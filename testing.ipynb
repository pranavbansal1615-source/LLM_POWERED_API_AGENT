{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34eedbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Bansal\\Documents\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Pranav Bansal\\Documents\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List,Any\n",
    "import chromadb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6892f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5.0.20241111\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "from PIL import Image\n",
    "import io\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def process_pdf_hybrid(pdf_path: str, text_threshold: int = 50):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    docs = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "\n",
    "        if len(text.strip()) < text_threshold:\n",
    "            # Fallback to OCR\n",
    "            pix = page.get_pixmap()\n",
    "            img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "            text = pytesseract.image_to_string(img)\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=text.strip(),\n",
    "                metadata={\n",
    "                    \"source_file\": pdf_path.split(\"\\\\\")[-1],\n",
    "                    \"page_number\": i + 1,\n",
    "                    \"file_type\": \"pdf\"\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs\n",
    "\n",
    "print(pytesseract.get_tesseract_version())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f653a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# loader = WebBaseLoader(\"https://docling.ai\")\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd312b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pages: 1495\n"
     ]
    }
   ],
   "source": [
    "pdf_files_path = \"C:\\\\Users\\\\Pranav Bansal\\\\Documents\\\\LLM_POWERED_API_AGENT\\\\pdf_files\"\n",
    "docs = []\n",
    "\n",
    "from pathlib import Path\n",
    "pdf_dir = Path(pdf_files_path)\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    doc = process_pdf_hybrid(str(pdf))\n",
    "    docs.extend(doc)\n",
    "\n",
    "print(\"Loaded pages:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb776e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 1, 'file_type': 'pdf'}, page_content=\"O'REILLY*\\n\\nApplied Machine\\nLearning and\\nAl for Engineers\\n\\nSolve Business Problems\\nThat Can't Be Solved\\nAlgorithmically\\n\\nJeff Prosise\\nForewor: rd by Adam Prosise\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 2, 'file_type': 'pdf'}, page_content='1\\nPraise for Applied Machine Learning and AI for Engineers\\nThis book is a fantastic guide to machine learning and AI\\nalgorithms. It’s succinct while being comprehensive, and\\nthe concrete examples with working code show how to take\\nthe theory into practice.\\nMark Russinovich, Azure CTO and Technical Fellow,\\nMicrosoft\\nWhen Jeff Prosise is passionate about something (whether\\nit be technology, his pet yellow-nape Amazon “Hawkeye,”\\nor his hobby of building and flying radio-controlled\\njets), you definitely want to listen in. He combines that\\npassion with a clarity of explanation that enables him to\\ncommunicate and teach complex topics better than anyone\\nI’ve ever known. He brings you along on a personal\\njourney of learning and understanding. Now Jeff brings\\nthese skills to the current and ongoing “technological\\ntsunami” (as he puts it) of machine learning and AI. In\\nthis new book, he builds your understanding from the\\nfoundations up, always emphasizing an intuitive approach\\nand connecting concepts and solutions to the real world.\\nIf you want to understand how AI and ML really work under\\nthe hood, and how these technologies have evolved and\\ncome to be, READ THIS BOOK.\\nTodd Fine, Chief Strategy Officer, Atmosera\\nJeff distills years of working AI/ML knowledge into a\\npractical and understandable guide for practitioners of\\nall levels.\\nKen Muse, 4x Azure MVP and Senior DevOps Architect,\\nGitHub\\nApplied Machine Learning and AI for Engineers is the book\\nI wish I had when first introduced to machine learning.\\nIt’s a fantastic introduction for novice ML engineers\\nand a great reference for those more experienced. It is\\nnow my go-to source when refreshing and enhancing my'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 3, 'file_type': 'pdf'}, page_content='2\\nunderstanding of various ML techniques and their\\napplicability. I love that Jeff uses real-world examples\\nand datasets to demonstrate each tool set and approach.\\nJust like a neural net, I have no idea how Jeff came up\\nwith this material, but it’s tremendously useful all the\\nsame.\\nBrent Rector, Principal Technical Program Manager,\\nAmazon; Founder, Wise Owl Consulting, LLC, Wise Owl\\nAviation Services, LLC, and Rector Aviation Law PC\\nI’ve known Jeff for decades and he has always possessed\\nthe ability to plainly explain complicated concepts. He\\nhas done it again with Applied Machine Learning and AI\\nfor Engineers. The examples, analogies, and color figures\\nmake the material truly understandable for the beginner\\nto the more advanced reader.\\nJeffrey Richter, Software Architect, Microsoft\\nThis book fills an extremely important gap for engineers\\nand scientists that want to bring the power of AI to bear\\non their most challenging data analysis problems. It\\nstrikes just the right balance between technical depth\\nand practical application, with tools and many hands-on\\nexamples, to empower the reader to become an effective AI\\npractitioner in their own application domain (including\\nbusiness, science, and other data-rich fields).\\nShaun S. Gleason, PhD, Director, Cyber Resilience and\\nIntelligence, Oak Ridge National Laboratory\\nApplied Machine Learning and AI for Engineers has the\\npotential to become the go-to for ML and AI enthusiasts.\\nWhat sets this book apart is how relevant the problem\\nstatements are in today’s fast-paced adoption of machine\\nlearning in the tech world. A must-read for newbies and\\nprofessionals alike!\\nLipi Deepaakshi Patnaik, Software Development\\nEngineer, Zeta Suite'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 4, 'file_type': 'pdf'}, page_content='3\\nJeff has always been able to wrap great stories around\\ndeep technical concepts that make learning markedly\\neasier. This might be his best book yet, and perhaps his\\nmost relevant subject matter as well. Reading this book\\nwill make you want to go build something.\\nDoug Turnure, Azure Specialist, Microsoft\\nApplied Machine Learning and AI for Engineers is a\\npractical handbook for building useful machine learning\\nsystems. It provides accessible guidance on how to apply\\ncutting-edge AI algorithms to solve contemporary business\\nproblems.\\nBrian Spiering, Data Science Instructor, Metis\\nThis book is a perfect start for engineers and software\\ndevelopers who want to get into machine learning. It\\ncovers good ground in ML and quickly gets you started for\\na wide variety of problems that are driven by data rather\\nthan algorithms.\\nDr. Manjeet Dahiya, VP and Head, AI and Machine\\nLearning, Ecom Express\\nWhether you’re new to applied ML or a seasoned developer\\nlooking for a reference, this book is a must-have, up-to-\\ndate, comprehensive guide to all major classes of machine\\nlearning algorithms (with clean code implementations to\\nreally solidify your understanding).\\nGoku Mohandas, Founder, Made With ML\\nApplied Machine Learning and AI for Engineers is a\\nbrilliant primer for beginner to intermediate readers in\\nAI/ML. The book builds a comfortable flow from\\nconventional ML to deep learning with the desirable bonus\\nof AI implementation on the cloud, making it a complete\\nend-to-end guide for any enthusiastic reader or\\nprofessional practitioner.\\nSatyarth Praveen, Computing Science Engineer,\\nLawrence Berkeley National Lab'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 5, 'file_type': 'pdf'}, page_content='4\\nJeff Prosise is one of the best teachers I have ever\\nencountered, and regardless of the platform (classroom,\\nblog, magazine article, webinar, book, etc.), he has a\\nspecial talent of taking complex topics and making them\\naccessible for the rest of us. In this book, Jeff goes\\nbeyond simply providing a very clear understanding of the\\nbasic concepts that undergird machine learning to provide\\neasy-to-follow examples that demonstrate those concepts\\nwithin the current environment. He delivers a great\\nintroduction/overview of a wide variety of topics within\\nmachine learning and gives clear guidance on how each can\\n(and should) be used. Jeff is one of the very few that\\ncould have taken this complex topic and put it into a\\nform that could be easily absorbed and applied. This book\\nis a MUST READ for engineers and other problem solvers\\nwho are looking to use machine learning to augment their\\nskill set.\\nLarry Clement, Assistant Professor, Computing,\\nSoftware, and Data Sciences, and former Department\\nChair, California Baptist University. Formerly a\\nsenior engineer-scientist with the Boeing Corporation\\non the C-17 program.\\nInfused with author Jeff Prosise’s iconic teaching style\\nthat has helped thousands of developers over the years,\\nApplied Machine Learning and AI for Engineers layers\\ncontext around complex topics and provides easy-to-\\nunderstand examples and tutorials. Highly recommended for\\nengineers who want to incorporate machine learning\\nconcepts and skills in their repertoire.\\nVani Mandava, Head of Engineering, Scientific\\nSoftware Engineering Center at University of\\nWashington eScience Institute'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 6, 'file_type': 'pdf'}, page_content='5\\nApplied Machine Learning and AI\\nfor Engineers\\nSolve Business Problems That Can’t Be Solved\\nAlgorithmically\\nJeff Prosise\\nForeword by Adam Prosise'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 7, 'file_type': 'pdf'}, page_content='6\\nApplied Machine Learning and AI for Engineers\\nby Jeff Prosise\\nCopyright © 2023 Jeff Prosise. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway\\nNorth, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business,\\nor sales promotional use. Online editions are also available\\nfor most titles (http://oreilly.com). For more information,\\ncontact our corporate/institutional sales department: 800-\\n998-9938 or corporate@oreilly.com.\\n\\xa0\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editor: Jill Leonard\\nProduction Editor: Gregory Hyman\\nCopyeditor: Audrey Doyle\\nProofreader: Piper Editorial Consulting, LLC\\nIndexer: Potomac Indexing, LLC\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 8, 'file_type': 'pdf'}, page_content='7\\n\\xa0\\nNovember 2022: First Edition\\nRevision History for the First Edition\\n\\xa0\\n2022-11-10: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492098058\\nfor release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly\\nMedia, Inc. Applied Machine Learning and AI for Engineers,\\nthe cover image, and related trade dress are trademarks of\\nO’Reilly Media, Inc.\\nThe views expressed in this work are those of the author and\\ndo not represent the publisher’s views. While the publisher\\nand the author have used good faith efforts to ensure that\\nthe information and instructions contained in this work are\\naccurate, the publisher and the author disclaim all\\nresponsibility for errors or omissions, including without\\nlimitation responsibility for damages resulting from the use\\nof or reliance on this work. Use of the information and\\ninstructions contained in this work is at your own risk. If\\nany code samples or other technology this work contains or\\ndescribes is subject to open source licenses or the\\nintellectual property rights of others, it is your\\nresponsibility to ensure that your use thereof complies with\\nsuch licenses and/or rights.\\n978-1-492-09805-8\\n[LSI]'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 9, 'file_type': 'pdf'}, page_content='8\\nDedication\\nFor my Wintellect family past and present'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 10, 'file_type': 'pdf'}, page_content='9\\nForeword\\nWhen your dad is insatiably curious about what you are\\nstudying, home is not a safe harbor.\\nI received my master’s in analytics in 2018. Through the\\ncourse of my graduate school program, my cohort learned how\\nto leverage machine learning, AI, and analytics to add value\\nto businesses and develop solutions for real-world\\nchallenges. I have a passion for these things—a passion that\\nI share with my dad, Jeff Prosise. In fact, I can’t tell you\\nthe number of times he asked if he could join me in class\\n(I’m not kidding) or launched a salvo of questions about\\nwhat we were studying in the kitchen when I escaped the grind\\nat my parents’ house.\\nIf you have ever been on an airplane experiencing turbulence\\nand the person next to you coped by striking up a\\nconversation about what a marvel of engineering modern\\njetliners are because they do not have a single point of\\nfailure, you know exactly how I felt.\\nOur love of data and analytics grew into a shared love of the\\nvalue it provides. Using the tools and techniques outlined in\\nthis book, one can draw certitude in the face of uncertainty.\\nData science allows you to find underlying truth—to discover\\nwhat is really happening and how it drives behaviors and\\noutcomes. The ability to peek behind the curtain using\\nanalytics, rather than intuition, is an appreciating skill\\nset in our information economy and is vital for people and\\ninstitutions navigating modern uncertainty.\\nEqually important is effectively communicating these findings\\nto a nontechnical audience while having a deep technical\\nunderstanding of just what is going on under the hood. This\\nlevel of communication can’t be faked (I’ve tried a time or\\ntwo during those kitchen discussions with my dad).\\nAnalytics, AI, and machine learning do not have\\ninsurmountable technical or deployment issues. Rather, the\\nchallenge and accessibility of understanding just what is'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 11, 'file_type': 'pdf'}, page_content='10\\nhappening and how they work is the impediment, because this\\nunderstanding is often shrouded in technical jargon and\\nindustry shibboleths. These barriers to entry act as a\\nlimiting principle and stunt the utilization of data science\\nto address problems and questions.\\nThat is what this book seeks to change: it removes the shroud\\nand jargon, making these tools and resources accessible.\\nAnd, to be completely honest, my dad is very good at writing\\nand teaching intimidating technical topics in a manner that\\nmakes learning them almost effortless, and he has been for\\nall my life. He has made subjects all the way back to DOS\\nunderstandable for the movers and shakers in today’s\\nbusiness world. He has made subjects that I frankly can’t\\nwrap my head around accessible for generations of programmers\\nover the last few decades. To put it simply, he’s the best.\\nThe secret is this: his ethos of how he approaches teaching a\\ntopic centers around “how would I want this explained to me\\nif I had never heard of it but was interested?” Given the\\nunique challenges data science poses and the myriad\\nperspectives professionals in this space come from, his\\napproach provides a level of accessibility not found in many\\nother places.\\nTake it from me: you couldn’t have a better guide through\\nthe complexities of ML and AI. If you’re already familiar,\\nthen this book will hone your understanding, as it did for\\nme. (Looking at you, Chapter\\xa013.) If you’re interested in\\nmachine learning, AI, analytics, and the value they add to\\nhumanity, you’re in the right place.\\nRegardless of which camp you fall in, you’ll come away with\\ndeeper knowledge of the subjects he outlines, empowering you\\nto use these tools and then tell the story of what you found.\\nUsually I would end something like this by saying, “I hope\\nyou enjoy the book and learn something,” but in this case,\\nthat would not be the truth. I don’t hope—I know. I know\\nyou will learn from and alongside my dad, as I have.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 12, 'file_type': 'pdf'}, page_content='11\\nThere’s no person I’ve tried to be more like in my life\\nthan Jeff Prosise, and I couldn’t be more excited to share\\nthis aspect of him with you. And maybe—just maybe—it will\\nignite a passion for this stuff, as it did with me.\\nI’ll leave you with this. I’ll say to you what he\\napocryphally told me as he held me on the day I was born:\\n“Welcome to the show, kid.”\\nAdam Prosise\\nProcess and Innovation Specialist, Delta Air Lines'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 13, 'file_type': 'pdf'}, page_content='12\\nPreface\\nI have witnessed three great technical revolutions in my\\nlifetime: first the personal computer, then the internet, and\\nlastly the smartphone. Machine learning (ML) and AI are just\\nas fundamentally important as all three and will have an\\nequally profound impact on our lives.\\nI first became interested in machine learning the day my\\ncredit card company called to confirm that I was trying to\\npurchase a $700 necklace. I was not, but I was curious: how\\ndid they know it wasn’t me? I use my card all over the\\nworld, and for the record, I do buy my wife nice things from\\ntime to time. Not once had the credit card company declined a\\nlegitimate purchase, but several times they had correctly\\nflagged fraudulent purchases, the one prior to this being an\\nattempt by someone in Brazil to use my card to buy an airline\\nticket. This time was different: the jewelry store was 2\\nmiles from my house. I tried to imagine an algorithm that\\ncould so reliably detect credit card fraud at the point of\\nsale. It didn’t take long to realize that something more\\npowerful than a mere algorithm was at work.\\nIt turned out that the credit card company runs every\\ntransaction through a sophisticated machine learning model\\nthat is incredibly adept at detecting fraud. That moment\\nchanged my life. It’s a splendid example of how ML and AI\\nare making the world a better place. Moreover, understanding\\nhow ML could analyze credit card transactions in real time\\nand pick out the bad ones while allowing legitimate charges\\nto go through became a mountain that I had to climb.\\nWho Should Read This Book'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 14, 'file_type': 'pdf'}, page_content='13\\nRecently, I received a call from the head of engineering at a\\nmanufacturing company. He started the conversation like this:\\n“Until last week, I didn’t know what ML and AI stood for.\\nNow my CEO has tasked me with figuring out how they can\\nimprove our business, and to do it before our competitors get\\nahead of us. I am starting at square one. Can you help?”\\nThe next call came from a government contracting firm\\ninterested in using machine learning to detect tax fraud and\\nmoney laundering. The team there was reasonably well versed\\nin machine learning theory but wondered how best to go about\\nbuilding the models they needed.\\nProfessionals everywhere are realizing that ML and AI\\nrepresent a technological tsunami, and they’re trying to get\\non top of the wave before it crashes over them. This book is\\nfor them: engineers, software developers, IT managers, and\\nothers whose goal is to build a practical understanding of ML\\nand AI and put that knowledge to work solving problems that\\nwere difficult or even intractable before. It seeks to impart\\nan intuitive understanding and resorts to equations only when\\nnecessary. Despite what you may have heard, you don’t have\\nto be an expert in calculus or linear algebra to build\\nsystems that recognize objects in photos, translate English\\nto French, or expose drug traffickers and tax cheats.\\nWhy I Wrote This Book\\nInside every author is a tiny gremlin that says they can tell\\nthe story in a way that no one else has. I wrote my first\\ncomputer book more than 30 years ago and my last one more\\nthan 20 years ago, and I didn’t intend to write another one.\\nBut now I have a story to tell. It’s an important story—one\\nthat every engineer and software developer should hear. I’m\\nnot entirely satisfied with the way others have told it, so I\\nwrote the book that I wish I had had when I was learning the\\ncraft. It starts with the basics and leads you on a journey\\nto the heights of ML and AI. By the end, you’ll understand'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 15, 'file_type': 'pdf'}, page_content='14\\nhow credit card companies detect fraud, how aircraft\\ncompanies use machine learning to perform predictive\\nmaintenance on jet engines, how self-driving cars see the\\nworld around them, how Google Translate translates text\\nbetween languages, and how facial recognition systems work.\\nMoreover, you’ll be able to build systems like them\\nyourself, or use existing systems to infuse AI into the apps\\nthat you write.\\nToday’s most advanced machine learning models are trained on\\ncomputers equipped with graphics processing units (GPUs) or\\ntensor processing units (TPUs), often at great time and\\nexpense. A point of emphasis in this book is presenting\\nexamples that can be built on a typical PC or laptop without\\na GPU. When we tackle computer-vision models that recognize\\nobjects in photos, I’ll describe how such models work and\\nhow they’re trained with millions of images on GPU clusters.\\nBut then I’ll show you how to use a technique called\\ntransfer learning to repurpose existing models to solve\\ndomain-specific problems and train them on an ordinary\\nlaptop.\\nThis book draws heavily from the classes and workshops that I\\nteach at companies and research institutions around the\\nworld. I love teaching because I love seeing the light bulbs\\ncome on. I often kick off classes on ML and AI by saying I’m\\nnot here to teach; I’m here to change your life. Here’s\\nhoping that your life will be a little bit different, and a\\nlittle bit better, than it was before you read this book.\\nRunning the Book’s Code Samples\\nEngineers learn best by doing, not merely by reading. This\\nbook contains numerous code samples that you can run to\\nreinforce the concepts presented in each chapter. Most are\\nwritten in Python and use popular open source libraries such\\nas Scikit-Learn, Keras, and TensorFlow. All are available in'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 16, 'file_type': 'pdf'}, page_content='15\\na public GitHub repo. It’s the single source of truth for\\nthe code samples because I can update it at any time.\\nThere are machine learning platforms that allow you to build\\nand train models with no code. But the best way to understand\\nwhat these platforms do and how they do it is to write code.\\nPython is a simple programming language. It’s easy to learn.\\nEngineers today have to be comfortable writing code. You can\\nlearn Python as you go by working the examples in this book,\\nand if you’re already comfortable with Python (and with\\nprogramming in general), then you’re ahead of the game.\\nTo run my samples on your PC or laptop, you need a 64-bit\\nversion of Python 3.7 or higher. You can download a Python\\nruntime from Python.org, or you can install a Python\\ndistribution such as Anaconda. You also need to make sure the\\nfollowing packages and their dependencies are installed:\\n\\xa0\\nScikit-Learn and TensorFlow for building machine\\nlearning models\\nPandas, Matplotlib, and Seaborn for data\\nwrangling and visualization\\nOpenCV and Pillow for handling images\\nFlask and Requests for calling REST APIs and\\nbuilding web services\\nSklearn-onnx and Onnxruntime for Open Neural\\nNetwork Exchange (ONNX) models\\nLibrosa for generating spectrogram images from\\naudio files\\nMTCNN and Keras-vggface for building facial\\nrecognition systems\\nKerasNLP, Transformers, Datasets, and PyTorch for\\nbuilding natural language processing (NLP) models'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 17, 'file_type': 'pdf'}, page_content='16\\nAzure-cognitiveservices-vision-computervision,\\nAzure-ai-textanalytics, and Azure-\\ncognitiveservices-speech for calling Azure\\nCognitive Services\\nYou can install most of these packages with pip install\\ncommands. If you installed Anaconda, many of these packages\\nare already there, and you can install the rest using conda\\ninstall commands or an equivalent.\\nSpeaking of environments, it’s never a bad idea to use\\nvirtual Python environments to prevent package installs from\\nconflicting with other package installs. If you’re not\\nfamiliar with virtual environments, you can read about them\\nat Python.org. If you use Anaconda, virtual environments are\\nbaked right in.\\nMost of my code samples were built for Jupyter notebooks,\\nwhich provide an interactive platform for writing and\\nexecuting Python code. Notebooks are incredibly popular in\\nthe data science community for exploring data and training\\nmachine learning models. You can run Jupyter notebooks\\nlocally by installing packages such as Notebook or\\nJupyterLab, or you can use cloud-hosted environments like\\nGoogle Colab. One of the advantages of Colab is that you\\ndon’t have to install anything on your computer—not even\\nPython. And in the rare cases in which my samples require a\\nGPU, Colab provides it for you.\\nPython development environments are notoriously finicky to\\nset up and maintain, especially on Windows. If you’d prefer\\nnot to have to create such an environment, or if you tried\\nbut failed to get it working, help is only a download away. I\\npackaged a complete development environment suitable for\\nrunning every sample in this book in a Docker container\\nimage. Assuming you have the Docker Engine installed on your\\ncomputer, you can launch the container with the following\\ncommand:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 18, 'file_type': 'pdf'}, page_content='17\\ndocker run -it -p 8888:8888 jeffpro/applied-machine-learning:latest\\nNavigate in your browser to the URL that appears in the\\noutput. You’ll land in a full Jupyter environment with all\\nmy code samples and everything needed to run them. They’re\\nin a folder named Applied-Machine-Learning cloned from the\\nGitHub repo of the same name. The downside to using a\\ncontainer is that changes you make aren’t persisted by\\ndefault. One way to remedy that is to use a -v switch in the\\ndocker command to bind to a local directory. For more\\ninformation, refer to “Use Bind Mounts” in the Docker\\ndocumentation.\\nNavigating This Book\\nThis book is organized into two parts:\\n\\xa0\\nPart\\xa0I (Chapters 1 through 7) teaches the ABCs\\nof machine learning and introduce popular\\nlearning algorithms such as logistic regression\\nand gradient boosting.\\nPart\\xa0II (Chapters 8 through 14) covers deep\\nlearning, which is synonymous with AI today and\\nuses deep neural networks to fit mathematical\\nmodels to data.\\nI highly encourage you to work the exercises as you read the\\nbook. You’ll come away with a deeper understanding of the\\nmaterial, and you’ll no doubt think of ways to modify my\\nexamples to play “what if?” with the code.\\nConventions Used in This Book\\nThe following typographical conventions are used in this\\nbook:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 19, 'file_type': 'pdf'}, page_content='18\\nItalic\\nIndicates new terms, URLs, email addresses,\\nfilenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within\\nparagraphs to refer to program elements such as\\nvariable or function names, databases, data types,\\nenvironment variables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed\\nliterally by the user.\\nTIP\\nThis element signifies a tip or suggestion.\\nNOTE\\nThis element signifies a general note.\\nWARNING\\nThis element indicates a warning or caution.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 20, 'file_type': 'pdf'}, page_content='19\\nUsing Code Examples\\nAs mentioned, supplemental material (code examples,\\nexercises, etc.) is available for download at\\nhttps://oreil.ly/applied-machine-learning-code.\\nIf you have a technical question or a problem using the code\\nexamples, please send email to bookquestions@oreilly.com.\\nThis book is here to help you get your job done. In general,\\nif example code is offered with this book, you may use it in\\nyour programs and documentation. You do not need to contact\\nus for permission unless you’re reproducing a significant\\nportion of the code. For example, writing a program that uses\\nseveral chunks of code from this book does not require\\npermission. Selling or distributing examples from O’Reilly\\nbooks does require permission. Answering a question by citing\\nthis book and quoting example code does not require\\npermission. Incorporating a significant amount of example\\ncode from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but generally do not require, attribution. An\\nattribution usually includes the title, author, publisher,\\nand ISBN. For example: “Applied Machine Learning and AI for\\nEngineers by Jeff Prosise (O’Reilly). Copyright 2023 Jeff\\nProsise, 978-1-492-09805-8.”\\nIf you feel your use of code examples falls outside fair use\\nor the permission given above, feel free to contact us at\\npermissions@oreilly.com.\\nO’Reilly Online Learning\\nNOTE\\nFor more than 40 years, O’Reilly Media has provided technology\\nand business training, knowledge, and insight to help companies'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 21, 'file_type': 'pdf'}, page_content='20\\nsucceed.\\nOur unique network of experts and innovators share their\\nknowledge and expertise through books, articles, and our\\nonline learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses,\\nin-depth learning paths, interactive coding environments, and\\na vast collection of text and video from O’Reilly and 200+\\nother publishers. For more information, visit\\nhttps://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to\\nthe publisher:\\n\\xa0\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 22, 'file_type': 'pdf'}, page_content='21\\nWe have a web page for this book, where we list errata,\\nexamples, and any additional information. You can access this\\npage at https://oreil.ly/applied-machine-learning.\\nEmail bookquestions@oreilly.com to comment or ask technical\\nquestions about this book.\\nFor news and information about our books and courses, visit\\nhttps://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-\\nmedia\\nFollow us on Twitter: https://twitter.com/oreillymedia\\nWatch us on YouTube: https://youtube.com/oreillymedia\\nAcknowledgments\\nWriting and publishing a book is a team effort. It starts\\nwith the author, but before it lands on shelves, it goes\\nthrough reviewers, developmental editors, copy editors,\\nartists, and production personnel.\\nI’d like to thank several friends and family members for\\nreviewing chapters as I wrote them and providing constructive\\nfeedback. They are engineers, mathematicians, data analysts,\\nand professors, and they happen to be among the smartest\\npeople I know: Larry Clement, Manjeet Dahiya, Tom Marshall,\\nDon Meyer, Goku Mohandas, Ken Muse, Lipi Deepaakshi Patnaik,\\nCharles Petzold, Abby Prosise, Adam Prosise, Jeffrey Richter,\\nBruce Schecter, Vishwesh Ravi Shrimali, Brian Spiering, and\\nRon Sumida.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 23, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 24, 'file_type': 'pdf'}, page_content='23\\nA big thank-you to the team at O’Reilly too, for turning my\\nwords into prose and my sketches into art. That includes Jill\\nLeonard, Audrey Doyle, Gregory Hyman, David Futato, Karen\\nMontgomery, and Nicole Butterfield. A special shout-out to\\nJon Hassell, who heard my vision for this book and said,\\n“Let’s do it.” I have had the privilege of working with\\nsome great publishing teams over the years. None were better\\nthan this one.\\nFinally, to Lori—my wife, travel companion, and partner in\\ncrime for the last 40 years. I couldn’t have done it without\\nyou. And I promise this will be the last book I write!'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 25, 'file_type': 'pdf'}, page_content='Part I. Machine Learning with\\nScikit-Learn'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 26, 'file_type': 'pdf'}, page_content='25\\nChapter 1. Machine Learning\\nMachine learning expands the boundaries of what’s possible\\nby allowing computers to solve problems that were intractable\\njust a few short years ago. From fraud detection and medical\\ndiagnoses to product recommendations and cars that “see”\\nwhat’s in front of them, machine learning impacts our lives\\nevery day. As you read this, scientists are using machine\\nlearning to unlock the secrets of the human genome. When we\\none day cure cancer, we will thank machine learning for\\nmaking it possible.\\nMachine learning is revolutionary because it provides an\\nalternative to algorithmic problem-solving. Given a recipe,\\nor algorithm, it’s not difficult to write an app that hashes\\na password or computes a monthly mortgage payment. You code\\nup the algorithm, feed it input, and receive output in\\nreturn. It’s another proposition altogether to write code\\nthat determines whether a photo contains a cat or a dog. You\\ncan try to do it algorithmically, but the minute you get it\\nworking, you’ll come across a cat or dog picture that breaks\\nthe algorithm.\\nMachine learning takes a different approach to turning input\\ninto output. Rather than relying on you to implement an\\nalgorithm, it examines a dataset of inputs and outputs and\\nlearns how to generate output of its own in a process known\\nas training. Under the hood, special algorithms called\\nlearning algorithms fit mathematical models to the data and\\ncodify the relationship between data going in and data coming\\nout. Once trained, a model can accept new inputs and generate\\noutputs consistent with the ones in the training data.\\nTo use machine learning to distinguish between cats and dogs,\\nyou don’t code a cat-versus-dog algorithm. Instead, you\\ntrain a machine learning model with cat and dog photos.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 27, 'file_type': 'pdf'}, page_content='26\\nSuccess depends on the learning algorithm used and the\\nquality and volume of the training data.\\nPart of becoming a machine learning engineer is familiarizing\\nyourself with the various learning algorithms and developing\\nan intuition for when to use one versus another. That\\nintuition comes from experience and from an understanding of\\nhow machine learning fits mathematical models to data. This\\nchapter represents the first step on that journey. It begins\\nwith an overview of machine learning and the most common\\ntypes of machine learning models, and it concludes by\\nintroducing two popular learning algorithms and using them to\\nbuild simple yet fully functional models.\\nWhat Is Machine Learning?\\nAt an existential level, machine learning (ML) is a means for\\nfinding patterns in numbers and exploiting those patterns to\\nmake predictions. ML makes it possible to train a model with\\nrows or sequences of 1s and 0s, and to learn from the data so\\nthat, given a new sequence, the model can predict what the\\nresult will be. Learning is the process by which ML finds\\npatterns that can be used to predict future outputs, and\\nit’s where the “learning” in “machine learning” comes\\nfrom.\\nAs an example, consider the table of 1s and 0s depicted in\\nFigure\\xa01-1. Each number in the fourth column is somehow\\nbased on the three numbers preceding it in the same row.\\nWhat’s the missing number?'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 28, 'file_type': 'pdf'}, page_content='27\\nFigure 1-1. Simple dataset consisting of 0s and 1s\\nOne possible solution is that for a given row, if the first\\nthree columns contain more 0s than 1s, then the fourth\\ncontains a 0. If the first three columns contain more 1s than\\n0s, then the answer is 1. By this logic, the empty box should\\ncontain a 1. Data scientists refer to the column containing\\nanswers (the red column in the figure) as the label column.\\nThe remaining columns are feature columns. The goal of a\\npredictive model is to find patterns in the rows in the\\nfeature columns that allow it to predict what the label will\\nbe.\\nIf all datasets were this simple, you wouldn’t need machine\\nlearning. But real-world datasets are larger and more\\ncomplex. What if the dataset contained millions of rows and\\nthousands of columns, which, as it happens, is common in\\nmachine learning? For that matter, what if the dataset\\nresembled the one in Figure\\xa01-2?'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 29, 'file_type': 'pdf'}, page_content='28\\nFigure 1-2. A more complex dataset\\nIt’s difficult for any human to examine this dataset and\\ncome up with a set of rules for predicting whether the red\\nbox should contain a 0 or a 1. (And no, it’s not as simple\\nas counting 1s and 0s.) Just imagine how much more difficult\\nit would be if the dataset really did have millions of rows\\nand thousands of columns.\\nThat’s what machine learning is all about: finding patterns\\nin massive datasets of numbers. It doesn’t matter whether\\nthere are 100 rows or 1,000,000 rows. In many cases, more is\\nbetter, because 100 rows might not provide enough samples for\\npatterns to be discerned.\\nIt isn’t an oversimplification to say that machine learning\\nsolves problems by mathematically modeling patterns in sets\\nof numbers. Most any problem can be reduced to a set of\\nnumbers. For example, one of the common applications for ML\\ntoday is sentiment analysis: looking at a text sample such as\\na movie review or a comment left on a website and assigning\\nit a 0 for negative sentiment (for example, “The food was\\nbland and the service was terrible.”) or a 1 for positive\\nsentiment (“Excellent food and service. Can’t wait to visit\\nagain!”). Some reviews might be mixed—for example, “The\\nburger was great but the fries were soggy”—so we use the\\nprobability that the label is a 1 as a sentiment score. A\\nvery negative comment might score a 0.1, while a very'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 30, 'file_type': 'pdf'}, page_content='29\\npositive comment might score a 0.9, as in there’s a 90%\\nchance that it expresses positive sentiment.\\nSentiment analyzers and other models that work with text are\\nfrequently trained on datasets like the one in Figure\\xa01-3,\\nwhich contains one row for every text sample and one column\\nfor every word in the corpus of text (all the words in the\\ndataset). A typical dataset like this one might contain\\nmillions of rows and 20,000 or more columns. Each row\\ncontains a 0 for negative sentiment in the label column, or a\\n1 for positive sentiment. Within each row are word counts—\\nthe number of times a given word appears in an individual\\nsample. The dataset is sparse, meaning it is mostly 0s with\\nan occasional nonzero number sprinkled in. But machine\\nlearning doesn’t care about the makeup of the numbers. If\\nthere are patterns that can be exploited to determine whether\\nthe next sample expresses positive or negative sentiment, it\\nwill find them. Spam filters use datasets such as these with\\n1s and 0s in the label column denoting spam and nonspam\\nmessages. This allows modern spam filters to achieve an\\nastonishing degree of accuracy. Moreover, these models grow\\nsmarter over time as they are trained with more and more\\nemails.\\nFigure 1-3. Dataset for sentiment analysis'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 31, 'file_type': 'pdf'}, page_content='30\\nSentiment analysis is an example of a text classification\\ntask: analyzing a text sample and classifying it as positive\\nor negative. Machine learning has proven adept at image\\nclassification as well. A simple example of image\\nclassification is looking at photos of cats and dogs and\\nclassifying each one as a cat picture (0) or a dog picture\\n(1). Real-world uses for image classification include\\nflagging defective parts coming off an assembly line,\\nidentifying objects in view of a self-driving car, and\\nrecognizing faces in photos.\\nImage classification models are trained with datasets like\\nthe one in Figure\\xa01-4, in which each row represents an image\\nand each column holds a pixel value. A dataset with 1,000,000\\nimages that are 200 pixels wide and 200 pixels high contains\\n1,000,000 rows and 40,000 columns. That’s 40 billion numbers\\nin all, or 120,000,000,000 if the images are color rather\\nthan grayscale. (In color images, pixel values comprise three\\nnumbers rather than one.) The label column contains a number\\nrepresenting the class or category to which the corresponding\\nimage belongs—in this case, the person whose face appears in\\nthe picture: 0 for Gerhard Schroeder, 1 for George W. Bush,\\nand so on.\\nFigure 1-4. Dataset for image classification'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 32, 'file_type': 'pdf'}, page_content='31\\nThese facial images come from a famous public dataset called\\nLabeled Faces in the Wild, or LFW for short. It is one of\\ncountless labeled datasets that are published in various\\nplaces for public consumption. Machine learning isn’t hard\\nwhen you have labeled datasets to work with—datasets that\\nothers (often grad students) have laboriously spent hours\\nlabeling with 1s and 0s. In the real world, engineers\\nsometimes spend the bulk of their time generating these\\ndatasets. One of the more popular repositories for public\\ndatasets is Kaggle.com, which makes lots of useful datasets\\navailable and holds competitions allowing budding ML\\npractitioners to test their skills.\\nMachine Learning Versus Artificial\\nIntelligence\\nThe terms machine learning and artificial intelligence (AI)\\nare used almost interchangeably today, but in fact, each term\\nhas a specific meaning, as shown in Figure\\xa01-5.\\nTechnically speaking, machine learning is a subset of AI,\\nwhich encompasses not only machine learning models but also\\nother types of models such as expert systems (systems that\\nmake decisions based on rules that you define) and\\nreinforcement learning systems, which learn behaviors by\\nrewarding positive outcomes while penalizing negative ones.\\nAn example of a reinforcement learning system is AlphaGo,\\nwhich was the first computer program to beat a professional\\nhuman Go player. It trains on games that have already been\\nplayed and learns strategies for winning on its own.\\nAs a practical matter, what most people refer to as AI today\\nis in fact deep learning, which is a subset of machine\\nlearning. Deep learning is machine learning performed with\\nneural networks. (There are forms of deep learning that\\ndon’t involve neural networks—deep Boltzmann machines are\\none example—but the vast majority of deep learning today\\ninvolves neural networks.) Thus, ML models can be divided'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 33, 'file_type': 'pdf'}, page_content='32\\ninto conventional models that use learning algorithms to\\nmodel patterns in data, and deep-learning models that use\\nneural networks to do the same.\\nFigure 1-5. Relationship between machine learning, deep learning, and AI\\nA BRIEF HISTORY OF AI\\nML and AI have surged in popularity in recent years. AI\\nwas a big deal in the 1980s, when it was widely believed\\nthat computers would soon be able to mimic the human'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 34, 'file_type': 'pdf'}, page_content='33\\nmind. But excitement waned, and for decades—up until\\n2010 or so—AI rarely made the news. Then a strange thing\\nhappened.\\nThanks to the availability of graphics processing units\\n(GPUs) from companies such as NVIDIA, researchers finally\\nhad the horsepower they needed to train advanced neural\\nnetworks. This led to advancements in the state of the\\nart, which led to renewed enthusiasm, which led to\\nadditional funding, which precipitated further\\nadvancements, and suddenly AI was a thing again. Neural\\nnetworks have been around (at least in theory) since the\\n1950s, but researchers lacked the computational power to\\ntrain them on large datasets. Today anyone can buy a GPU\\nor spin up a GPU cluster in the cloud. AI is advancing\\nmore rapidly now than ever before, and with that progress\\ncomes the ability to do things in software that engineers\\ncould only have dreamed about as recently as a decade\\nago.\\nOver time, data scientists have devised special types of\\nneural networks that excel at certain tasks, including tasks\\ninvolving computer vision—for example, distilling\\ninformation from images—and tasks that involve human\\nlanguages such as translating English to French. We’ll take\\na deep dive into neural networks beginning in Chapter\\xa08, and\\nyou’ll learn specifically how deep learning has elevated\\nmachine learning to new heights.\\nSupervised Versus Unsupervised Learning\\nMost ML models fall into one of two broad categories:\\nsupervised learning models and unsupervised learning models.\\nThe purpose of supervised learning models is to make\\npredictions. You train them with labeled data so that they\\ncan take future inputs and predict what the labels will be.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 35, 'file_type': 'pdf'}, page_content='34\\nMost of the ML models in use today are supervised learning\\nmodels. A great example is the model that the US Postal\\nService uses to turn handwritten zip codes into digits that a\\ncomputer can recognize to sort the mail. Another example is\\nthe model that your credit card company uses to authorize\\npurchases.\\nUnsupervised learning models, by contrast, don’t require\\nlabeled data. Their purpose is to provide insights into\\nexisting data, or to group data into categories and\\ncategorize future inputs accordingly. A classic example of\\nunsupervised learning is inspecting records regarding\\nproducts purchased from your company and the customers who\\npurchased them to determine which customers might be most\\ninterested in a new product you are launching and then\\nbuilding a marketing campaign that targets those customers.\\nA spam filter is a supervised learning model. It requires\\nlabeled data. A model that segments customers based on\\nincomes, credit scores, and purchasing history is an\\nunsupervised learning model, and the data that it consumes\\ndoesn’t have to be labeled. To help drive home the\\ndifference, the remainder of this chapter explores supervised\\nand unsupervised learning in greater detail.\\nUnsupervised Learning with k-Means Clustering\\nUnsupervised learning frequently employs a technique called\\nclustering. The purpose of clustering is to group data by\\nsimilarity. The most popular clustering algorithm is k-means\\nclustering, which takes n data samples and groups them into m\\nclusters, where m is a number you specify.\\nGrouping is performed using an iterative process that\\ncomputes a centroid for each cluster and assigns samples to\\nclusters based on their proximity to the cluster centroids.\\nIf the distance from a particular sample to the centroid of\\ncluster 1 is 2.0 and the distance from the same sample to the'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 36, 'file_type': 'pdf'}, page_content='35\\ncenter of cluster 2 is 3.0, then the sample is assigned to\\ncluster 1. In Figure\\xa01-6, 200 samples are loosely arranged\\nin three clusters. The diagram on the left shows the raw,\\nungrouped samples. The diagram on the right shows the cluster\\ncentroids (the red dots) with the samples colored by cluster.\\nFigure 1-6. Data points grouped using k-means clustering\\nHow do you code up an unsupervised learning model that\\nimplements k-means clustering? The easiest way to do it is to\\nuse the world’s most popular machine learning library:\\nScikit-Learn. It’s free, it’s open source, and it’s\\nwritten in Python. The documentation is great, and if you\\nhave a question, chances are you’ll find an answer by\\nGoogling it. I’ll use Scikit for most of the examples in the\\nfirst half of this book. The book’s Preface describes how to\\ninstall Scikit and configure your computer to run my examples\\n(or use a Docker container to do the same), so if you\\nhaven’t done so already, now’s a great time to set up your\\nenvironment.\\nTo get your feet wet with k-means clustering, start by\\ncreating a new Jupyter notebook and pasting the following\\nstatements into the first cell:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 37, 'file_type': 'pdf'}, page_content='36\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\nRun that cell, and then run the following code in the next\\ncell to generate a semirandom assortment of x and y\\ncoordinate pairs. This code uses Scikit’s make_blobs\\nfunction to generate the coordinate pairs, and Matplotlib’s\\nscatter function to plot them:\\n\\nfrom\\nsklearn.datasets\\nimport\\nmake_blobs\\n\\npoints,\\ncluster_indexes\\n=\\nmake_blobs(n_samples=300,\\ncenters=4,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cluster_std=0.8,\\nrandom_state=0)\\n\\nx\\n=\\npoints[:,\\n0]\\ny\\n=\\npoints[:,\\n1]\\n\\nplt.scatter(x,\\ny,\\ns=50,\\nalpha=0.7)\\nYour output should be identical to mine, thanks to the\\nrandom_state parameter that seeds the random-number generator\\nused internally by make_blobs:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 38, 'file_type': 'pdf'}, page_content=\"37\\nNext, use k-means clustering to divide the coordinate pairs\\ninto four groups. Then render the cluster centroids in red\\nand color-code the data points by cluster. Scikit’s KMeans\\nclass does the heavy lifting, and once it’s fit to the\\ncoordinate pairs, you can get the locations of the centroids\\nfrom KMeans’ cluster_centers_ attribute:\\n\\nfrom\\nsklearn.cluster\\nimport\\nKMeans\\n\\nkmeans\\n=\\nKMeans(n_clusters=4,\\nrandom_state=0)\\nkmeans.fit(points)\\npredicted_cluster_indexes\\n=\\nkmeans.predict(points)\\n\\nplt.scatter(x,\\ny,\\nc=predicted_cluster_indexes,\\ns=50,\\nalpha=0.7,\\ncmap='viridis')\\n\\ncenters\\n=\\nkmeans.cluster_centers_\\nplt.scatter(centers[:,\\n0],\\ncenters[:,\\n1],\\nc='red',\\ns=100)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 39, 'file_type': 'pdf'}, page_content='38\\nHere is the result:\\nTry setting n_clusters to other values, such as 3 and 5, to\\nsee how the points are grouped with different cluster counts.\\nWhich begs the question: how do you know what the right\\nnumber of clusters is? The answer isn’t always obvious from\\nlooking at a plot, and if the data has more than three\\ndimensions, you can’t plot it anyway.\\nOne way to pick the right number is with the elbow method,\\nwhich plots inertias (the sum of the squared distances of the\\ndata points to the closest cluster center) obtained from\\nKMeans.inertia_ as a function of cluster counts. Plot\\ninertias this way and look for the sharpest elbow in the\\ncurve:\\ninertias\\n=\\n[]\\n\\nfor\\ni\\nin\\nrange(1,\\n10):'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 40, 'file_type': 'pdf'}, page_content=\"39\\n\\xa0\\xa0\\xa0 kmeans\\n=\\nKMeans(n_clusters=i,\\nrandom_state=0)\\n\\xa0\\xa0\\xa0 kmeans.fit(points)\\n\\xa0\\xa0\\xa0 inertias.append(kmeans.inertia_)\\n\\nplt.plot(range(1,\\n10),\\ninertias)\\nplt.xlabel('Number of clusters')\\nplt.ylabel('Inertia')\\nIn this example, it appears that 4 is the right number of\\nclusters:\\nIn real life, the elbow might not be so distinct. That’s OK,\\nbecause by clustering the data in different ways, you\\nsometimes obtain insights that you wouldn’t obtain\\notherwise.\\nApplying k-Means Clustering to Customer Data\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 41, 'file_type': 'pdf'}, page_content=\"40\\nLet’s use k-means clustering to tackle a real problem:\\nsegmenting customers to identify ones to target with a\\npromotion to increase their purchasing activity. The dataset\\nthat you’ll use is a sample customer segmentation dataset\\nnamed customers.csv. Start by creating a subdirectory named\\nData in the folder where your notebooks reside, downloading\\ncustomers.csv, and copying it into the Data subdirectory.\\nThen use the following code to load the dataset into a Pandas\\nDataFrame and display the first five rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ncustomers\\n=\\npd.read_csv('Data/customers.csv')\\ncustomers.head()\\nFrom the output, you learn that the dataset contains five\\ncolumns, two of which describe the customer’s annual income\\nand spending score. The latter is a value from 0 to 100. The\\nhigher the number, the more this customer has spent with your\\ncompany in the past:\\nNow use the following code to plot the annual incomes and\\nspending scores:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 42, 'file_type': 'pdf'}, page_content=\"41\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\npoints\\n=\\ncustomers.iloc[:,\\n3:5].values\\nx\\n=\\npoints[:,\\n0]\\ny\\n=\\npoints[:,\\n1]\\n\\nplt.scatter(x,\\ny,\\ns=50,\\nalpha=0.7)\\nplt.xlabel('Annual Income (k$)')\\nplt.ylabel('Spending Score')\\nFrom the results, it appears that the data points fall into\\nroughly five clusters:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 43, 'file_type': 'pdf'}, page_content=\"42\\nUse the following code to segment the customers into five\\nclusters and highlight the clusters:\\n\\nfrom\\nsklearn.cluster\\nimport\\nKMeans\\n\\nkmeans\\n=\\nKMeans(n_clusters=5,\\nrandom_state=0)\\nkmeans.fit(points)\\npredicted_cluster_indexes\\n=\\nkmeans.predict(points)\\n\\nplt.scatter(x,\\ny,\\nc=predicted_cluster_indexes,\\ns=50,\\nalpha=0.7,\\ncmap='viridis')\\nplt.xlabel('Annual Income (k$)')\\nplt.ylabel('Spending Score')\\n\\ncenters\\n=\\nkmeans.cluster_centers_\\nplt.scatter(centers[:,\\n0],\\ncenters[:,\\n1],\\nc='red',\\ns=100)\\nHere is the result:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 44, 'file_type': 'pdf'}, page_content=\"43\\nThe customers in the lower-right quadrant of the chart might\\nbe good ones to target with a promotion to increase their\\nspending. Why? Because they have high incomes but low\\nspending scores. Use the following statements to create a\\ncopy of the DataFrame and add a column named Cluster\\ncontaining cluster indexes:\\n\\ndf\\n=\\ncustomers.copy()\\ndf['Cluster']\\n=\\nkmeans.predict(points)\\ndf.head()\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 45, 'file_type': 'pdf'}, page_content=\"44\\nNow use the following code to output the IDs of customers who\\nhave high incomes but low spending scores:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\n# Get the cluster index for a customer with a high income and low spending\\nscore\\ncluster\\n=\\nkmeans.predict(np.array([[120,\\n20]]))[0]\\n\\n# Filter the DataFrame to include only customers in that cluster\\nclustered_df\\n=\\ndf[df['Cluster']\\n==\\ncluster]\\n\\n# Show the customer IDs\\nclustered_df['CustomerID'].values\\nYou could easily use the resulting customer IDs to extract\\nnames and email addresses from a customer database:\\n\\narray([125,\\n129,\\n131,\\n135,\\n137,\\n139,\\n141,\\n145,\\n147,\\n149,\\n151,\\n153,\\n155,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 157,\\n159,\\n161,\\n163,\\n165,\\n167,\\n169,\\n171,\\n173,\\n175,\\n177,\\n179,\\n181,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 183,\\n185,\\n187,\\n189,\\n191,\\n193,\\n195,\\n197,\\n199],\\ndtype=int64)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 46, 'file_type': 'pdf'}, page_content='45\\nThe key here is that you used clustering to group customers\\nby annual income and spending score. Once customers are\\ngrouped in this manner, it’s a simple matter to enumerate\\nthe customers in each cluster.\\nSegmenting Customers Using More Than Two\\nDimensions\\nThe previous\\xa0 example was an easy one because you used just\\ntwo variables: annual incomes and spending scores. You could\\nhave done the same without help from machine learning. But\\nnow let’s segment the customers again, this time using\\neverything except the customer IDs. Start by replacing the\\nstrings \"Male\" and \"Female\" in the Gender column with 1s and\\n0s, a process known as label encoding. This is necessary\\nbecause machine learning can only deal with numerical data:\\n\\nfrom\\nsklearn.preprocessing\\nimport\\nLabelEncoder\\n\\ndf\\n=\\ncustomers.copy()\\nencoder\\n=\\nLabelEncoder()\\ndf[\\'Gender\\']\\n=\\nencoder.fit_transform(df[\\'Gender\\'])\\ndf.head()\\nThe Gender column now contains 1s and 0s:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 47, 'file_type': 'pdf'}, page_content=\"46\\nExtract the gender, age, annual income, and spending score\\ncolumns. Then use the elbow method to determine the optimum\\nnumber of clusters based on these features:\\n\\npoints\\n=\\ndf.iloc[:,\\n1:5].values\\ninertias\\n=\\n[]\\n\\nfor\\ni\\nin\\nrange(1,\\n10):\\n\\xa0\\xa0\\xa0 kmeans\\n=\\nKMeans(n_clusters=i,\\nrandom_state=0)\\n\\xa0\\xa0\\xa0 kmeans.fit(points)\\n\\xa0\\xa0\\xa0 inertias.append(kmeans.inertia_)\\n\\nplt.plot(range(1,\\n10),\\ninertias)\\nplt.xlabel('Number of Clusters')\\nplt.ylabel('Inertia')\\nThe elbow is less distinct this time, but 5 appears to be a\\nreasonable number:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 48, 'file_type': 'pdf'}, page_content=\"47\\nSegment the customers into five clusters and add a column\\nnamed Cluster containing the index of the cluster (0-4) to\\nwhich the customer was assigned:\\n\\nkmeans\\n=\\nKMeans(n_clusters=5,\\nrandom_state=0)\\nkmeans.fit(points)\\n\\ndf['Cluster']\\n=\\nkmeans.predict(points)\\ndf.head()\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 49, 'file_type': 'pdf'}, page_content=\"48\\nYou have a cluster number for each customer, but what does it\\nmean? You can’t plot gender, age, annual income, and\\nspending score in a two-dimensional chart the way you plotted\\nannual income and spending score in the previous example. But\\nyou can get the mean (average) of these values for each\\ncluster from the cluster centroids. Create a new DataFrame\\nwith columns for average age, average income, and so on, and\\nthen show the results in a table:\\n\\nresults\\n=\\npd.DataFrame(columns\\n=\\n['Cluster',\\n'Average Age',\\n'Average Income',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Average Spending Index',\\n'Number of\\nFemales',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Number of Males'])\\n\\nfor\\ni,\\ncenter\\nin\\nenumerate(kmeans.cluster_centers_):\\n\\xa0\\xa0\\xa0 age\\n=\\ncenter[1]\\xa0\\xa0\\xa0 # Average age for current cluster\\n\\xa0\\xa0\\xa0 income\\n=\\ncenter[2]\\n# Average income for current cluster\\n\\xa0\\xa0\\xa0 spend\\n=\\ncenter[3]\\xa0 # Average spending score for current cluster\\n\\n\\xa0\\xa0\\xa0 gdf\\n=\\ndf[df['Cluster']\\n==\\ni]\\n\\xa0\\xa0\\xa0 females\\n=\\ngdf[gdf['Gender']\\n==\\n0].shape[0]\\n\\xa0\\xa0\\xa0 males\\n=\\ngdf[gdf['Gender']\\n==\\n1].shape[0]\\n\\n\\xa0\\xa0\\xa0 results.loc[i]\\n=\\n([i,\\nage,\\nincome,\\nspend,\\nfemales,\\nmales])\\n\\nresults.head()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 50, 'file_type': 'pdf'}, page_content='49\\nThe output is as follows:\\nBased on this, if you were going to target customers with\\nhigh incomes but low spending scores for a promotion, which\\ngroup of customers (which cluster) would you choose? Would it\\nmatter whether you targeted males or females? For that\\nmatter, what if your goal was to create a loyalty program\\nrewarding customers with high spending scores, but you wanted\\nto give preference to younger customers who might be loyal\\ncustomers for a long time? Which cluster would you target\\nthen?\\nAmong the more interesting insights that clustering reveals\\nis that some of the biggest spenders are young people\\n(average age = 25.5) with modest incomes. Those customers are\\nmore likely to be female than male. All of this is useful\\ninformation to have if you’re growing a company and want to\\nbetter understand the demographics that you serve.\\nNOTE\\nk-means might be the most commonly used clustering algorithm, but\\nit’s not the only one. Others include agglomerative clustering,\\nwhich clusters data points in a hierarchical manner, and DBSCAN,\\nwhich stands for density-based spatial clustering of applications\\nwith noise. DBSCAN doesn’t require the cluster count to be\\nspecified ahead of time. It can also identify points that fall\\noutside the clusters it identifies, which is useful for detecting\\noutliers—anomalous data points that don’t fit in with the rest.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 51, 'file_type': 'pdf'}, page_content='50\\nScikit-Learn provides implementations of both algorithms in its\\nAgglomerative\\u200bClus\\u2060ter\\u2060ing and DBSCAN classes.\\nDo real companies use clustering to extract insights from\\ncustomer data? Indeed they do. During grad school, my son,\\nnow a data analyst for Delta Air Lines, interned at a pet\\nsupplies company. He used k-means clustering to determine\\nthat the number one reason that leads coming in through the\\ncompany’s website weren’t converted to sales was the length\\nof time between when the lead came in and Sales first\\ncontacted the customer. As a result, his employer introduced\\nadditional automation to the sales workflow to ensure that\\nleads were acted on quickly. That’s unsupervised learning at\\nwork. And it’s a splendid example of a company using machine\\nlearning to improve its business processes.\\nSupervised Learning\\nUnsupervised learning is an important branch of machine\\nlearning, but when most people hear the term machine learning\\nthey think about supervised learning. Recall that supervised\\nlearning models make predictions. For example, they predict\\nwhether a credit card transaction is fraudulent or a flight\\nwill arrive on time. They’re also trained with labeled data.\\nSupervised learning models come in two varieties: regression\\nmodels and classification models. The purpose of a regression\\nmodel is to predict a numeric outcome such as the price that\\na home will sell for or the age of a person in a photo.\\nClassification models, by contrast, predict a class or\\ncategory from a finite set of classes defined in the training\\ndata. Examples include whether a credit card transaction is\\nlegitimate or fraudulent and what number a handwritten digit\\nrepresents. The former is a binary classification model\\nbecause there are just two possible outcomes: the transaction'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 52, 'file_type': 'pdf'}, page_content='51\\nis legitimate or it’s not. The latter is an example of\\nmulticlass classification. Because there are 10 digits (0–9)\\nin the Western Arabic numeral system, there are 10 possible\\nclasses that a handwritten digit could represent.\\nThe two types of supervised learning models are pictured in\\nFigure\\xa01-7. On the left, the goal is to input an x and\\npredict what y will be. On the right, the goal is to input an\\nx and a y and predict what class the point corresponds to: a\\ntriangle or an ellipse. In both cases, the purpose of\\napplying machine learning to the problem is to build a model\\nfor making predictions. Rather than build that model\\nyourself, you train a machine learning model with labeled\\ndata and allow it to devise a mathematical model for you.\\nFigure 1-7. Regression versus classification\\nFor these datasets, you could easily build mathematical\\nmodels without resorting to machine learning. For a\\nregression model, you could draw a line through the data\\npoints and use the equation of that line to predict a y given\\nan x (Figure\\xa01-8). For a classification model, you could\\ndraw a line that cleanly separates triangles from ellipses—\\nwhat data scientists call a classification boundary—and\\npredict which class a new point represents by determining'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 53, 'file_type': 'pdf'}, page_content='52\\nwhether the point falls above or below the line. A point just\\nabove the line would be a triangle, while a point just below\\nit would classify as an ellipse.\\nFigure 1-8. Regression line and linear separation boundary\\nIn the real world, datasets are rarely this orderly. They\\ntypically look more like the ones in Figure\\xa01-9, in which\\nthere is no single line you can draw to correlate the x and y\\nvalues on the left or cleanly separate the classes on the\\nright. The goal, therefore, is to build the best model you\\ncan. That means picking the learning algorithm that produces\\nthe most accurate model.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 54, 'file_type': 'pdf'}, page_content='53\\nFigure 1-9. Real-world datasets\\nThere are many supervised learning algorithms. They go by\\nnames such as linear regression, random forests, gradient-\\nboosting machines (GBMs), and support vector machines (SVMs).\\nMany, but not all, can be used for regression and\\nclassification. Even seasoned data scientists frequently\\nexperiment to determine which learning algorithm produces the\\nmost accurate model. These and other learning algorithms will\\nbe covered in subsequent chapters.\\nk-Nearest Neighbors\\nOne of the simplest supervised learning algorithms is k-\\nnearest neighbors. The premise behind it is that given a set\\nof data points, you can predict a label for a new point by\\nexamining the points nearest it. For a simple regression\\nproblem in which each data point is characterized by x and y\\ncoordinates, this means that given an x, you can predict a y\\nby finding the n points with the nearest xs and averaging\\ntheir ys. For a classification problem, you find the n points\\nclosest to the point whose class you want to predict and\\nchoose the class with the highest occurrence count. If n = 5\\nand the five nearest neighbors include three triangles and'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 55, 'file_type': 'pdf'}, page_content='54\\ntwo ellipses, then the answer is a triangle, as pictured in\\nFigure\\xa01-10.\\nFigure 1-10. Classification with k-nearest neighbors\\nHere’s an example involving regression. Suppose you have 20\\ndata points describing how much programmers earn per year\\nbased on years of experience. Figure\\xa01-11 plots years of\\nexperience on the x-axis and annual income on the y-axis.\\nYour goal is to predict what someone with 10 years of\\nexperience should earn. In this example, x = 10, and you want\\nto predict what y should be.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 56, 'file_type': 'pdf'}, page_content='55\\nFigure 1-11. Programmers’ salaries in dollars versus years of experience\\nApplying k-nearest neighbors with n = 10 identifies the\\npoints highlighted in orange in Figure\\xa01-12 as the nearest\\nneighbors—the 10 whose x coordinates are closest to x = 10.\\nThe average of these points’ y coordinates is 94,838.\\nTherefore, k-nearest neighbors with n = 10 predicts that a\\nprogrammer with 10 years of experience will earn $94,838, as\\nindicated by the red dot.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 57, 'file_type': 'pdf'}, page_content='56\\nFigure 1-12. Regression with k-nearest neighbors and n = 10\\nThe value of n that you use with k-nearest neighbors\\nfrequently influences the outcome. Figure\\xa01-13 shows the\\nsame solution with n = 5. The answer is slightly different\\nthis time because the average y for the five nearest\\nneighbors is 98,713.\\nIn real life, it’s a little more nuanced because while the\\ndataset has just one label column, it probably has several\\nfeature columns—not just x, but x1, x2, x3, and so on. You\\ncan compute distances in n-dimensional space easily enough,\\nbut there are several ways to measure distances to identify a\\npoint’s nearest neighbors, including Euclidean distance,\\nManhattan distance, and Minkowski distance. You can even use\\nweights so that nearby points contribute more to the outcome\\nthan faraway points. And rather than find the n nearest\\nneighbors, you can select all the neighbors within a given\\nradius, a technique known as radius neighbors. Still, the\\nprinciple is the same regardless of the number of dimensions\\nin the dataset, the method used to measure distance, or'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 58, 'file_type': 'pdf'}, page_content='57\\nwhether you choose n nearest neighbors or all the neighbors\\nwithin a specified radius: find data points that are similar\\nto the target point and use them to regress or classify the\\ntarget.\\nFigure 1-13. Regression with k-nearest neighbors and n = 5\\nUsing k-Nearest Neighbors to Classify Flowers\\nScikit-Learn includes classes named KNeighborsRegressor and\\nKNeighbors\\u200bClassi\\u2060fier to help you train regression and\\nclassification models using the k-nearest neighbors learning\\nalgorithm. It also includes classes named\\nRadiusNeigh\\u2060borsRe\\u2060gres\\u2060sor and RadiusNeighborsClassifier that\\naccept a radius rather than a number of neighbors. Let’s\\nlook at an example that uses KNeighbor\\u2060s\\u200bClassifier to classify\\nflowers using the famous Iris dataset. That dataset includes\\n150 samples, each representing one of three species of iris.\\nEach row contains four measurements—sepal length, sepal\\nwidth, petal length, and petal width, all in centimeters—'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 59, 'file_type': 'pdf'}, page_content=\"58\\nplus a label: 0 for a setosa iris, 1 for versicolor, and 2\\nfor virginica. Figure\\xa01-14 shows an example of each species\\nand illustrates the difference between petals and sepals.\\nFigure 1-14. Iris dataset (Middle panel: “Blue Flag Flower Close-Up [Iris\\nVersicolor]” by Danielle Langlois is licensed under CC BY-SA 2.5,\\nhttps://creativecommons.org/licenses/by-sa/2.5/deed.en; rightmost panel:\\n“Image of Iris Virginica Shrevei BLUE FLAG” by Frank Mayfield is licensed\\nunder CC BY-SA 2.0, https://creativecommons.org/licenses/by-sa/2.0/deed.en)\\nTo train a machine learning model to differentiate between\\nspecies of iris based on sepal and petal measurements, begin\\nby running the following code in a Jupyter notebook to load\\nthe dataset, add a column containing the class name, and show\\nthe first five rows:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nload_iris\\n\\niris\\n=\\nload_iris()\\ndf\\n=\\npd.DataFrame(iris.data,\\ncolumns=iris.feature_names)\\ndf['class']\\n=\\niris.target\\ndf['class name']\\n=\\niris.target_names[iris['target']]\\ndf.head()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 60, 'file_type': 'pdf'}, page_content='59\\nThe Iris dataset is one of several sample datasets included\\nwith Scikit. That’s why you can load it by calling Scikit’s\\nload_iris function rather than reading it from an external\\nfile. Here’s the output from the code:\\nBefore you train a machine learning model from the data, you\\nneed to split the dataset into two datasets: one for training\\nand one for testing. That’s important, because if you don’t\\ntest a model with data it hasn’t seen before—that is, data\\nit wasn’t trained with—you have no idea how accurate it is\\nat making predictions.\\nFortunately, Scikit’s train_test_split function makes it\\neasy to split a dataset using a fractional split that you\\nspecify. Use the following statements to perform an 80/20\\nsplit with 80% of the rows set aside for training and 20%\\nreserved for testing:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 iris.data,\\niris.target,\\ntest_size=0.2,\\nrandom_state=0)\\nNow, x_train and y_train hold 120 rows of randomly selected\\nmeasurements and labels, while x_test and y_test hold the\\nremaining 30. Although 80/20 splits are customary for small\\ndatasets like this one, there’s no rule saying you have to\\nsplit 80/20. The more data you train with, the more accurate'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 61, 'file_type': 'pdf'}, page_content='60\\nthe model is. (That’s not strictly true, but generally\\nspeaking, you always want as much training data as you can\\nget.) The more data you test with, the more confidence you\\nhave in measurements of the model’s accuracy. For a small\\ndataset, 80/20 is a reasonable place to start.\\nThe next step is to train a machine learning model. Thanks to\\nScikit, that requires just a few lines of code:\\n\\nfrom\\nsklearn.neighbors\\nimport\\nKNeighborsClassifier\\n\\nmodel\\n=\\nKNeighborsClassifier()\\nmodel.fit(x_train,\\ny_train)\\nIn Scikit, you create a machine learning model by\\ninstantiating the class encapsulating the learning algorithm\\nyou selected—in this case, KNeighborsClassifier. Then you\\ncall fit on the model to train it by fitting it to the\\ntraining data. With just 120 rows of training data, training\\nhappens very quickly.\\nThe final step is to use the 30 rows of test data split off\\nfrom the original dataset to measure the model’s accuracy.\\nIn Scikit, that’s accomplished by calling the model’s score\\nmethod:\\n\\nmodel.score(x_test,\\ny_test)\\nIn this example, score returns 0.966667, which means the\\nmodel got it right about 97% of the time when making\\npredictions with the features in x_test and comparing the\\npredicted labels to the actual labels in y_test.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 62, 'file_type': 'pdf'}, page_content='61\\nOf course, the whole purpose of training a predictive model\\nis to make predictions with it. In Scikit, you make a\\nprediction by calling the model’s predict method. Use the\\nfollowing statements to predict the class—0 for setosa, 1\\nfor versicolor, and 2 for virginica—identifying the species\\nof an iris whose sepal length is 5.6 cm, sepal width is 4.4\\ncm, petal length is 1.2 cm, and petal width is 0.4 cm:\\n\\nmodel.predict([[5.6,\\n4.4,\\n1.2,\\n0.4]])\\nThe predict method can make multiple predictions in a single\\ncall. That’s why you pass it a list of lists rather than\\njust a list. It returns a list whose length equals the number\\nof lists you passed in. Since you passed just one list to\\npredict, the return value is a list with one value. In this\\nexample, the predicted class is 0, meaning the model\\npredicted that an iris whose sepal length is 5.6 cm, sepal\\nwidth is 4.4 cm, petal length is 1.2 cm, and petal width is\\n0.4 cm is mostly likely a setosa iris.\\nWhen you create a KNeighborsClassifier without specifying the\\nnumber of neighbors, it defaults to 5. You can specify the\\nnumber of neighbors this way:\\n\\nmodel\\n=\\nKNeighborsClassifier(n_neighbors=10)\\nTry fitting (training) and scoring the model again using\\nn_neighbors=10. Does the model score the same? Does predict\\nstill predict class 0? Feel free to experiment with other\\nn_neighbors values to get a feel for their effect on the\\noutcome.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 63, 'file_type': 'pdf'}, page_content='62\\nKNEIGHBORSCLASSIFIER INTERNALS\\nk-nearest neighbors is sometimes referred to as a lazy\\nlearning algorithm because most of the work is done when\\nyou call predict rather than when you call fit. In fact,\\ntraining technically doesn’t have to do anything except\\nmake a copy of the training data for when predict is\\ncalled. So what happens inside KNeighborsClassifier’s\\nfit method?\\nIn most cases, fit constructs a binary tree in memory\\nthat makes predict faster by preventing it from having to\\nperform a brute-force search for neighboring samples. If\\nit determines that a binary tree won’t help,\\nKNeighborsClassifier resorts to brute force when making\\npredictions. This typically happens when the training\\ndata is sparse—that is, mostly zeros with a few nonzero\\nvalues sprinkled in.\\nOne of the wonderful things about Scikit-Learn is that it\\nis open source. If you care to know more about how a\\nparticular class or method works, you can go straight to\\nthe source code on GitHub. You’ll find the source code\\nfor KNeighborsClassifier and RadiusNeighborsClassifier on\\nGitHub.\\nThe process employed here—load the data, split the data,\\ncreate a classifier or regressor, call fit to fit it to the\\ntraining data, call score to assess the model’s accuracy\\nusing test data, and finally, call predict to make\\npredictions—is one that you will use over and over with\\nScikit. In the real world, data frequently requires cleaning\\nbefore it’s used for training and testing. For example, you\\nmight have to remove rows with missing values or dedupe the\\ndata to eliminate redundant rows. You’ll see plenty of\\nexamples of this later, but in this example, the data was\\ncomplete and well structured right out of the box, and\\ntherefore required no further preparation.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 64, 'file_type': 'pdf'}, page_content='63\\nSummary\\nMachine learning offers engineers and software developers an\\nalternative approach to problem-solving. Rather than use\\ntraditional computer algorithms to transform input into\\noutput, machine learning relies on learning algorithms to\\nbuild mathematical models from training data. Then it uses\\nthose models to turn future inputs into outputs.\\nMost machine learning models fall into either of two\\ncategories. Unsupervised learning models are widely used to\\nanalyze datasets by highlighting similarities and\\ndifferences. They don’t require labeled data. Supervised\\nlearning models learn from labeled data in order to make\\npredictions—for example, to predict whether a credit card\\ntransaction is legitimate. Supervised learning can be used to\\nsolve regression problems or classification problems.\\nRegression models predict numeric outcomes, while\\nclassification models predict classes (categories).\\nk-means clustering is a popular unsupervised learning\\nalgorithm, while k-nearest neighbors is a simple yet\\neffective supervised learning algorithm. Many, but not all,\\nsupervised learning algorithms can be used for regression and\\nfor classification. Scikit-Learn’s KNeighborsRegressor\\nclass, for example, applies k-nearest neighbors to regression\\nproblems, while KNeighborsClassifier applies the same\\nalgorithm to classification problems.\\nEducators often use k-nearest neighbors to introduce\\nsupervised learning because it’s easily understood and it\\nperforms reasonably well in a variety of problem domains.\\nWith k-nearest neighbors under your belt, the next step on\\nthe road to machine learning proficiency is getting to know\\nother supervised learning algorithms. That’s the focus of\\nChapter\\xa02, which introduces several popular learning\\nalgorithms in the context of regression modeling.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 65, 'file_type': 'pdf'}, page_content='64\\nChapter 2. Regression Models\\nYou learned in Chapter\\xa01 that supervised learning models\\ncome in two varieties: regression models and classification\\nmodels. You also learned that regression models predict\\nnumeric outcomes, such as the price that a home will sell for\\nor the number of visitors a website will attract. Regression\\nmodeling is a vital and sometimes underappreciated aspect of\\nmachine learning. Retailers use it to forecast demand. Banks\\nuse it to screen loan applications, factoring in variables\\nsuch as credit scores, debt-to-income ratios, and loan-to-\\nvalue ratios. Insurance companies use it to set premiums.\\nWhenever you need numerical predictions, regression modeling\\nis the right tool for the job.\\nWhen building a regression model, the first and most\\nimportant decision you make is what learning algorithm to\\nuse. Chapter\\xa01 presented a simple three-class classification\\nmodel that used the k-nearest neighbors learning algorithm to\\nidentify a species of iris given the flower’s sepal and\\npetal measurements. k-nearest neighbors can be used for\\nregression too, but it’s one of many you can choose from for\\nmaking numerical predictions. Other learning algorithms\\nfrequently produce more accurate models.\\nThis chapter introduces common regression algorithms, many of\\nwhich can be used for classification also, and guides you\\nthrough the process of building a regression model that\\npredicts taxi fares using data published by the New York City\\nTaxi and Limousine Commission. It also describes various\\nmeans for assessing a regression model’s accuracy and\\nintroduces an important technique for measuring accuracy\\ncalled cross-validation.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 66, 'file_type': 'pdf'}, page_content='65\\nLinear Regression\\nNext to k-nearest neighbors, linear regression is perhaps the\\nsimplest learning algorithm of all. It works best with data\\nthat is relatively linear—that is, data points that fall\\nroughly along a line. Thinking back to high school math\\nclass, you’ll recall that the equation for a line in two\\ndimensions is:\\nwhere m is the slope of the line and b is where the line\\nintersects the y-axis. The income-versus-years-of-experience\\ndataset in Figure\\xa01-11 lends itself well to linear\\nregression. Figure\\xa02-1 shows a regression line fit to the\\ndata points. Predicting the income for a programmer with 10\\nyears of experience is as simple as finding the point on the\\nline where x = 10. The equation of the line is y = 3,984x +\\n60,040. Plugging 10 into that equation for x, the predicted\\nincome is $99,880.\\nFigure 2-1. Linear regression'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 67, 'file_type': 'pdf'}, page_content='66\\nThe goal when training a linear regression model is to find\\nvalues for m and b that produce the most accurate\\npredictions. This is typically done using an iterative\\nprocess that starts with assumed values for m and b and\\nrepeats until it converges on suitable values.\\nThe most common technique for fitting a line to a set of\\npoints is ordinary least squares regression, or OLS for\\nshort. It works by squaring the distance in the y direction\\nbetween each point and the regression line, summing the\\nsquares, and dividing by the number of points to compute the\\nmean squared error, or MSE. (Squaring each distance prevents\\nnegative distances from offsetting positive distances.) Then\\nit adjusts m and b to reduce the MSE the next time around and\\nrepeats until the MSE is sufficiently low. I won’t go into\\nthe details of how it determines in which direction to adjust\\nm and b (it’s not hard, but it involves a smidgeon of\\ncalculus—specifically, using partial derivatives of the MSE\\nfunction to determine whether to increase or decrease m and b\\nin the next iteration), but OLS can often fit a line to a set\\nof points with a dozen or fewer iterations.\\nScikit-Learn\\xa0 has a number of classes to help you build\\nlinear regression models, including the LinearRegression\\nclass, which embodies OLS, and the Polynomial\\u200bFea\\u2060tures class,\\nwhich fits a polynomial curve rather than a straight line to\\nthe training data. Training a linear regression model can be\\nas simple as this:\\n\\nmodel\\n=\\nLinearRegression()\\nmodel.fit(x,\\ny)\\nScikit has other linear regression classes with names such as\\nRidge and Lasso. One scenario in which they’re useful is\\nwhen the training data contains outliers. Recall from\\nChapter\\xa01 that outliers are data points that don’t conform'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 68, 'file_type': 'pdf'}, page_content='67\\nwith the rest. Outliers can bias a model or make it less\\naccurate. Ridge and Lasso add regularization, which mitigates\\nthe effect of outliers by lessening their influence on the\\noutcome as coefficients are adjusted during training. An\\nalternate approach to dealing with outliers is to remove them\\naltogether, which is what you’ll do in the taxi-fare example\\nat the end of this chapter.\\nNOTE\\nLasso regression has a secondary benefit too. If the training data\\nsuffers from multicollinearity, a condition in which two or more\\ninput variables are linearly correlated so that one can be\\npredicted from another with a reasonable degree of accuracy, Lasso\\neffectively ignores the redundant data.\\nA classic example of multicollinearity occurs when a dataset\\nincludes one column specifying the number of rooms in a house and\\nanother column specifying the square footage. More rooms generally\\nmeans more area, so the two variables are correlated to some\\ndegree.\\nLinear regression isn’t limited to two dimensions (x and y\\nvalues); it works with any number of dimensions. Linear\\nregression with one independent variable (x) is known as\\nsimple linear regression, while linear regression with two or\\nmore independent variables—for example, x1, x2, x3, and so\\non—is called multiple linear regression. If a dataset is two\\ndimensional, it’s simple enough to plot the data to\\ndetermine its shape. You can plot three-dimensional data too,\\nbut plotting datasets with four or five dimensions is more\\nchallenging, and datasets with hundreds or thousands of\\ndimensions are impossible to visualize.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 69, 'file_type': 'pdf'}, page_content='68\\nHow do you determine whether a high-dimensional dataset might\\nlend itself to linear regression? One way to do it is to\\nreduce n dimensions to two or three using techni\\u2060ques such as\\nprincipal component analysis (PCA) and t-distributed\\nstochastic neighbor embedding (t-SNE) so that you can plot\\nthem. These techniques are covered in Chapter\\xa06. Both reduce\\nthe dimensionality of a dataset without incurring a\\ncommensurate loss of information. With PCA, for example, it\\nisn’t uncommon to reduce the number of dimensions by 90%\\nwhile retaining 90% of the information in the original\\ndataset. It might sound like magic, but it’s not. It’s\\nmath.\\nIf the number of dimensions is relatively small, a simpler\\ntechnique for visualizing high-dimensional datasets is pair\\nplots, which plot pairs of dimensions in conventional 2D\\ncharts. Figure\\xa02-2 shows a pair plot charting sepal length\\nversus petal length, sepal width versus petal width, and\\nother parameter pairs for the Iris dataset introduced in\\nChapter\\xa01.\\nSeaborn’s pairplot function makes it easy to create pair\\nplots. The plot in Figure\\xa02-2 was generated with one line of\\ncode:\\n\\nsns.pairplot(df)\\nThe pair plot not only helps you visualize relationships in\\nthe dataset, but in this example, the histogram in the lower-\\nright corner reveals that the dataset is balanced too. There\\nis an equal number of samples of all three classes, and for\\nreasons you’ll learn in Chapter\\xa03, you always prefer to\\ntrain classification models with balanced datasets.\\nLinear regression is a parametric learning algorithm, which\\nmeans that its purpose is to examine a dataset and find the'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 70, 'file_type': 'pdf'}, page_content='69\\noptimum values for parameters in an equation—for example, m\\nand b. k-nearest neighbors, by contrast, is a nonparametric\\nlearning algorithm because it doesn’t fit data to an\\nequation. Why does it matter whether a learning algorithm is\\nparametric or nonparametric? Because datasets used to train\\nparametric models frequently need to be normalized. At its\\nsimplest, normalizing data means making sure all the values\\nin all the columns have consistent ranges. I’ll cover\\nnormalization in Chapter\\xa05, but for now, realize that\\ntraining parametric models with unnormalized data—for\\nexample, a dataset that contains values from 0 to 1 in one\\ncolumn and 0 to 1,000,000 in another—can make those models\\nless accurate or prevent them from converging on a solution\\naltogether. This is particularly true with support vector\\nmachines and neural networks, but it applies to other\\nparametric models as well. Even k-nearest neighbors models\\nwork best with normalized data because while the learning\\nalgorithm isn’t parametric, it uses distance-based\\ncalculations internally.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 71, 'file_type': 'pdf'}, page_content='70\\nFigure 2-2. Pair plot revealing relationships between variable pairs\\nDecision Trees\\nEven if you’ve never taken a computer science course, you\\nprobably know what a binary tree is. In machine learning, a\\ndecision tree is a tree structure that predicts an outcome by\\nanswering a series of questions. Most decision trees are\\nbinary trees, in which case the questions require simple yes-\\nor-no answers.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 72, 'file_type': 'pdf'}, page_content='71\\nFigure\\xa02-3 shows a decision tree built by Scikit from the\\nincome-versus-experience dataset introduced in Chapter\\xa01.\\nThe tree is simple because the dataset contains just one\\nfeature column (years of experience) and I limited the\\ntree’s depth to 3, but the technique extends to trees of\\nunlimited size and complexity. In this example, predicting a\\nsalary for a programmer with 10 years of experience requires\\njust three yes/no decisions, as indicated by the red arrows.\\nThe answer is about $100K, which is pretty close to what k-\\nnearest neighbors and linear regression predicted when\\napplied to the same dataset.\\nFigure 2-3. Decision tree\\nDecision trees can be used for regression and classification.\\nFor a regressor, the leaf nodes (the nodes that lack\\nchildren) represent regression values. For a classifier, they\\nrepresent classes. The output from a decision tree regressor\\nisn’t continuous. The output will always be one of the\\nvalues assigned to a leaf node, and the number of leaf nodes\\nis finite. The output from a linear regression model, by\\ncontrast, is continuous. It can assume any value along the\\nline fit to the training data. In the previous example, you\\nget the same answer if you ask the tree to predict a salary\\nfor someone with 10 years of experience and someone with 13\\nyears of experience. Bump years of experience up to 14,\\nhowever, and the predicted salary jumps to $125K (Figure\\xa02-\\n4). If you allow the tree to grow deeper, the answers become'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 73, 'file_type': 'pdf'}, page_content='72\\nmore refined. But allowing it to grow too deep can lead to\\nbig problems for reasons we’ll cover momentarily.\\nOnce a decision tree model is trained—that is, once the tree\\nis built—predictions are made quickly. But how do you decide\\nwhat decisions to make at each node? For example, why is the\\nnumber of years represented by the root node in Figure\\xa02-3\\nequal to 13.634? Why not 10.000 or 8.742 or some other\\nnumber? For that matter, if the dataset has multiple feature\\ncolumns, how do you decide which column to break on at each\\ndecision node?\\nFigure 2-4. Mathematical model created from a decision tree\\nDecision trees are built by recursively splitting the\\ntraining data. The fundamental decisions that the splitting\\nalgorithm makes when it adds a node to the tree are 1) which\\ncolumn will this node split, and 2) what is the value that\\nthe split is based upon. In each iteration, the goal is to\\nselect a column and split value that does the most to reduce\\nthe “impurity” of the remaining data for classification'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 74, 'file_type': 'pdf'}, page_content='73\\nproblems or the variance of the remaining data for regression\\nproblems. A common impurity measure for classifiers is Gini,\\nwhich roughly quantifies the percentage of samples that a\\nsplit value would misclassify. For regressors, the sum of the\\nsquared error or absolute error, where “error” is the\\ndifference between the split value and the values on either\\nside of the split, is typically used instead. The tree-\\nbuilding process starts at the root node and works its way\\nrecursively downward until the tree is fully leafed out or\\nexternal constraints (such as a limit on maximum depth)\\nprevent further growth.\\nScikit’s DecisionTreeRegressor class and DecisionTree\\u200b\\nClas\\u2060sifier class make building decision trees easy. Each\\nimplements the well-known CART algorithm for building binary\\ntrees, and each lets you choose from a handful of criteria\\nfor measuring impurity or variance. Each also supports\\nparameters such as max_depth, min\\u200b_sam\\u2060ples_split, and\\nmin_samples_leaf that let you constrain a decision tree’s\\ngrowth. If you accept the default values, building a decision\\ntree can be as simple as this:\\nmodel\\n=\\nDecisionTreeRegressor()\\nmodel.fit(x,\\ny)\\nDecision trees are nonparametric. Training a decision tree\\nmodel involves building a binary tree, not fitting an\\nequation to a dataset. This means data used to build a\\ndecision tree doesn’t have to be normalized.\\nDecision trees have a big upside: they work as well with\\nnonlinear data as they do with linear data. In fact, they\\nlargely don’t care how the data is shaped. But there’s a\\ndownside too. It’s a big one, and it’s one of the reasons\\nstandalone decision trees are rarely used in machine\\nlearning. That reason is overfitting.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 75, 'file_type': 'pdf'}, page_content='74\\nDecision trees are highly prone to overfitting. If allowed to\\ngrow large enough, a decision tree can essentially memorize\\nthe training data. It might appear to be accurate, but if\\nit’s fit too tightly to the training data, it might not\\ngeneralize well. That means it won’t be as accurate when\\nit’s asked to make predictions with data it hasn’t seen\\nbefore. Figure\\xa02-5 shows a decision tree fit to the income-\\nversus-experience dataset with no constraints on depth. The\\njagged path followed by the red line as it passes through all\\nthe points is a clear sign of overfitting. Overfitting is the\\nbane of data scientists. The only thing worse than a model\\nthat’s inaccurate is one that appears to be accurate but in\\nreality is not.\\nFigure 2-5. Decision tree overfit to the training data\\nOne way to prevent overfitting when using decision trees is\\nto constrain their growth so that they can’t memorize the\\ntraining data. Another way is to use groups of decision trees\\ncalled random forests.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 76, 'file_type': 'pdf'}, page_content='75\\nRandom Forests\\nA random forest is a collection of decision trees (often\\nhundreds of them), each trained differently on the same data,\\nas depicted in Figure\\xa02-6. Typically, each tree is trained\\non randomly selected rows in the dataset, and branching is\\nbased on columns that are randomly selected at every split.\\nThe model can’t fit too tightly to the training data because\\nevery tree trains on a different subset of the data. The\\ntrees are built independently, and when the model makes a\\nprediction, it runs the input through all the decision trees\\nand averages the result. Because the trees are constructed\\nindependently, training can be parallelized on hardware that\\nsupports it.\\nFigure 2-6. Random forests\\nIt’s a simple concept, and one that works well in practice.\\nRandom forests can be used for both regression and\\nclassification, and Scikit provides classes such as\\nRandomFores\\u2060t\\u200bRegressor and RandomForestClassifier to help out.\\nThey feature a number of tunable parameters, including\\nn_estimators, which specifies the number of trees in the\\nrandom forest (default = 100); max_depth, which limits the\\ndepth of each tree; and max_samples, which specifies the\\nfraction of the rows in the training data used to build\\nindividual trees. Figure\\xa02-7 shows how RandomForestRegressor\\nfits to the income-versus-experience dataset with max_depth=3'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 77, 'file_type': 'pdf'}, page_content='76\\nand max_samples=0.5, meaning no tree sees more than 50% of\\nthe rows in the dataset.\\nFigure 2-7. Mathematical model created from a random forest\\nBecause decision trees are nonparametric, random forests are\\nnonparametric also. And even though Figure\\xa02-7 shows how a\\nrandom forest fits a linear dataset, random forests are\\nperfectly capable of modeling nonlinear datasets too.\\nGradient-Boosting Machines\\nRandom forests are proof of the supposition that you can take\\nmany weak learners—models that by themselves are not strong\\npredictors—and combine them to form accurate models. No\\nindividual tree in a random forest can predict an outcome\\nwith a great deal of accuracy. But put all the trees together\\nand average the results and they often outperform other\\nmodels. Data scientists refer to this as ensemble modeling or\\nensemble learning.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 78, 'file_type': 'pdf'}, page_content='77\\nAnother way to exploit ensemble modeling is gradient\\nboosting. Models that use it are called gradient-boosting\\nmachines, or GBMs. Most GBMs use decision trees and are\\nsometimes referred to as gradient-boosted decision trees\\n(GBDTs). Like random forests, GBDTs comprise collections of\\ndecision trees. But rather than build independent decision\\ntrees from random subsets of the data, GBDTs build dependent\\ndecision trees, one after another, training each using output\\nfrom the last. The first decision tree models the dataset.\\nThe second decision tree models the error in the output from\\nthe first, the third models the error in the output from the\\nsecond, and so on. To make a prediction, a GBDT runs the\\ninput through each decision tree and sums all the outputs to\\narrive at a result. With each addition, the result becomes\\nslightly more accurate, giving rise to the term additive\\nmodeling. It’s like driving a golf ball down the fairway and\\nhitting successively shorter shots until you finally reach\\nthe hole.\\nEach decision tree in a GBDT model is a weak learner. In\\nfact, GBDTs typically use decision tree stumps, which are\\ndecision trees with depth 1 (a root node and two child\\nnodes), as shown in Figure\\xa02-8. During training, you start\\nby taking the mean of all the target values in the training\\ndata to create a baseline for predictions. Then you subtract\\nthe mean from the target values to generate a new set of\\ntarget values or residuals for the first tree to predict.\\nAfter training the first tree, you run the input through it\\nto generate a set of predictions. Then you add the\\npredictions to the previous set of predictions, generate a\\nnew set of residuals by subtracting the sum from the original\\n(actual) target values, and train a second tree to predict\\nthose residuals. Repeating this process for n trees, where n\\nis typically 100 or more, produces an ensemble model. To help\\nensure that each decision tree is a weak learner, GBDT models\\nmultiply the output from each decision tree by a learning\\nrate to reduce their influence on the outcome. The learning'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 79, 'file_type': 'pdf'}, page_content='78\\nrate is usually a small number such as 0.1 and is a parameter\\nthat you can specify when using classes that implement GBMs.\\nFigure 2-8. Gradient-boosting machines\\nScikit includes classes named GradientBoostingRegressor and\\nGradientBoostingClassifier to help you build GBDTs. But if you\\nreally want to understand how GBDTs work, you can build one\\nyourself with Scikit’s DecisionTreeRegressor class. The code\\nin Example\\xa02-1 implements a GBDT with 100 decision tree\\nstumps and predicts the annual income of a programmer with 10\\nyears of experience.\\nExample 2-1. Gradient-boosted decision tree implementation\\n\\nlearning_rate\\n=\\n0.1\\n# Learning rate\\nn_trees\\n=\\n100\\n# Number of decision trees\\ntrees\\n=\\n[]\\n# Trees that comprise the model\\n\\n# Compute the mean of all the target values\\ny_pred\\n=\\nnp.array([y.mean()]\\n*\\nlen(y))\\nbaseline\\n=\\ny_pred\\n\\n\\n# Create n_trees and train each with the error\\n# in the output from the previous tree'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 80, 'file_type': 'pdf'}, page_content='79\\nfor\\ni\\nin\\nrange(n_trees):\\n\\n\\xa0\\xa0\\xa0 error\\n=\\ny\\n-\\ny_pred\\n\\xa0\\xa0\\xa0 tree\\n=\\nDecisionTreeRegressor(max_depth=1,\\nrandom_state=0)\\n\\n\\xa0\\xa0\\xa0 tree.fit(x,\\nerror)\\n\\n\\xa0\\xa0\\xa0 predictions\\n=\\ntree.predict(x)\\n\\xa0\\xa0\\xa0 y_pred\\n=\\ny_pred\\n+\\n(learning_rate\\n*\\npredictions)\\n\\xa0\\xa0\\xa0 trees.append(tree)\\n\\n\\n# Predict a y for x=10\\ny_pred\\n=\\nnp.array([baseline[0]]\\n*\\nlen(x))\\n\\n\\nfor\\ntree\\nin\\ntrees:\\n\\n\\xa0\\xa0\\xa0 y_pred\\n=\\ny_pred\\n+\\n(learning_rate\\n*\\ntree.predict([[10.0]]))\\n\\ny_pred[0]\\nThe diagram on the left in Figure\\xa02-9 shows the output from\\na single decision tree stump applied to the income-versus-\\nexperience dataset. That model is such a weak learner that it\\ncan predict only two different income levels. The diagram on\\nthe right shows the output from the model in Example\\xa02-1.\\nThe additive effect of the weak learners produces a strong\\nlearner that predicts a programmer with 10 years of\\nexperience should earn $99,082 per year, which is consistent\\nwith the predictions made by other models.\\nFigure 2-9. Single decision tree versus gradient-boosted decision trees'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 81, 'file_type': 'pdf'}, page_content='80\\nGBDTs can be used for regression and classification, and they\\nare nonparametric. Aside from neural networks and support\\nvector machines, GBDTs are frequently the ones that data\\nscientists find most capable of modeling complex datasets.\\nUnlike linear regression models and random forests, GBDTs are\\nsusceptible to overfitting. One way to mitigate overfitting\\nwhen using GradientBoostingRegressor and\\nGradientBoostingClassifier is to use the subsample parameter\\nto prevent individual trees from seeing the entire dataset,\\nanalogous to what max_samples does for random forests.\\nAnother way is to use the learning_rate parameter to lower\\nthe learning rate, which defaults to 0.1.\\nSupport Vector Machines\\nI will save a full treatment of support vector machines\\n(SVMs) for Chapter\\xa05, but along with GBMs, they represent\\nthe cutting edge of statistical machine learning. They can\\noften fit models to highly nonlinear datasets that other\\nlearning algorithms cannot. They’re so important that they\\nmerit separate treatment from all other algorithms. They work\\nby employing a mathematical device called kernel tricks to\\nsimulate the effect of adding dimensions to data. The idea is\\nthat data that isn’t separable in m dimensions might be\\nseparable in n dimensions. Here’s a quick example.\\nThe classes in the two-dimensional dataset on the left in\\nFigure\\xa02-10 can’t be separated with a line. But if you add\\na third dimension so that points closer to the center have\\nhigher z values and points farther from the center have lower\\nz values, as shown on the right, you can slide a plane\\nbetween the red points and the purple points and achieve 100%\\nseparation of the classes. That is the principle by which\\nSVMs work. It is mathematically complex when generalized to\\nwork with arbitrary datasets, but it is an extremely powerful\\ntechnique that is vastly simplified by Scikit.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 82, 'file_type': 'pdf'}, page_content='81\\nFigure 2-10. Support vector machines\\nSVMs are primarily used for classification, but they can be\\nused for regression as well. Scikit includes classes for\\ndoing both, including SVC for classification problems and SVR\\nfor regression problems. You will learn all about these\\nclasses in Chapter\\xa05. For now, drop the term support vector\\nmachine at the next machine learning gathering you attend and\\nyou will instantly become the life of the party.\\nAccuracy Measures for Regression Models\\nAs you learned in Chapter\\xa01, you need one set of data for\\ntraining a model and another set for testing it, and you can\\nscore a model for accuracy by passing test data to the\\nmodel’s score method. Testing quantifies how accurate the\\nmodel is at making predictions. It is incredibly important to\\ntest a model with a dataset other than the one it was trained\\nwith because it will probably learn the training data\\nreasonably well, but that doesn’t mean it will generalize\\nwell—that is, make accurate predictions. And if you don’t\\ntest a model, you don’t know how accurate it is.\\nEngineers frequently use Scikit’s train_test_split function\\nto split a dataset into a training dataset and a test\\ndataset. But when you split a small dataset this way, you'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 83, 'file_type': 'pdf'}, page_content='82\\ncan’t necessarily trust the score returned by the model’s\\nscore method. And what does the score mean, anyway? The\\nanswer is different for regression models and classification\\nmodels, and while there are a handful of ways to score\\nregression models, there are many ways to score\\nclassification models. Let’s take a moment to understand the\\nnumber that score returns for a regression model, and what\\nyou can do to have more faith in that number.\\nTo demonstrate why you need to be somewhat skeptical of the\\nvalue returned by score when dealing with small datasets, try\\na simple experiment. Use the following code to load Scikit’s\\nCalifornia housing dataset, shuffle the rows, and extract the\\nfirst 1,000 rows:\\n\\nfrom\\nsklearn.utils\\nimport\\nshuffle\\nfrom\\nsklearn.datasets\\nimport\\nfetch_california_housing\\n\\ndf\\n=\\nfetch_california_housing(as_frame=True).frame\\ndf\\n=\\nshuffle(df,\\nrandom_state=0)\\ndf\\n=\\ndf.head(1000)\\ndf.head()\\nThe dataset contains columns with names such as MedInc\\n(median income) and MedHouseVal (median home value), as shown\\nin the following figure. The details aren’t important for\\nnow. What is important is that you are going to build a model\\nthat uses the values in all the other columns to predict\\nMedHouseVal values:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 84, 'file_type': 'pdf'}, page_content=\"83\\nUse the following code to split the data 80/20, train a\\nlinear regression model with 80% of the data to predict the\\nprice of a house, and score the model with 20% of the data\\nsplit off for testing:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLinearRegression\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\n=\\ndf.drop(['MedHouseVal'],\\naxis=1)\\ny\\n=\\ndf['MedHouseVal']\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\n\\nmodel\\n=\\nLinearRegression()\\nmodel.fit(x_train,\\ny_train)\\nmodel.score(x_test,\\ny_test)\\nIn this example, score returns 0.5863, which ostensibly\\nindicates that the model was about 59% accurate using the\\nfeatures in the test data to make predictions. So far, so\\ngood.\\nNow change the random_state value passed to train_test_split\\nfrom 0 to 1 and run the code again:\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=1)\\n\\nmodel\\n=\\nLinearRegression()\\nmodel.fit(x_train,\\ny_train)\\nmodel.score(x_test,\\ny_test)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 85, 'file_type': 'pdf'}, page_content='84\\nThis time, score returns 0.6255. So which is it? Is the model\\n59% accurate at making predictions or 63% accurate? Why did\\nscore return two different values?\\nWhen train_test_split splits a dataset, it randomly selects\\nrows for the training dataset and the test dataset. The\\nrandom_state parameter seeds the random-number generator used\\nto make selections. By specifying two different seed values,\\nyou train the model with two different datasets and test it\\nwith two different datasets too. Sure, there is overlap\\nbetween them. But the fact remains that the number you seed\\ntrain_test_split’s random-number generator with affects the\\noutcome. The smaller the dataset, the greater that effect is\\nlikely to be.\\nThe solution is cross-validation. To cross-validate a model,\\nyou partition the dataset into folds, as pictured in\\nFigure\\xa02-11. Using five folds is common, but you can use any\\nnumber of folds you like. Then you train the model five times\\n—once for each fold—using a different 80% of the dataset\\nfor training and a different 20% for testing each time, and\\naverage the scores to generate a cross-validated score. That\\nscore is more reliable than the score returned by score\\nbecause it is less sensitive to how the data is split. This\\nprocess is known as k-fold cross-validation.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 86, 'file_type': 'pdf'}, page_content='85\\nFigure 2-11. 5-fold cross-validation\\nThe downside to cross-validation is that it takes longer.\\nWith 5-fold cross-validation, you’re training the model five\\ntimes. The good news is that cross-validation is generally\\napplied only to small datasets, and if the dataset is small,\\nthe model probably trains quickly.\\nYou could write the code to do cross-validation yourself, but\\nyou don’t have to because Scikit does it for you. Cross-\\nvalidating a model requires just one line of code:\\n\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nThe cross-validated score in this example should come out to\\nabout 0.61, which lies between the two values generated by\\nthe model’s score method. It is a more accurate measure of\\nthe model’s accuracy than either of the other scores.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 87, 'file_type': 'pdf'}, page_content='86\\nYou\\xa0 don’t have to train a model before cross-validating it\\nsince cross_val_score trains it for you. However,\\ncross_val_score trains a copy of the model, not the model\\nitself, so once you’ve used cross-validation to gauge the\\nmodel’s accuracy, you still need to call fit before making\\npredictions:\\n\\nmodel.fit(x,\\ny)\\nNote that you pass the entire dataset to fit rather than a\\nsubset split off for training. That’s another benefit of\\ncross-validation: you train your model with all of the data,\\nwhich is a big deal if the dataset is small to begin with.\\nThere’s no longer a hard requirement to hold some of it out\\nfor testing. However, even with cross-validation, it’s still\\nuseful to score the model with data reserved exclusively for\\ntesting if such data is available. Remember: you don’t truly\\nknow how accurate a model is until you know how it responds\\nto data it wasn’t trained with.\\nTIP\\nAs a rule of thumb, you should reserve cross-validation for small\\ndatasets and use train/test splits for large datasets. The larger\\nthe dataset, the less sensitive it is to how the data is split.\\nWhich begs the question: precisely what does the value\\nreturned when you score or cross-validate a model represent?\\nFor a regression model, it’s the coefficient of\\ndetermination, also known as the R-squared score or simply\\nR2. The coefficient of determination is usually a value from'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 88, 'file_type': 'pdf'}, page_content='87\\n0 to 1 (“usually” because it can, in certain circumstances,\\ngo negative) which quantifies the variance in the output that\\ncan be explained by the variables in the input. A simple way\\nto think of it is that an R2 score of 0.8 means that the\\nmodel should, on average, be about 80% accurate in making\\npredictions, or get the answer right within 20%. The higher\\nthe R2 score, the more accurate the model. There are other\\nways to measure the accuracy of a regression model, including\\nmean squared error (MSE) and mean absolute error (MAE). Those\\nnumbers are meaningful only in the context of the range of\\noutput values, whereas R2 gives you one simple number that is\\nindependent of range. You can read more about regression\\nmetrics and methods for retrieving them in the Scikit\\ndocumentation.\\nThe value returned by a model’s score method is completely\\ndifferent for a classification model. I will address the\\nvarious ways to quantify the accuracy of classification\\nmodels in Chapter\\xa03.\\nUsing Regression to Predict Taxi Fares\\nImagine that you work for a taxi company, and one of your\\ncustomers’ biggest complaints is that they don’t know how\\nmuch a ride will cost until it’s over. That’s because\\ndistance isn’t the only variable that determines a fare\\namount. You decide to do something about it by building a\\nmobile app that customers can use when they climb into a taxi\\nto estimate what the fare will be. To provide the\\nintelligence for the app, you intend to use the massive\\namounts of fare data the company has collected over the years\\nto build a machine learning model.\\nLet’s train a regression model to predict a fare amount\\ngiven the time of day, the day of the week, and the pickup\\nand drop-off locations. Start by downloading the CSV file\\ncontaining the dataset and copying it into the Data\\nsubdirectory where your Jupyter notebooks are hosted. Then'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 89, 'file_type': 'pdf'}, page_content=\"88\\nuse the following code to load the dataset into a notebook.\\nThe dataset contains about 55,000 rows and is a subset of a\\nmuch larger dataset used in Kaggle’s New York City Taxi Fare\\nPrediction competition. The data requires a fair amount of\\nprep work before it’s of any use—something that’s not\\nuncommon in machine learning. Data scientists often find that\\ncollecting and preparing data accounts for 90% or more of\\ntheir time:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/taxi-fares.csv',\\nparse_dates=['pickup_datetime'])\\ndf.head()\\nNote the use of the read_csv function’s parse_dates\\nparameter to parse the strings in the pickup_datetime column\\ninto Python datetime objects. Here’s the output from the\\ncode:\\nEach row represents a taxi ride and contains information such\\nas the fare amount, the pickup and drop-off locations\\n(expressed as latitudes and longitudes), and the passenger\\ncount. It’s the fare amount that we want to predict. Use the\\nfollowing code to draw a histogram showing how many rows\\ncontain a passenger count of 1, how many contain a passenger\\ncount of 2, and so on:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 90, 'file_type': 'pdf'}, page_content=\"89\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nsns.countplot(x=df['passenger_count'])\\nHere is the output:\\nMost of the rows in the dataset have a passenger count of 1.\\nSince we’re interested in predicting the fare amount only\\nfor single passengers, use the following code to remove all\\nrows with multiple passengers and remove the key column from\\nthe dataset since that column isn’t needed—in other words,\\nit’s not one of the features that we will try to predict on:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 91, 'file_type': 'pdf'}, page_content=\"90\\n\\ndf\\n=\\ndf[df['passenger_count']\\n==\\n1]\\ndf\\n=\\ndf.drop(['key',\\n'passenger_count'],\\naxis=1)\\ndf.head()\\nThat leaves 38,233 rows in the dataset, which you can see for\\nyourself with the following statement:\\n\\ndf.shape\\nNow use Pandas’ corr method to find out how much influence\\ninput variables such as latitude and longitude have on the\\nvalues in the fare_amount column:\\n\\ncorr_matrix\\n=\\ndf.corr()\\ncorr_matrix['fare_amount'].sort_values(ascending=False)\\nThe output looks like this:\\n\\nfare_amount\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 1.000000\\ndropoff_longitude\\xa0\\xa0\\xa0 0.020438\\npickup_longitude\\xa0\\xa0\\xa0\\xa0 0.015742\\npickup_latitude\\xa0\\xa0\\xa0\\xa0 -0.015915\\ndropoff_latitude\\xa0\\xa0\\xa0 -0.021711\\nName: fare_amount, dtype: float64\\nThe numbers don’t look very encouraging. Latitudes and\\nlongitudes have little to do with fare amounts, at least in\\ntheir present form. And yet, intuitively, they should have a\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 92, 'file_type': 'pdf'}, page_content=\"91\\nlot to do with fare amounts since they specify starting and\\nending points, and longer rides incur higher fares.\\nNow comes the fun part: creating whole new columns of data\\nthat have more impact on the outcome—columns whose values\\nare computed from values in other columns. Add columns\\nspecifying the day of the week (0=Monday, 1=Sunday, and so\\non), the hour of the day that the passenger was picked up (0\\n–23), and the distance (by air, not on the street) in miles\\nthat the ride covered. To compute distances, this code\\nassumes that most rides are short and that it is therefore\\nsafe to ignore the curvature of the Earth:\\n\\nfrom\\nmath\\nimport\\nsqrt\\n\\nfor\\ni,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 dt\\n=\\nrow['pickup_datetime']\\n\\xa0\\xa0\\xa0 df.at[i,\\n'day_of_week']\\n=\\ndt.weekday()\\n\\xa0\\xa0\\xa0 df.at[i,\\n'pickup_time']\\n=\\ndt.hour\\n\\xa0\\xa0\\xa0 x\\n=\\n(row['dropoff_longitude']\\n-\\nrow['pickup_longitude'])\\n*\\n54.6\\n\\xa0\\xa0\\xa0 y\\n=\\n(row['dropoff_latitude']\\n-\\nrow['pickup_latitude'])\\n*\\n69.0\\n\\xa0\\xa0\\xa0 distance\\n=\\nsqrt(x**2\\n+\\ny**2)\\n\\xa0\\xa0\\xa0 df.at[i,\\n'distance']\\n=\\ndistance\\n\\ndf.head()\\nYou no longer need all the columns, so use these statements\\nto remove the ones that won’t be used:\\n\\ndf.drop(columns=['pickup_datetime',\\n'pickup_longitude',\\n'pickup_latitude',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'dropoff_longitude',\\n'dropoff_latitude'],\\ninplace=True)\\ndf.head()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 93, 'file_type': 'pdf'}, page_content=\"92\\nLet’s check the correlation again:\\n\\ncorr_matrix\\n=\\ndf.corr()\\ncorr_matrix['fare_amount'].sort_values(ascending=False)\\nThere still isn’t a strong correlation between distance\\ntraveled and fare amount. Perhaps this will explain why:\\n\\ndf.describe()\\nHere is the output:\\nThe dataset contains outliers, and outliers frequently skew\\nthe results of machine learning models. Filter the dataset by\\neliminating negative fare amounts and placing reasonable\\nlimits on fares and distance, and then run a correlation\\nagain:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 94, 'file_type': 'pdf'}, page_content=\"93\\n\\ndf\\n=\\ndf[(df['distance']\\n>\\n1.0)\\n&\\n(df['distance']\\n<\\n10.0)]\\ndf\\n=\\ndf[(df['fare_amount']\\n>\\n0.0)\\n&\\n(df['fare_amount']\\n<\\n50.0)]\\n\\ncorr_matrix\\n=\\ndf.corr()\\ncorr_matrix['fare_amount'].sort_values(ascending=False)\\nOnce more, here is the output:\\n\\nfare_amount\\xa0\\xa0\\xa0 1.000000\\ndistance\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.851913\\nday_of_week\\xa0\\xa0 -0.003570\\npickup_time\\xa0\\xa0 -0.023085\\nName: fare_amount, dtype: float64\\nThat looks better! Most (85%) of the variance in fare amounts\\nis explained by the distance traveled. The correlation\\nbetween the day of the week, the hour of the day, and the\\nfare amount is still weak, but that’s not surprising since\\ndistance traveled is the main factor that drives taxi fares.\\nLet’s leave those columns in since it makes sense that it\\nmight take longer to get from point A to point B during rush\\nhour, or that traffic at 5:00 p.m. on Friday might be\\ndifferent than traffic at 5:00 p.m. on Saturday.\\nNow it’s time to train a regression model. Let’s try three\\ndifferent learning algorithms to determine which one yields\\nthe most accurate fit, and use cross-validation to gauge\\naccuracy. Start with a linear regression model:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLinearRegression\\nfrom\\nsklearn.model_selection\\nimport\\ncross_val_score\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 95, 'file_type': 'pdf'}, page_content=\"94\\nx\\n=\\ndf.drop(['fare_amount'],\\naxis=1)\\ny\\n=\\ndf['fare_amount']\\n\\nmodel\\n=\\nLinearRegression()\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nTry a RandomForestRegressor with the same dataset and see how\\nits accuracy compares. Recall that random-forest models train\\nmultiple decision trees on the data and average the results\\nof all the trees to make a prediction:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nRandomForestRegressor\\n\\nmodel\\n=\\nRandomForestRegressor(random_state=0)\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nFinally, try GradientBoostingRegressor. Gradient-boosting\\nmachines use multiple decision trees, each of which is\\ntrained to compensate for the error in the output from the\\nprevious one:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nGradientBoostingRegressor\\n\\nmodel\\n=\\nGradientBoostingRegressor(random_state=0)\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nAssuming the GradientBoostingRegressor produced the highest\\ncross-validated coefficient of determination, train it using\\nthe entire dataset:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 96, 'file_type': 'pdf'}, page_content=\"95\\n\\nmodel.fit(x,\\ny)\\nFinish up by using the trained model to make a pair of\\npredictions. First, estimate what it will cost to hire a taxi\\nfor a 2-mile trip at 5:00 p.m. on Friday afternoon. Because\\nyou passed DataFrames containing column names to the fit\\nmethod, recent versions of Scikit will display a warning if\\nyou pass lists or NumPy arrays to predict. Therefore, input a\\nDataFrame instead:\\n\\nmodel.predict(pd.DataFrame({\\n'day_of_week':\\n[4],\\n'pickup_time':\\n[17],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'distance':\\n[2.0]\\n}))\\nNow predict the fare amount for a 2-mile trip taken at 5:00\\np.m. one day later (on Saturday):\\n\\nmodel.predict(pd.DataFrame({\\n'day_of_week':\\n[5],\\n'pickup_time':\\n[17],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'distance':\\n[2.0]\\n}))\\nDoes the model predict a higher or lower fare amount for the\\nsame trip on Saturday afternoon? Do the answers make sense\\ngiven that the data comes from New York City cabs? Consider\\nthat rush-hour traffic is likely to be heavier on Friday\\nafternoon than on Saturday afternoon.\\nSummary\\nRegression models are supervised learning models that predict\\nnumeric outcomes such as the cost of a taxi ride. Prominent\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 97, 'file_type': 'pdf'}, page_content='96\\nlearning algorithms used for regression include the\\nfollowing:\\nLinear regression\\nModels training data by fitting it to the equation of\\na line\\nDecision trees\\nUse binary trees to predict an outcome by answering a\\nseries of yes-and-no questions\\nRandom forests\\nUse multiple independent decision trees to model the\\ndata and are resistant to overfitting\\nGradient-boosting machines\\nUse multiple dependent decision trees, each modeling\\nthe error in the output from the last\\nSupport vector machines\\nTake an entirely different approach to modeling data\\nby adding dimensionality under the supposition that\\ndata that isn’t linearly separable in the original\\nproblem space might be linearly separable in higher-\\ndimensional space\\nScikit provides convenient implementations of these and other\\nlearning algorithms in classes such as LinearRegression,\\nRandomForestRegressor, and GradientBoostingRegressor.\\nA common metric for quantifying the accuracy of regression\\nmodels is the R2 score, also known as the coefficient of\\ndetermination. It’s typically a value from 0 to 1, with\\nhigher numbers indicating higher accuracy. Technically, it’s\\na measure of the variance in the output that can be explained\\nby the values in the input. For small datasets, k-fold cross-\\nvalidation gives you more confidence in the R2 score than'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 98, 'file_type': 'pdf'}, page_content='97\\nsimply splitting the data once for training and testing. k-\\nfold trains the model k times, each time with the dataset\\nsplit differently.\\nReal-world datasets tend to be messy and often require\\nfurther preparation to be useful for machine learning. As the\\ntaxi-fare example demonstrated, outliers in training data can\\naffect a model’s accuracy or prevent the model from being\\nuseful at all. One solution is to identify the outliers and\\nremove them before training the model. Another is to employ a\\nlearning algorithm such as ridge regression or lasso\\nregression that supports regularization.\\nRegression models are common in machine learning, but\\nclassification models are more common still. Chapter\\xa03\\ntackles classification models head-on, introduces another\\nleading learning algorithm called logistic regression, and\\nbuilds on what you learned in this chapter.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 99, 'file_type': 'pdf'}, page_content='98\\nChapter 3. Classification Models\\nThe machine learning model featured in the previous chapter\\nused various forms of regression to predict taxi fares based\\non distance to travel, the day of the week, and the time of\\nday. Regression models predict numerical outcomes and are\\nwidely used in industry to forecast sales, prices, demand,\\nand other numbers that drive business decisions. Equally\\nimportant are classification models, which predict\\ncategorical outcomes such as whether a credit card\\ntransaction is fraudulent or which letter of the alphabet a\\nhandwritten character represents.\\nMost classification models fall into two categories: binary\\nclassification models, in which there are just two possible\\noutcomes, and multiclass classification models, in which\\nthere are more than two possible outcomes. In both instances,\\nthe model assigns a single class, or class label, to an\\ninput. Less common are multilabel classification models,\\nwhich can classify a single input as belonging to several\\nclasses—for example, predicting that a document is both a\\npaper on machine learning and a paper on genomics. Some can\\npredict that an input belongs to none of the possible classes\\ntoo.\\nMuch of what you know about regression models also applies to\\nclassification models. For example, many of the learning\\nalgorithms that power regression models work equally well\\nwith classification models. One substantive difference\\nbetween regression and classification is how you measure a\\nmodel’s accuracy. There’s no such thing as an R2 score for\\na classification model. In its place are an abundance of\\nmeasures, such as precision, recall, specificity,\\nsensitivity, and F1 score, to name but a few. One of the keys\\nto becoming proficient with classification models is getting'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 100, 'file_type': 'pdf'}, page_content='99\\ncomfortable with the various accuracy metrics and, more\\nimportantly, understanding which one (or ones) to use based\\non the model’s intended application.\\nYou’ve seen one example of multiclass classification in the\\niris tutorial in Chapter\\xa01. It’s time to delve deeper into\\nmachine learning classifiers, starting with one of the most\\ntried-and-true learning algorithms of all, one that works\\nonly for classification models: logistic regression.\\nLogistic Regression\\nMany learning algorithms exist for classification problems.\\nIn Chapter\\xa02, you learned how decision trees, random\\nforests, and gradient-boosting machines (GBMs) fit regression\\nmodels to training data. These algorithms can be used for\\nclassification as well, and Scikit helps out by offering\\nclasses such as DecisionTreeClassifier,\\nRandomForestClassifier, and GradientBoostingClassifier. In\\nChapter\\xa01, you used Scikit’s KNeighborsClassifier class to\\nbuild a three-class classification model with k-nearest\\nneighbors as the learning algorithm.\\nThese are important learning algorithms, and they see use in\\nmany contemporary machine learning models. But one of the\\nmost popular classification algorithms is logistic\\nregression, which analyzes a distribution of data and fits an\\nequation to it that defines the probability that a given\\nsample belongs to each of two possible classes. It might\\ndetermine, for example, that there’s a 10% chance the values\\nin a sample correspond to class 0 and a 90% chance they\\ncorrespond to class 1. In this case, logistic regression will\\npredict that the sample corresponds to class 1. Despite the\\nname, logistic regression is a classification algorithm, not\\na regression algorithm. Its purpose is not to create\\nregression models but to quantify probabilities for the\\npurpose of classifying input samples.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 101, 'file_type': 'pdf'}, page_content='100\\nAs an example, consider the data points in Figure\\xa03-1, which\\nbelong to two classes: 0 (blue) and 1 (red). Let’s assume\\nthat the x-axis specifies the number of hours a person\\nstudied for an exam and the y-axis indicates whether they\\npassed (1) or failed (0). The blues fall in the range x = 0\\nto x = 10, while the reds fall in the range x = 5 to x = 15.\\nYou can’t pick a value for x that separates the classes\\nsince both have values between x = 5 and x = 10. (Try drawing\\na vertical line that has only reds on one side and only blues\\non the other.) But you can draw a curve that, given an x,\\nshows the probability that a point with that x belongs to\\nclass 1. As x increases, so too does the likelihood that the\\npoint represents class 1 (pass) rather than class 0 (fail).\\nFrom the curve, you can see that if x = 2, there is less than\\na 5% chance that the point corresponds to class 1. But if x =\\n10, there is about a 76% chance that it’s class 1. If asked\\nto classify that point as a red or a blue, you would conclude\\nthat it’s a red because it’s much more likely to be red\\nthan blue.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 102, 'file_type': 'pdf'}, page_content='101\\nFigure 3-1. Logistic regression\\nThe curve\\xa0in Figure\\xa03-1 is a sigmoid curve. It charts a\\nfunction known as the logistic function (also known as the\\nlogit function) that has been used in statistics for decades.\\nFor logistic regression, the function is defined this way,\\nwhere x is the input value and m and b are parameters learned\\nduring training:\\nThis equation reveals why logistic regression is called\\nlogistic regression, despite the fact that it’s a\\nclassification algorithm. The exponent of e happens to be the\\nequation for linear regression.\\nThe logistic regression learning algorithm fits the logistic\\nfunction to a distribution of data and uses the resulting y\\nvalues as probabilities to classify data points. It works\\nwith any number of features (not just x, but x1, x2, x3, and\\nso on), and it is a parametric learning algorithm since it\\nuses training data to find optimum values for m and b. How it\\nfinds the optimum values is an implementational detail that\\nlibraries such as Scikit-Learn handle for you. Scikit\\ndefaults to a numerical optimization algorithm known as\\nLimited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)\\nbut supports other optimization methods as well. This,\\nincidentally, is one reason why Scikit is so popular in the\\nmachine learning community. It’s not difficult to calculate\\nm and b from the training data for a linear regression model,\\nbut it’s harder to do it for a logistic regression model,\\nnot to mention more sophisticated parametric models such as\\nsupport vector machines.\\nScikit’s LogisticRegression class is logistic regression in\\na box. With it, training a logistic regression model can be\\nas simple as this:\\n\\nmodel\\n=\\nLogisticRegression()'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 103, 'file_type': 'pdf'}, page_content=\"102\\nmodel.fit(x,\\ny)\\nOnce the model is trained, you can call its predict method to\\npredict which class the input belongs to, or its\\npredict_proba method to get the computed probabilities for\\neach class. If you fit a LogisticRegression model to the\\ndataset in Figure\\xa03-1, the following statement predicts\\nwhether x = 10 corresponds to class 0 or class 1:\\n\\npredicted_class\\n=\\nmodel.predict([[10.0]])[0]\\nprint(predicted_class)\\n# Outputs 1\\nAnd these statements show the probabilities computed for each\\nclass:\\n\\npredicted_probabilities\\n=\\nmodel.predict_proba([[10.0]])[0]\\nprint(f'Class 0: {predicted_probabilities[0]}')\\n# 0.23508543966167028\\nprint(f'Class 1: {predicted_probabilities[1]}')\\n# 0.7649145603383297\\nScikit also includes the LogisticRegressionCV class for\\ntraining logistic regression models with built-in cross-\\nvalidation. (If you need a refresher, cross-validation was\\nintroduced in Chapter\\xa02.) At the expense of additional\\ntraining time, the following statements train a logistic\\nregression model using five folds:\\n\\nmodel\\n=\\nLogisticRegressionCV(cv=5)\\nmodel.fit(x,\\ny)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 104, 'file_type': 'pdf'}, page_content='103\\nLogistic regression is technically a binary classification\\nalgorithm, but it can be used for multiclass classification\\ntoo. I’ll say more about this toward the end of the chapter.\\nFor now, think of logistic regression as a machine learning\\nalgorithm that uses the well-known logistic function to\\nquantify the probability that an input corresponds to each of\\ntwo classes, and you’ll have an accurate conceptual\\nunderstanding of what logistic regression is.\\nAccuracy Measures for Classification Models\\nYou can quantify the accuracy of a classification model the\\nsame way you do for a regression model: by calling the\\nmodel’s score method. For a classifier, score returns the\\nsum of the true positives and the true negatives divided by\\nthe total number of samples. If the test data includes 10\\npositives (samples of class 1) and 10 negatives (samples of\\nclass 0) and the model correctly identifies 8 of the\\npositives and 7 of the negatives, then the score is (8 + 7) /\\n20, or 0.75. This is sometimes referred to as the model’s\\naccuracy score.\\nThere are many other ways to score a classification model,\\nand which one is “right” often depends on how the model\\nwill be used. Rather than compute an accuracy score, data\\nscientists sometimes measure a classification model’s\\nprecision and recall instead:\\nPrecision\\nComputed by dividing the number of true positives by\\nthe sum of the true positives and false positives\\nRecall\\nComputed by dividing the number of true positives by\\nthe sum of the true positives and false negatives'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 105, 'file_type': 'pdf'}, page_content='104\\nIn effect, precision imposes a penalty on false positives\\n(instances in which the model incorrectly predicts a 1),\\nwhile recall penalizes false negatives by lowering the score\\nwhen a model incorrectly predicts a 0.\\nFigure\\xa03-2 illustrates the difference. Suppose you train a\\nmodel to differentiate between polar bear images and walrus\\nimages, and to test it you submit three polar bear images and\\nthree walrus images. Furthermore, assume that the model\\ncorrectly classifies two of the polar bear images, but\\nincorrectly classifies two walrus images as polar bear\\nimages, as indicated by the red boxes. In this case, the\\nmodel’s precision in identifying polar bears is 50% because\\nonly two of the four images the model classified as polar\\nbears were in fact polar bears. But recall is 67% since the\\nmodel correctly identified two of the three polar bear\\nimages. That’s precision and recall in a nutshell. The\\nformer quantifies how confident you can be that a positive\\nprediction is accurate, while the latter quantifies the\\nmodel’s ability to accurately identify positive samples. The\\ntwo can be combined into one score called the F1 score (also\\nknown as the F-score) using a simple formula.\\nFigure 3-2. Using precision and recall to measure the accuracy of a\\nclassifier\\nScikit provides helpful functions such as precision_score,\\nrecall_score, and f1_score for retrieving classification\\nmetrics. Whether you prefer precision or recall depends on\\nwhich is higher: the cost of false positives or the cost of'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 106, 'file_type': 'pdf'}, page_content='105\\nfalse negatives. Use precision when the cost of false\\npositives is high—for example, when “positive” means an\\nemail is spam. You would rather a spam filter send a few spam\\nmails to your inbox than route a legitimate (and potentially\\nimportant) email to your junk folder. By contrast, use recall\\nif the cost of false negatives is high. A great example is\\nwhen using machine learning to spot tumors in X-rays and MRI\\nscans. You would much rather mistakenly send a patient to a\\ndoctor due to a false positive than tell that patient there\\nare no tumors when there really are.\\nSPOTTING POLAR BEARS IN THE WILD\\nThe polar bear versus walrus example was taken from a\\ntutorial I wrote for Microsoft that began with the\\nfollowing introduction:\\nYou’re the leader of a group of climate scientists\\nwho are concerned about the dwindling polar bear\\npopulation in the Arctic. To address the problem,\\nyour team has placed hundreds of motion-activated\\ncameras at strategic locations throughout the region.\\nInstead of manually examining each photo that’s\\ntaken to determine whether it contains a polar bear,\\nyour challenge is to devise an automated system that\\nprocesses data from these cameras in real time and\\ndisplays an alert on a map when one of your cameras\\nphotographs a polar bear. You need a solution that\\nuses artificial intelligence (AI) to determine with a\\nhigh degree of accuracy whether a photo contains a\\npolar bear. And you need it fast, because climate\\nchange won’t wait.\\nThe tutorial combines several Azure services to form an\\nend-to-end solution, and it uses Microsoft’s Power BI\\nfor visualizations. If you’re interested, you can check\\nit out online.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 107, 'file_type': 'pdf'}, page_content='106\\nAccuracy score, precision, recall, and F1 score apply to\\nbinary classification and mul\\u2060ticlass classification models.\\nAn additional metric—one that applies to binary\\nclassification only—is the receiver operating characteristic\\n(ROC) curve, which plots the true-positive rate (TPR) against\\nthe false-positive rate (FPR) at various probability\\nthresholds. A sample ROC curve is shown in Figure\\xa03-3. A\\nstraight line stretching from the lower left to the upper\\nright would indicate that the model gets it right just 50% of\\nthe time, which is no better than guessing for a binary\\nclassifier. The more the curve arches toward the upper-left\\ncorner, the more accurate the model. Data scientists often\\nuse the area under the curve (AUC, or ROC AUC) as an overall\\nmeasure of accuracy. Scikit provides a class named\\nRocCurveDisplay for plotting ROC curves, and a function named\\nroc_auc_score for retrieving ROC AUC scores. Scores returned\\nby this function are values from 0.0 to 1.0. The higher the\\nscore, the more accurate the model.\\nYet another way to assess the accuracy of a classification\\nmodel is to plot a confusion matrix like the one in\\nFigure\\xa03-4. It works for binary and multiclass\\nclassification, and it shows for each class how the model\\nperformed during testing. In this example, the model was\\nasked to differentiate between images containing masked faces\\nand images containing unmasked faces. It got it right 78 out\\nof 85 times when presented with pictures of people wearing\\nmasks, and 41 out of 58 times when presented with pictures of\\npeople not wearing masks. Scikit offers a confusion_matrix\\nfunction for computing a confusion matrix, and a\\nConfusionMatrixDisplay class with methods named from_estimator\\nand from_predictions for plotting confusion matrices.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 108, 'file_type': 'pdf'}, page_content='107\\nFigure 3-3. Receiver operating characteristic curve'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 109, 'file_type': 'pdf'}, page_content='108\\nFigure 3-4. Confusion matrix\\nNOTE\\nCountless code samples online use Scikit’s plot_confusion_matrix\\nfunction to display confusion matrices. ConfusionMatrixDisplay was\\nintroduced in Scikit 1.0 and is the proper way to generate\\nconfusion matrices. plot_confusion_matrix is slated to be removed\\nfrom the library in version 1.2.\\nOther terms you might come across when discussing the\\naccuracy of classification models include sensitivity and'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 110, 'file_type': 'pdf'}, page_content='109\\nspecificity. Sensitivity is identical to recall, so Scikit\\ndoesn’t include a separate method for computing it.\\nSpecificity is recall for the negative class rather than the\\npositive class and is calculated by dividing the number of\\ntrue negatives by the sum of the true negatives and false\\npositives. Scikit doesn’t provide a dedicated function for\\ncalculating specificity, either, but you can do it easily\\nenough by calling recall_score with a pos_label parameter\\nindicating that 0 (rather than 1) is the positive label:\\n\\nrecall_score(y_test,\\ny_predicted,\\npos_label=0)\\nSensitivity and specificity are frequently used in drug\\ntesting and cancer screening. Suppose you’re traveling\\nabroad and require a negative COVID test before returning\\nhome. If you don’t have COVID, what are the chances that a\\ntest will incorrectly say you do? (I have asked myself that\\nquestion many times recently while traveling overseas.) The\\nanswer is the test’s specificity—a measure of how accurate\\nthe test is at identifying negative samples. Sensitivity, on\\nthe other hand, reveals how likely the test is to be correct\\nif it says you do have COVID. It’s a subtle distinction, but\\nan important one if you’re concerned that a faulty test\\nmight keep you from going home (specificity) or if you want\\nto be certain you don’t have COVID before visiting an\\nelderly relative (sensitivity).\\nCategorical Data\\nMachine learning finds patterns in numbers. It works only\\nwith numbers. Yet many datasets have columns containing\\nstring values such as \"male\" and \"female\" or \"red\", \"green\",\\nand \"blue\". Data scientists refer to these as categorical\\nvalues and the columns that contain them as categorical'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 111, 'file_type': 'pdf'}, page_content=\"110\\ncolumns. Machine learning can’t handle categorical values\\ndirectly. To use them in a model, you must convert them into\\nnumbers.\\nTwo popular techniques exist for converting categorical\\nvalues into numerical values. One is label encoding, which\\nyou briefly saw in the k-means clustering example in\\nChapter\\xa01. Label encoding replaces categorical values with\\nintegers. If there are three unique values in a column, label\\nencoding replaces them with 0s, 1s, and 2s. To demonstrate,\\nrun the following code in a Jupyter notebook:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.preprocessing\\nimport\\nLabelEncoder\\n\\ndata\\n=\\n[[10,\\n'red'],\\n[20,\\n'blue'],\\n[12,\\n'red'],\\n[16,\\n'green'],\\n[22,\\n'blue']]\\ndf\\n=\\npd.DataFrame(data,\\ncolumns=['Length',\\n'Color'])\\n\\nencoder\\n=\\nLabelEncoder()\\ndf['Color']\\n=\\nencoder.fit_transform(df['Color'])\\ndf.head()\\nThis code creates a DataFrame containing a numeric column\\nnamed Length and a categorical column named Color, which\\ncontains three different categorical values. Here’s what the\\ndataset looks like before encoding:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 112, 'file_type': 'pdf'}, page_content='111\\nAnd here’s how it looks after the values in the Color column\\nare label-encoded using Scikit’s LabelEncoder class:\\nThe encoded dataset can be used to train a machine learning\\nmodel. The unencoded dataset cannot. You can get an ordered\\nlist of the classes that were encoded from the encoder’s\\nclasses_ attribute.\\nThe other, more common means for converting categorical\\nvalues into numeric values is one-hot encoding, which adds'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 113, 'file_type': 'pdf'}, page_content=\"112\\none column to the dataset for each unique value in a\\ncategorical column and fills the encoded columns with 1s and\\n0s. One-hot encoding can be performed with Scikit’s\\nOneHotEncoder class or by calling get\\u200b_dum\\u2060mies on a Pandas\\nDataFrame. Here is how the latter is used to encode the\\ndataset:\\n\\ndata\\n=\\n[[10,\\n'red'],\\n[20,\\n'blue'],\\n[12,\\n'red'],\\n[16,\\n'green'],\\n[22,\\n'blue']]\\ndf\\n=\\npd.DataFrame(data,\\ncolumns=['Length',\\n'Color'])\\n\\ndf\\n=\\npd.get_dummies(df,\\ncolumns=['Color'])\\ndf.head()\\nAnd here are the results:\\nLabel encoding and one-hot encoding are used with regression\\nproblems and classification problems. The obvious question\\nis, which one should you use? Generally speaking, data\\nscientists prefer one-hot encoding to label encoding. The\\nformer gives every unique value an equal weight, whereas the\\nlatter implies that some values may be more important than\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 114, 'file_type': 'pdf'}, page_content='113\\nothers—for example, that \"red\" (2) is more important than\\n\"blue\" (0). Label encoding, on the other hand, is more memory\\nefficient. The number of columns is the same before and after\\nencoding, whereas one-hot encoding adds one column per unique\\nvalue. For very large datasets with thousands of unique\\nvalues in a categorical column, label encoding requires\\nsubstantially less memory.\\nThe accuracy of a machine learning model is rarely impacted\\nby the encoding method you choose. If you’re in doubt,\\nyou’ll rarely go wrong with one-hot encoding. If you want to\\nbe certain, you can encode the data both ways and compare the\\nresults after training a machine learning model.\\nBinary Classification\\nBinary classifiers are supervised learning models trained\\nwith labeled data: 0s for the negative class and 1s for the\\npositive. The predictions they make are 0s and 1s too. They\\nalso divulge a probability for each class that you can factor\\ninto your conclusions. For example, a credit card company\\nmight decide that a transaction will be declined only if the\\nmodel predicts with at least 99% certainty that the\\ntransaction is fraudulent. In that case, the probability that\\nthe model computed is more important than the raw prediction\\nthat it made.\\nTo help bring home everything presented thus far regarding\\nbinary classification, let’s use Scikit to build a couple of\\nmodels: first a simple one that demonstrates core principles,\\nfollowed by a second one that solves a genuine business\\nproblem.\\nClassifying Passengers Who Sailed on the\\nTitanic'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 115, 'file_type': 'pdf'}, page_content=\"114\\nOne of the more famous public datasets in machine learning is\\nthe Titanic dataset, which contains information regarding\\nhundreds of passengers who sailed on the ill-fated voyage of\\nthe RMS Titanic, including which ones survived and which ones\\ndid not. Let’s use logistic regression to build a binary\\nclassification model from the dataset and see if we can\\npredict the odds that a passenger will survive given that\\nperson’s gender, age, and fare class (whether they traveled\\nin first, second, or third class).\\nThe first step is to download the dataset and copy it to the\\nData subdirectory of the directory that hosts your Jupyter\\nnotebooks. Then run the following code in a notebook to load\\nthe dataset and get a feel for its contents:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/titanic.csv')\\ndf.head()\\nHere is the output:\\nThe dataset contains 891 rows and 12 columns. Some of the\\ncolumns, such as PassengerId and Name, aren’t relevant to a\\nmachine learning model. Others are very relevant. The ones\\nwe’ll focus on are:\\nSurvived\\nIndicates whether the passenger survived the voyage\\n(1) or did not (0)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 116, 'file_type': 'pdf'}, page_content=\"115\\nPclass\\nIndicates whether the passenger was traveling in\\nfirst class (1), second class (2), or third class (3)\\nSex\\nIndicates the passenger’s gender\\nAge\\nIndicates the passenger’s age\\nThe Survived column is the label column—the one we’ll try\\nto predict. The other columns are relevant because first-\\nclass passengers were more likely to survive the sinking\\nbecause their cabins were closer to the top deck of the ship\\nand nearer the lifeboats. Plus, women and children were more\\nlikely to be given space in lifeboats.\\nNow use the following statement to see if the dataset is\\nmissing any values:\\n\\ndf.info()\\nHere’s the output:\\n\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 891 entries, 0 to 890\\nData columns (total 12 columns):\\n#\\xa0\\xa0 Column\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Non-Null Count\\xa0 Dtype\\xa0 \\n---\\xa0 ------\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 --------------\\xa0 -----\\xa0 \\n0\\xa0\\xa0 PassengerId\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n1\\xa0\\xa0 Survived\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n2\\xa0\\xa0 Pclass\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 117, 'file_type': 'pdf'}, page_content=\"116\\n3\\xa0\\xa0 Name\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 object \\n4\\xa0\\xa0 Sex\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 object \\n5\\xa0\\xa0 Age\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 714 non-null\\xa0\\xa0\\xa0 float64\\n6\\xa0\\xa0 SibSp\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n7\\xa0\\xa0 Parch\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n8\\xa0\\xa0 Ticket\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 object \\n9\\xa0\\xa0 Fare\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 float64\\n10\\xa0 Cabin\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 204 non-null\\xa0\\xa0\\xa0 object \\n11\\xa0 Embarked\\xa0\\xa0\\xa0\\xa0 889 non-null\\xa0\\xa0\\xa0 object \\ndtypes: float64(2), int64(5), object(5)\\nmemory usage: 83.7+ KB\\nThe Cabin column is missing a lot of values, but we don’t\\ncare since we’re not using that column. We will use the Age\\ncolumn, and that column is missing some values as well. We\\ncould replace the missing values with the mean of all the\\nother ages—an approach that data scientists refer to as\\nimputing missing values—but we’ll take the simpler approach\\nof removing rows with missing values. Use the following\\nstatements to remove the columns that aren’t needed, drop\\nrows with missing values, and one-hot-encode the values in\\nthe Sex and Pclass columns:\\n\\ndf\\n=\\ndf[['Survived',\\n'Age',\\n'Sex',\\n'Pclass']]\\ndf\\n=\\npd.get_dummies(df,\\ncolumns=['Sex',\\n'Pclass'])\\ndf.dropna(inplace=True)\\ndf.head()\\nHere is the resulting dataset:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 118, 'file_type': 'pdf'}, page_content=\"117\\nThe next task is to split the dataset for training and\\ntesting:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\n=\\ndf.drop('Survived',\\naxis=1)\\ny\\n=\\ndf['Survived']\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y,\\nrandom_state=0)\\nNote the stratify=y parameter passed to train_test_split.\\nThat’s important, because of the 714 samples remaining after\\nrows with missing values are removed, 290 represent\\npassengers who survived and 424 represent passengers who did\\nnot. We want the training dataset and the test dataset to\\ncontain similar proportions of both classes, and stratify=y\\naccomplishes that. Without stratification, the model might\\nappear to be more or less accurate than it really is.\\nNow create a logistic regression model, train it with the\\ndata split off for training, and score it with the test data:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 119, 'file_type': 'pdf'}, page_content=\"118\\n\\nmodel\\n=\\nLogisticRegression(random_state=0)\\nmodel.fit(x_train,\\ny_train)\\nmodel.score(x_test,\\ny_test)\\nScore the model again using cross-validation in order to have\\nmore confidence in the score. Remember that this is the\\naccuracy score computed by summing the true positives and\\ntrue negatives and dividing by the total number of samples:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ncross_val_score\\n\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nUse the following statements to display a confusion matrix\\nshowing precisely how the model performed during testing:\\n\\n%matplotlib\\ninline\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=['Perished',\\n'Survived'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nObserve that the model is more accurate when predicting that\\npassengers won’t survive than when predicting that they\\nwill. That’s because the dataset used to train the model\\ncontained more examples of passengers who perished than of\\npassengers who survived. You always prefer to train a binary\\nclassification model with a perfectly balanced dataset\\ncontaining an equal number of positive and negative samples,\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 120, 'file_type': 'pdf'}, page_content=\"119\\nbut it’s acceptable to train with an imbalanced dataset if\\nyou take the imbalance into account when assessing the\\nmodel’s accuracy.\\nNow use Scikit’s precision_score and recall_score functions\\nto compute the model’s precision, recall, sensitivity, and\\nspecificity:\\n\\nfrom\\nsklearn.metrics\\nimport\\nprecision_score,\\nrecall_score\\n\\ny_pred\\n=\\nmodel.predict(x_test)\\nprecision\\n=\\nprecision_score(y_test,\\ny_pred)\\nrecall\\n=\\nrecall_score(y_test,\\ny_pred)\\nsensitivity\\n=\\nrecall\\nspecificity\\n=\\nrecall_score(y_test,\\ny_pred,\\npos_label=0)\\n\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'Sensitivity: {sensitivity}')\\nprint(f'Specificity: {specificity}')\\nIs the high specificity score consistent with the observation\\nthat the model is more adept at identifying passengers who\\nwon’t survive than those who will? How would you explain the\\nrelatively low recall and sensitivity scores?\\nNow let’s use the trained model to make some predictions.\\nFirst, find out whether a 30-year-old female traveling in\\nfirst class is likely to survive the voyage. Since the model\\nwas trained with a DataFrame containing column names, we’ll\\nuse the same column names to formulate an input:\\n\\nfemale\\n=\\npd.DataFrame({\\n'Age':\\n[30],\\n'Sex_female':\\n[1],\\n'Sex_male':\\n[0],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Pclass_1':\\n[1],\\n'Pclass_2':\\n[0],\\n'Pclass_3':\\n[0]\\n})\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 121, 'file_type': 'pdf'}, page_content=\"120\\n\\nmodel.predict(female)[0]\\nThe model predicts she will survive, but what are the odds\\nthat she we will survive?\\n\\nprobability\\n=\\nmodel.predict_proba(female)[0][1]\\nprint(f'Probability of survival: {probability:.1%}')\\nA 30-year-old female traveling in first class is more than\\n90% likely to survive the voyage, but what about a 60-year-\\nold male traveling in third class?\\n\\nmale\\n=\\npd.DataFrame({\\n'Age':\\n[60],\\n'Sex_female':\\n[0],\\n'Sex_male':\\n[1],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Pclass_1':\\n[0],\\n'Pclass_2':\\n[0],\\n'Pclass_3':\\n[1]\\n})\\n\\nprobability\\n=\\nmodel.predict_proba(male)[0][1]\\nprint(f'Probability of survival: {probability:.1%}')\\nFeel free to experiment with other inputs to see what the\\nmodel says. How likely, for example, is a 12-year-old boy\\ntraveling in second class to survive the sinking of the\\nTitanic?\\nDetecting Credit Card Fraud\\nOne of the most compelling uses for machine learning today is\\nspotting fraudulent financial transactions. Credit card\\ncompanies apply machine learning at the point of sale to\\ndecide whether to accept or decline individual charges. While\\nthese companies are understandably reluctant to publish the\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 122, 'file_type': 'pdf'}, page_content='121\\ndetails of how they do it or the data they use to train their\\nmodels, at least one such dataset has been published for\\npublic consumption. The data in it was anonymized using a\\ntechnique called \\xa0principal component analysis (PCA), which\\nI’ll introduce in Chapter\\xa06.\\nThe dataset is pictured in Figure\\xa03-5. The data comes from\\nreal transactions made by European credit card holders in\\nSeptember 2013. Most of the columns have uninformative names,\\nsuch as V1 and V2, and contain similarly opaque values. Three\\ncolumns—Time, Amount, and Class—have real names and\\nunaltered values revealing when the transaction took place,\\nthe amount of the transaction, and whether the transaction\\nwas legitimate (Class=0) or fraudulent (Class=1).\\nFigure 3-5. The fraud-detection dataset\\nEach row represents one transaction. Of the 284,807\\ntransactions in the dataset, only 492 are fraudulent. The\\ndataset is highly imbalanced, so you would expect a machine\\nlearning model trained on it to be much better at classifying\\nlegitimate transactions than fraudulent ones. That’s not\\nnecessarily a problem, because credit card companies would\\nrather misclassify fraudulent transactions and allow 100 of'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 123, 'file_type': 'pdf'}, page_content=\"122\\nthem to slide through than misclassify one legitimate\\ntransaction and anger a customer.\\nBegin by downloading a ZIP file containing the dataset. Copy\\ncreditcard.csv from the ZIP file into your notebooks’ Data\\nsubdirectory, and then run the following code in a Jupyter\\nnotebook to load the dataset and show the first several rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/creditcard.csv')\\ndf.head()\\nFind out how many rows the dataset contains and whether any\\nof those rows have missing values:\\n\\ndf.info()\\nThe dataset contains 284,807 rows, and none of them are\\nmissing values. Split the data for training and testing, and\\nuse train_test_split’s stratify parameter to ensure that the\\nratio of legitimate and fraudulent transactions is consistent\\nin the training dataset and the testing dataset:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\n=\\ndf.drop(['Time',\\n'Class'],\\naxis=1)\\ny\\n=\\ndf['Class']\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y,\\nrandom_state=0)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 124, 'file_type': 'pdf'}, page_content=\"123\\nTrain a logistic regression model to separate the classes:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\n\\nlr_model\\n=\\nLogisticRegression(random_state=0,\\nmax_iter=5000)\\nlr_model.fit(x_train,\\ny_train)\\nNote the max_iter=5000 parameter passed to the\\nLogisticRegression function. max_iter specifies the maximum\\nnumber of iterations allowed to converge on a solution when\\nfitting the logistic function to a dataset. The default is\\n100, which isn’t enough in this example. Raising the limit\\nto 5,000 gives the internal solver the headroom it needs to\\nfind a solution.\\nA typical accuracy score computed by dividing the sum of the\\ntrue positives and true negatives by the number of test\\nsamples isn’t very helpful because the dataset is so\\nimbalanced. Fraudulent transactions represent less than 0.2%\\nof all the samples, which means that the model could simply\\nguess that every transaction is legitimate and get it right\\nabout 99.8% of the time. Use a confusion matrix to visualize\\nhow the model performs during testing:\\n\\n%matplotlib\\ninline\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nlabels\\n=\\n['Legitimate',\\n'Fraudulent']\\ncmd.from_estimator(lr_model,\\nx_test,\\ny_test,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 125, 'file_type': 'pdf'}, page_content=\"124\\nA logistic regression model correctly identified 56,853\\ntransactions as legitimate while misclassifying legitimate\\ntransactions as fraudulent just 11 times. We want to minimize\\nthe latter number because we don’t want to annoy customers\\nby declining legitimate transactions. Let’s see if a random-\\nforest classifier can do better:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nRandomForestClassifier\\n\\nrf_model\\n=\\nRandomForestClassifier(random_state=0)\\nrf_model.fit(x_train,\\ny_train)\\n\\ncmd.from_estimator(rf_model,\\nx_test,\\ny_test,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 126, 'file_type': 'pdf'}, page_content=\"125\\nA random forest mistook just four legitimate transactions as\\nfraudulent. That’s an improvement over logistic regression.\\nLet’s see if a gradient-boosting classifier can do better\\nstill:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nGradientBoostingClassifier\\n\\ngbm_model\\n=\\nGradientBoostingClassifier(random_state=0)\\ngbm_model.fit(x_train,\\ny_train)\\n\\ncmd.from_estimator(gbm_model,\\nx_test,\\ny_test,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 127, 'file_type': 'pdf'}, page_content='126\\nThe GBM misclassified more legitimate transactions than the\\nrandom forest, so we’ll stick with the random forest. Out of\\n56,864 legitimate transactions, the random forest correctly\\nclassified 56,860 of them. This means that legitimate\\ntransactions are classified correctly more than 99.99% of the\\ntime. Meanwhile, the model caught about 75% of the fraudulent\\ntransactions.\\nUse the following statements to measure the random-forest\\nclassifier’s precision, recall, sensitivity, and\\nspecificity:\\n\\nfrom\\nsklearn.metrics\\nimport\\nprecision_score,\\nrecall_score'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 128, 'file_type': 'pdf'}, page_content=\"127\\ny_pred\\n=\\nrf_model.predict(x_test)\\nprecision\\n=\\nprecision_score(y_test,\\ny_pred)\\nrecall\\n=\\nrecall_score(y_test,\\ny_pred)\\nsensitivity\\n=\\nrecall\\nspecificity\\n=\\nrecall_score(y_test,\\ny_pred,\\npos_label=0)\\n\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'Sensitivity: {sensitivity}')\\nprint(f'Specificity: {specificity}')\\nHere is the output:\\n\\nPrecision: 0.9466666666666667\\nRecall: 0.7244897959183674\\nSensitivity: 0.7244897959183674\\nSpecificity: 0.9999296567248172\\nGiven credit card companies’ desire to keep customers happy\\nand spending money, which of these metrics do you think\\nthey’re most interested in? If you answered specificity, you\\nanswered correctly. Specificity is a measure of how reliable\\nthe test is at not falsely classifying a negative sample as\\npositive—in this case, at not classifying a legitimate\\ntransaction as fraudulent.\\nUnfortunately, you can’t make predictions with this model\\nbecause you don’t know the meaning of the numbers in the V1\\nthrough V28 columns, and you can’t generate columnar values\\nfrom a new transaction because you don’t have the transform\\napplied to the original dataset. You don’t even know what\\nthe original dataset looks like. Most likely each row\\ncontains information about the card holder—for example,\\nannual income, credit score, age, country of residence, and\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 129, 'file_type': 'pdf'}, page_content='128\\namount of money spent on the card last year—plus information\\nabout the product that was purchased and where it was\\npurchased. The feature-engineering aspect of machine learning\\n—figuring out what data is relevant to the model you’re\\nattempting to build—is just as challenging, if not more so,\\nthan data preparation and choosing a learning algorithm.\\nIn real life, the models that credit card companies use to\\ndetect fraud are more sophisticated than this, and they often\\nincorporate several models since no single model is 100%\\naccurate. A company might build three models, for example,\\nand allow them to vote on whether a given transaction is\\nlegitimate. Regardless, you have proven the principle that,\\ngiven the right features, you can build a classification\\nmodel that is reasonably accurate at detecting credit card\\nfraud. And you have seen firsthand how easy Scikit makes it\\nto experiment with different learning algorithms to determine\\nwhich produces the most useful model.\\nMulticlass Classification\\nNow it’s time to tackle multiclass classification, in which\\nthere are n possible outcomes rather than just two. A great\\nexample of multiclass classification is performing optical\\ncharacter recognition: examining a handwritten digit and\\npredicting which digit 0 through 9 it corresponds to. Another\\nexample is looking at a facial photo and identifying the\\nperson in the photo by running it through a model trained to\\nrecognize hundreds of people.\\nVirtually everything you learned about binary classification\\napplies to multiclass classification too. In Scikit, any\\nclassifier that works with binary classification also works\\nwith multiclass classification models. The importance of this\\ncan’t be overstated. Some learning algorithms, such as\\nlogistic regression, work only in binary classification\\nscenarios. Many machine learning libraries make you write\\nexplicit code to extend logistic regression to perform'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 130, 'file_type': 'pdf'}, page_content=\"129\\nmulticlass classification—or use a different form of\\nlogistic regression altogether. Scikit doesn’t. Instead, it\\nmakes sure classifiers such as LogisticRegression work in\\neither scenario, and when necessary, it does extra work\\nbehind the scenes to make it happen.\\nFor logistic regression, Scikit uses one of two strategies to\\nextend the algorithm to work in multiclass scenarios. (You\\ncan specify which strategy to use with the Logisti\\u2060c\\u200bRegression\\nclass’s multi_class parameter, or accept the default of\\n'auto' and allow Scikit to choose.) One is multinomial\\nlogistic regression, which replaces the logistic function\\nwith a softmax function that yields multiple probabilities—\\none per class. The other is one-vs-rest, also known as one-\\nvs-all, which trains n binary classification models, where n\\nis the number of classes that the model can predict. Each of\\nthe n models pairs one class against all the other classes,\\nand when the model is asked to make a prediction, it runs the\\ninput through all n models and uses the output from the one\\nthat yields the highest probability. This strategy is\\ndepicted in Figure\\xa03-6.\\nFigure 3-6. Strategies for extending binary classification algorithms to\\nsupport multiclass classification\\nThe one-vs-rest approach works well for logistic regression,\\nbut for some binary-only classification algorithms, Scikit\\nuses a one-vs-one approach instead. When you use Scikit’s\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 131, 'file_type': 'pdf'}, page_content='130\\nSVC class (a support vector machine classifier that you’ll\\nlearn about in Chapter\\xa05) to perform multiclass\\nclassification, for example, Scikit builds one model for each\\npair of classes. If the model includes four possible classes,\\nScikit builds no fewer than seven models under the hood.\\nYou don’t have to know any of this to build a multiclass\\nclassification model. But it does explain why some multiclass\\nclassification models require more memory and train more\\nslowly than others. Some classification algorithms, such as\\nrandom forests and GBMs, support multiclass classification\\nnatively. For algorithms that don’t, Scikit has your back.\\nIt fills the gap and does so as transparently as possible.\\nTo reiterate: all Scikit classifiers are capable of\\nperforming binary classification and multiclass\\nclassification. This simplifies the code you write and lets\\nyou focus on building and training models rather than\\nunderstanding the underlying mechanics of the algorithms.\\nBuilding a Digit Recognition Model\\nWant to experience multiclass classification firsthand? How\\nabout a model that examines scanned, handwritten digits and\\npredicts what digits 0–9 they correspond to? The US Postal\\nService built a similar model many years ago to recognize\\nhandwritten zip codes as part of an effort to automate mail\\nsorting. We’ll use a sample dataset that’s built into\\nScikit: the University of California Irvine’s Optical\\nRecognition of Handwritten Digits dataset, which contains\\nalmost 1,800 handwritten digits. Each digit is represented by\\nan 8 × 8 array of numbers from 0 to 16, with higher numbers\\nindicating darker pixels. We will use logistic regression to\\nmake predictions from the data. Figure\\xa03-7 shows the first\\n50 digits in the dataset.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 132, 'file_type': 'pdf'}, page_content=\"131\\nFigure 3-7. First 50 digits in the Optical Recognition of Handwritten Digits\\ndataset\\nStart by creating a Jupyter notebook and executing the\\nfollowing statements in the first cell:\\n\\nfrom\\nsklearn\\nimport\\ndatasets\\n\\ndigits\\n=\\ndatasets.load_digits()\\nprint('digits.images: '\\n+\\nstr(digits.images.shape))\\nprint('digits.target: '\\n+\\nstr(digits.target.shape))\\nHere’s what the first digit looks like in numerical form:\\n\\ndigits.images[0]\\nAnd here’s how it looks to the eye:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 133, 'file_type': 'pdf'}, page_content=\"132\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n\\nplt.tick_params(axis='both',\\nwhich='both',\\nbottom=False,\\ntop=False,\\nleft=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 right=False,\\nlabelbottom=False,\\nlabelleft=False)\\nplt.imshow(digits.images[0],\\ncmap=plt.cm.gray_r)\\nIt’s obviously a 0, but you can confirm that from its label:\\n\\ndigits.target[0]\\nPlot the first 50 images and show the corresponding labels:\\n\\nfig,\\naxes\\n=\\nplt.subplots(5,\\n10,\\nfigsize=(12,\\n7),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\n\\nfor\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0 ax.imshow(digits.images[i],\\ncmap=plt.cm.gray_r)\\n\\xa0\\xa0\\xa0 ax.text(0.45,\\n1.05,\\nstr(digits.target[i]),\\ntransform=ax.transAxes)\\nClassification models work best with balanced datasets. Use\\nthe following statements to plot the distribution of the\\nsamples:\\n\\nplt.xticks([])\\nplt.hist(digits.target,\\nrwidth=0.9)\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 134, 'file_type': 'pdf'}, page_content='133\\nThe dataset is almost perfectly balanced, so let’s split it\\nand train a logistic regression model:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 digits.data,\\ndigits.target,\\ntest_size=0.2,\\nrandom_state=0)\\n\\nmodel\\n=\\nLogisticRegression(max_iter=5000)\\nmodel.fit(x_train,\\ny_train)\\nUse the score method to quantify the model’s accuracy:\\n\\nmodel.score(x_test,\\ny_test)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 135, 'file_type': 'pdf'}, page_content=\"134\\nUse a confusion matrix to see how the model performs on the\\ntest dataset:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nfig,\\nax\\n=\\nplt.subplots(figsize=(8,\\n8))\\nax.grid(False)\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\ncmap='Blues',\\ncolorbar=False,\\nax=ax)\\nThe resulting output paints an encouraging picture: large\\nnumbers and dark colors along the diagonal, and small numbers\\nand light colors outside the diagonal. A perfect model would\\nhave all zeros outside the diagonal, but of course, perfect\\nmodels don’t exist:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 136, 'file_type': 'pdf'}, page_content=\"135\\nPick one of the digits from the dataset and plot it to see\\nwhat it looks like:\\n\\nsns.reset_orig()\\n# Undo sns.set()\\nplt.tick_params(axis='both',\\nwhich='both',\\nbottom=False,\\ntop=False,\\nleft=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 right=False,\\nlabelbottom=False,\\nlabelleft=False)\\nplt.imshow(digits.images[100],\\ncmap=plt.cm.gray_r)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 137, 'file_type': 'pdf'}, page_content='136\\nPass it to the model and see what digit the model predicts it\\nis:\\n\\nmodel.predict([digits.data[100]])[0]\\nWhat probabilities does the model predict for each possible\\ndigit?\\n\\nmodel.predict_proba([digits.data[100]])\\nWhat is the probability that the digit is a 4?\\n\\nmodel.predict_proba([digits.data[100]])[0][4]\\nWhen used for binary classification, predict_proba returns\\ntwo probabilities: one for the negative class and one for the\\npositive class. For multiclass classification, predict_proba\\nreturns probabilities for each possible class. This permits\\nyou to assess the model’s confidence in the prediction\\nreturned by predict. Not surprisingly, predict returns the\\nclass assigned the highest probability.\\nSummary\\nClassification models are widely used in industry to predict\\ncategorical outcomes such as whether a credit card\\ntransaction should be accepted or declined. Binary\\nclassification models predict either of two outcomes, while\\nmulticlass classification models predict more than two'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 138, 'file_type': 'pdf'}, page_content='137\\noutcomes. One of the most widely used learning algorithms for\\nclassification models is logistic regression, which fits an\\nequation to training data and uses it to predict outcomes by\\ncomputing the possibility that each is the correct one and\\nselecting the outcome with the highest probability.\\nThere are many ways to score classification models. Which\\nmethod is correct depends on how you intend to use the model.\\nFor example, when the cost of false positives is high, use\\nprecision to assess the model’s accuracy. Precision is\\ncomputed by dividing the number of true positives by the sum\\nof the true positives and false positives. On the other hand,\\nif the cost of false negatives is high, use recall instead.\\nRecall is computed by dividing the number of true positives\\nby the sum of the true positives and false negatives. Closely\\nrelated to precision and recall are sensitivity and\\nspecificity. Sensitivity is identical to recall, while\\nspecificity is recall for the negative class rather than the\\npositive class. Confusion matrices offer a convenient way to\\nvisualize how a model performs on test data without reducing\\nthe accuracy to a single number.\\nSome learning algorithms work only with binary\\nclassification, but Scikit works some magic under the hood to\\nmake sure any learning algorithm can be used for binary or\\nmulticlass classification. This isn’t true of all machine\\nlearning libraries. Some restrict their learning algorithms\\nto specific scenarios or require you to write extra code to\\nuse a binary classification algorithm in a multiclass model.\\nAll the models in this chapter were trained with numerical\\ndata, even though some of the datasets contained categorical\\nvalues—values that are strings rather than numbers—that had\\nto be converted to numbers using one-hot encoding. You may\\nwonder how to build a classification model that works solely\\non text—for example, a model that scores restaurant reviews\\nfor sentiment or classifies emails as spam or not spam. It’s\\na perfectly legitimate question to ask. And it happens to be\\nthe subject of Chapter\\xa04.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 139, 'file_type': 'pdf'}, page_content='138\\nChapter 4. Text Classification\\nOne of the more novel uses for binary classification is\\nsentiment analysis, which examines a sample of text such as a\\nproduct review, a tweet, or a comment left on a website and\\nscores it on a scale of 0.0 to 1.0, where 0.0 represents\\nnegative sentiment and 1.0 represents positive sentiment. A\\nreview such as “great product at a great price” might score\\n0.9, while “overpriced product that barely works” might\\nscore 0.1. The score is the probability that the text\\nexpresses positive sentiment. Sentiment analysis models are\\ndifficult to build algorithmically but are relatively easy to\\ncraft with machine learning. For examples of how sentiment\\nanalysis is used in business today, see the article “8\\nSentiment Analysis Real-World Use Cases” by Nicholas\\nBianchi.\\nSentiment analysis is one example of a task that involves\\nclassifying textual data rather than numerical data. Because\\nmachine learning works with numbers, you must convert text to\\nnumbers before training a sentiment analysis model, a model\\nthat identifies spam emails, or any other model that\\nclassifies text. A common approach is to build a table of\\nword frequencies called a bag of words. Scikit-Learn provides\\nclasses to help. It also includes support for normalizing\\ntext so that, for example, “awesome” and “Awesome” don’t\\ncount as two different words.\\nThis chapter begins by describing how to prepare text for use\\nin classification models. After building a sentiment analysis\\nmodel, you’ll learn about another popular learning algorithm\\ncalled Naive Bayes that works particularly well with text and\\nuse it to build a model that distinguishes between legitimate\\nemails and spam emails. Finally, you’ll learn about a\\nmathematical technique for measuring the similarity of two'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 140, 'file_type': 'pdf'}, page_content='139\\ntext samples and use it to build an app that recommends\\nmovies based on other movies you enjoy.\\nPreparing Text for Classification\\nBefore you train a model to classify text, you must convert\\nthe text into numbers, a process known as vectorization.\\nChapter\\xa01 presented the illustration reproduced in\\nFigure\\xa04-1, which demonstrates a common technique for\\nvectorizing text. Each row represents a text sample such as a\\ntweet or a movie review, and each column represents a word in\\nthe training text. The numbers in the rows are word counts,\\nand the final number in each row is a label: 0 for negative\\nand 1 for positive.\\nFigure 4-1. Dataset for sentiment analysis\\nText is typically cleaned before it’s vectorized. Examples\\nof cleaning include converting characters to lowercase (so,\\nfor example, “Excellent” is equivalent to “excellent”),\\nremoving punctuation symbols, and optionally removing stop\\nwords—common words such as the and and that are likely to\\nhave little impact on the outcome. Once cleaned, sentences\\nare divided into individual words (tokenized) and the words\\nare used to produce datasets like the one in Figure\\xa04-1.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 141, 'file_type': 'pdf'}, page_content='140\\nScikit-Learn has three classes that handle the bulk of the\\nwork of cleaning and vectorizing text:\\nCountVectorizer\\nCreates a dictionary (vocabulary) from the corpus of\\nwords in the training text and generates a matrix of\\nword counts like the one in Figure\\xa04-1\\nHashingVectorizer\\nUses word hashes rather than an in-memory vocabulary\\nto produce word counts and is therefore more memory\\nefficient\\nTfidfVectorizer\\nCreates a dictionary from words provided to it and\\ngenerates a matrix similar to the one in Figure\\xa04-1,\\nbut rather than containing integer word counts, the\\nmatrix contains term frequency-inverse document\\nfrequency (TFIDF) values between 0.0 and 1.0\\nreflecting the relative importance of individual\\nwords\\nAll three classes are capable of converting text to\\nlowercase, removing punctuation symbols, removing stop words,\\nsplitting sentences into individual words, and more. They\\nalso support n-grams, which are combinations of two or more\\nconsecutive words (you specify the number n) that should be\\ntreated as a single word. The idea is that words such as\\ncredit and score might be more meaningful if they appear next\\nto each other in a sentence than if they appear far apart.\\nWithout n-grams, the relative proximity of words is ignored.\\nThe downside to using n-grams is that it increases memory\\nconsumption and training time. Used judiciously, however, it\\ncan make text classification models more accurate.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 142, 'file_type': 'pdf'}, page_content=\"141\\nNOTE\\nNeural networks have other, more powerful ways of taking word\\norder into account that don’t require related words to occur next\\nto each other. A conventional machine learning model can’t\\nconnect the words blue and sky in the sentence “I like blue, for\\nit is the color of the sky,” but a neural network can. I will\\nshed more light on this in Chapter\\xa013.\\nHere’s an example demonstrating what CountVectorizer does\\nand how it’s used:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nlines\\n=\\n[\\n\\xa0\\xa0\\xa0 'Four score and 7 years ago our fathers brought forth,',\\n\\xa0\\xa0\\xa0 '... a new NATION, conceived in liberty $$$,',\\n\\xa0\\xa0\\xa0 'and dedicated to the PrOpOsItIoN that all men are created equal',\\n\\xa0\\xa0\\xa0 'One nation\\\\'s freedom equals #freedom for another $nation!'\\n]\\n\\n# Vectorize the lines\\nvectorizer\\n=\\nCountVectorizer(stop_words='english')\\nword_matrix\\n=\\nvectorizer.fit_transform(lines)\\n\\n# Show the resulting word matrix\\nfeature_names\\n=\\nvectorizer.get_feature_names_out()\\nline_names\\n=\\n[f'Line {(i\\n+\\n1):d}'\\nfor\\ni,\\n_\\nin\\nenumerate(word_matrix)]\\n\\ndf\\n=\\npd.DataFrame(data=word_matrix.toarray(),\\nindex=line_names,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 columns=feature_names)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 143, 'file_type': 'pdf'}, page_content=\"142\\n\\ndf.head()\\nHere’s the output:\\nThe corpus of text in this case is four strings in a Python\\nlist. CountVectorizer broke the strings into words, removed\\nstop words and symbols, and converted all remaining words to\\nlowercase. Those words comprise the columns in the dataset,\\nand the numbers in the rows show how many times a given word\\nappears in each string. The stop_words='english' parameter\\ntells CountVectorizer to remove stop words using a built-in\\ndictionary of more than 300 English-language stop words. If\\nyou prefer, you can provide your own list of stop words in a\\nPython list. (Or you can leave the stop words in there; it\\noften doesn’t matter.) And if you’re training with text\\nwritten in another language, you can get lists of\\nmultilanguage stop words from other Python libraries such as\\nthe Natural Language Toolkit (NLTK) and Stop-words.\\nObserve from the output that equal and equals count as\\nseparate words, even though they have similar meaning. Data\\nscientists sometimes go a step further when preparing text\\nfor machine learning by stemming or lemmatizing words. If the\\npreceding text were stemmed, all occurrences of equals would\\nbe converted to equal. Scikit lacks support for stemming and\\nlemmatization, but you can get it from other libraries such\\nas NLTK.\\nCountVectorizer removes punctuation symbols, but it doesn’t\\nremove numbers. It ignored the 7 in line 1 because it ignores\\nsingle characters. But if you changed 7 to 777, the term 777\\nwould appear in the vocabulary. One way to fix that is to\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 144, 'file_type': 'pdf'}, page_content=\"143\\ndefine a function that removes numbers and pass it to\\nCountVectorizer via the preprocessor parameter:\\n\\nimport\\nre\\n\\ndef\\npreprocess_text(text):\\n\\xa0\\xa0\\xa0 return\\nre.sub(r'\\\\d+',\\n'',\\ntext).lower()\\n\\nvectorizer\\n=\\nCountVectorizer(stop_words='english',\\npreprocessor=preprocess_text)\\nword_matrix\\n=\\nvectorizer.fit_transform(lines)\\nNote the call to lower to convert the text to lowercase.\\nCountVectorizer doesn’t convert text to lowercase if you\\nprovide a preprocessing function, so the preprocessing\\nfunction must convert it itself. It still removes punctuation\\ncharacters, however.\\nAnother useful parameter to CountVectorizer is min_df, which\\nignores words that appear fewer than the specified number of\\ntimes. It can be an integer specifying a minimum count (for\\nexample, ignore words that appear fewer than five times in\\nthe training text, or min_df=5), or it can be a floating-\\npoint value from 0.0 to 1.0 specifying the minimum percentage\\nof samples in which a word must appear—for example, ignore\\nwords that appear in less than 10% of the samples\\n(min_df=0.1). It’s great for filtering out words that\\nprobably aren’t meaningful anyway, and it reduces memory\\nconsumption and training time by decreasing the size of the\\nvocabulary. Count\\u200bVec\\u2060tor\\u2060izer also supports a max_df parameter\\nfor eliminating words that appear too frequently.\\nThe preceding examples use CountVectorizer, which probably\\nleaves you wondering when (and why) you would use\\nHashingVectorizer or TfidfVectorizer instead. HashingVectorizer\\nis useful when dealing with large datasets. Rather than store\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 145, 'file_type': 'pdf'}, page_content='144\\nwords in memory, it hashes each word and uses the hash as an\\nindex into an array of word counts. It can therefore do more\\nwith less memory and is very useful for reducing the size of\\nvectorizers when serializing them so that you can restore\\nthem later—a topic I’ll say more about in Chapter\\xa07. The\\ndownside to HashingVectorizer is that it doesn’t let you\\nwork backward from vectorized text to the original text.\\nCount\\u200bVec\\u2060tor\\u2060izer does, and it provides an inverse_transform\\nmethod for that purpose.\\nTfidfVectorizer is frequently used to perform keyword\\nextraction: examining a document or set of documents and\\nextracting keywords that characterize their content. It\\nassigns words numerical weights reflecting their importance,\\nand it uses two factors to determine the weights: how often a\\nword appears in individual documents, and how often it\\nappears in the overall document set. Words that appear more\\nfrequently in individual documents but occur in fewer\\ndocuments receive higher weights. I won’t go further into it\\nhere, but if you’re curious to learn more, this book’s\\nGitHub repo contains a notebook that uses Tfidf\\u200bVec\\u2060tor\\u2060izer to\\nextract keywords from the manuscript of Chapter\\xa01.\\nSentiment Analysis\\nTo train a sentiment analysis model, you need a labeled\\ndataset. Several such datasets are available in the public\\ndomain. One of those is the IMDB movie review dataset, which\\ncontains 25,000 samples of negative reviews and 25,000\\nsamples of positive reviews posted on the Internet Movie\\nDatabase website. Each review is meticulously labeled with a\\n0 for negative sentiment or a 1 for positive sentiment. To\\ndemonstrate how sentiment analysis works, let’s build a\\nbinary classification model and train it with this dataset.\\nWe’ll use logistic regression as the learning algorithm. A\\nsentiment analysis score yielded by this model is simply the\\nprobability that the input expresses positive sentiment,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 146, 'file_type': 'pdf'}, page_content=\"145\\nwhich is easily retrieved by calling LogisticRegres\\u2060sion’s\\npredict_proba method.\\nStart by downloading the dataset and copying it to the Data\\nsubdirectory of the directory that hosts your Jupyter\\nnotebooks. Then run the following code in a notebook to load\\nthe dataset and show the first five rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/reviews.csv',\\nencoding='ISO-8859-1')\\ndf.head()\\nThe encoding attribute is necessary because the CSV file uses\\nISO-8859-1 character encoding rather than UTF-8. The output\\nis as follows:\\nFind out how many rows the dataset contains and confirm that\\nthere are no missing values:\\n\\ndf.info()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 147, 'file_type': 'pdf'}, page_content=\"146\\nUse the following statement to see how many instances there\\nare of each class (0 for negative and 1 for positive):\\n\\ndf.groupby('Sentiment').describe()\\nHere is the output:\\nThere is an even number of positive and negative samples, but\\nin each case, the number of unique samples is less than the\\nnumber of samples for that class. That means the dataset has\\nduplicate rows, and duplicate rows could bias a machine\\nlearning model. Use the following statements to delete the\\nduplicate rows and check for balance again:\\n\\ndf\\n=\\ndf.drop_duplicates()\\ndf.groupby('Sentiment').describe()\\nNow there are no duplicate rows, and the number of positive\\nand negative samples is roughly equal.\\nNext, use CountVectorizer to prepare and vectorize the text\\nin the Text column. Set min_df to 20 to ignore words that\\nappear infrequently in the training text. This reduces the\\nlikelihood of out-of-memory errors and will probably make the\\nmodel more accurate as well. Also use the ngram_range\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 148, 'file_type': 'pdf'}, page_content=\"147\\nparameter to allow Count\\u200bVec\\u2060tor\\u2060izer to include word pairs as\\nwell as individual words:\\n\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nvectorizer\\n=\\nCountVectorizer(ngram_range=(1,\\n2),\\nstop_words='english',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 min_df=20)\\n\\nx\\n=\\nvectorizer.fit_transform(df['Text'])\\ny\\n=\\ndf['Sentiment']\\nNow split the dataset for training and testing. We’ll use a\\n50/50 split since there are almost 50,000 samples in total:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\ntest_size=0.5,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nThe next step is to train a classifier. We’ll use Scikit’s\\nLogisticRegression class, which uses logistic regression to\\nfit a model to the data:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\n\\nmodel\\n=\\nLogisticRegression(max_iter=1000,\\nrandom_state=0)\\nmodel.fit(x_train,\\ny_train)\\nValidate the trained model with the 50% of the dataset set\\naside for testing and show the results in a confusion matrix:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 149, 'file_type': 'pdf'}, page_content=\"148\\n\\n%matplotlib\\ninline\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=['Negative',\\n'Positive'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nThe confusion matrix reveals that the model correctly\\nidentified 10,795 negative reviews while misclassifying 1,574\\nof them. It correctly identified 10,966 positive reviews and\\ngot it wrong 1,456 times:\\nNow comes the fun part: analyzing text for sentiment. Use the\\nfollowing statements to produce a sentiment score for the\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 150, 'file_type': 'pdf'}, page_content=\"149\\nsentence “The long lines and poor customer service really\\nturned me off”:\\n\\ntext\\n=\\n'The long lines and poor customer service really turned me off'\\nmodel.predict_proba(vectorizer.transform([text]))[0][1]\\nHere’s the output:\\n\\n0.09183447847778639\\nNow do the same for “The food was great and the service was\\nexcellent!”:\\n\\ntext\\n=\\n'The food was great and the service was excellent!'\\nmodel.predict_proba(vectorizer.transform([text]))[0][1]\\nIf you expected a higher score for this one, you won’t be\\ndisappointed:\\n\\n0.8536277207125618\\nFeel free to try sentences of your own and see if you agree\\nwith the sentiment scores the model predicts. It’s not\\nperfect, but it’s good enough that if you run hundreds of\\nreviews or comments through it, you should get a reliable\\nindication of the sentiment expressed in the text.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 151, 'file_type': 'pdf'}, page_content=\"150\\nNOTE\\nSometimes CountVectorizer’s built-in list of stop words lowers the\\naccuracy of a model because the list is so broad. As an\\nexperiment, remove stop_words='english' from CountVectorizer and run\\nthe code again. Check the confusion matrix. Does the accuracy\\nincrease or decrease? Feel free to vary other parameters such as\\nmin_df and ngram_range too. In the real world, data scientists often\\ntry many different parameter combinations to determine which one\\nproduces the best results.\\nNaive Bayes\\nLogistic regression is a go-to algorithm for classification\\nmodels and is often very effective at classifying text. But\\nin scenarios involving text classification, data scientists\\noften turn to another learning algorithm called Naive Bayes.\\nIt’s a classification algorithm based on Bayes’ theorem,\\nwhich provides a means for calculating conditional\\nprobabilities. Mathematically, Bayes’ theorem is stated this\\nway:\\nThis says the probability that A is true given that B is true\\nis equal to the probability that B is true given that A is\\ntrue multiplied by the probability that A is true divided by\\nthe probability that B is true. That’s a mouthful, and while\\naccurate, it doesn’t explain why Naive Bayes is so useful\\nfor classifying text—or how you apply it, for example, to a\\ncollection of emails to determine which ones are spam.\\nLet’s start with a simple example. Suppose 10% of all the\\nemails you receive are spam. That’s P(A). Analysis reveals\\nthat 5% of the spam emails you receive contain the word\\ncongratulations, but just 1% of all your emails contain the\\nsame word. Therefore, P(B|A) is 0.05 and P(B) is 0.01. The\\nprobability of an email being spam if it contains the word\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 152, 'file_type': 'pdf'}, page_content='151\\ncongratulations is P(A|B), which is (0.05 x 0.10) / 0.01, or\\n0.50.\\nOf course, a spam filter must consider all the words in an\\nemail, not just one. It turns out that if you make some\\nsimple (naive) assumptions—that the order of the words in an\\nemail doesn’t matter, and that every word has equal weight—\\nyou can write Bayes’ equation this way for a spam\\nclassifier:\\nIn plain English, the probability that a message is spam is\\nproportional to the product\\xa0of:\\n\\xa0\\nThe probability that any message in the dataset\\nis spam, or P(S)\\nThe probability that each word in the message\\nappears in a spam message, or P(word|S)\\nP(S) can be calculated easily enough: it’s simply the\\nfraction of the messages in the dataset that are spam\\nmessages. If you train a machine learning model with 1,000\\nmessages and 500 of them are spam, then P(S) = 0.5. For a\\ngiven word, P(word|S) is simply the number of times the word\\nappears in spam messages divided by the number of words in\\nall the spam messages. The entire problem reduces to word\\ncounts. You can do a similar calculation to compute the\\nprobability that the message is not spam, and then use the\\nhigher of the two probabilities to make a prediction.\\nHere’s an example involving four sample emails. The emails\\nare:\\n\\n\\xa0Text\\xa0\\n\\xa0Spam\\xa0\\n\\xa0Raise your credit score in minutes\\xa0\\n\\xa01\\xa0\\n\\xa0Here are the minutes from yesterday’s meeting\\xa0 \\xa00\\xa0\\n\\xa0Meeting tomorrow to review yesterday’s scores\\xa0 \\xa00\\xa0\\n\\xa0Score tomorrow’s meds at yesterday’s prices\\xa0\\n\\xa01'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 153, 'file_type': 'pdf'}, page_content='152\\nIf you remove stop words, convert characters to lowercase,\\nand stem the words such that tomorrow’s becomes tomorrow,\\nyou’re left with this:\\n\\n\\xa0Text\\xa0\\n\\xa0Spam\\xa0\\n\\xa0raise credit score minute\\xa0\\n\\xa01\\xa0\\n\\xa0minute yesterday meeting\\xa0\\n\\xa00\\xa0\\n\\xa0meeting tomorrow review yesterday score\\xa0 \\xa00\\xa0\\n\\xa0score tomorrow med yesterday price\\xa0\\n\\xa01\\xa0\\nBecause two of the four messages are spam and two are not,\\nthe probability that any message is spam (P(S)) is 0.5. The\\nsame goes for the probability that any message is not spam\\n(P(N) = 0.5). In addition, the spam messages contain nine\\nunique words, while the nonspam messages contain a total of\\neight.\\nThe next step is to build the following table of word\\nfrequencies. Take the word yesterday as an example. It\\nappears once in a message that’s labeled as spam, so\\nP(yesterday|S) is 1/9, or 0.111. It appears twice in nonspam\\nmessages, so P(yesterday|N) is 2/8, or 0.250:\\n\\n\\xa0Word\\xa0\\n\\xa0P(word|S\\xa0) \\xa0P(word|N\\xa0)\\n\\xa0raise\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000\\xa0\\n\\xa0credit\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000\\xa0\\n\\xa0score\\xa0\\n\\xa02/9 = 0.222\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0minute\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0yesterday\\xa0 \\xa01/9 = 0.111\\xa0 \\xa02/8 = 0.250\\xa0\\n\\xa0meeting\\xa0\\n\\xa00/9 = 0.000\\xa0 \\xa02/8 = 0.250\\xa0\\n\\xa0tomorrow\\xa0 \\xa01/9 = 0.111\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0review\\xa0\\n\\xa00/9 = 0.000\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0med\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000\\xa0\\n\\xa0price\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 154, 'file_type': 'pdf'}, page_content='153\\nThis works up to a point, but the zeros in the table are a\\nproblem. Let’s say you want to determine whether “Scores\\nmust be reviewed by tomorrow” is spam. Removing stop words\\nleaves you with “score review tomorrow.” You can compute\\nthe probability that the message is spam this way:\\n\\n\\nThe result is 0 because review doesn’t appear in a spam\\nmessage, and 0 times anything is 0. The algorithm simply\\ncan’t assign a spam probability to “Scores must be reviewed\\nby tomorrow.”\\nA common way to resolve this is to apply Laplace smoothing,\\nalso known as additive smoothing. Typically, this involves\\nadding 1 to each numerator and the number of unique words in\\nthe dataset (in this case, 10) to each denominator. Now,\\nP(review|S) evaluates to (0 + 1) / (9 + 10), which equals\\n0.053. It’s not much, but it’s better than nothing\\n(literally). Here are the word frequencies again, this time\\nrevised with Lap\\u2060lace smoothing:\\n\\n\\xa0Word\\xa0\\n\\xa0P(word|S\\xa0)\\n\\xa0P(word|N\\xa0)\\n\\xa0raise\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056\\xa0\\n\\xa0credit\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056\\xa0\\n\\xa0score\\xa0\\n\\xa0(2 + 1) / (9 + 10) = 0.158\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0minute\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0yesterday\\xa0 \\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(2 + 1) / (8 + 10) = 0.167\\xa0\\n\\xa0meeting\\xa0\\n\\xa0(0 + 1) / (9 + 10) = 0.053\\xa0 \\xa0(2 + 1) / (8 + 10) = 0.167\\xa0\\n\\xa0tomorrow\\xa0 \\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0review\\xa0\\n\\xa0(0 + 1) / (9 + 10) = 0.053\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0med\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056\\xa0\\n\\xa0price\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 155, 'file_type': 'pdf'}, page_content='154\\nNow you can determine whether “Scores must be reviewed by\\ntomorrow” is spam by performing two simple calculations:\\n\\nBy this measure, “Scores must be reviewed by tomorrow” is\\nlikely not to be spam. The probabilities are relative, but\\nyou could normalize them and conclude there’s about a 40%\\nchance the message is spam and a 60% chance it’s not based\\non the emails the model was trained with.\\nFortunately, you don’t have to do these computations by\\nhand. Scikit-Learn provides several classes to help out,\\nincluding the MultinomialNB class, which works great with\\ntables of word counts produced by CountVectorizer.\\nSpam Filtering\\nIt’s no coincidence that modern spam filters are remarkably\\nadept at identifying spam. Virtually all of them rely on\\nmachine learning. Such models are difficult to implement\\nalgorithmically because an algorithm that uses keywords such\\nas credit and score to determine whether an email is spam is\\neasily fooled. Machine learning, by contrast, looks at a body\\nof emails and uses what it learns to classify the next email.\\nSuch models often achieve more than 99% accuracy. And they\\nget smarter over time as they’re trained with more and more\\nemails.\\nThe previous example used logistic regression to predict\\nwhether text input to it expresses positive or negative\\nsentiment. It used the probability that the text expresses\\npositive sentiment as a sentiment score, and you saw that\\nexpressions such as “The long lines and poor customer\\nservice really turned me off” score close to 0.0, while\\nexpressions such as “The food was great and the service was\\nexcellent” score close to 1.0. Now let’s build a binary\\nclassification model that classifies emails as spam or not'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 156, 'file_type': 'pdf'}, page_content=\"155\\nspam and use Naive Bayes to fit the model to the training\\ndata.\\nThere are several spam classification datasets available in\\nthe public domain. Each contains a collection of emails with\\nsamples labeled with 1s for spam and 0s for not spam. We’ll\\nuse a relatively small dataset containing 1,000 samples.\\nBegin by downloading the dataset and copying it into your\\nnotebooks’ Data subdirectory. Then load the data and display\\nthe first five rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/ham-spam.csv')\\ndf.head()\\nNow check for duplicate rows in the dataset:\\n\\ndf.groupby('IsSpam').describe()\\nThe dataset contains one duplicate row. Let’s remove it and\\ncheck for balance:\\n\\ndf\\n=\\ndf.drop_duplicates()\\ndf.groupby('IsSpam').describe()\\nThe dataset now contains 499 samples that are not spam, and\\n500 that are. The next step is to use CountVectorizer to\\nvectorize the emails. Once more, we’ll allow CountVectorizer\\nto consider word pairs as well as individual words and remove\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 157, 'file_type': 'pdf'}, page_content=\"156\\nstop words using Scikit’s built-in dictionary of English\\nstop words:\\n\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nvectorizer\\n=\\nCountVectorizer(ngram_range=(1,\\n2),\\nstop_words='english')\\nx\\n=\\nvectorizer.fit_transform(df['Text'])\\ny\\n=\\ndf['IsSpam']\\nSplit the dataset so that 80% can be used for training and\\n20% for testing:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nThe next step is to train a Naive Bayes classifier using\\nScikit’s MultinomialNB class:\\nfrom\\nsklearn.naive_bayes\\nimport\\nMultinomialNB\\n\\nmodel\\n=\\nMultinomialNB()\\nmodel.fit(x_train,\\ny_train)\\nValidate the trained model with the 20% of the dataset set\\naside for testing using a confusion matrix:\\n\\n%matplotlib\\ninline\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 158, 'file_type': 'pdf'}, page_content=\"157\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=['Not Spam',\\n'Spam'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nThe model correctly identified 101 of 102 legitimate emails\\nas not spam, and 95 of 98 spam emails as spam:\\nUse the score method to get a rough measure of the model’s\\naccuracy:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 159, 'file_type': 'pdf'}, page_content='158\\n\\nmodel.score(x_test,\\ny_test)\\nNow use Scikit’s RocCurveDisplay class to visualize the ROC\\ncurve:\\n\\nfrom\\nsklearn.metrics\\nimport\\nRocCurveDisplay\\nas\\nrcd\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nrcd.from_estimator(model,\\nx_test,\\ny_test)\\nThe results are encouraging. Trained with just 999 samples,\\nthe area under the ROC curve (AUC) indicates the model is\\nmore than 99.9% accurate at classifying emails as spam or not\\nspam:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 160, 'file_type': 'pdf'}, page_content=\"159\\nLet’s see how the model classifies a few emails that it\\nhasn’t seen before, starting with one that isn’t spam. The\\nmodel’s predict method predicts a class—0 for not spam, or\\n1 for spam:\\n\\nmsg\\n=\\n'Can you attend a code review on Tuesday to make sure the logic is\\nsolid?'\\ninput\\n=\\nvectorizer.transform([msg])\\nmodel.predict(input)[0]\\nThe model says this message is not spam, but what’s the\\nprobability that it’s not spam? You can get that from\\npredict_proba, which returns an array containing two values:\\nthe probability that the predicted class is 0, and the\\nprobability that the predicted class is 1, in that order:\\n\\nmodel.predict_proba(input)[0][0]\\nThe model seems very sure that this email is legitimate:\\n\\n0.9999497111473539\\nNow test the model with a spam message:\\n\\nmsg\\n=\\n'Why pay more for expensive meds when you can order them online ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'and save $$$?'\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 161, 'file_type': 'pdf'}, page_content='160\\ninput\\n=\\nvectorizer.transform([msg])\\nmodel.predict(input)[0]\\nWhat is the probability that the message is not spam?\\n\\nmodel.predict_proba(input)[0][0]\\nThe answer is:\\n\\n0.00021423891260677753\\nWhat is the probability that the message is spam?\\n\\nmodel.predict_proba(input)[0][1]\\nAnd the answer is:\\n\\n0.9997857610873945\\nObserve that predict and predict_proba accept a list of\\ninputs. Based on that, could you classify an entire batch of\\nemails with one call to either method? How would you get the\\nresults for each email?\\nRecommender Systems'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 162, 'file_type': 'pdf'}, page_content='161\\nAnother branch of machine learning that has proven its mettle\\nin recent years is recommender systems—systems that\\nrecommend products or services to customers. Amazon’s\\nrecommender system reportedly drives 35% of its sales. The\\ngood news is that you don’t have to be Amazon to benefit\\nfrom a recommender system, nor do you have to have Amazon’s\\nresources to build one. They’re relatively simple to create\\nonce you learn a few basic principles.\\nRecommender systems come in many forms. Popularity-based\\nsystems present options to customers based on what products\\nand services are popular at the time—for example, “Here are\\nthis week’s bestsellers.” Collaborative systems make\\nrecommendations based on what others have selected, as in\\n“People who bought this book also bought these books.”\\nNeither of these systems requires machine learning.\\nContent-based systems, by contrast, benefit greatly from\\nmachine learning. An example of a content-based system is one\\nthat says “if you bought this book, you might like these\\nbooks also.” These systems require a means for quantifying\\nsimilarity between items. If you liked the movie Die Hard,\\nyou might or might not like Monty Python and the Holy Grail.\\nIf you liked Toy Story, you’ll probably like A Bug’s Life\\ntoo. But how do you make that determination algorithmically?\\nContent-based recommenders require two ingredients: a way to\\nvectorize—convert to numbers—the attributes that\\ncharacterize a service or product, and a means for\\ncalculating similarity between the resulting vectors. The\\nfirst one is easy. Count\\u200bVec\\u2060tor\\u2060izer converts text into tables\\nof word counts. All you need is a way to measure similarity\\nbetween rows of word counts and you can build a recommender\\nsystem. And one of the simplest and most effective ways to do\\nthat is a technique called cosine similarity.\\nCosine Similarity'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 163, 'file_type': 'pdf'}, page_content='162\\nCosine similarity is a mathematical means for computing the\\nsimilarity between pairs of vectors (or rows of numbers\\ntreated as vectors). The basic idea is to take each value in\\na sample—for example, word counts in a row of vectorized\\ntext—and use them as endpoint coordinates for a vector, with\\nthe other endpoint at the origin of the coordinate system. Do\\nthat for two samples, and then compute the cosine between\\nvectors in m-dimensional space, where m is the number of\\nvalues in each sample. Because the cosine of 0 is 1, two\\nidentical vectors have a similarity of 1. The more dissimilar\\nthe vectors, the closer the cosine will be to 0.\\nHere’s an example in two-dimensional space to illustrate.\\nSuppose you have three rows containing two values each:\\n\\n\\xa01\\xa0 \\xa02\\xa0\\n\\xa02\\xa0 \\xa03\\xa0\\n\\xa03\\xa0 \\xa01\\xa0\\nYou want to determine whether row 2 is more similar to row 1\\nor row 3. It’s hard to tell just by looking at the numbers,\\nand in real life, there are many more numbers. If you simply\\nadded the numbers in each row and compared the sums, you\\nwould conclude that row 2 is more similar to row 3. But what\\nif you treated each row as a vector, as shown in Figure\\xa04-2?\\n\\xa0\\nRow 1: (0, 0) → (1, 2)\\nRow 2: (0, 0) → (2, 3)\\nRow 3: (0, 0) → (3, 1)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 164, 'file_type': 'pdf'}, page_content='163\\nFigure 4-2. Cosine similarity\\nNow you can plot each row as a vector, compute the cosines of\\nthe angles formed by 1 and 2 and 2 and 3, and determine that\\nrow 2 is more like row 1 than row 3. That’s cosine\\nsimilarity in a nutshell.\\nCosine similarity isn’t limited to two dimensions; it works\\nin higher-dimensional space as well. To help compute cosine\\nsimilarities regardless of the number of dimensions, Scikit\\noffers the cosine_similarity function. The following code\\ncomputes the cosine similarities of the three samples in the\\npreceding example:\\n\\ndata\\n=\\n[[1,\\n2],\\n[2,\\n3],\\n[3,\\n1]]\\ncosine_similarity(data)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 165, 'file_type': 'pdf'}, page_content=\"164\\nThe return value is a similarity matrix containing the\\ncosines of every vector pair. The width and height of the\\nmatrix equals the number of samples:\\n\\narray([[1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ,\\n0.99227788,\\n0.70710678],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [0.99227788,\\n1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ,\\n0.78935222],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [0.70710678,\\n0.78935222,\\n1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ]])\\nFrom this, you can see that the similarity of rows 1 and 2 is\\n0.992, while the similarity of rows 2 and 3 is 0.789. In\\nother words, row 2 is more similar to row 1 than it is to row\\n3. There is also more similarity between rows 2 and 3 (0.789)\\nthan there is between rows 1 and 3 (0.707).\\nBuilding a Movie Recommendation System\\nLet’s put cosine similarity to work building a content-based\\nrecommender system for movies. Start by downloading the\\ndataset, which is one of several movie datasets available\\nfrom Kaggle.com. This one has information for about 4,800\\nmovies, including title, budget, genres, keywords, cast, and\\nmore. Place the CSV file in your Jupyter notebooks’ Data\\nsubdirectory. Then load the dataset and peruse its contents:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/movies.csv')\\ndf.head()\\nThe dataset contains 24 columns, only a few of which are\\nneeded to describe a movie. Use the following statements to\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 166, 'file_type': 'pdf'}, page_content=\"165\\nextract key columns such as title and genres and fill missing\\nvalues with empty strings:\\ndf\\n=\\ndf[['title',\\n'genres',\\n'keywords',\\n'cast',\\n'director']]\\ndf\\n=\\ndf.fillna('')\\n# Fill missing values with empty strings\\ndf.head()\\nNext, add a column named features that combines all the words\\nin the other columns:\\n\\ndf['features']\\n=\\ndf['title']\\n+\\n' '\\n+\\ndf['genres']\\n+\\n' '\\n+ \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 df['keywords']\\n+\\n' '\\n+\\ndf['cast']\\n+\\n' '\\n+ \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 df['director']\\nUse CountVectorizer to vectorize the text in the features\\ncolumn:\\n\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nvectorizer\\n=\\nCountVectorizer(stop_words='english',\\nmin_df=20)\\nword_matrix\\n=\\nvectorizer.fit_transform(df['features'])\\nword_matrix.shape\\nThe table of word counts contains 4,803 rows—one for each\\nmovie—and 918 columns. The next task is to compute cosine\\nsimilarities for each row pair:\\n\\nfrom\\nsklearn.metrics.pairwise\\nimport\\ncosine_similarity\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 167, 'file_type': 'pdf'}, page_content=\"166\\n\\nsim\\n=\\ncosine_similarity(word_matrix)\\nUltimately, the goal of this system is to input a movie title\\nand identify the n movies that are most similar to that\\nmovie. To that end, define a function named get\\u200b\\n_recom\\u2060mendations that accepts a movie title, a DataFrame\\ncontaining information about all the movies, a similarity\\nmatrix, and the number of movie titles to return:\\n\\ndef\\nget_recommendations(title,\\ndf,\\nsim,\\ncount=10):\\n\\xa0\\xa0\\xa0 # Get the row index of the specified title in the DataFrame\\n\\xa0\\xa0\\xa0 index\\n=\\ndf.index[df['title'].str.lower()\\n==\\ntitle.lower()]\\n\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 # Return an empty list if there is no entry for the specified title\\n\\xa0\\xa0\\xa0 if\\n(len(index)\\n==\\n0):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n[]\\n\\n\\xa0\\xa0\\xa0 # Get the corresponding row in the similarity matrix\\n\\xa0\\xa0\\xa0 similarities\\n=\\nlist(enumerate(sim[index[0]]))\\n\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 # Sort the similarity scores in that row in descending order\\n\\xa0\\xa0\\xa0 recommendations\\n=\\nsorted(similarities,\\nkey=lambda\\nx:\\nx[1],\\nreverse=True)\\n\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 # Get the top n recommendations, ignoring the first entry in the list since\\n\\xa0\\xa0\\xa0 # it corresponds to the title itself (and thus has a similarity of 1.0)\\n\\xa0\\xa0\\xa0 top_recs\\n=\\nrecommendations[1:count\\n+\\n1]\\n\\n\\xa0\\xa0\\xa0 # Generate a list of titles from the indexes in top_recs\\n\\xa0\\xa0\\xa0 titles\\n=\\n[]\\n\\n\\xa0\\xa0\\xa0 for\\ni\\nin\\nrange(len(top_recs)):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 title\\n=\\ndf.iloc[top_recs[i][0]]['title']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 titles.append(title)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 168, 'file_type': 'pdf'}, page_content=\"167\\n\\n\\xa0\\xa0\\xa0 return\\ntitles\\nThis function sorts the cosine similarities in descending\\norder to identify the count movies most like the one\\nidentified by the title parameter. Then it returns the titles\\nof those movies.\\nNow use get_recommendations to search the database for\\nsimilar movies. First ask for the 10 movies that are most\\nsimilar to the James Bond thriller Skyfall:\\n\\nget_recommendations('Skyfall',\\ndf,\\nsim)\\nHere is the output:\\n\\n['Spectre',\\n'Quantum of Solace',\\n'Johnny English Reborn',\\n'Clash of the Titans',\\n'Die Another Day',\\n'Diamonds Are Forever',\\n'Wrath of the Titans',\\n'I Spy',\\n'Sanctum',\\n'Blackthorn']\\nCall get_recommendations again to list movies that are like\\nMulan:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 169, 'file_type': 'pdf'}, page_content=\"168\\n\\nget_recommendations('Mulan',\\ndf,\\nsim)\\nFeel free to try other movies as well. Note that you can only\\ninput movie titles that are in the dataset. Use the following\\nstatements to print a complete list of titles:\\n\\npd.set_option('display.max_rows',\\nNone)\\nprint(df['title'])\\nI think you’ll agree that the system does a pretty credible\\njob of picking similar movies. Not bad for about 20 lines of\\ncode!\\nSummary\\nMachine learning models that classify text are common and see\\na variety of uses in industry and in everyday life. What\\nrational human being doesn’t wish for a magic wand that\\neradicates all spam mails, for example?\\nText used to train a text classification model must be\\nprepared and vectorized prior to training. Preparation\\nincludes converting characters to lowercase and removing\\npunctuation characters, and may include removing stop words,\\nremoving numbers, and stemming or lemmatizing. Once prepared,\\ntext is vectorized by converting it into a table of word\\nfrequencies. Scikit’s CountVectorizer class makes short work\\nof the vectorization process and handles some of the\\npreparation duties too.\\nLogistic regression and other popular classification\\nalgorithms can be used to classify text once it’s converted\\nto numerical form. For text classification tasks, however,\\nthe Naive Bayes learning algorithm frequently outperforms\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 170, 'file_type': 'pdf'}, page_content='169\\nother algorithms. By making a few “naive” assumptions such\\nas that the order in which words appear in a text sample\\ndoesn’t matter, Naive Bayes reduces to a process of word\\ncounting. Scikit’s MultinomialNB class provides a handy\\nNaive Bayes implementation.\\nCosine similarity is a mathematical means for computing the\\nsimilarity between two rows of numbers. One use for it is\\nbuilding systems that recommend products or services based on\\nother products or services that a customer has purchased.\\nWord frequency tables produced from textual descriptions by\\nCountVectorizer can be combined with cosine similarity to\\ncreate intelligent recommender systems intended to supplement\\na company’s bottom line.\\nFeel free to use this chapter’s examples as a starting point\\nfor experiments of your own. For instance, see if you can\\ntweak the parameters passed to CountVectorizer in any of the\\nexamples and increase the accuracy of the resulting model.\\nData scientists call the search for the optimum parameter\\ncombination hyperparameter tuning, and it’s a subject\\nyou’ll learn about in the next chapter.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 171, 'file_type': 'pdf'}, page_content='170\\nChapter 5. Support Vector\\nMachines\\nSupport vector machines (SVMs) represent the cutting edge of\\nmachine learning. They are most often used to solve\\nclassification problems, but they can also be used for\\nregression. Due to the unique way in which they fit\\nmathematical models to data, SVMs often succeed at finding\\nseparation between classes when other models do not. They\\ntechnically perform binary classification only, but Scikit-\\nLearn enables them to do multiclass classification as well\\nusing techniques discussed in Chapter\\xa03.\\nScikit-Learn makes building SVMs easy with classes such as\\nSVC (short for support vector classifier) for classification\\nmodels and SVR (support vector regressor) for regression\\nmodels. You can use these classes without understanding how\\nSVMs work, but you’ll get more out of them if you do\\nunderstand how they work. It’s also important to know how to\\ntune SVMs for individual datasets and how to prepare data\\nbefore you train a model. Toward the end of this chapter,\\nwe’ll build an SVM that performs facial recognition. But\\nfirst, let’s look behind the scenes and discover why SVMs\\nare often the go-to mechanism for modeling real-world\\ndatasets.\\nHow Support Vector Machines Work\\nFirst, why are they called support vector machines? The\\npurpose of an SVM classifier is the same as any other\\nclassifier: to find a decision boundary that cleanly\\nseparates the classes. SVMs do this by finding a line in 2D\\nspace, a plane in 3D space, or a hyperplane in higher-'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 172, 'file_type': 'pdf'}, page_content='171\\ndimensional space that allows them to distinguish between\\ndifferent classes with the greatest certainty possible. In\\nthe example in Figure\\xa05-1, there are an infinite number of\\nlines you can draw to separate the two classes, but the best\\nline is the one that produces the widest margin (the one\\nshown on the right). The width of the margin is the distance\\nbetween the points closest to the boundary in each class\\nalong a line perpendicular to the boundary. These points are\\ncalled support vectors and are circled in red.\\nFigure 5-1. Maximum-margin classification\\nOf course, real data rarely lends itself to such clean\\nseparation. Overlap between classes inevitably prevents a\\nperfect fit. To accommodate this, SVMs support a\\nregularization parameter usually referred to as C that can be\\nadjusted to loosen or tighten the fit. Lower values of C\\nproduce a wider margin with more errors on either side of the\\ndecision boundary, as shown in Figure\\xa05-2. Higher values\\nyield a tighter fit to the training data with a\\ncorrespondingly thinner margin and fewer errors. If C is too\\nhigh, the model might not generalize well. The optimum value\\nvaries by dataset. Data scientists typically try different\\nvalues of C to determine which one performs the best against\\ntest data.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 173, 'file_type': 'pdf'}, page_content='172\\nAll of the aforementioned is true, but none of it explains\\nwhy SVMs are so good at what they do. SVMs aren’t the only\\nmodels that mathematically look for boundaries separating the\\nclasses. What makes SVMs special are kernels, some of which\\nadd dimensions to data to find boundaries that don’t exist\\nat lower dimensions. Consider Figure\\xa05-3. You can’t draw a\\nline that completely separates the red dots from the purple\\ndots. But if you add a third dimension as shown on the right\\n—a z dimension whose value is based on a point’s distance\\nfrom the center—then you can slide a plane between the\\npurples and the reds and achieve 100% separation. In this\\nexample, data that isn’t linearly separable in two\\ndimensions is linearly separable in three dimensions. The\\nprinciple at work is Cover’s theorem, which states that data\\nthat isn’t linearly separable might be linearly separable if\\nprojected into higher-dimensional space using a nonlinear\\ntransform.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 174, 'file_type': 'pdf'}, page_content='173\\nFigure 5-2. Effect of C on margin width\\nFigure 5-3. Adding dimensions to achieve linear separability'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 175, 'file_type': 'pdf'}, page_content='174\\nThe kernel transformation used in this example, which\\nprojects two-dimensional data to three dimensions by adding a\\nz to every x and y, works well with this particular dataset.\\nBut for SVMs to be broadly useful, you need a kernel that\\nisn’t tied to the shape of a specific dataset.\\nKernels\\nScikit-Learn has several general-purpose kernels built in,\\nincluding the linear kernel, the RBF kernel,1 the polynomial\\nkernel, and the sigmoid kernel. The linear kernel doesn’t\\nadd dimensions. It works well with data that is linearly\\nseparable out of the box, but it doesn’t perform very well\\nwith data that isn’t. Applying it to the problem in\\nFigure\\xa05-3 produces the decision boundary on the left in\\nFigure\\xa05-4. Applying the RBF kernel to the same data\\nproduces the decision boundary on the right. The RBF kernel\\nprojects the x and y values into a higher-dimensional space\\nand finds a hyperplane that cleanly separates the purples\\nfrom the reds. When projected back to two dimensions, the\\ndecision boundary roughly forms a circle. Similar results can\\nbe achieved on this dataset with a properly tuned polynomial\\nkernel, but generally speaking, the RBF kernel can find\\ndecision boundaries in nonlinear data that the pol\\u2060ynomial\\nkernel cannot. That’s why RBF is the default kernel type in\\nScikit if you don’t specify otherwise.\\nA logical question to ask is, did the RBF kernel add a z to\\nevery x and y? The short answer is no. It effectively\\nprojected the data points into a space with an infinite\\nnumber of dimensions. The key word is effectively. Kernels\\nuse mathematical shortcuts called kernel tricks to measure\\nthe effect of adding new dimensions without actually\\ncomputing values for them. This is where the math for SVMs\\ngets hairy. Kernels are carefully designed to compute the dot\\nproduct between two n-dimensional vectors in m-dimensional\\nspace (where m is greater than n and can even be infinite)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 176, 'file_type': 'pdf'}, page_content='175\\nwithout generating all those new dimensions, and ultimately,\\nthe dot products are all an SVM needs to compute a decision\\nboundary. It’s the mathematical equivalent of having your\\ncake and eating it too, and it’s the secret sauce that makes\\nSVMs awesome. SVMs can take a long time to train on large\\ndatasets, but one of the benefits of an SVM is that it tends\\nto do better on smaller datasets with fewer rows or samples\\nthan other learning algorithms.\\nFigure 5-4. Linear kernel versus RBF kernel\\nKernel Tricks\\nWant to see an example of how kernel tricks are used to\\ncompute dot products in high-dimensional spaces without\\ncomputing values for the new dimensions? The following\\nexplanation is completely optional. But if you, like me,\\nlearn better from concrete examples, then you might find this\\nsection helpful.\\nLet’s start with the two-dimensional circular dataset\\npresented earlier, but this time let’s project it into\\nthree-dimensional space with the following equations:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 177, 'file_type': 'pdf'}, page_content='176\\n\\n\\nIn other words, we’ll compute x and y in three-dimensional\\nspace (x′ and y′\\u200a) by squaring x and y in two-dimensional\\nspace, and we’ll add a z that’s the product of the original\\nx and y and the square root of 2. Projecting the data this\\nway produces a clean separation between purples and reds, as\\nshown in Figure\\xa05-5.\\nFigure 5-5. Projecting 2D points to 3D to separate two classes\\nThe efficacy of SVMs depends on their ability to compute the\\ndot product of two vectors (or points, which can be treated\\nas vectors) in higher-dimensional space without projecting\\nthem into that space—that is, using only the values in the\\noriginal space. Let’s manufacture a couple of points to work\\nwith:\\n\\nWe can compute the dot product of these two points this way:\\nOf course, the dot product in two dimensions isn’t very\\nhelpful. An SVM needs the dot product of these points in 3D'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 178, 'file_type': 'pdf'}, page_content=\"177\\nspace. Let’s use the preceding equations to project a and b\\nto 3D, and then compute the dot product of the result:\\n\\n\\nWe now have the dot product of a pair of 2D points in 3D\\nspace, but we had to generate coordinates in 3D space to get\\nit. Here’s where it gets interesting. The following\\nfunction, or kernel trick, produces the same result using\\nonly the values in the original 2D space:\\n⟨a, b⟩ is simply the dot product of a and b, so ⟨a, b⟩2 is the\\nsquare of the dot product of a and b. We already know how to\\ncompute the dot product of a and b. Therefore:\\nThis agrees with the result computed by explicitly projecting\\nthe points, but with no projection required. That’s the\\nkernel trick in a nutshell. It saves time and memory when\\ngoing from two dimensions to three. Just imagine the savings\\nwhen projecting to an infinite number of dimensions—which,\\nyou’ll recall, is exactly what the RBF kernel does.\\nThe kernel trick used here wasn’t manufactured from thin\\nair. It happens to be the one used by a degree-2 polynomial\\nkernel. With Scikit, you can fit an SVM classifier with a\\ndegree-2 polynomial kernel to a dataset this way:\\n\\nmodel\\n=\\nSVC(kernel='poly',\\ndegree=2)\\nmodel.fit(x,\\ny)\\nIf you apply this to the preceding circular dataset and plot\\nthe decision boundary (Figure\\xa05-6, right), the result is\\nalmost identical to the one generated by the RBF kernel.\\nInterestingly, a degree-1 polynomial kernel (Figure\\xa05-6,\\nleft) produces the same decision boundary as the linear\\nkernel since a line is just a first-degree polynomial.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 179, 'file_type': 'pdf'}, page_content='178\\nKernel tricks are special. Each one is designed to simulate a\\nspecific projection into higher dimensions. Scikit gives you\\na handful of kernels to work with, but there are others that\\nScikit doesn’t build in. You can extend Scikit with kernels\\nof your own, but the ones that it provides are sufficient for\\nthe vast majority of use cases.\\nFigure 5-6. Degree-1 versus degree-2 polynomial kernel\\nHyperparameter Tuning\\nAt the outset, it’s difficult to know which of the built-in\\nkernels will produce the most accurate model. It’s also\\ndifficult to know what the right value of C is—that is, the\\nvalue that provides the best balance between underfitting and\\noverfitting the training data and yields the best results\\nwhen the model is run with test data. For the RBF and\\npolynomial kernels, there’s a third value called gamma that\\naffects accuracy. And for polynomial kernels, the degree\\nparameter impacts the model’s ability to learn from the\\ntraining data.\\nThe C parameter controls how aggressively the model fits to\\nthe training data. The higher the value, the tighter the fit'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 180, 'file_type': 'pdf'}, page_content='179\\nand the higher the risk of overfitting. Figure\\xa05-7 shows how\\nthe RBF kernel fits a model to a set of training data\\ncontaining three classes with different values of C. The\\ndefault is C=1 in Scikit, but you can specify a different\\nvalue to adjust the fit. You can see the danger of\\noverfitting in the lower-right diagram. A point that lies to\\nthe extreme right would be classified as a blue, even though\\nit probably belongs to the yellow or brown class.\\nUnderfitting is a problem too. In the upper-left example,\\nvirtually any data point that isn’t a brown will be\\nclassified as a blue.\\nFigure 5-7. Effect of C on the RBF kernel\\nAn SVM that uses the RBF kernel isn’t properly tuned until\\nyou have the right value for gamma too. gamma controls how\\nfar the influence of a single data point reaches in computing\\ndecision boundaries. Lower values use more points and produce\\nsmoother decision boundaries; higher values involve fewer'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 181, 'file_type': 'pdf'}, page_content='180\\npoints and fit more tightly to the training data. This is\\nillustrated in Figure\\xa05-8, where increasing gamma while\\nholding C constant closes the decision boundary more tightly\\naround clusters of classes. gamma can be any nonzero positive\\nvalue, but values between 0 and 1 are the most common. Rather\\nthan hardcode a default value for gamma, Scikit picks a\\ndefault value algorithmically if you don’t specify one.\\nIn practice, data scientists experiment with different\\nkernels and different parameter values to find the\\ncombination that produces the most accurate model, a process\\nknown as hyperparameter tuning. The usefulness of\\nhyperparameter tuning isn’t unique to SVMs, but you can\\nalmost always make an SVM more accurate by finding the\\noptimum combination of kernel type, C, and gamma (and for\\npolynomial kernels, degree).\\nFigure 5-8. Effect of gamma on the RBF kernel'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 182, 'file_type': 'pdf'}, page_content=\"181\\nTo aid in the process of hyperparameter tuning, Scikit\\nprovides a family of optimizers that includes GridSearchCV,\\nwhich tries all combinations of a specified set of parameter\\nvalues with built-in cross-validation to determine which\\ncombination produces the most accurate model. These\\noptimizers prevent you from having to write code to do a\\nbrute-force search using all the unique combinations of\\nparameter values. To be clear, they do brute-force searches\\nthemselves by training the model multiple times, each time\\nwith a different combination of values. At the end, you can\\nretrieve the most accurate model from the best_estimator_\\nattribute, the parameter values that produced the most\\naccurate model from the best_params_ attribute, and the best\\nscore from the best_score_ attribute.\\nHere’s an example that uses Scikit’s SVC class to implement\\nan SVM classifier. For starters, you can create an SVM\\nclassifier that uses default parameter values and fit it to a\\ndataset with two lines of code:\\n\\nmodel\\n=\\nSVC()\\nmodel.fit(x,\\ny)\\nThis uses the RBF kernel with C=1. You can specify the kernel\\ntype and values for C and gamma this way:\\n\\nmodel\\n=\\nSVC(kernel='poly',\\nC=10,\\ngamma=0.1)\\nmodel.fit(x,\\ny)\\nSuppose you wanted to try two different kernels and five\\nvalues each for C and gamma to see which combination produces\\nthe best results. Rather than write a nested for loop, you\\ncould do this:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 183, 'file_type': 'pdf'}, page_content=\"182\\n\\nmodel\\n=\\nSVC()\\n\\ngrid\\n=\\n{\\n\\xa0\\xa0\\xa0 'C':\\n[0.01,\\n0.1,\\n1,\\n10,\\n100],\\n\\xa0\\xa0\\xa0 'gamma':\\n[0.01,\\n0.25,\\n0.5,\\n0.75,\\n1.0],\\n\\xa0\\xa0\\xa0 'kernel':\\n['rbf',\\n'poly']\\n}\\n\\ngrid_search\\n=\\nGridSearchCV(estimator=model,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameter combinations\\nThe call to fit won’t return for a while. It trains the\\nmodel 250 times since there are 50 different combinations of\\nkernel, C, and gamma, and cv=5 says to use fivefold cross-\\nvalidation to assess the results. Once training is complete,\\nyou retrieve the best model this way:\\n\\nbest_model\\n=\\ngrid_search.best_estimator_\\nIt is not uncommon to run a search regimen such as this one\\nmultiple times—the first time with course parameter values,\\nand each time thereafter with narrower ranges of values\\ncentered on the values obtained from best_params_. More\\ntraining time up front is the price you pay for an accurate\\nmodel. To reiterate, you can almost always make an SVM more\\naccurate by finding the optimum combination of parameters.\\nAnd for better or worse, brute force is the most effective\\nway to identify the best combination.\\nNOTE\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 184, 'file_type': 'pdf'}, page_content='183\\nOne nuance to be aware of regarding the SVC class is that it\\ndoesn’t compute probabilities by default. If you want to call\\npredict_proba on an SVC instance, you must set probability to True\\nwhen creating the instance:\\n\\nmodel\\n=\\nSVC(probability=True)\\nThe model will train more slowly, but you’ll be able to retrieve\\nprobabilities as well as predictions. Furthermore, the Scikit\\ndocumentation warns that “predict_proba may be inconsistent with\\npredict.” For more information, see Section 1.4.1.2 in the\\ndocumentation.\\nData Normalization\\nIn Chapter\\xa02, I noted that some learning algorithms work\\nbetter with normalized data. Unnormalized data contains\\ncolumns of numbers with vastly different ranges—for example,\\nvalues from 0 to 1 in one column and from 0 to 1,000,000 in\\nanother. SVM is a parametric learning algorithm. Training\\nwith normalized data is important because SVMs use distances\\nto compute margins. If one dimension spans much larger\\ndistances than another, the internal algorithm used to find\\nthe maximum margins might have trouble converging on a\\nsolution.\\nThe importance of training machine learning models with\\nnormalized data isn’t limited to SVMs. Decision trees and\\nlearning algorithms such as random forests and gradient-\\nboosted decision trees that rely on decision trees are\\nnonparametric, so they work equally well with normalized and\\nunnormalized data. They are the exception, however. Most'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 185, 'file_type': 'pdf'}, page_content='184\\nother learning algorithms benefit to one degree or another\\nfrom normalized data. That includes k-nearest neighbors,\\nwhich although nonparametric uses distance-based calculations\\ninternally to discriminate between classes.\\nScikit offers several classes for normalizing data. The most\\ncommonly used are MinMaxScaler and StandardScaler. The former\\nnormalizes data by proportionally reducing the values in each\\ncolumn to values from 0.0 to 1.0. Mathematically, it’s\\nsimple. For each column in a dataset, MinMaxScaler subtracts\\nthe minimum value in that column from all the column’s\\nvalues, then it divides each value by the difference between\\nthe minimum and maximum values. In the resulting column, the\\nminimum value is 0.0 and the maximum is 1.0.\\nTo demonstrate, I extracted subsets of two columns with\\nvastly different ranges from the breast cancer dataset built\\ninto Scikit. Each column contains 100 values. Here are the\\nfirst 10 rows:\\n\\n[[1.001e+03 3.001e-01]\\n[1.326e+03 8.690e-02]\\n[1.203e+03 1.974e-01]\\n[3.861e+02 2.414e-01]\\n[1.297e+03 1.980e-01]\\n[4.771e+02 1.578e-01]\\n[1.040e+03 1.127e-01]\\n[5.779e+02 9.366e-02]\\n[5.198e+02 1.859e-01]\\n[4.759e+02 2.273e-01]]\\nThe values in the first column range from 201.9 to 1,878.0;\\nthe values in the second column range from 0.000692 to\\n0.3754. Figure\\xa05-9 shows how the data looks if plotted with\\nthe x- and y-axis equally scaled. Because the values in the\\nfirst column are much larger than the values in the second,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 186, 'file_type': 'pdf'}, page_content='185\\nthe data points appear to form a line. If you adjust the\\nscale of the axes to match the ranges of values in each\\ncolumn, you get a completely different picture (Figure\\xa05-\\n10).\\nFigure 5-9. Unnormalized data plotted with equally scaled axes'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 187, 'file_type': 'pdf'}, page_content='186\\nFigure 5-10. Unnormalized data plotted with proportionally scaled axes\\nData that is this highly unnormalized can pose a problem for\\nparametric learning algorithms. One way to address that is to\\napply MinMaxScaler to the data:\\n\\nfrom\\nsklearn.preprocessing\\nimport\\nMinMaxScaler\\n\\nscaler\\n=\\nMinMaxScaler()\\nnormalized_data\\n=\\nscaler.fit_transform(data)\\nHere are the first 10 rows after min-max normalization:\\n\\n[[0.47676153 0.79904352]\\n[0.67066404 0.23006715]\\n[0.5972794\\xa0 0.52496344]'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 188, 'file_type': 'pdf'}, page_content='187\\n[0.10989798 0.64238821]\\n[0.65336197 0.52656469]\\n[0.16419068 0.41928115]\\n[0.50002983 0.29892076]\\n[0.22433029 0.24810786]\\n[0.18966649 0.49427287]\\n[0.16347473 0.60475891]]\\nFigure\\xa05-11 shows a plot of the normalized data with equal\\naxes. The shape of the data didn’t change. What did change\\nis that both columns now contain values ranging from 0.0 to\\n1.0.\\nFigure 5-11. Data normalized with MinMaxScaler\\nSVMs almost always train better with normalized data, but the\\nsimple normalization performed by MinMaxScaler sometimes\\nisn’t enough. SVMs tend to respond better to data that is\\nnormalized to unit variance using a technique called'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 189, 'file_type': 'pdf'}, page_content='188\\nstandardization or Z-score normalization. Unit variance is\\nachieved by doing the following to each column in a dataset:\\n\\xa0\\nComputing the mean and standard deviations of all\\nthe values in the column\\nSubtracting the mean from each value in the\\ncolumn\\nDividing each value in the column by the standard\\ndeviation\\nThis is precisely the transform that Scikit’s StandardScaler\\nclass performs on a dataset. Applying unit variance to a\\ndataset is as simple as this:\\n\\nfrom\\nsklearn.preprocessing\\nimport\\nStandardScaler\\n\\nscaler\\n=\\nStandardScaler()\\nnormalized_data\\n=\\nscaler.fit_transform(data)\\nThe values in the original dataset may vary wildly from one\\ncolumn to the next, but the transformed dataset will contain\\ncolumns of numbers anchored around 0 with ranges that are\\nproportional to each column’s standard deviation. Applying\\nStandardScaler to the dataset produces the following values\\nin the first 10 rows:\\n\\n[[ 0.93457642\\xa0 2.36212718]\\n[ 1.95483237 -0.35495682]\\n[ 1.56870474\\xa0 1.05328794]\\n[-0.99574783\\xa0 1.61403698]\\n[ 1.86379415\\xa0 1.06093451]\\n[-0.71007617\\xa0 0.5486138 ]'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 190, 'file_type': 'pdf'}, page_content='189\\n[ 1.05700714 -0.02615397]\\n[-0.39363986 -0.26880538]\\n[-0.57603023\\xa0 0.90672853]\\n[-0.71384326\\xa0 1.4343424 ]]\\nAnd it produces the distribution shown in Figure\\xa05-12. Once\\nmore, the shape of the data didn’t change, but the values\\nthat define that shape changed substantially.\\nSVMs typically perform best when trained with standardized\\ndata, even if all the columns have similar ranges. (The same\\nis true of neural networks, by the way.) The classic case in\\nwhich columns have similar ranges but benefit from\\nnormalization anyway is image data, where each column holds\\npixel values from 0 to 255. There are exceptions, but it is\\nusually a mistake to throw a bunch of data at an SVM without\\nunderstanding the distribution of the data—specifically,\\nwhether it has unit variance.\\nFigure 5-12. Data normalized with StandardScaler'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 191, 'file_type': 'pdf'}, page_content='190\\nPipelining\\nIf you normalize or standardize the values used to train a\\nmachine learning model, you must apply the same transform to\\nvalues input to the model’s predict method. In other words,\\nif you train a model this way:\\n\\nmodel\\n=\\nSVC()\\nscaler\\n=\\nStandardScaler()\\nx\\n=\\nscaler.fit_transform(x)\\nmodel.fit(x,\\ny)\\nyou make predictions with it this way:\\n\\ninput\\n=\\n[0,\\n1,\\n2,\\n3,\\n4]\\nmodel.predict([scaler.transform([input])\\nOtherwise, you’ll get nonsensical predictions.\\nTo simplify your code and make it harder to forget to\\ntransform training data and prediction data the same way,\\nScikit offers the make_pipeline function. make_pipeline lets\\nyou combine predictive models—what Scikit calls estimators,\\nor instances of classes such as SVC—with transforms applied\\nto data input to those models. Here’s how you use\\nmake_pipeline to ensure that any data input to the model is\\ntransformed with StandardScaler:\\n\\n# Train the model\\npipe\\n=\\nmake_pipeline(StandardScaler(),\\nSVC())\\npipe.fit(x,\\ny)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 192, 'file_type': 'pdf'}, page_content=\"191\\n\\n# Make a prediction with the model\\ninput\\n=\\n[0,\\n1,\\n2,\\n3,\\n4]\\npipe.predict([input])\\nNow data used to train the model has StandardScaler applied\\nto it, and data input to make predictions is transformed the\\nsame way.\\nWhat if you wanted to use GridSearchCV to find the optimum\\nset of parameters for a pipeline that combines a data\\ntransform and estimator? It’s not hard, but there’s a trick\\nyou need to know about. It involves using class names\\nprefaced with double underscores in the param_grid dictionary\\npassed to GridSearchCV. Here’s an example:\\n\\npipe\\n=\\nmake_pipeline(StandardScaler(),\\nSVC())\\n\\ngrid\\n=\\n{\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.01,\\n0.1,\\n1,\\n10,\\n100],\\n\\xa0\\xa0\\xa0 'svc__gamma':\\n[0.01,\\n0.25,\\n0.5,\\n0.75,\\n1.0],\\n\\xa0\\xa0\\xa0 'svc__kernel':\\n['rbf',\\n'poly']\\n}\\n\\ngrid_search\\n=\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameter combinations\\nThis example trains the model 250 times to find the best\\ncombination of kernel, C, and gamma for the SVC instance in\\nthe pipeline. Note the “svc__” nomenclature, which maps to\\nthe SVC instance passed to the make_pipeline function.\\nUsing SVMs for Facial Recognition\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 193, 'file_type': 'pdf'}, page_content='192\\nModern facial recognition is often accomplished with neural\\nnetworks, but support vector machines can do a credible job\\ntoo. Let’s demonstrate by building a model that recognizes\\nfaces. The dataset we’ll use is the Labeled Faces in the\\nWild (LFW) dataset, which contains more than 13,000 facial\\nimages of famous people collected from around the web and is\\nbuilt into Scikit as a sample dataset. Of the more than 5,000\\npeople represented in the dataset, 1,680 have two or more\\nfacial images, while only five have 100 or more. We’ll set\\nthe minimum number of faces per person to 100, which means\\nthat five sets of faces corresponding to five famous people\\nwill be imported. Each facial image is labeled with the name\\nof the person the face belongs to.\\nStart by creating a new Jupyter notebook and using the\\nfollowing statements to load the dataset:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\n=\\nfetch_lfw_people(min_faces_per_person=100)\\nprint(faces.target_names)\\nprint(faces.images.shape)\\nIn total, 1,140 facial images were loaded. Each image\\nmeasures 47 × 62 pixels for a total of 2,914 pixels per\\nimage. That means the dataset contains 2,914 features. Use\\nthe following code to show the first 24 images in the dataset\\nand the people to whom the faces belong:\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 194, 'file_type': 'pdf'}, page_content=\"193\\nfig,\\nax\\n=\\nplt.subplots(3,\\n8,\\nfigsize=(18,\\n10))\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nHere is the output:\\nCheck the balance in the dataset by generating a histogram\\nshowing how many facial images were imported for each person:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nfrom\\ncollections\\nimport\\nCounter\\ncounts\\n=\\nCounter(faces.target)\\nnames\\n=\\n{}\\n\\nfor\\nkey\\nin\\ncounts.keys():\\n\\xa0\\xa0\\xa0 names[faces.target_names[key]]\\n=\\ncounts[key]\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 195, 'file_type': 'pdf'}, page_content=\"194\\ndf\\n=\\npd.DataFrame.from_dict(names,\\norient='index')\\ndf.plot(kind='bar')\\nThe output reveals that there are far more images of George\\nW. Bush than of anyone else in the dataset:\\nClassification models are best trained with balanced\\ndatasets. Use the following code to reduce the dataset to 100\\nimages of each person:\\n\\nmask\\n=\\nnp.zeros(faces.target.shape,\\ndtype=bool)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 196, 'file_type': 'pdf'}, page_content=\"195\\n\\nfor\\ntarget\\nin\\nnp.unique(faces.target):\\n\\xa0\\xa0\\xa0 mask[np.where(faces.target\\n==\\ntarget)[0][:100]]\\n=\\n1\\n\\xa0\\xa0\\xa0\\xa0 \\nx\\n=\\nfaces.data[mask]\\ny\\n=\\nfaces.target[mask]\\nx.shape\\nNote that x contains 500 facial images and y contains the\\nlabels that go with them: 0 for Colin Powell, 1 for Donald\\nRumsfeld, and so on. Now let’s see if an SVM can make sense\\nof the data. We’ll train three different models: one that\\nuses a linear kernel, one that uses a polynomial kernel, and\\none that uses an RBF kernel. In each case, we’ll use\\nGridSearchCV to optimize hyperparameters. Start with a linear\\nmodel and four different values of C:\\n\\nfrom\\nsklearn.svm\\nimport\\nSVC\\nfrom\\nsklearn.model_selection\\nimport\\nGridSearchCV\\n\\nsvc\\n=\\nSVC(kernel='linear')\\n\\ngrid\\n=\\n{\\n\\xa0\\xa0\\xa0 'C':\\n[0.1,\\n1,\\n10,\\n100]\\n}\\n\\ngrid_search\\n=\\nGridSearchCV(estimator=svc,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameters\\ngrid_search.best_score_\\nThis model achieves a cross-validated accuracy of 84.4%.\\nIt’s possible that accuracy can be improved by standardizing\\nthe image data. Run the same grid search again, but this time\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 197, 'file_type': 'pdf'}, page_content=\"196\\nuse StandardScaler to apply unit variance to all the pixel\\nvalues:\\n\\nfrom\\nsklearn.pipeline\\nimport\\nmake_pipeline\\nfrom\\nsklearn.preprocessing\\nimport\\nStandardScaler\\n\\nscaler\\n=\\nStandardScaler()\\nsvc\\n=\\nSVC(kernel='linear')\\npipe\\n=\\nmake_pipeline(scaler,\\nsvc)\\n\\ngrid\\n=\\n{\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.1,\\n1,\\n10,\\n100]\\n}\\n\\ngrid_search\\n=\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\ngrid_search.best_score_\\nStandardizing the data produced an incremental improvement in\\naccuracy. What value of C produced that accuracy?\\n\\ngrid_search.best_params_\\nIs it possible that a polynomial kernel could outperform a\\nlinear kernel? There’s an easy way to find out. Note the\\nintroduction of the gamma and degree parameters to the\\nparameter grid. These parameters, along with C, can greatly\\ninfluence a polynomial kernel’s ability to fit to the\\ntraining data:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 198, 'file_type': 'pdf'}, page_content=\"197\\n\\nscaler\\n=\\nStandardScaler()\\nsvc\\n=\\nSVC(kernel='poly')\\npipe\\n=\\nmake_pipeline(scaler,\\nsvc)\\n\\ngrid\\n=\\n{\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.1,\\n1,\\n10,\\n100],\\n\\xa0\\xa0\\xa0 'svc__gamma':\\n[0.01,\\n0.25,\\n0.5,\\n0.75,\\n1],\\n\\xa0\\xa0\\xa0 'svc__degree':\\n[1,\\n2,\\n3,\\n4,\\n5]\\n}\\n\\ngrid_search\\n=\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameter combinations\\ngrid_search.best_score_\\nThe polynomial kernel achieved the same accuracy as the\\nlinear kernel. What parameter values led to this result?\\n\\ngrid_search.best_params_\\nThe best_params_ attribute reveals that the optimum value of\\ndegree was 1, which means the polynomial kernel acted like a\\nlinear kernel. It’s not surprising, then, that it achieved\\nthe same accuracy. Could an RBF kernel do better?\\n\\nscaler\\n=\\nStandardScaler()\\nsvc\\n=\\nSVC(kernel='rbf')\\npipe\\n=\\nmake_pipeline(scaler,\\nsvc)\\n\\ngrid\\n=\\n{\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.1,\\n1,\\n10,\\n100],\\n\\xa0\\xa0\\xa0 'svc__gamma':\\n[0.01,\\n0.25,\\n0.5,\\n0.75,\\n1.0]\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 199, 'file_type': 'pdf'}, page_content='198\\n}\\n\\ngrid_search\\n=\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\ngrid_search.best_score_\\nThe RBF kernel didn’t perform as well as the linear and\\npolynomial kernels. There’s a lesson here. The RBF kernel\\noften fits to nonlinear data better than other kernels, but\\nit doesn’t always fit better. That’s why the best strategy\\nwith an SVM is to try different kernels with different\\nparameter values. The best combination will vary from dataset\\nto dataset. For the LFW dataset, it seems that a linear\\nkernel is best. That’s convenient, because the linear kernel\\nis the fastest of all the kernels Scikit provides.\\nNOTE\\nIn addition to the SVC class, Scikit-Learn includes SVM\\nclassifiers named LinearSVC and NuSVC. The latter supports the same\\nassortment of kernels as the SVC class, but it replaces C with a\\nregularization parameter called nu that controls tightness of fit\\ndifferently. NuSVC doesn’t scale as well as SVC to large datasets,\\nand in my experience it is rarely used. LinearSVC implements the\\nlinear kernel only, but it uses a different optimization algorithm\\nthat trains faster. If training is slow with SVC and you determine\\nthat a linear kernel yields the best model, consider swapping SVC\\nfor LinearSVC. Faster training times make a difference even for\\nmodestly sized datasets if you’re using GridSearchCV to train a\\nmodel hundreds of times. For a great summary of the functional\\ndifferences between the two classes, see the article “SVM with\\nScikit-Learn: What You Should Know” by Angela Shi.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 200, 'file_type': 'pdf'}, page_content=\"199\\nConfusion matrices are a great way to visualize a model’s\\naccuracy. Let’s split the dataset, train an optimized linear\\nmodel with 80% of the images, test it with the remaining 20%,\\nand show the results in a confusion matrix.\\nThe first step is to split the dataset. Note the stratify=y\\nparameter, which ensures that the training dataset and the\\ntest dataset have the same proportion of samples of each\\nclass as the original dataset. In this example, the training\\ndataset will contain 20 samples of each of the five people:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\ntrain_size=0.8,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y,\\nrandom_state=0)\\nNow train a linear SVM with the optimum C value revealed by\\nthe grid search:\\n\\nscaler\\n=\\nStandardScaler()\\nsvc\\n=\\nSVC(kernel='linear',\\nC=0.1)\\npipe\\n=\\nmake_pipeline(scaler,\\nsvc)\\npipe.fit(x_train,\\ny_train)\\nCross-validate the model to confirm its accuracy:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ncross_val_score\\n\\ncross_val_score(pipe,\\nx,\\ny,\\ncv=5).mean()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 201, 'file_type': 'pdf'}, page_content=\"200\\nUse a confusion matrix to see how the model performs against\\nthe test data:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nfig,\\nax\\n=\\nplt.subplots(figsize=(6,\\n6))\\nax.grid(False)\\ncmd.from_estimator(pipe,\\nx_test,\\ny_test,\\ndisplay_labels=faces.target_names,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 202, 'file_type': 'pdf'}, page_content='201\\nThe model correctly identified Colin Powell 19 times out of\\n20, Donald Rumsfeld 20 times out of 20, and so on. That’s\\nnot bad. And it’s a great example of support vector machines\\nat work.\\nSummary\\nSupport vector machines, or SVMs, frequently fit to datasets\\nbetter than other learning algorithms. SVMs are maximum-\\nmargin classifiers that use kernel tricks to simulate adding\\ndimensions to data. The theory is that data that isn’t\\nlinearly separable in m dimensions might be separable in n'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 203, 'file_type': 'pdf'}, page_content='202\\ndimensions if n is higher than m. SVMs are most often used\\nfor classification, but they can perform regression too. As\\nan experiment, try replacing GradientBoostingRegressor in the\\ntaxi-fare example in Chapter\\xa02 with SVR and using\\nGridSearchCV to optimize the model’s hyperparameters. Which\\nmodel produces the highest cross-validated coefficient of\\ndetermination?\\nSVMs usually train better with data that is normalized to\\nunit variance. That’s true even if the values in all the\\ncolumns have similar ranges, but it’s especially true if\\nthey don’t have similar ranges. Scikit’s StandardScaler\\nclass applies unit variance to data. Unit variance is\\nachieved by dividing the values in a column by the mean of\\nall the values in the column and dividing by the standard\\ndeviation. Scikit’s make_pipeline function enables you to\\ncombine transformers such as StandardScaler and classifiers\\nsuch as SVC into one logical unit to ensure that data passed\\nto fit, predict, and predict_proba undergoes the same\\ntransformations.\\nSVMs require tuning in order to achieve optimum accuracy.\\nTuning means finding the right values for parameters such as\\nC, gamma, and kernel, and it entails trying different\\nparameter combinations and assessing the results. Scikit\\nprovides classes such as GridSearchCV to help, but they\\nincrease training time by training the model once for each\\nunique combination of parameter values.\\nSVMs can seem magical in their ability to fit mathematical\\nmodels to complex datasets. But in my view, that magic takes\\na back seat to the numerical gymnastics performed by\\nprincipal component analysis (PCA), which solves a variety of\\nproblems routinely encountered in machine learning. I often\\nintroduce PCA by telling audiences that it’s the best-kept\\nsecret in machine learning. After Chapter\\xa06, it will be a\\nsecret no longer.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 204, 'file_type': 'pdf'}, page_content='RBF is short for radial basis function\\n\\n203'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 205, 'file_type': 'pdf'}, page_content='204\\nChapter 6. Principal Component\\nAnalysis\\nPrincipal component analysis, or PCA, is one of the minor\\nmiracles of machine learning. It’s a dimensionality\\nreduction technique that reduces the number of dimensions in\\na dataset without sacrificing a commensurate amount of\\ninformation. While that might seem underwhelming on the face\\nof it, it has profound implications for engineers and\\nsoftware developers working to build predictive models from\\ntheir data.\\nWhat if I told you that you could take a dataset with 1,000\\ncolumns, use PCA to reduce it to 100 columns, and retain 90%\\nor more of the information in the original dataset? That’s\\nrelatively common, believe it or not. And it lends itself to\\na variety of practical uses, including:\\n\\xa0\\nReducing high-dimensional data to two or three\\ndimensions so that it can be plotted and explored\\nReducing the number of dimensions in a dataset\\nand then restoring the original number of\\ndimensions, which finds application in anomaly\\ndetection and noise filtering\\nAnonymizing datasets so that they can be shared\\nwith others without revealing the nature or\\nmeaning of the data\\nAnd that’s not all. A side effect of applying PCA to a\\ndataset is that less important features—columns of data that\\nhave less relevance to the outcome of a predictive model—are\\nremoved, while dependencies between columns is eliminated.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 206, 'file_type': 'pdf'}, page_content='205\\nAnd in datasets with a low ratio of samples (rows) to\\nfeatures (columns), PCA can be used to increase that ratio.\\nAs a rule of thumb, you typically want a dataset used for\\nmachine learning to have at least five times as many rows as\\nit has columns. If you can’t add rows, an alternative is to\\nuse PCA to shave columns.\\nOnce you learn about PCA, you’ll wonder how you lived\\nwithout it. Let’s take a few moments to understand what it\\nis and how it works. Then we’ll look at some examples\\ndemonstrating why it’s such an indispensable tool.\\nUnderstanding Principal Component Analysis\\nOne way to wrap your head around PCA is to see how it reduces\\na two-dimensional dataset to one dimension. Figure\\xa06-1\\ndepicts a 2D dataset comprising a somewhat random collection\\nof x and y values. If you reduced this dataset to a single\\ndimension by simply dropping the x column or the y column,\\nyou’d be left with a horizontal or vertical line that bears\\nlittle resemblance to the original dataset.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 207, 'file_type': 'pdf'}, page_content='206\\nFigure 6-1. Two-dimensional dataset\\nFigure\\xa06-2 adds arrows representing the dataset’s two\\nprincipal components. Essentially, the coordinate system has\\nbeen transformed so that one axis (the longer of the two\\narrows) captures most of the variance in the dataset. This is\\nthe dataset’s primary principal component. The other axis\\ncontains a narrower range of values and represents the\\nsecondary principal component. The number of principal\\ncomponents equals the number of dimensions in a dataset, so\\nin this example, there are two principal components.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 208, 'file_type': 'pdf'}, page_content='207\\nFigure 6-2. Arrows depicting the principal components of a two-dimensional\\ndataset\\nTo reduce a two-dimensional dataset to one dimension, PCA\\nfinds the two principal components and eliminates the one\\nwith less variance. This effectively projects the data points\\nonto the primary principal component axis, as shown in\\nFigure\\xa06-3. The red data points don’t retain all of the\\ninformation in the original dataset, but they contain most of\\nit. In this example, the PCAed dataset retains more than 95%\\nof the information in the original. PCA reduced the number of\\ndimensions by 50%, but it sacrificed less than 5% of the\\nmeaningful information in the dataset. That’s the gist of\\nPCA: reducing the number of dimensions without incurring a\\ncommensurate loss of information.\\nUnder the hood, PCA works its magic by building a covariance\\nmatrix that quantifies the variance of each dimension with\\nrespect to the others, and from the matrix computing\\neigenvectors and eigenvalues that identify the dataset’s\\nprincipal components. If you’d like to dig deeper, I suggest'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 209, 'file_type': 'pdf'}, page_content='208\\nreading “A Step-by-Step Explanation of Principal Component\\nAnalysis (PCA)” by Zakaria Jaadi. The good news is that you\\ndon’t have to understand the math to make PCA work, because\\nScikit-Learn’s PCA class does the math for you. The\\nfollowing statements reduce the dataset x to five dimensions,\\nregardless of the number of dimensions it originally\\ncontains:\\n\\npca\\n=\\nPCA(n_components=5)\\nx\\n=\\npca.fit_transform(x)\\nFigure 6-3. Two-dimensional dataset (blue) reduced to one dimension (red)\\nwith PCA\\nYou can also invert a PCA transform to restore the original\\nnumber of dimensions:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 210, 'file_type': 'pdf'}, page_content=\"209\\n\\nx\\n=\\npca.inverse_transform(x)\\nThe inverse_transform method restores the dataset to its\\noriginal number of dimensions, but it doesn’t restore the\\noriginal dataset. The information that was discarded when the\\nPCA transform was applied will be missing from the restored\\ndataset.\\nYou can visualize the loss of information when a PCA\\ntransform is applied and then inverted using the Labeled\\nFaces in the Wild (LFW) dataset introduced in Chapter\\xa05. To\\ndemonstrate, fire up a Jupyter notebook and run the following\\ncode:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\n=\\nfetch_lfw_people(min_faces_per_person=100)\\nfig,\\nax\\n=\\nplt.subplots(3,\\n8,\\nfigsize=(18,\\n10))\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nThe output shows the first 24 images in the dataset:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 211, 'file_type': 'pdf'}, page_content=\"210\\nEach image measures 47 × 62 pixels, for a total of 2,914\\npixels per image. That means the dataset has 2,914\\ndimensions. Now use the following code to reduce the number\\nof dimensions to 150 (roughly 5% of the original number),\\nrestore the original 2,914 dimensions, and plot the restored\\nimages:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\npca\\n=\\nPCA(n_components=150,\\nrandom_state=0)\\npca_faces\\n=\\npca.fit_transform(faces.data)\\nunpca_faces\\n=\\npca.inverse_transform(pca_faces).reshape(1140,\\n62,\\n47)\\n\\nfig,\\nax\\n=\\nplt.subplots(3,\\n8,\\nfigsize=(18,\\n10))\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(unpca_faces[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 212, 'file_type': 'pdf'}, page_content='211\\nEven though you removed almost 95% of the dimensions in the\\ndataset, little meaningful information was discarded. The\\nrestored images are slightly blurrier than the originals, but\\nthe faces are still recognizable:\\nTo reiterate, you reduced the number of dimensions from 2,914\\nto 150, but because PCA found 2,914 principal components and\\nremoved the ones that are least important (the ones with the\\nleast variance), you retained the bulk of the information in\\nthe original dataset. Which begs a question: precisely how\\nmuch of the original information was retained?\\nAfter a PCA object is fit to a dataset, you can find out how\\nmuch variance is encoded in each principal component from the\\nexplained_variance_ratio_ attribute. It’s an array with one\\nelement for each principal component in the transformed\\ndataset. Here’s how it looks after the LFW dataset is\\nreduced to 150 dimensions:\\n\\narray([0.18075283,\\n0.15304269,\\n0.07271618,\\n0.05843799,\\n0.05164209,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.03046084,\\n0.02518216,\\n0.02159553,\\n0.02021552,\\n0.01913318,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 213, 'file_type': 'pdf'}, page_content='212\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.01555639,\\n0.01456686,\\n0.01256744,\\n0.01084539,\\n0.00984127,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.0093953\\n,\\n0.00916603,\\n0.00868945,\\n0.00813571,\\n0.00727695,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00677768,\\n0.00641177,\\n0.00598251,\\n0.00584093,\\n0.00560558,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00510269,\\n0.00502014,\\n0.00471686,\\n0.00459556,\\n0.00417527,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00388964,\\n0.00382498,\\n0.00369323,\\n0.00351382,\\n0.00336695,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00334725,\\n0.00322673,\\n0.00311035,\\n0.00296509,\\n0.00291989,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00284639,\\n0.00269707,\\n0.00258668,\\n0.00250783,\\n0.00242382,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.0024114\\n,\\n0.00238748,\\n0.00231979,\\n0.00227626,\\n0.00220951,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00208082,\\n0.00205139,\\n0.0020204\\n,\\n0.00193762,\\n0.00190936,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00187478,\\n0.00181047,\\n0.00179518,\\n0.0017543\\n,\\n0.0017069\\n,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00167668,\\n0.00163358,\\n0.00161024,\\n0.00157452,\\n0.00153261,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00149059,\\n0.00147311,\\n0.00146492,\\n0.00144425,\\n0.00141883,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00138223,\\n0.00134285,\\n0.00131313,\\n0.00129011,\\n0.00125677,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00124584,\\n0.00123509,\\n0.00120691,\\n0.00118097,\\n0.00116907,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00114366,\\n0.00114065,\\n0.00112442,\\n0.00109727,\\n0.00107732,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.0010587\\n,\\n0.00103661,\\n0.0010168\\n,\\n0.00101077,\\n0.00099613,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00095741,\\n0.00094375,\\n0.00093824,\\n0.00091516,\\n0.00089966,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00089489,\\n0.00088575,\\n0.00086848,\\n0.00085914,\\n0.00085092,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00081995,\\n0.00081599,\\n0.00080637,\\n0.00077363,\\n0.00076949,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00075849,\\n0.00074882,\\n0.00073402,\\n0.00073062,\\n0.00071469,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00070963,\\n0.00070081,\\n0.00069565,\\n0.00068256,\\n0.00066753,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00065527,\\n0.00065092,\\n0.00063932,\\n0.00062695,\\n0.00062127,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00060888,\\n0.0006006\\n,\\n0.00058937,\\n0.00058177,\\n0.00057769,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00056761,\\n0.00056366,\\n0.00055619,\\n0.00054269,\\n0.00053866,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00052962,\\n0.00052751,\\n0.00051337,\\n0.00050697,\\n0.00050601,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00049807,\\n0.00049578,\\n0.00048296,\\n0.00047389,\\n0.0004729\\n,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00046955,\\n0.00046098,\\n0.00045423,\\n0.00044775,\\n0.0004431\\n,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00043341,\\n0.00042795,\\n0.00042292,\\n0.00041978,\\n0.00041305],\\n\\xa0\\xa0\\xa0\\xa0\\xa0 dtype=float32)\\nThis reveals that 18% of the variance in the dataset is\\nexplained by the primary principal component, 15% is\\nexplained by the secondary principal component, and so on.\\nObserve that the numbers decrease as the index increases. By\\ndefinition, each principal component in a PCAed dataset\\ncontains more information than the principal component after'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 214, 'file_type': 'pdf'}, page_content=\"213\\nit. In this example, the 2,764 principal components that were\\ndiscarded contained so little information that their loss was\\nbarely noticeable when the transform was inverted. In fact,\\nthe sum of the 150 numbers in the preceding example is\\n0.9480211. This means reducing the dataset from 2,914\\ndimensions to 150 retained 94.8% of the information in the\\noriginal dataset. In other words, you reduced the number of\\ndimensions by almost 95%, and yet you retained almost 95% of\\nthe information in the dataset. If that’s not awesome, I\\ndon’t know what is.\\nA logical question to ask is, what is the “right” number of\\ncomponents? In other words, what number of components strikes\\nthe best balance between reducing the number of dimensions in\\nthe dataset and retaining most of the information? One way to\\nfind that number is with a scree plot, which charts the\\nproportion of explained variance for each dimension. The\\nfollowing code produces a scree plot for the PCA transform\\nused on the facial images:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nplt.plot(pca.explained_variance_ratio_)\\nplt.xlabel('Principal Component')\\nplt.ylabel('Explained Variance')\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 215, 'file_type': 'pdf'}, page_content=\"214\\nAnother way to look at it is to plot the cumulative sum of\\nthe variances as a function of component count:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\\nplt.xlabel('Number of Components')\\nplt.ylabel('Cumulative Explained Variance');\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 216, 'file_type': 'pdf'}, page_content='215\\nEither way you look at it, the bulk of the information is\\ncontained in the first 50 to 100 dimensions. Based on these\\nplots, if you reduced the number of dimensions to 50 instead\\nof 150, would you expect the restored facial images to look\\nsubstantially different? If you’re not sure, try it and see.\\nFiltering Noise\\nOne very practical use for PCA is to filter noise from data.\\nNoise is data that is random, corrupt, or otherwise\\nmeaningless, and it’s particularly likely to occur when the\\ndata comes from physical devices such as pressure sensors or\\naccelerometers. The basic approach to using PCA for noise\\nreduction is to PCA-transform the data and then invert the\\ntransform, reducing the dataset from m dimensions to n and\\nthen restoring it to m. Because PCA discards the least\\nimportant information when reducing dimensions and noise\\ntends to have little or no informational value, this ideally'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 217, 'file_type': 'pdf'}, page_content=\"216\\neliminates much of the noise while retaining most of the\\nmeaningful data.\\nYou can test this supposition with the LFW dataset. Use the\\nfollowing statements to add noise to the facial images using\\na random-number generator and plot the first 24 images:\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\nimport\\nnumpy\\nas\\nnp\\n\\nfaces\\n=\\nfetch_lfw_people(min_faces_per_person=100)\\n\\nnp.random.seed(0)\\nnoisy_faces\\n=\\nnp.random.normal(faces.data,\\n20)\\n\\nfig,\\nax\\n=\\nplt.subplots(3,\\n8,\\nfigsize=(18,\\n10))\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(noisy_faces[i].reshape(62,\\n47),\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nThe resulting facial images resemble a staticky 1960s TV\\nscreen:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 218, 'file_type': 'pdf'}, page_content='217\\nNow use PCA to reduce the number of dimensions. Rather than\\nspecify the number of dimensions (components), we’ll specify\\nthat we want to reduce the amount of information in the\\ndataset to 80%. We’ll let Scikit decide how many dimensions\\nwill remain, and then show the count:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\npca\\n=\\nPCA(0.8,\\nrandom_state=0)\\npca_faces\\n=\\npca.fit_transform(noisy_faces)\\npca.n_components_\\nPCA reduced the number of dimensions from 2,914 to 179, but\\nthe remaining dimensions contain 80% of the information in\\nthe original 2,914. Now reconstruct the facial images from\\nthe PCAed faces and show the results:\\n\\nunpca_faces\\n=\\npca.inverse_transform(pca_faces)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 219, 'file_type': 'pdf'}, page_content=\"218\\n\\nfig,\\nax\\n=\\nplt.subplots(3,\\n8,\\nfigsize=(18,\\n10))\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(unpca_faces[i].reshape(62,\\n47),\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nHere is the output:\\nThe reconstructed dataset isn’t quite as clean as the\\noriginal, but it’s clean enough that you can make out the\\nfaces in the photos.\\nAnonymizing Data\\nChapter\\xa03 demonstrated how to use various learning\\nalgorithms to build a binary classification model that\\ndetects credit card fraud. The dataset used in the example\\ncontained real credit card data that had been anonymized to\\nprotect the card holders (and the credit card company’s\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 220, 'file_type': 'pdf'}, page_content='219\\nintellectual property). The first 10 rows of that dataset are\\npictured in Figure\\xa06-4.\\nFigure 6-4. Anonymized fraud detection dataset\\nAnother practical use for PCA is to anonymize data in this\\nmanner. It’s generally a two-step process:\\n\\xa0\\n1. \\nUse PCA to “reduce” the dataset from m\\ndimensions to m, where m is the original number\\nof dimensions (as well as the number of\\ndimensions after “reduction”).\\n2. \\nNormalize the data so that it has unit variance.\\nThe second step isn’t required, but it does make the ranges\\nof values more uniform. Data anonymized this way can still be\\nused to train a machine learning model, but its original\\nmeaning can’t be inferred.\\nTry it with a dataset of your own. First, use the following\\ncode to load Scikit’s breast cancer dataset and display the\\nfirst five rows:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 221, 'file_type': 'pdf'}, page_content=\"220\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nload_breast_cancer\\n\\ndata\\n=\\nload_breast_cancer()\\ndf\\n=\\npd.DataFrame(data=data.data,\\ncolumns=data.feature_names)\\npd.set_option('display.max_columns',\\n6)\\ndf.head()\\nThe output is as follows:\\nThe dataset contains 30 columns, not counting the label\\ncolumn. Now use the following statements to find the 30\\nprincipal components and apply StandardScaler to the\\ntransformed data:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\nfrom\\nsklearn.preprocessing\\nimport\\nStandardScaler\\n\\npca\\n=\\nPCA(n_components=30,\\nrandom_state=0)\\npca_data\\n=\\npca.fit_transform(df)\\n\\nscaler\\n=\\nStandardScaler()\\nanon_df\\n=\\npd.DataFrame(scaler.fit_transform(pca_data))\\npd.set_option('display.max_columns',\\n8)\\nanon_df.head()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 222, 'file_type': 'pdf'}, page_content='221\\nThe result is as follows:\\nThe dataset is unrecognizable after the PCA transform.\\nWithout the transform, it’s impossible to work backward and\\nreconstruct the original data. Yet the sum of the\\nexplained_variance_ratio_ values is 1.0, which means no\\ninformation was lost. You can prove it this way:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nnp.sum(pca.explained_variance_ratio_)\\nThe PCAed dataset is just as useful for machine learning as\\nthe original. Furthermore, if you want to share the dataset\\nwith others so that they can train models of their own, there\\nis no risk of divulging sensitive or proprietary information.\\nVisualizing High-Dimensional Data\\nYet another use for PCA is to reduce a dataset to two or\\nthree dimensions so that it can be plotted with libraries\\nsuch as Matplotlib. You can’t plot a dataset that has 1,000\\ncolumns. You can plot a dataset that has two or three\\ncolumns. The fact that PCA can reduce high-dimensional data\\nto two or three dimensions while retaining much of the'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 223, 'file_type': 'pdf'}, page_content=\"222\\noriginal information makes it a great tool for exploring data\\nand visualizing relationships between classes.\\nSuppose you’re building a classification model and want to\\nassess up front whether there is sufficient separation\\nbetween classes to support such a model. Take the Optical\\nRecognition of Handwritten Digits dataset built into Scikit,\\nfor example. Each digit in the dataset is represented by an 8\\n× 8 array of pixel values, meaning the dataset has 64\\ndimensions. If you could plot a 64-dimensional diagram, you\\nmight be able to inspect the dataset and look for separation\\nbetween classes. But 64 dimensions is 61 too many for most\\nhumans.\\nEnter PCA. The following code loads the dataset, uses PCA to\\nreduce it to two dimensions, and plots the result, with\\ndifferent colors representing different classes (digits):\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\nfrom\\nsklearn.datasets\\nimport\\nload_digits\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndigits\\n=\\nload_digits()\\npca\\n=\\nPCA(n_components=2,\\nrandom_state=0)\\npca_digits\\n=\\npca.fit_transform(digits.data)\\n\\nplt.figure(figsize=(12,\\n8))\\nplt.scatter(pca_digits[:,\\n0],\\npca_digits[:,\\n1],\\nc=digits.target,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap=plt.cm.get_cmap('Paired',\\n10))\\nplt.colorbar(ticks=range(10))\\nplt.clim(-0.5,\\n9.5)\\nThe resulting plot provides an encouraging sign that you\\nmight be able to train a classifier with the data. While\\nthere is clearly some overlap between classes, the different\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 224, 'file_type': 'pdf'}, page_content='223\\nclasses form rather distinct clusters. There is significant\\noverlap between red (the digit 4) and light purple (the digit\\n6), indicating that a model might have some difficulty\\ndistinguishing between 4s and 6s. However, 0s and 1s lie at\\nthe top and bottom, while 3s and 4s fall on the far left and\\nfar right. A model would presumably be proficient at telling\\nthese digits apart:\\nYou can better visualize relationships between classes with a\\n3D plot. The following code uses PCA to reduce the dataset to\\nthree dimensions and Mplot3D to produce an interactive plot.\\nNote that if you run this code in Jupyter Lab, you’ll\\nprobably have to change the first line to %matplotlib widget:\\n\\n%matplotlib\\nnotebook\\nfrom\\nmpl_toolkits.mplot3d\\nimport\\nAxes3D'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 225, 'file_type': 'pdf'}, page_content=\"224\\n\\ndigits\\n=\\nload_digits()\\npca\\n=\\nPCA(n_components=3,\\nrandom_state=0)\\npca_digits\\n=\\npca.fit_transform(digits.data)\\n\\nax\\n=\\nplt.figure(figsize=(12,\\n8)).add_subplot(111,\\nprojection='3d')\\nax.scatter(xs\\n=\\npca_digits[:,\\n0],\\nys\\n=\\npca_digits[:,\\n1],\\nzs\\n=\\npca_digits[:,\\n2],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 c=digits.target,\\ncmap=plt.cm.get_cmap('Paired',\\n10))\\nYou can rotate the resulting plot in 3D and look at it from\\ndifferent angles. Here we can see that there is more\\nseparation between 4s and 6s than was evident in two\\ndimensions:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 226, 'file_type': 'pdf'}, page_content='225\\nPCA isn’t the only way to reduce a dataset to two or three\\ndimensions for plotting. You can also use Scikit’s Isomap\\nclass or its TSNE class. TSNE implements t-distributed\\nstochastic neighbor embedding, or t-SNE for short. t-SNE is a\\ndimensionality reduction algorithm that is used almost\\nexclusively for visualizing high-dimensional data. Whereas\\nPCA uses a linear function to transform data, t-SNE uses a\\nnonlinear transform that tends to heighten the separation\\nbetween classes by keeping similar data points close together\\nin low-dimensional space. (PCA, by contrast, focuses on\\nkeeping dissimilar points far apart.) Here’s an example that'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 227, 'file_type': 'pdf'}, page_content=\"226\\nplots the Digits dataset in two dimensions after reducing it\\nwith t-SNE:\\n\\n%matplotlib\\ninline\\nfrom\\nsklearn.manifold\\nimport\\nTSNE\\n\\ndigits\\n=\\nload_digits()\\ntsne\\n=\\nTSNE(n_components=2,\\ninit='pca',\\nlearning_rate='auto',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\ntsne_digits\\n=\\ntsne.fit_transform(digits.data)\\n\\nplt.figure(figsize=(12,\\n8))\\nplt.scatter(tsne_digits[:,\\n0],\\ntsne_digits[:,\\n1],\\nc=digits.target,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap=plt.cm.get_cmap('Paired',\\n10))\\nplt.colorbar(ticks=range(10))\\nplt.clim(-0.5,\\n9.5)\\nAnd here is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 228, 'file_type': 'pdf'}, page_content='227\\nt-SNE does a better job of separating groups of digits into\\nclusters, indicating there are patterns in the data that\\nmachine learning can exploit. The chief drawback is that t-\\nSNE is compute intensive, which means it can take a\\nprohibitively long time to run on large datasets. One way to\\nmitigate that is to run t-SNE on a subset of rows rather than\\nthe entire dataset. Another strategy is to use PCA to reduce\\nthe number of dimensions, and then subject the PCAed dataset\\nto t-SNE.\\nAnomaly Detection\\nAnomaly detection is a branch of machine learning that seeks\\nto identify anomalies in datasets or data streams. Airbus\\nuses it to predict failures in jet engines and detect\\nanomalies in telemetry data beamed down from the\\nInternational Space Station. Credit card companies use it to'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 229, 'file_type': 'pdf'}, page_content='228\\ndetect credit card fraud. The goal of anomaly detection is to\\nidentify outliers in data—samples that aren’t “normal”\\nwhen compared to others. In the case of credit card fraud,\\nthe assumption is that if transactions are subjected to an\\nanomaly detection algorithm, fraudulent transactions will\\nshow up as anomalous, while legitimate transactions will not.\\nThere are many ways to perform anomaly detection. They go by\\nnames such as isolation forests, one-class SVMs, and local\\noutlier factor (LOF). Most rely on unsupervised learning\\nmethods and therefore do not require labeled data. They\\nsimply look at a collection of samples and determine which\\nones are anomalous. Unsupervised anomaly detection is\\nparticularly interesting because it doesn’t require a priori\\nknowledge of what constitutes an anomaly, nor does it require\\nan unlabeled dataset to be meticulously labeled.\\nOne of the most popular forms of anomaly detection relies on\\nprincipal component analysis. You already know that PCA can\\nbe used to reduce data from m dimensions to n, and that a PCA\\ntransform can be inverted to restore the original m\\ndimensions. You also know that inverting the transform\\ndoesn’t recover the data that was lost when the transform\\nwas applied. The gist of PCA-based anomaly detection is that\\nan anomalous sample should exhibit more loss or\\nreconstruction error (the difference between the original\\ndata and the same data after a PCA transform is applied and\\ninverted) than a normal one. In other words, the loss\\nincurred when an anomalous sample is PCAed and un-PCAed\\nshould be higher than the loss incurred when the same\\noperation is applied to a normal sample. Let’s see if this\\nassumption holds up in the real world.\\nUsing PCA to Detect Credit Card Fraud\\nSupervised learning isn’t the only option for detecting\\ncredit card fraud. Here’s an alternative approach that uses\\nPCA-based anomaly detection to identify fraudulent'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 230, 'file_type': 'pdf'}, page_content='229\\ntransactions. Begin by loading the dataset, separating the\\nsamples by class into one dataset representing legitimate\\ntransactions and another representing fraudulent\\ntransactions, and dropping the Time and Class columns. If you\\ndidn’t download the dataset in Chapter\\xa03, you can get it\\nnow from the ZIP file.\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv(\\'Data/creditcard.csv\\')\\ndf.head()\\n\\n# Separate the samples by class\\nlegit\\n=\\ndf[df[\\'Class\\']\\n==\\n0]\\nfraud\\n=\\ndf[df[\\'Class\\']\\n==\\n1]\\n\\n# Drop the \"Time\" and \"Class\" columns\\nlegit\\n=\\nlegit.drop([\\'Time\\',\\n\\'Class\\'],\\naxis=1)\\nfraud\\n=\\nfraud.drop([\\'Time\\',\\n\\'Class\\'],\\naxis=1)\\nUse PCA to reduce the two datasets from 29 to 26 dimensions,\\nand then invert the transform to restore each dataset to 29\\ndimensions. The transform is fitted to legitimate\\ntransactions only because we need a baseline value for\\nreconstruction error that allows us to discriminate between\\nlegitimate and fraudulent transactions. It is applied,\\nhowever, to both datasets:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\npca\\n=\\nPCA(n_components=26,\\nrandom_state=0)\\nlegit_pca\\n=\\npd.DataFrame(pca.fit_transform(legit),\\nindex=legit.index)\\nfraud_pca\\n=\\npd.DataFrame(pca.transform(fraud),\\nindex=fraud.index)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 231, 'file_type': 'pdf'}, page_content='230\\n\\nlegit_restored\\n=\\npd.DataFrame(pca.inverse_transform(legit_pca),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index=legit_pca.index)\\n\\nfraud_restored\\n=\\npd.DataFrame(pca.inverse_transform(fraud_pca),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index=fraud_pca.index)\\nSome information was lost in the transition. Hopefully, the\\nfraudulent transactions incurred more loss than the\\nlegitimate ones, and we can use that to differentiate between\\nthem. The next step is to compute the loss for each row in\\nthe two datasets by summing the squares of the differences\\nbetween the values in the original rows and the restored\\nrows:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\ndef\\nget_anomaly_scores(df_original,\\ndf_restored):\\n\\xa0\\xa0\\xa0 loss\\n=\\nnp.sum((np.array(df_original)\\n-\\nnp.array(df_restored))\\n**\\n2,\\naxis=1)\\n\\xa0\\xa0\\xa0 loss\\n=\\npd.Series(data=loss,\\nindex=df_original.index)\\n\\xa0\\xa0\\xa0 return\\nloss\\n\\nlegit_scores\\n=\\nget_anomaly_scores(legit,\\nlegit_restored)\\nfraud_scores\\n=\\nget_anomaly_scores(fraud,\\nfraud_restored)\\nNow plot the losses incurred when the legitimate transactions\\nwere transformed and restored:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 232, 'file_type': 'pdf'}, page_content='231\\n\\nlegit_scores.plot(figsize\\n=\\n(12,\\n6))\\nHere is the result:\\nNext, plot the losses for the fraudulent transactions:\\n\\nfraud_scores.plot(figsize\\n=\\n(12,\\n6))\\nHere is the result:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 233, 'file_type': 'pdf'}, page_content=\"232\\nThe plots reveal that most of the rows in the dataset\\nrepresenting legitimate transactions incurred a loss of less\\nthan 200, while many of the rows in the dataset representing\\nfraudulent transactions incurred a loss greater than 200.\\nSeparate the rows on this basis—classifying transactions\\nwith a loss of less than 200 as legitimate and transactions\\nwith a higher loss as fraudulent—and use a confusion matrix\\nto visualize the results:\\n\\nthreshold\\n=\\n200\\n\\ntrue_neg\\n=\\nlegit_scores[legit_scores\\n<\\nthreshold].count()\\nfalse_pos\\n=\\nlegit_scores[legit_scores\\n>=\\nthreshold].count()\\ntrue_pos\\n=\\nfraud_scores[fraud_scores\\n>=\\nthreshold].count()\\nfalse_neg\\n=\\nfraud_scores[fraud_scores\\n<\\nthreshold].count()\\n\\nlabels\\n=\\n['Legitimate',\\n'Fraudulent']\\nmat\\n=\\n[[true_neg,\\nfalse_pos],\\n[false_neg,\\ntrue_pos]]\\n\\nsns.heatmap(mat,\\nsquare=True,\\nannot=True,\\nfmt='d',\\ncbar=False,\\ncmap='Blues',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 xticklabels=labels,\\nyticklabels=labels)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 234, 'file_type': 'pdf'}, page_content=\"233\\nplt.xlabel('Predicted label')\\nplt.ylabel('True label')\\nHere is the result:\\nThe results aren’t quite as good as they were with the\\nrandom forest, but the model still caught about 50% of the\\nfraudulent transactions while mislabeling just 76 out of\\n284,315 legitimate transactions. That’s an error rate of\\nless than 0.03% for legitimate transactions, compared to\\n0.007% for the supervised learning model.\\nTwo parameters in this model drive the error rate: the number\\nof dimensions the datasets were reduced to (26), and the\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 235, 'file_type': 'pdf'}, page_content=\"234\\nthreshold chosen to distinguish between legitimate and\\nfraudulent transactions (200). You can tweak the accuracy by\\nexperimenting with different values. I did some informal\\ntesting and concluded that this was a reasonable combination.\\nPicking a lower threshold improves the model’s ability to\\nidentify fraudulent transactions, but at the cost of\\nmisclassifying more legitimate transactions. In the end, you\\nhave to decide what error rate you’re willing to live with,\\nkeeping in mind that declining a legitimate credit card\\npurchase is likely to anger a customer.\\nUsing PCA to Predict Bearing Failure\\nOne of the classic uses for anomaly detection is to predict\\nfailures in rotating machinery. Let’s apply PCA-based\\nanomaly detection to a subset of a dataset published by NASA\\nto predict failures in bearings. The dataset contains\\nvibration data for four bearings supporting a rotating shaft\\nwith a radial load of 6,000 pounds applied to it. The\\nbearings were run to failure, and vibration data was captured\\nby high-sensitivity quartz accelerometers at regular\\nintervals until failure occurred.\\nFirst, download the CSV file containing the subset that I\\nculled from the larger NASA dataset. Then create a Jupyter\\nnotebook and load the data:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/bearings.csv',\\nindex_col=0,\\nparse_dates=[0])\\ndf.head()\\nHere are the first five rows in the dataset:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 236, 'file_type': 'pdf'}, page_content='235\\nThe dataset contains 984 samples. Each sample contains\\nvibration data for four bearings, and the samples were taken\\n10 minutes apart. Plot the vibration data for all four\\nbearings as a time series:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\ndf.plot(figsize\\n=\\n(12,\\n6))\\nHere is the output:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 237, 'file_type': 'pdf'}, page_content=\"236\\nAbout four days into the test, vibrations in bearing 1 began\\nincreasing. They spiked a day later, and about two days after\\nthat, bearing 1 suffered a catastrophic failure. Our goal is\\nto build a model that recognizes increased vibration in any\\nbearing as a sign of impending failure, and to do it without\\na labeled dataset.\\nThe next step is to extract samples representing “normal”\\noperation from the dataset (x_train in the following code)\\nand reduce four dimensions to one using PCA—essentially\\ncombining the data from all four bearings. Then apply the\\nsame PCA transform to the remainder of the dataset (x_test),\\ncombine the two partial datasets, and plot the result:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\nx_train\\n=\\ndf['2004-02-12 10:32:39':'2004-02-13 23:42:39']\\nx_test\\n=\\ndf['2004-02-13 23:52:39':]\\n\\npca\\n=\\nPCA(n_components=1,\\nrandom_state=0)\\nx_train_pca\\n=\\npd.DataFrame(pca.fit_transform(x_train))\\nx_train_pca.index\\n=\\nx_train.index\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 238, 'file_type': 'pdf'}, page_content='237\\n\\nx_test_pca\\n=\\npd.DataFrame(pca.transform(x_test))\\nx_test_pca.index\\n=\\nx_test.index\\n\\ndf_pca\\n=\\npd.concat([x_train_pca,\\nx_test_pca])\\ndf_pca.plot(figsize\\n=\\n(12,\\n6))\\nplt.legend().remove()\\nThe output is shown here:\\nNow invert the PCA transform and plot the “restored”\\ndataset:\\n\\ndf_restored\\n=\\npd.DataFrame(pca.inverse_transform(df_pca),\\nindex=df_pca.index)\\ndf_restored.plot(figsize\\n=\\n(12,\\n6))\\nThe results are as follows:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 239, 'file_type': 'pdf'}, page_content='238\\nIt is obvious that a loss was incurred by applying and\\ninverting the transform. Let’s define a function that\\ncomputes the loss in a range of samples, then apply that\\nfunction to all of the samples in the original dataset and\\nthe restored dataset and plot the differences over time:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\ndef\\nget_anomaly_scores(df_original,\\ndf_restored):\\n\\xa0\\xa0\\xa0 loss\\n=\\nnp.sum((np.array(df_original)\\n-\\nnp.array(df_restored))\\n**\\n2,\\naxis=1)\\n\\xa0\\xa0\\xa0 loss\\n=\\npd.Series(data=loss,\\nindex=df_original.index)\\n\\xa0\\xa0\\xa0 return\\nloss\\n\\nscores\\n=\\nget_anomaly_scores(df,\\ndf_restored)\\nscores.plot(figsize\\n=\\n(12,\\n6))\\nHere is the output:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 240, 'file_type': 'pdf'}, page_content='239\\nThe loss is very small when all four bearings are operating\\nnormally, but it begins to rise when one or more bearings\\nexhibit greater-than-normal vibration. From the chart, it’s\\napparent that when the loss rises above a threshold value of\\napproximately 0.002, that’s an indication a bearing might\\nfail.\\nNow that you’ve selected a tentative loss threshold, you can\\nuse it to detect anomalous behavior in the bearings. Begin by\\ndefining a function that takes a sample and returns True or\\nFalse indicating whether the sample is anomalous by applying\\nand inverting a PCA transform, measuring the loss for each\\nbearing, and comparing it to a specified loss threshold:\\n\\ndef\\nis_anomaly(row,\\npca,\\nthreshold):\\n\\xa0\\xa0\\xa0 pca_row\\n=\\npca.transform(row)\\n\\xa0\\xa0\\xa0 restored_row\\n=\\npca.inverse_transform(pca_row)\\n\\xa0\\xa0\\xa0 losses\\n=\\nnp.sum((row\\n-\\nrestored_row)\\n**\\n2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 for\\nloss\\nin\\nlosses:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nloss\\n>\\nthreshold:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\nTrue;'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 241, 'file_type': 'pdf'}, page_content=\"240\\n\\n\\xa0\\xa0\\xa0 return\\nFalse\\nApply the function to a row early in the time series that\\nrepresents normal behavior and confirm that it returns False:\\n\\nx\\n=\\ndf.loc[['2004-02-16 22:52:39']]\\nis_anomaly(x,\\npca,\\n0.002)\\nApply the function to a row later in the time series that\\nrepresents anomalous behavior and confirm that it returns\\nTrue:\\n\\nx\\n=\\ndf.loc[['2004-02-18 22:52:39']]\\nis_anomaly(x,\\npca,\\n0.002)\\nNow apply the function to all the samples in the dataset and\\nshade anomalous samples red in order to visualize when\\nanomalous behavior is detected:\\n\\ndf.plot(figsize\\n=\\n(12,\\n6))\\n\\nfor\\nindex,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 if\\nis_anomaly(pd.DataFrame([row]),\\npca,\\n0.002):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 plt.axvline(row.name,\\ncolor='r',\\nalpha=0.2)\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 242, 'file_type': 'pdf'}, page_content=\"241\\nRepeat this procedure, but this time use a loss threshold of\\n0.0002 rather than 0.002:\\n\\ndf.plot(figsize\\n=\\n(12,\\n6))\\n\\nfor\\nindex,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 if\\nis_anomaly(pd.DataFrame([row]),\\npca,\\n0.0002):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 plt.axvline(row.name,\\ncolor='r',\\nalpha=0.2)\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 243, 'file_type': 'pdf'}, page_content='242\\nYou can adjust the sensitivity of the model by adjusting the\\nthreshold value used to detect anomalies. Using a loss\\nthreshold of 0.002 predicts bearing failure about two days\\nbefore it occurs, while a loss threshold of 0.0002 predicts\\nthe failure about three days before. You typically want to\\nchoose a loss threshold that predicts failure as early as\\npossible without raising false alarms.\\nMultivariate Anomaly Detection\\nCould we have predicted failure in the preceding example by\\nsimply monitoring individual bearings? Perhaps. But what if\\nimpending failure is indicated by marginally elevated\\nvibrations in two bearings rather than just one? Engineers\\nfrequently find that it isn’t individual sensors but a\\ncombination of readings from several sensors that signal\\nimpending trouble. These readings may come from sensors of\\ndifferent types: temperature sensors and pressure gauges in\\nautomotive and aerospace applications, for example, or heart\\nmonitors and blood pressure monitors in health-care\\napplications. Reducing the number of dimensions to one with\\nPCA is an attempt to capture relationships between data\\nemanating from individual sensors and treat the readings'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 244, 'file_type': 'pdf'}, page_content='243\\nsystemically, a technique known as multivariate anomaly\\ndetection.\\nOne limitation of using PCA to detect anomalies in\\nmultivariate systems is that because it uses linear\\ntransforms, PCA is better at modeling linear relationships\\nbetween variables than nonlinear relationships. Neural\\nnetworks, by contrast, excel at modeling nonlinear data.\\nThat’s the primary reason why state-of-the-art multivariate\\nanomaly detection today commonly relies on deep learning.\\nAs the number of variables increases, so too does the\\nchallenge of modeling the interdependencies between them. It\\nis not uncommon for overall system health to be determined by\\ndozens of otherwise independent variables. In September 2020,\\na team of researchers at Microsoft and Peking University\\npublished a paper titled “Multivariate Time-series Anomaly\\nDetection via Graph Attention Network” that proposed a novel\\narchitecture for multivariate anomaly detection. It combines\\ntwo deep-learning models: one that relies on prediction error\\nand another that relies on reconstruction error. Microsoft\\nuses this architecture in its Azure Multivariate Anomaly\\nDetector service, which can model dependencies between up to\\n300 independent data sources and is used by companies such as\\nAirbus and Siemens to detect irregularities in space-station\\ntelemetry and to test medical devices before they’re sent to\\nmarket. The Azure Multivariate Anomaly Detector service is\\npart of Azure Cognitive Services, which is covered in\\nChapter\\xa014.\\nSummary\\nPrincipal component analysis is a technique for reducing the\\nnumber of dimensions in a dataset without incurring a\\ncommensurate loss of information. It enjoys a number of uses\\nin machine learning, including visualizing high-dimensional\\ndata, anonymizing data, reducing noise, and increasing the\\nratio of rows to columns by reducing the number of'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 245, 'file_type': 'pdf'}, page_content='244\\ndimensions. It can also be used to perform anomaly detection\\nby measuring the loss incurred when a PCA transform is\\napplied and then inverted. Anomalous samples tend to incur\\nmore loss.\\nWhen I teach classes, I often introduce PCA as “the best-\\nkept secret in machine learning.” It shouldn’t remain a\\nsecret, because it’s an indispensable tool in the hands of\\nmachine learning engineers. Now that you know about it, I can\\njust about guarantee that you’ll find ways to put it to\\nwork.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 246, 'file_type': 'pdf'}, page_content='245\\nChapter 7. Operationalizing\\nMachine Learning Models\\nAll of the machine learning models presented so far in this\\nbook are written in Python. Models don’t have to be written\\nin Python, but many are, thanks in part to the numerous\\nworld-class Python libraries available, including Pandas and\\nScikit-Learn. ML models written in Python are easily consumed\\nin Python apps. Calling them from other languages such as\\nC++, Java, and C# requires a little more work. You can’t\\nsimply call a Python function from C++ as if it were a C++\\nfunction. So how do you invoke models written in Python from\\napps written in other languages? Put another way, how do you\\noperationalize Python models such that they are usable in any\\napp on any platform written in any programming language?\\nThe diagram on the left in Figure\\xa07-1 shows one strategy:\\nwrap the model in a web service and expose its functionality\\nthrough a REST API. Then any client that can generate an\\nHTTP(S) request can invoke the model. It’s relatively easy\\nto do the wrapping with help from Python frameworks such as\\nFlask. The web service can be hosted locally or in the cloud,\\nand it can be containerized for easy deployment using tools\\nsuch as Docker.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 247, 'file_type': 'pdf'}, page_content='246\\nFigure 7-1. Architectures for consuming Python models in other languages\\nThe diagram on the right shows another strategy—one that’s\\nrelatively new but rapidly growing in popularity. It involves\\nexporting a Python model to a platform-agnostic format called\\nONNX, short for Open Neural Network Exchange, and then using\\nan ONNX runtime to load the model in Java, C++, C#, and other\\nprogramming languages. Once loaded, the model can be called\\nvia the ONNX runtime.\\nOf course, if the client app and the model are written in the\\nsame language, you need neither a web service nor ONNX. In\\nthis chapter, I’ll walk you through several scenarios:\\n\\xa0\\nHow to save a trained Python model and invoke it\\nfrom a Python client\\nHow to invoke a Python model from a non-Python\\nclient using a web service\\nHow to containerize a Python model (and web\\nservice) for easy deployment\\nHow to use ONNX to invoke a Python model from\\nother programming languages\\nHow to write machine learning models in C# rather\\nthan Python'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 248, 'file_type': 'pdf'}, page_content=\"247\\nI’ll finish up by demonstrating a novel way to\\noperationalize a machine learning model by exposing its\\nfunctionality through Microsoft Excel. There’s a lot to\\ncover, so let’s get started.\\nConsuming a Python Model from a Python Client\\nOstensibly, invoking a Python model from a Python client is\\nsimple: just call predict (or, for a classifier,\\npredict_proba) on the model. Of course, you don’t want to\\nhave to retrain the model every time you use it. You want to\\ntrain it once, and then empower a client app to re-create the\\nmodel in its trained state. For that, Python programmers use\\nthe Python pickle module.\\nTo demonstrate, the following code builds and trains the\\nTitanic model featured in Chapter\\xa03. Rather than use the\\nmodel to make predictions, however, it saves the model to a\\n.pkl file (it “pickles” the model) with a call to\\npickle.dump on the final line:\\n\\nimport\\npickle\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\n\\ndf\\n=\\npd.read_csv('Data/titanic.csv')\\ndf\\n=\\ndf[['Survived',\\n'Age',\\n'Sex',\\n'Pclass']]\\ndf\\n=\\npd.get_dummies(df,\\ncolumns=['Sex',\\n'Pclass'])\\ndf.dropna(inplace=True)\\n\\nx\\n=\\ndf.drop('Survived',\\naxis=1)\\ny\\n=\\ndf['Survived']\\n\\nmodel\\n=\\nLogisticRegression(random_state=0)\\nmodel.fit(x,\\ny)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 249, 'file_type': 'pdf'}, page_content=\"248\\n\\npickle.dump(model,\\nopen('titanic.pkl',\\n'wb'))\\nTo invoke the model, a Python client uses pickle.load to\\ndeserialize the model from the .pkl file, re-creating the\\nmodel in its trained state, and calls predict_proba to\\ncompute the odds of a passenger’s survival:\\n\\nimport\\npickle\\nimport\\npandas\\nas\\npd\\n\\nmodel\\n=\\npickle.load(open('titanic.pkl',\\n'rb'))\\n\\nfemale\\n=\\npd.DataFrame({\\n'Age':\\n[30],\\n'Sex_female':\\n[1],\\n'Sex_male':\\n[0],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Pclass_1':\\n[1],\\n'Pclass_2':\\n[0],\\n'Pclass_3':\\n[0]\\n})\\n\\nprobability\\n=\\nmodel.predict_proba(female)[0][1]\\nprint(f'Probability of survival: {probability:.1%}')\\nNow the client can use the model to make a prediction without\\nretraining it. And once the model is loaded, it can persist\\nfor the lifetime of the client and be called upon for\\npredictions whenever needed.\\nChapter\\xa05 introduced Scikit’s make_pipeline function, which\\nallows estimators (objects that make predictions) and\\ntransformers (objects that transform data input to a model)\\nto be combined into a single unit, or pipeline. The pickle\\nmodule can be used to serialize and deserialize pipelines\\ntoo. Example\\xa07-1 recasts the sentiment analysis model\\nfeatured in Chapter\\xa04 to use make_pipeline to combine a\\nCountVec\\u2060tor\\u2060izer for vectorizing text with a LogisticRegression\\nobject for classifying text. The call to pickle.dump saves\\nthe model, CountVectorizer and all.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 250, 'file_type': 'pdf'}, page_content='249\\nExample 7-1. Training and saving a sentiment analysis\\npipeline\\n\\nimport\\npickle\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\nfrom\\nsklearn.pipeline\\nimport\\nmake_pipeline\\n\\ndf\\n=\\npd.read_csv(\\'Data/reviews.csv\\',\\nencoding=\"ISO-8859-1\")\\ndf\\n=\\ndf.drop_duplicates()\\n\\nx\\n=\\ndf[\\'Text\\']\\ny\\n=\\ndf[\\'Sentiment\\']\\n\\nvectorizer\\n=\\nCountVectorizer(ngram_range=(1,\\n2),\\nstop_words=\\'english\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 min_df=20)\\n\\nmodel\\n=\\nLogisticRegression(max_iter=1000,\\nrandom_state=0)\\npipe\\n=\\nmake_pipeline(vectorizer,\\nmodel)\\npipe.fit(x,\\ny)\\n\\npickle.dump(pipe,\\nopen(\\'sentiment.pkl\\',\\n\\'wb\\'))\\nA Python client can deserialize the pipeline and call\\npredict_proba to score a line of text for sentiment with a\\nfew simple lines of code:\\n\\nimport\\npickle\\n\\npipe\\n=\\npickle.load(open(\\'sentiment.pkl\\',\\n\\'rb\\'))\\nscore\\n=\\npipe.predict_proba([\\'Great food and excellent service!\\'])[0][1]\\nprint(score)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 251, 'file_type': 'pdf'}, page_content=\"250\\nPickling in this manner works not just with CountVectorizer\\nbut with other transformers as well, such as StandardScaler.\\nNOTE\\nPickling a pipeline containing a CountVectorizer can produce a\\nlarge .pkl file. In this example, sentiment.pkl is 50 MB in length\\nbecause it contains the entire vocabulary built by CountVectorizer\\nfrom the training text. Remove the min_df=20 parameter and the file\\nswells to nearly 90 MB.\\nThe solution is to replace CountVectorizer with Hashing\\u200bVec\\u2060tor\\u2060izer,\\nwhich doesn’t create a vocabulary but instead uses word hashes to\\nindex a table of word frequencies. That reduces sentiment.pkl to 8\\nMB—without the min_df parameter, which HashingVectorizer doesn’t\\nsupport anyway.\\nIf you’d like to write a standalone Python client that\\nperforms sentiment analysis, run the code in Example\\xa07-1 in\\na Jupyter notebook to generate sentiment.pkl. Optionally, you\\ncan change CountVectorizer to HashingVectorizer and remove the\\nmin_df parameter to reduce the size of the .pkl file. Then\\ncreate a Python script named sentiment.py containing the\\nfollowing code:\\n\\nimport\\npickle,\\nsys\\n\\n# Get the text to analyze\\nif\\nlen(sys.argv)\\n>\\n1:\\n\\xa0\\xa0\\xa0 text\\n=\\nsys.argv[1]\\nelse:\\n\\xa0\\xa0\\xa0 text\\n=\\ninput('Text to analyze: ')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 252, 'file_type': 'pdf'}, page_content='251\\n\\n# Load the pipeline containing the model and the vectorizer\\npipe\\n=\\npickle.load(open(\\'sentiment.pkl\\',\\n\\'rb\\'))\\n\\n# Pass the input text to the pipeline and print the result\\nscore\\n=\\npipe.predict_proba([text])[0][1]\\nprint(score)\\nCopy sentiment.pkl into the same directory as sentiment.py,\\nand then pop out to the command line and run the script:\\n\\npython\\nsentiment.py\\n\"Great food and excellent service!\"\\nThe output should look something like this, which is proof\\nthat you succeeded in re-creating the model in its trained\\nstate and invoking it to analyze the input text for\\nsentiment:\\nNote that the sentiment score will differ slightly if you\\nreplaced CountVectorizer with HashingVectorizer, in part due'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 253, 'file_type': 'pdf'}, page_content='252\\nto the omission of the min_df parameter.\\nVersioning Pickle Files\\nGenerally speaking, a model pickled (saved) with one version\\nof Scikit can’t be unpickled with another version. Sometimes\\nthe result is warning messages; other times, it doesn’t work\\nat all. Be sure to save models with the same version of\\nScikit that you use to consume them. This requires a bit of\\nplanning from an engineering perspective, because if you\\nstore serialized models in a centralized repository and\\nupdate the version of Scikit used in your apps, you’ll need\\nto update the saved models too.\\nWhich prompts some interesting questions: How do you set up a\\nrepository for machine learning models? How do you deploy\\nmodels from the repository to the devices that host them? For\\nthat matter, how do you version those models as well as the\\ndatasets you train them with?\\nThe answers come from a nascent field known as MLOps, which\\nis short for ML operations. I don’t cover MLOps in this book\\nbecause it’s a rich subject that is a book unto itself. If\\nyou want to learn more, I recommend reading Practical MLOps:\\nOperationalizing Machine Learning Models by Noah Gift and\\nAlfredo Deza (O’Reilly).\\nConsuming a Python Model from a C# Client\\nSuppose you wanted to invoke the sentiment analysis model in\\nthe previous section from an app written in another language\\n—say, C#. You can’t directly call a Python function from\\nC#, but you can wrap a Python model in a web service and\\nexpose its predict (or predict_proba) method using a REST\\nAPI. One way to code the web service is to use Flask, a\\npopular framework for building websites and web services in\\nPython.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 254, 'file_type': 'pdf'}, page_content=\"253\\nTo see for yourself, make sure Flask is installed on your\\ncomputer. Then create a file named app.py and paste in the\\nfollowing code. This code uses Flask to implement a Python\\nweb service that listens on port 5000:\\n\\nimport\\npickle\\nfrom\\nflask\\nimport\\nFlask,\\nrequest\\n\\napp\\n=\\nFlask(__name__)\\npipe\\n=\\npickle.load(open('sentiment.pkl',\\n'rb'))\\n\\n@app.route('/analyze',\\nmethods=['GET'])\\ndef\\nanalyze():\\n\\xa0\\xa0\\xa0 if\\n'text'\\nin\\nrequest.args:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n=\\nrequest.args.get('text')\\n\\xa0\\xa0\\xa0 else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n'No string to analyze'\\n\\n\\xa0\\xa0\\xa0 score\\n=\\npipe.predict_proba([text])[0][1]\\n\\xa0\\xa0\\xa0 return\\nstr(score)\\n\\nif __name__ ==\\n'__main__':\\n\\xa0\\xa0\\xa0 app.run(debug=True,\\nport=5000,\\nhost='0.0.0.0')\\nAt startup, the service deserializes the pipeline comprising\\nthe sentiment analysis model in sentiment.pkl. The @app.route\\nstatement decorating the analyze function tells Flask to call\\nthe function when the service’s analyze method is called. If\\nthe service is hosted locally, the following request invokes\\nanalyze and returns a string containing a sentiment score for\\nthe text in the query string:\\n\\nhttp://localhost:5000/analyze?text=Great food and excellent service!\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 255, 'file_type': 'pdf'}, page_content='254\\nTo demonstrate, go to the directory where app.py is located\\n(make sure sentiment.pkl is there too) and start Flask by\\ntyping:\\n\\nflask run\\nThen go to a separate command prompt and use a curl command\\nto fire off a request to the URL:\\ncurl -G -w \"\\\\n\" http://localhost:5000/analyze --data-urlencode \"text=Great food\\nand excellent service!\"\\nHere’s the output:\\nIf you have Visual Studio or Visual Studio Code installed on\\nyour computer and are set up to compile and run C# apps, you\\ncan use the following code in a C# console app to invoke the\\nweb service and score a text string for sentiment. Of course,\\nyou’re not limited to invoking the web service (and by'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 256, 'file_type': 'pdf'}, page_content='255\\nextension, the model) from C#. Any language will do, because\\nvirtually all modern programming languages provide a means\\nfor sending HTTP requests.\\n\\nusing\\nSystem;\\nusing\\nSystem.Net.Http;\\nusing\\nSystem.Threading.Tasks;\\n\\nclass\\nProgram\\n{\\n\\xa0\\xa0\\xa0 static\\nasync\\nTask\\nMain(string[]\\nargs)\\n\\xa0\\xa0\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 string\\ntext;\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Get the text to analyze\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(args.Length\\n>\\n0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n=\\nargs[0];\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 else\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.Write(\"Text to analyze: \");\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n=\\nConsole.ReadLine();\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Pass the text to the web service\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nclient\\n=\\nnew\\nHttpClient();\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nurl\\n=\\n$\"http://localhost:5000/analyze?text={text}\";\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nresponse\\n=\\nawait\\nclient.GetAsync(url);\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nscore\\n=\\nawait\\nresponse.Content.ReadAsStringAsync();\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Show the sentiment score\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.WriteLine(score);\\n\\xa0\\xa0\\xa0 }\\n}'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 257, 'file_type': 'pdf'}, page_content='256\\nThis web service is a simple one that reads input from a\\nquery string and returns a string. For more complex input and\\noutput, you can serialize the input into JSON and transmit it\\nin the body of an HTTP POST, and you can return a JSON\\npayload in the response. For a tutorial demonstrating how to\\ndo it in Python, see the article “Python Post JSON Using\\nRequests Library”.\\nContainerizing a Machine Learning Model\\nOne downside to wrapping a machine learning model in a web\\nservice and running it locally is that the client computer\\nmust have Python installed, as well as all the packages that\\nthe model and web service require. An alternative is to host\\nthe web service in the cloud where it can be called via the\\ninternet. It’s not hard to go out to Azure or AWS, spin up a\\nvirtual machine (VM), and install the software there. But\\nthere’s a better way. That better way is containers.\\nContainers have revolutionized the way software is built and\\ndeployed. A container includes an app and everything the app\\nneeds to run, including a runtime (for example, Python), the\\npackages the app relies on, and even a virtual filesystem. If\\nyou’re not familiar with containers, think of them as\\nlightweight VMs that start quickly and consume far less\\nmemory. Docker is the world’s most popular container\\nplatform, although it is rapidly being supplanted by\\nKubernetes.\\nContainers are created from container images, which serve as\\nblueprints for containers in the same way that classes in\\nobject-oriented programming languages constitute blueprints\\nfor objects. The first step in creating a Docker container\\nimage containing the sentiment analysis model and web service\\nis to create a file named Dockerfile (no filename extension)\\nin the same directory as app.py and sentiment.pkl and then\\npaste the following statements into it:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 258, 'file_type': 'pdf'}, page_content='257\\n\\nFROM\\npython:3.8\\nRUN\\npip\\ninstall\\nflask\\nnumpy\\nscipy\\nscikit-learn\\n&& \\\\\\n\\xa0\\xa0\\xa0 mkdir\\n/app\\nCOPY\\napp.py\\n/app\\nCOPY\\nsentiment.pkl\\n/app\\nWORKDIR\\n/app\\nEXPOSE\\n5000\\nENTRYPOINT\\n[\"python\"]\\nCMD\\n[\"app.py\"]\\nA Dockerfile contains instructions for building a container\\nimage. This one creates a container image that includes a\\nPython runtime, several Python packages such as Flask and\\nScikit-Learn, and app.py and sentiment.pkl. It also instructs\\nthe Docker runtime that hosts the container to open port 5000\\nfor HTTP requests and to execute app.py when the container\\nstarts.\\nThere are several ways to build a container image from a\\nDockerfile. If Docker is installed on your computer, you can\\nuse a docker build command:\\ndocker build -t sentiment-server .\\nAlternatively, you can upload the Dockerfile to a cloud\\nplatform such as Microsoft Azure and build it there. This\\nprevents you from having to have Docker installed on the\\nlocal machine, and it makes it easy to store the resulting\\ncontainer image in the cloud. Container images are stored in\\ncontainer registries, and modern cloud platforms host\\ncontainer registries as well as containers. If you launch a\\ncontainer instance in Azure, the web service in the container\\ncan be invoked with a URL similar to this one:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 259, 'file_type': 'pdf'}, page_content='258\\nhttp://wintellect.northcentralus.azurecontainer.io:5000/analyze?text=Great food\\nand excellent service!\\nOne of the benefits of hosting the container instance in the\\ncloud is that it can be reached from any client app running\\non any machine and any operating system, and the computer\\nthat hosts the app needs nothing special installed.\\nContainers can be beneficial even if you host the web service\\nlocally rather than in the cloud. As long as you deploy a\\ncontainer stack such as the Docker runtime to the local\\nmachine, you don’t have to install Python and all the\\npackages that the web service requires. You just launch a\\ncontainer instance and direct HTTP requests to it via\\nlocalhost.\\nUsing ONNX to Bridge the Language Gap\\nIs it possible to bridge the gap between a C# client and a\\nPython ML model without using a web service as a middleman?\\nIn a word, yes! The solution lies in a four-letter acronym:\\nONNX. As mentioned earlier, ONNX stands for Open Neural\\nNetwork Exchange, and it was originally devised to allow\\ndeep-learning models written with one framework—for example,\\nTensorFlow—to be used with other frameworks such as PyTorch.\\nBut today it can be used with Scikit models too. I’ll say\\nmore about ONNX in Chapter\\xa012, but for now, let’s use it to\\ncall a Python model directly from an app written in C#—no\\nweb service required.\\nThe first step in using ONNX to bridge the language gap is to\\ninstall Skl2onnx in your Python environment. Then use that\\npackage’s convert_sklearn method to save a trained Scikit\\nmodel to a .onnx file. Here’s a short code snippet that\\nsaves the sentiment analysis model in Example\\xa07-1 to a file\\nnamed sentiment.onnx. The initial_types parameter specifies'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 260, 'file_type': 'pdf'}, page_content=\"259\\nthat the model expects one input value: a string containing\\nthe text to score for sentiment:\\n\\nfrom\\nskl2onnx\\nimport\\nconvert_sklearn\\nfrom\\nskl2onnx.common.data_types\\nimport\\nStringTensorType\\n\\ninitial_type\\n=\\n[('string_input',\\nStringTensorType([None,\\n1]))]\\nonnx\\n=\\nconvert_sklearn(pipe,\\ninitial_types=initial_type)\\n\\nwith\\nopen('sentiment.onnx',\\n'wb')\\nas\\nf:\\n\\xa0\\xa0\\xa0 f.write(onnx.SerializeToString())\\nIf you wish to consume the model from Python, you first\\ninstall a Python package named Onnxruntime containing the\\nONNX runtime, also known as the ORT. This provides support\\nfor loading and running ONNX models. Then you call the\\nruntime’s InferenceSession method with a path to a .onnx\\nfile to deserialize the model, and call run on the returned\\nsession object to call the model’s predict or predict_proba\\nmethod. Here’s how it looks in code:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\nonnxruntime\\nas\\nrt\\n\\nsession\\n=\\nrt.InferenceSession('sentiment.onnx')\\ninput_name\\n=\\nsession.get_inputs()[0].name\\nlabel_name\\n=\\nsession.get_outputs()[1].name\\n# 0 = predict, 1 = predict_proba\\n\\ninput\\n=\\nnp.array('Great food and excellent service!').reshape(1,\\n-1)\\nscore\\n=\\nsession.run([label_name],\\n{\\ninput_name:\\ninput\\n})[0][0][1]\\nprint(score)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 261, 'file_type': 'pdf'}, page_content='260\\nNote that the string input to the model is passed as a NumPy\\narray. The value returned from the run method reveals the\\nsentiment score returned by predict_proba. If you’d rather\\ncall predict to predict a class, change ses\\u2060sion\\u200b.get_outputs()\\n[1].name to session.get\\u200b_outputs()[0].name.\\nNOTE\\nIn real life, you wouldn’t load the model every time you call it.\\nYou’d load it once, allow it to persist for the lifetime of the\\nclient, and call it whenever you want to make a prediction.\\nOf course, the whole point of this discussion is to call the\\nmodel from C# instead of Python. That’s not difficult\\neither, thanks to a NuGet package from Microsoft called\\nMicrosoft.ML.OnnxRuntime. You can install it from the command\\nline or using an integrated development environment such as\\nVisual Studio. Then it’s a relatively simple matter to write\\na C# console app that re-creates the trained sentiment\\nanalysis model from the .onnx file and calls it to score a\\ntext string:\\n\\nusing\\nSystem;\\nusing\\nMicrosoft.ML.OnnxRuntime;\\nusing\\nMicrosoft.ML.OnnxRuntime.Tensors;\\nusing\\nSystem.Collections.Generic;\\nusing\\nSystem.Linq;\\n\\nclass\\nProgram\\n{\\n\\xa0\\xa0\\xa0 static\\nvoid\\nMain(string[]\\nargs)\\n\\xa0\\xa0\\xa0 {'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 262, 'file_type': 'pdf'}, page_content='261\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 string\\ntext;\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Get the text to analyze\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(args.Length\\n>\\n0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n=\\nargs[0];\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 else\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.Write(\"Text to analyze: \");\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n=\\nConsole.ReadLine();\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Create the model and pass the text to it\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\ntensor\\n=\\nnew\\nDenseTensor<string>(new\\nstring[]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\ntext\\n},\\nnew\\nint[]\\n{\\n1,\\n1\\n});\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\ninput\\n=\\nnew\\nList<NamedOnnxValue>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 NamedOnnxValue.CreateFromTensor<string>(\"string_input\",\\ntensor)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 };\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nsession\\n=\\nnew\\nInferenceSession(\"sentiment.onnx\");\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\noutput\\n=\\nsession.Run(input)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 .ToList().Last().AsEnumerable<NamedOnnxValue>();\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nscore\\n=\\noutput.First().AsDictionary<Int64,\\nfloat>()[1];\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Show the sentiment score\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.WriteLine(score);\\n\\xa0\\xa0\\xa0 }\\n}\\nThis code assumes that sentiment.onnx is present in the\\ncurrent directory. Instantiating the InferenceSession object'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 263, 'file_type': 'pdf'}, page_content=\"262\\ncreates the model, and calling Run on the object invokes the\\nmodel. Under the hood, Run calls the model’s predict and\\npredict_proba methods and makes the results of both\\navailable. There’s a little work required to convert C#\\ntypes into ONNX types and vice versa, but once you get the\\nhang of it, it’s pretty remarkable that you can call a\\nmachine learning model written in Python directly from C#.\\nHere’s another example. Suppose you wanted to consume\\nChapter\\xa02’s taxi-fare regression model in C#. Recall that\\nthe model accepts three floating-point values as input—the\\nday of the week (0–6), the hour of day (0–23), and the\\ndistance to travel in miles—and returns a predicted taxi\\nfare. The following Python code saves the trained model in an\\nONNX file:\\n\\nfrom\\nskl2onnx\\nimport\\nconvert_sklearn\\nfrom\\nskl2onnx.common.data_types\\nimport\\nFloatTensorType\\n\\ninitial_type\\n=\\n[('float_input',\\nFloatTensorType([None,\\n3]))]\\nonnx\\n=\\nconvert_sklearn(model,\\ninitial_types=initial_type)\\n\\nwith\\nopen('taxi.onnx',\\n'wb')\\nas\\nf:\\n\\xa0\\xa0\\xa0 f.write(onnx.SerializeToString())\\nThe following C# code loads the model and makes a prediction.\\nThe big difference between this and the previous example is\\nthat you pass the model an array of three floating-point\\nvalues rather than a string:\\n\\n// Package the input\\nvar\\ninput\\n=\\nnew\\nfloat[]\\n{\\n\\xa0\\xa0\\xa0 4.0f,\\xa0 // Day of week\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 264, 'file_type': 'pdf'}, page_content='263\\n\\xa0\\xa0\\xa0 17.0f,\\n// Pickup time (hour of day)\\n\\xa0\\xa0\\xa0 2.0f\\xa0\\xa0 // Distance to travel\\n};\\n\\nvar\\ntensor\\n=\\nnew\\nDenseTensor<float>(input,\\nnew\\nint[]\\n{\\n1,\\n3\\n});\\n\\n// Create the model and pass the input to it\\nvar\\nsession\\n=\\nnew\\nInferenceSession(\"taxi.onnx\");\\n\\nvar\\noutput\\n=\\nsession.Run(new\\nList<NamedOnnxValue>\\n{\\n\\xa0\\xa0\\xa0 NamedOnnxValue.CreateFromTensor(\"float_input\",\\ntensor)\\n});\\n\\nvar\\nscore\\n=\\noutput.First().AsTensor<float>().First();\\n\\n// Show the predicted fare\\nConsole.WriteLine($\"{score:#.00}\");\\nSo which is faster? Calling a machine learning model wrapped\\nin a web service, or calling the same model using ONNX? I\\nwrote a simple test harness to answer that question. On my\\ncomputer, calling the sentiment analysis model in a Flask web\\nservice running locally required slightly more than 2 seconds\\nper round trip. Calling the same model through the Python\\nONNX runtime took 0.001 seconds on average. That’s a\\ndifference of more than three orders of magnitude. And you\\nwould incur additional latency if the web service was hosted\\non a remote server.\\nSignificantly, ONNX isn’t limited to C# and Python. ONNX\\nruntimes are available for Python, C, C++, C#, Java,\\nJavaScript, and Objective-C, and they run on Windows, Linux,\\nmacOS, Android, and iOS. When it comes to projecting machine\\nlearning models written in Python to other platforms and\\nlanguages, ONNX is a game changer. For more information,\\ncheck out the ONNX runtime website.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 265, 'file_type': 'pdf'}, page_content='264\\nBuilding ML Models in C# with ML.NET\\nScikit-Learn is arguably the world’s most popular machine\\nlearning framework. The efficacy of the library, the\\ndocumentation that accompanies it, and the mindshare that\\nsurrounds it are the primary reasons more ML models are\\nwritten in Python than any other language. But Scikit isn’t\\nthe only machine learning framework. Others exist for other\\nlanguages, and if you can code an ML model in the same\\nprogramming language as the client that consumes it, you can\\navoid jumping through hoops to operationalize the model.\\nML.NET is Microsoft’s free, open source, cross-platform\\nmachine learning library for .NET developers. It does most of\\nwhat Scikit does and a few things that Scikit doesn’t do.\\nAnd when it comes to writing ML/AI solutions in C#, there is\\nno better tool for the job.\\nML.NET derives from an internal library that was developed by\\nMicrosoft—and has been used in Microsoft products—for more\\nthan a decade. The ML algorithms that it implements have been\\ntried and tested in the real world and tuned to optimize\\nperformance and accuracy. Because ML.NET is consumed from C#,\\nyou get all the benefits of a compiled programming language,\\nincluding type safety and fast execution.\\nA paper published by the ML.NET team at Microsoft in 2019\\ndiscusses the motivations and design goals behind ML.NET. It\\nalso compares ML.NET’s accuracy and performance to that of\\nScikit-Learn and another machine learning framework named\\nH2O. Using a 9 GB Amazon review dataset, ML.NET trained a\\nsentiment analysis model to 95% accuracy. Neither Scikit nor\\nH2O could process the dataset due to its size. When all three\\nframeworks were trained on 10% of the dataset, ML.NET\\nachieved the highest accuracy, and trained six times faster\\nthan Scikit and almost 10 times faster than H2O.\\nML.NET is compatible with Windows, Linux, and macOS. Thanks\\nto an innovation called IDataView, it can handle datasets of\\nvirtually unlimited size. While it can’t be used to build'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 266, 'file_type': 'pdf'}, page_content='265\\nneural networks from scratch, it does have the ability to\\nload existing neural networks and use a technique called\\ntransfer learning to repurpose those networks to solve\\ndomain-specific problems. (Transfer learning will be covered\\nin Chapter\\xa010.) It also builds in ONNX support that allows\\nit to load sophisticated models, including deep-learning\\nmodels, stored in ONNX format. Finally, it can be consumed in\\nPython and even combined with Scikit-Learn using a set of\\nPython bindings called NimbusML.\\nIf you’re a .NET developer who is interested in machine\\nlearning, there has never been a better time to get\\nacquainted with ML.NET. This section isn’t meant to provide\\nan exhaustive treatment of ML.NET but to introduce it, show\\nthe basics of building ML models with it, and hopefully whet\\nyour appetite enough to motivate you to learn more. There are\\nplenty of great resources available online, including the\\nofficial ML.NET documentation, a GitHub repo containing\\nML.NET samples, and the ML.NET cookbook.\\nSentiment Analysis with ML.NET\\nThe following C# code uses ML.NET to build and train a\\nsentiment analysis model. It’s equivalent to the Python\\nimplementation in Example\\xa07-1:\\n\\nvar\\ncontext\\n=\\nnew\\nMLContext(seed:\\n0);\\n\\n// Load the data\\nvar\\ndata\\n=\\ncontext.Data.LoadFromTextFile<Input>(\"reviews.csv\",\\n\\xa0\\xa0\\xa0 hasHeader:\\ntrue,\\nseparatorChar:\\n\\',\\',\\nallowQuoting:\\ntrue);\\n\\n// Split the data into a training set and a test set\\nvar\\ntrainTestData\\n=\\ncontext.Data.TrainTestSplit(data,\\n\\xa0\\xa0\\xa0 testFraction:\\n0.2,\\nseed:\\n0);'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 267, 'file_type': 'pdf'}, page_content='266\\nvar\\ntrainData\\n=\\ntrainTestData.TrainSet;\\nvar\\ntestData\\n=\\ntrainTestData.TestSet;\\n\\n// Build and train the model\\nvar\\npipeline\\n=\\ncontext.Transforms.Text.FeaturizeText\\n\\xa0\\xa0\\xa0 (outputColumnName:\\n\"Features\",\\ninputColumnName:\\n\"Text\")\\n\\xa0\\xa0\\xa0 .Append(context.BinaryClassification.Trainers.SdcaLogisticRegression());\\n\\nvar\\nmodel\\n=\\npipeline.Fit(trainData);\\n\\n// Evaluate the model\\nvar\\npredictions\\n=\\nmodel.Transform(testData);\\nvar\\nmetrics\\n=\\ncontext.BinaryClassification.Evaluate(predictions);\\nConsole.WriteLine($\"AUC: {metrics.AreaUnderPrecisionRecallCurve:P2}\");\\n\\n// Score a line of text for sentiment\\nvar\\npredictor\\n=\\ncontext.Model.CreatePredictionEngine<Input,\\nOutput>(model);\\nvar\\ninput\\n=\\nnew\\nInput\\n{\\nText\\n=\\n\"Among the best movies I have ever seen\"};\\nvar\\nprediction\\n=\\npredictor.Predict(input);\\n\\n// Show the score\\nConsole.WriteLine($\"Sentiment score: {prediction.Probability}\");\\nEvery ML.NET app begins by creating an instance of the\\nMLContext class. The optional seed parameter initializes the\\nrandom-number generator used by ML.NET so that you get\\nrepeatable results from one run to the next. MLContext\\nexposes a number of properties through which large parts of\\nthe ML.NET API are accessed. One example of this is the call\\nto LoadFromTextFile, which is a DataOperationsCatalog method\\naccessed through MLContext’s Data property.\\nLoadFromTextFile is one of several methods ML.NET provides\\nfor loading data from text files, databases, and other data\\nsources. It returns a data view, which is an object that\\nimplements the IDataView interface. Data views in ML.NET are\\nsimilar to DataFrames in Pandas, with one important'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 268, 'file_type': 'pdf'}, page_content='267\\ndifference: whereas DataFrames have to fit in memory, data\\nviews do not. Internally, data views use a SQL-like cursor to\\naccess data. This means they can wrap a theoretically\\nunlimited amount of data. That’s why ML.NET was able to\\nprocess the entire 9 GB Amazon dataset, while Scikit and H2O\\nwere not.\\nAfter loading the data and splitting it for training and\\ntesting, the preceding code creates a pipeline containing a\\nTextFeaturizingEstimator object—created with the\\nFeaturizeText method—and an\\nSdcaLogisticRegressionBinaryTrainer object—created with the\\nSdca\\u200bLo\\u2060gisticRegression method. This is analogous in Scikit to\\ncreating a pipeline containing a CountVectorizer object for\\nvectorizing input text and a LogisticRegression object for\\nfitting a model to the data. Calling Fit on the pipeline\\ntrains the model, just like calling fit in Scikit. It’s no\\ncoincidence that ML.NET employs some of the same patterns as\\nScikit. This was done intentionally to impart a sense of\\nfamiliarity to programmers who use Scikit.\\nAfter evaluating the model’s accuracy by computing the area\\nunder the precision-recall curve, a call to\\nModelOperationsCatalog.CreatePredictionEngine creates a\\nprediction engine whose Predict method makes predictions.\\nUnlike Scikit, which has you call predict on the estimator\\nitself, ML.NET encapsulates prediction capability in a\\nseparate object, in part so that multiple prediction engines\\ncan be created to achieve scalability in high-traffic\\nscenarios.\\nIn this example, Predict accepts an Input object as input and\\nreturns an Output object. One of the benefits of building\\nmodels with ML.NET is strong typing. LoadFromTextFile is a\\ngeneric method that accepts a class name as a type parameter\\n—in this case, Input. Similarly, CreatePredictionEngine uses\\ntype parameters to specify schemas for input and output. The\\nInput and Output classes are application specific and in this\\ninstance are defined as follows:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 269, 'file_type': 'pdf'}, page_content='268\\n\\npublic\\nclass\\nInput\\n{\\n\\xa0\\xa0\\xa0 [LoadColumn(0)]\\n\\xa0\\xa0\\xa0 public\\nstring\\nText;\\n\\n\\xa0\\xa0\\xa0 [LoadColumn(1), ColumnName(\"Label\")]\\n\\xa0\\xa0\\xa0 public\\nbool\\nSentiment;\\n}\\n\\npublic\\nclass\\nOutput\\n{\\n\\xa0\\xa0\\xa0 [ColumnName(\"PredictedLabel\")]\\n\\xa0\\xa0\\xa0 public\\nbool\\nPrediction\\n{\\nget;\\nset;\\n}\\n\\n\\xa0\\xa0\\xa0 public\\nfloat\\nProbability\\n{\\nget;\\nset;\\n}\\n}\\nThe LoadColumn attributes map columns in the data file to\\nproperties in the Input class. Here, they tell ML.NET that\\nvalues for the Text field come from column 0 in the input\\nfile, and values for Sentiment (the 1s and 0s indicating\\nwhether the sentiment expressed in the text is positive or\\nnegative) come from column 1. The Colum\\u2060n\\u200bName(\"Label\")\\nattribute identifies the second column as the label column—\\nthe one containing the values that the model will attempt to\\npredict.\\nThe Output class defines the output schema. In this example,\\nit contains properties named Prediction and Probability,\\nwhich, following a prediction, hold the predicted label (0 or\\n1) and the probability that the sample belongs to the\\npositive class, which doubles as a sentiment score. The\\nColumnName(\"PredictedLabel\") attribute maps the value returned\\nby Predict to the Output object’s Prediction property.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 270, 'file_type': 'pdf'}, page_content='269\\nNOTE\\nThere is nothing magic about the class names Input and Output. You\\ncould name them SentimentData and SentimentPrediction and the code\\nwould work just the same.\\nSaving and Loading ML.NET Models\\nEarlier, you learned how to use Python’s pickle module to\\nsave and load trained models. You do the same in ML.NET by\\ncalling ModelOperationsCatalog.Save and\\nModelOperationsCatalog.Load through the MLContext object’s\\nModel property:\\n\\n//\\nSave\\na\\ntrained\\nmodel\\nto\\na\\nzip\\nfile\\ncontext.Model.Save(model,\\ndata.Schema,\\n\"model.zip\");\\n\\n//\\nLoad\\na\\ntrained\\nmodel\\nfrom\\na\\nzip\\nfile\\nvar\\nmodel\\n=\\ncontext.Model.Load(\"model.zip\",\\nout\\nDataViewSchema\\nschema);\\nThis enables clients to re-create a model in its trained\\nstate and use it to make predictions without having to train\\nthe model repeatedly.\\nAdding Machine Learning Capabilities to Excel\\nWant to see a novel way to operationalize a machine learning\\nmodel? Imagine that you’re a software developer at an\\ninternet vacation rentals firm. The company’s communications\\ndepartment has asked you to create a spreadsheet that lets\\nthem analyze text for sentiment. The idea is that if'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 271, 'file_type': 'pdf'}, page_content='270\\nsentiment toward the company turns negative on social media,\\nthe communications team can get out in front of it.\\nYou already know how to build, train, and save a sentiment\\nanalysis model in Python using Scikit. Excel supports user-\\ndefined functions (UDFs), which enable users to write custom\\nfunctions that are called just like SUM(), AVG(), and other\\nfunctions built into Excel. But UDFs are written in Visual\\nBasic for Applications (VBA), not Python. To marry Scikit\\nwith Excel, you need to write UDFs in Python.\\nFortunately, there are libraries that let you do just that.\\nOne of them is Xlwings, an open source library that combines\\nthe power of Excel with the versatility of Python. With it,\\nyou can write Python code that loads or creates Excel\\nspreadsheets and manipulates their content, write Python\\nmacros triggered by button clicks in Excel, access Excel\\nspreadsheets from Jupyter notebooks, and more. You can also\\nuse Xlwings to write Python UDFs for Excel for Windows.\\nThe first step in building the spreadsheet the communications\\nteam wants is to configure Excel to trust VBA add-ins. Launch\\nMicrosoft Excel and use the File → Options command to open\\nthe Excel Options dialog. Click Trust Center in the menu on\\nthe left, and then click the Trust Center Settings button.\\nClick Macro Settings on the left, and check the “Trust\\naccess to the VBA project object model” box, as shown in\\nFigure\\xa07-2. Then click OK to dismiss the Trust Center\\ndialog, followed by OK to dismiss the Excel Options dialog.\\nThe next step is to install Xlwings on your computer using a\\npip install xlwings command or equivalent for your Python\\nenvironment. Afterward, go to the command line and use the\\nfollowing command to install the Xlwings add-in in Excel:\\n\\nxlwings addin install'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 272, 'file_type': 'pdf'}, page_content='271\\nNow create a project directory on your computer and cd to it.\\nThen execute the following command:\\n\\nxlwings quickstart sentiment\\nFigure 7-2. Configuring Excel to trust VBA add-ins\\nThis command creates a subdirectory named sentiment in the\\ncurrent directory and initializes it with a pair of files: a\\nspreadsheet named sentiment.xlsm and a Python file named\\nsentiment.py. It is the latter of these in which you will\\nwrite a UDF that analyzes text for sentiment.\\nNext, copy sentiment.pkl into the sentiment directory. (If\\nyou didn’t generate that file earlier, run the code in\\nExample\\xa07-1 in a Jupyter notebook to generate it now.) Then\\nopen sentiment.py in your favorite text editor and replace\\nits contents with the following code. This code loads the\\nsentiment analysis model from the .pkl file and stores a\\nreference to the model in the variable named model. Then it'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 273, 'file_type': 'pdf'}, page_content=\"272\\ndefines a UDF named analyze_text that can be called from\\nExcel. analyze_text vectorizes the text passed to it and\\ninputs it to the model’s predict_proba method. That method\\nreturns a number from 0.0 to 1.0, with 0.0 representing\\nnegative sentiment and 1.0 representing positive sentiment.\\nWhen you’re done, save your changes to sentiment.py. The UDF\\nis written. Now it’s time to call it from Excel.\\n\\nimport\\npickle,\\nos\\nimport\\nxlwings\\nas\\nxw\\n\\n# Load the model and the vocabulary and create a CountVectorizer\\nmodel_path\\n=\\nos.path.abspath(os.path.join(os.path.dirname(__file__),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'sentiment.pkl'))\\n\\nmodel\\n=\\npickle.load(open(model_path,\\n'rb'))\\n\\n@xw.func\\ndef\\nanalyze_text(text):\\n\\xa0\\xa0\\xa0 score\\n=\\nmodel.predict_proba([text])[0][1]\\n\\xa0\\xa0\\xa0 return\\nscore\\nThe elegance of Xlwings is that once you’ve written a UDF\\nsuch as analyze_text, you can call it the same way you call\\nfunctions built into Excel. But first you must import the\\nUDF. To do that, open sentiment.xlsm in Excel. Go to the\\n“xlwings” tab and click Import Functions to import the\\nanalyze_text function, as shown in Figure\\xa07-3. Excel\\ndoesn’t tell you if the import was successful, but it does\\nlet you know if the import failed—if, for example, you\\nforgot to copy sentiment.pkl into the directory where\\nsentiment.py is stored.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 274, 'file_type': 'pdf'}, page_content='273\\nFigure 7-3. Importing the analyze_text function\\nType Great food and excellent service into cell A1 (Figure\\xa07-\\n4).\\nFigure 7-4. Entering a string of text to analyze\\nType the following expression into cell B1 to pass the text\\nin cell A1 to the analyze_text function imported from\\nsentiment.py:\\n\\n=analyze_text(A1)\\nConfirm that a number from 0.0 to 1.0 appears in cell B1, as\\nshown in Figure\\xa07-5. This is the score that the machine'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 275, 'file_type': 'pdf'}, page_content='274\\nlearning model assigned to the text “Great food and\\nexcellent service.” Do you agree with the score? Finish up\\nby entering some text strings of your own to see how they\\nscore for sentiment.\\nFigure 7-5. Sentiment analysis in Excel\\nUDFs written in Python present Excel users with a new whole\\nnew world of possibilities thanks to the rich ecosystem of\\nPython libraries available for machine learning, statistical\\nanalysis, and other tasks. And they provide a valuable\\nopportunity for Excel users to operationalize machine\\nlearning models written in Python.\\nSummary\\nWriting a Python client that invokes a Python machine\\nlearning model requires little more than an extra line of\\ncode to deserialize the model from a .pkl file. One way for a\\nnon-Python client to invoke a Python model is to wrap the\\nmodel in a Python web service and invoke the model using REST\\nAPIs. The web service can be hosted locally or in the cloud,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 276, 'file_type': 'pdf'}, page_content='275\\nand containerizing the web service (and the model) simplifies\\ndeployment and makes the software more portable.\\nAn alternative approach is to use ONNX to bridge the language\\ngap. With ONNX, you can save a Scikit model to a .onnx file\\nand load the model from a variety of programming languages,\\nincluding C, C++, C#, Java, and JavaScript. Once loaded, the\\nmodel can be invoked just as if it were called from Python.\\nAnother option for invoking machine learning models from non-\\nPython clients is to write the model in the same language as\\nthe client. Microsoft’s ML.NET, which is free, cross-\\nplatform, and open source, is a great option for C#\\ndevelopers. Other libraries include Java-ML for Java\\ndevelopers and Caret and Tidymodels for R developers. The\\nAPIs supported by these libraries are different from the APIs\\nin Scikit, but the principles embodied in them are the same.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 277, 'file_type': 'pdf'}, page_content='276\\nPart II. Deep Learning with\\nKeras and TensorFlow'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 278, 'file_type': 'pdf'}, page_content='277\\nChapter 8. Deep Learning\\nEvery model in Part\\xa0I\\xa0of this book employed classic machine\\nlearning algorithms that form the core of ML itself: logistic\\nregression, random forests, and so on. Such models are often\\nreferred to as traditional machine learning models to\\ndifferentiate them from deep-learning models. Recall from\\nChapter\\xa01 that deep learning is a subset of machine learning\\nthat relies primarily on neural networks, and that most of\\nwhat’s considered AI today is accomplished with deep\\nlearning. From recognizing objects in photos to real-time\\nspeech translation to using computers to generate art, music,\\npoetry, and photorealistic faces, deep learning allows\\ncomputers to perform feats that traditional machine learning\\ndoes not.\\nI frequently introduce deep learning to software developers\\nby challenging them to devise an algorithmic means for\\ndetermining whether a photo contains a dog. If they offer a\\nsolution, I’ll counter with a dog picture that foils the\\nalgorithm. Traditional ML models can partially solve the\\nproblem, but when it comes to recognizing objects in images,\\ndeep learning represents the state of the art. It’s not\\nterribly difficult to train a neural network to recognize dog\\npictures, sometimes more accurately than humans. Once you\\nlearn how to do that, it’s a small step forward to\\nrecognizing defective parts coming off an assembly line or\\nbicycles passing in front of a self-driving car.\\nNeural networks have been around for decades, but it’s only\\nin the past 10 years or so that sufficient compute power has\\nbeen available to train sophisticated networks. Cutting-edge\\nneural networks are trained on graphics processing units\\n(GPUs) and tensor processing units (TPUs), often attached to\\nhigh-performance computing clusters. GPUs are great for'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 279, 'file_type': 'pdf'}, page_content='278\\ngaming because they deliver high-performance graphics. They\\nare also efficient parallel processing machines that allow\\ndata scientists to train neural networks in a fraction of the\\ntime required on ordinary CPUs. Today, any researcher with a\\ncredit card can purchase an NVIDIA GPU or spin up GPUs in\\nAzure or AWS and have access to compute power that\\nresearchers 20 years ago could only have dreamed of. This,\\nmore than anything else, has driven AI’s resurgence and\\nprecipitated continual advances in the state of the art.\\nThis chapter is the first of several focused on deep\\nlearning. In it, you’ll learn:\\n\\xa0\\nWhat a neural network is and where the “deep”\\nin deep learning comes from\\nHow a neural network transforms input into output\\nusing simple mathematical operations\\nWhat happens when a neural network is trained, as\\nwell as the challenges that training entails\\nYou won’t start building and training neural networks just\\nyet; that begins in Chapter\\xa09. Before you build a house, you\\nneed a foundation to build upon. That foundation begins right\\nnow.\\nUnderstanding Neural Networks\\nNeural networks come in many varieties. Convolutional neural\\nnetworks (CNNs), for example, excel at computer-vision tasks\\nsuch as classifying images. Recurrent neural networks (RNNs)\\nfind application in handwriting recognition and natural\\nlanguage processing (NLP), while generative adversarial\\nnetworks, or GANs, enable computers to create art, music, and\\nother content. But the first step in wrapping your head\\naround deep learning is to understand what a neural network\\nis and how it works.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 280, 'file_type': 'pdf'}, page_content='279\\nThe simplest type of neural network is the multilayer\\nperceptron. It consists of nodes or neurons arranged in\\nlayers. The depth of the network is the number of layers; the\\nwidth is the number of neurons in each layer, which can be\\ndifferent for every layer. State-of-the-art neural networks\\nsometimes contain 100 or more layers and thousands of neurons\\nin individual layers. A deep neural network is one that\\ncontains many layers, and it’s where the term deep learning\\nis derived from.\\nThe multilayer perceptron in Figure\\xa08-1 contains three\\nlayers: an input layer with two neurons, a middle layer (also\\nknown as a hidden layer) with three neurons, and an output\\nlayer with one neuron. Because the input layer is often\\nignored when counting layers, some would argue that this\\nnetwork contains two layers, not three. Regardless, the\\nnetwork’s job is to take two floating-point values as input\\nand produce a single floating-point number as output. Neural\\nnetworks work with floating-point numbers. They only work\\nwith floating-point numbers. As with traditional machine\\nlearning models, a neural network can only process non-\\nnumeric data—for example, text strings—if the data is first\\nconverted to numbers.\\nFigure 8-1. Multilayer perceptron\\nThe orange arrows in Figure\\xa08-1 represent connections\\nbetween neurons. Each neuron in each layer is connected to\\neach neuron in the next layer, giving rise to the term fully'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 281, 'file_type': 'pdf'}, page_content='280\\nconnected layers. Each connection is assigned a weight, which\\nis typically a small floating-point number. In addition, each\\nneuron outside the input layer is assigned a bias, which is\\nalso a small floating-point number. Figure\\xa08-2 shows a set\\nof weights and biases that enable the network to sum two\\ninputs (for example, to add 2 and 2). The blocks labeled\\n“ReLU” represent activation functions, which apply simple\\nnonlinear transforms to values propagated through the\\nnetwork. The most commonly used activation function is the\\nrectified linear units (ReLU) function, which passes positive\\nnumbers through unchanged while converting negative numbers\\nto 0s. Without activation functions, neural networks would\\nstruggle to model nonlinear data. And it’s no secret that\\nreal-world data tends to be nonlinear.\\nFigure 8-2. Weights and biases\\nNeurons perform simple linear transformations on data input\\nto them. For a neuron with a single input x, the neuron’s\\nvalue y is computed by multiplying x by the weight m assigned\\nto the input and adding b, the neuron’s bias:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 282, 'file_type': 'pdf'}, page_content='281\\nLook familiar? That’s the equation for linear regression.\\nScikit-Learn has a Perceptron class that models this behavior\\nand can be used to build neural linear regression models. It\\neven offers classes named MLPRegressor and MLPClassifier for\\nbuilding simple multilayer perceptrons. Scikit is not,\\nhowever, a deep-learning library. Real deep-learning\\nlibraries do more to support advanced neural networks.\\nNOTE\\nThe combination of neurons that perform linear transformations and\\nactivation functions that apply nonlinear transforms is an\\nembodiment of the universal approximation theorem, which states\\nthat you can approximate any function f by summing the output from\\nlinear functions and transforming it with a nonlinear function.\\nTextbooks often say that activation functions “add nonlinearity”\\nto neural networks. Now you know why.\\nTo turn inputs into outputs, a neural network assigns the\\ninput values to the neurons in the input layer. Then it\\nmultiplies the values of the input neurons by the weights\\nconnecting them to the neurons in the next layer, sums the\\ninputs for each neuron, and adds the biases. It repeats this\\nprocess to propagate values from left to right all the way to'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 283, 'file_type': 'pdf'}, page_content='282\\nthe output layer. Figure\\xa08-3 shows what happens in the first\\ntwo layers when the network in Figure\\xa08-2 adds 2 and 2.\\nFigure 8-3. Flow of data from the input layer to the hidden layer when\\nadding 2 and 2\\nValues propagate from the hidden layer to the output layer\\nthe same way, with one exception: they are transformed by an\\nactivation function before they’re multiplied by weights.\\nRemember that the ReLU activation function turns negative\\nnumbers into 0s. In Figure\\xa08-4, the –1.83 calculated for\\nthe middle neuron in the hidden layer is converted to 0 when\\nforwarded to the output layer, effectively eliminating that\\nneuron’s contribution to the output.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 284, 'file_type': 'pdf'}, page_content='283\\nFigure 8-4. Flow of data from the hidden layer to the output layer when\\nadding 2 and 2\\nGiven a set of weights and biases, it isn’t difficult to\\ncode a neural network by hand. The following Python code\\nmodels the network in Figure\\xa08-2:\\n\\n# Weights\\nw0\\n=\\n0.9907079\\nw1\\n=\\n1.0264927\\nw2\\n=\\n0.01417504\\nw3\\n=\\n-0.8950311\\nw4\\n=\\n0.88046944\\nw5\\n=\\n0.7524377\\nw6\\n=\\n0.794296\\nw7\\n=\\n1.1687347\\nw8\\n=\\n0.2406084\\n\\n# Biases\\nb0\\n=\\n-0.00070612\\nb1\\n=\\n-0.06846002'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 285, 'file_type': 'pdf'}, page_content='284\\nb2\\n=\\n-0.00055442\\nb3\\n=\\n-0.00000929\\n\\ndef\\nrelu(x):\\n\\xa0\\xa0\\xa0 return\\nmax(0,\\nx)\\n\\ndef\\npredict(x1,\\nx2):\\n\\xa0\\xa0\\xa0 h1\\n=\\n(x1\\n*\\nw0)\\n+\\n(x2\\n*\\nw1)\\n+\\nb0\\n\\xa0\\xa0\\xa0 h2\\n=\\n(x1\\n*\\nw2)\\n+\\n(x2\\n*\\nw3)\\n+\\nb1\\n\\xa0\\xa0\\xa0 h3\\n=\\n(x1\\n*\\nw4)\\n+\\n(x2\\n*\\nw5)\\n+\\nb2\\n\\xa0\\xa0\\xa0 y\\n=\\n(relu(h1)\\n*\\nw6)\\n+\\n(relu(h2)\\n*\\nw7)\\n+\\n(relu(h3)\\n*\\nw8)\\n+\\nb3\\n\\xa0\\xa0\\xa0 return\\ny\\nIf you’d like to see for yourself, paste the code into a\\nJupyter notebook and call the predict function with the\\ninputs 2 and 2. The answer should be very close to the actual\\nsum of 2 and 2.\\nFor a given problem, there is an infinite combination of\\nweights and biases that produces the desired outcome.\\nFigure\\xa08-5 shows the same network with a completely\\ndifferent set of weights and biases. Yet, if you plug the\\nvalues into the preceding code (or propagate values through\\nthe network by hand), you’ll find that the network is\\nequally capable of adding 2 and 2—or other small values, for\\nthat matter.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 286, 'file_type': 'pdf'}, page_content='285\\nFigure 8-5. Adding 2 and 2 with a different set of weights and biases\\nGiven a set of weights and biases, using a neural network to\\nmake predictions is simplicity itself. It’s little more than\\nmultiplication and addition. But coming up with a set of\\nweights and biases to begin with is a challenge. It’s why\\nneural networks must be trained.\\nTraining Neural Networks\\nTraining a traditional machine learning model fits it to a\\ndataset. Neural networks require training too, and it is\\nduring training that weights and biases are calculated.\\nWeights are typically initialized with small random numbers.\\nBiases are usually initialized with 0s. In its untrained\\nstate, a neural network can do little more than generate\\nrandom outputs. Once training is complete, the weights and\\nbiases enable the network to distinguish dogs from cats,\\ntranslate a book review to another language, or do whatever\\nelse it was designed to do.\\nWhat happens when a neural network is trained? At a high\\nlevel, training samples are fed through the network, the\\nerror (the difference between the computed output and the\\ncorrect output) is computed using a loss function, and a'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 287, 'file_type': 'pdf'}, page_content='286\\nbackpropagation algorithm goes backward through the network\\nadjusting the weights and biases (Figure\\xa08-6). This is done\\nrepeatedly until the error is sufficiently small. With each\\niteration, the weights and biases become incrementally more\\nrefined and the error commensurately smaller.\\nFigure 8-6. Adjusting weights and biases during training\\nThe most critical component of the backpropagation regimen is\\nthe optimizer, which on each backward pass decides how much\\nand in which direction, positive or negative, to adjust the\\nweights and biases. Data scientists work constantly to find\\nbetter and more efficient optimizers to train networks more\\naccurately and in less time.\\nDo a search on “neural networks” and you’ll turn up lots\\nof articles with lots of complex math. Most of the math is\\nrelated to optimization. An optimizer can’t just guess how\\nto adjust the weights and biases due to their sheer numbers.\\nA neural network containing two hidden layers with 1,000\\nneurons each has 1,000,000 connections between layers, and\\ntherefore 1,000,000 weights to adjust. Training would take\\nforever if the optimization strategy were simply randomly\\nguessing. An optimizer must be intelligent enough to make'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 288, 'file_type': 'pdf'}, page_content='287\\nadjustments that reduce the error in each successive\\niteration.\\nData scientists use plots like the one in Figure\\xa08-7 to\\nvisualize what optimizers do. The plot is called a loss\\nlandscape. It has been reduced to three dimensions for\\nvisualization purposes, but in reality, it contains many\\ndimensions—sometimes millions of them. The multicolored\\ncontour charts the error for different combinations of\\nweights and biases. The optimizer’s goal is to navigate the\\ncontour and find the combination that produces the least\\nerror, which corresponds to the lowest point, or global\\nminimum, in the loss landscape.\\nFigure 8-7. Loss landscape (Source: Alexander Amini, Ava Soleimany, Sertac\\nKaraman, and Daniela Rus, “Spatial Uncertainty Sampling for End-to-End'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 289, 'file_type': 'pdf'}, page_content='288\\nControl,” NeurIPS Bayesian Deep Learning [2017],\\nhttps://arxiv.org/pdf/1805.04829)\\nThe optimizer’s job isn’t an easy one. It involves partial\\nderivatives (calculating the slope of the contour with\\nrespect to each weight and bias), gradient descent (adjusting\\nthe weights and biases to go down the slope rather than up it\\nor sideways), and learning rates, which drive the fractional\\nadjustments made to the weights and biases in each\\nbackpropagation pass. If the learning rate is too great, the\\noptimizer might miss the global minimum. If it’s too small,\\nthe network will take a long time to train. Modern optimizers\\nuse adaptive learning rates that take smaller steps as they\\napproach a minimum, where the slope of the contour is 0. To\\ncomplicate matters, the optimizer must avoid getting trapped\\nin local minima so that it can continue traversing the\\ncontour toward the global minimum where the error is the\\nsmallest. It also has to be wary of “saddle points” where\\nthe slope increases in one direction but falls off in a\\nperpendicular direction.\\nNOTE\\nIf you research gradient descent, you’ll encounter terms such as\\nstochastic gradient descent (SGD) and mini-batch gradient descent\\n(MBGD). Optimization via gradient descent is an iterative process\\nin which samples are fed forward through the network, gradients\\nare computed, and the gradients are combined with the learning\\nrate to update weights and biases. Updating the weights and biases\\nafter every sample is fed forward through the network is\\ncomputationally expensive, so training typically involves running\\nbatches of perhaps 30 to 40 samples through the network, averaging\\nthe error, and then performing a backpropagation pass. That’s\\nMBGD. It speeds training and helps the optimizer bypass local\\nminima. For more information, and for a very readable introduction'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 290, 'file_type': 'pdf'}, page_content='289\\nto the challenges inherent to training neural networks, see the\\narticle “How Neural Networks Are Trained”.\\nNeural networks are fundamentally simple. Training them is\\nmathematically complex. Fortunately, you don’t have to\\nunderstand everything that happens during training in order\\nto build them. Deep-learning libraries such as Keras and\\nTensorFlow insulate you from the math and provide cutting-\\nedge optimizers to do the heavy lifting. But now when you use\\none of these libraries and it asks you to pick a loss\\nfunction and an optimizer, you’ll understand what it’s\\nasking for and why.\\nSummary\\nDeep learning is a subset of machine learning that relies on\\ndeep neural networks, and it is the root of modern AI. It’s\\nhow computers identify objects in images, translate text and\\nspeech into other languages, generate artwork and music, and\\nperform other tasks that were virtually impossible a few\\nyears ago.\\nThe multilayer perceptron is a simple neural network\\ncomprising layers of neurons. Each neuron turns input into\\noutput using a simple mathematical formula. Activation\\nfunctions further transform the data as it passes between\\nlayers by introducing nonli\\u2060nearities, enabling neural\\nnetworks to fit to a variety of datasets. Hidden layers\\nbetween the input layer and the output layer perform the bulk\\nof the computational work, and a multilayer perceptron with\\nmany hidden layers is referred to as a deep neural network.\\nTraining a neural network fits it to a dataset by iteratively\\nadjusting weights and biases—the weights connecting neurons\\nin adjacent layers and the biases assigned to the neurons'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 291, 'file_type': 'pdf'}, page_content='290\\nthemselves—to produce the desired outcome. The\\nbackpropagation passes that adjust the weights and biases are\\nthe heart of the training regimen. The component responsible\\nfor making adjustments is the optimizer; its ultimate goal is\\nto find the optimum combination of weights and biases with as\\nfew backpropagation passes as possible.\\nNow that you understand how neural networks work, the next\\nstep is to learn how to build and train them. For that, data\\nscientists rely on frameworks such as Keras and TensorFlow.\\nChapter\\xa09 begins a deep dive into both.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 292, 'file_type': 'pdf'}, page_content='291\\nChapter 9. Neural Networks\\nMachine learning isn’t hard when you have a properly\\nengineered dataset to work with. The reason it’s not hard is\\nlibraries such as Scikit-Learn and ML.NET, which reduce\\ncomplex learning algorithms to a few lines of code. Deep\\nlearning isn’t difficult either, thanks to libraries such as\\nthe Microsoft Cognitive Toolkit (CNTK), Theano, and PyTorch.\\nBut the library\\xa0 that most of the world has settled on for\\nbuilding neural networks is TensorFlow, an open source\\nframework created by Google that was released under the\\nApache License 2.0 in 2015.\\nTensorFlow isn’t limited to building neural networks. It is\\na framework for performing fast mathematical operations at\\nscale using tensors, which are generalized arrays. Tensors\\ncan represent scalar values (0-dimensional tensors), vectors\\n(1D tensors), matrices (2D tensors), and so on. A neural\\nnetwork is basically a workflow for transforming tensors. The\\nthree-layer perceptron featured in Chapter\\xa08 takes a 1D\\ntensor containing two values as input, transforms it into a\\n1D tensor containing three values, and produces a 0D tensor\\nas output. TensorFlow lets you define directed graphs that in\\nturn define how tensors are computed. And unlike Scikit, it\\nsupports GPUs.\\nThe learning curve for TensorFlow is rather steep. Another\\nlibrary, named Keras, provides a simplified Python interface\\nto TensorFlow and has emerged as the Scikit of deep learning.\\nKeras is all about neural networks. It began life as a\\nstandalone project in 2015 but was integrated into TensorFlow\\nin 2019. Any code that you write using TensorFlow’s built-in\\nKeras module ultimately executes in (and is optimized for)\\nTensorFlow. Even Google recommends using the Keras API.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 293, 'file_type': 'pdf'}, page_content=\"292\\nKeras offers two APIs for building neural networks: a\\nsequential API and a functional API. The former is simpler\\nand is sufficient for most neural networks. The latter is\\nuseful in more advanced scenarios such as networks with\\nmultiple inputs or outputs—for example, a classification\\noutput and a regression output, which is common in neural\\nnetworks that perform object detection—or shared layers.\\nMost of the examples in this book use the sequential API. If\\ncuriosity compels you to learn more about the functional API,\\nsee “How to Use the Keras Functional API for Deep Learning”\\nby Jason Brownlee for a very readable introduction.\\nBuilding Neural Networks with Keras and\\nTensorFlow\\nCreating a neural network using Keras’s sequential API is\\nsimple. You first create an instance of the Sequential class.\\nThen you call add on the Sequential object to add layers. The\\nlayers are instances of classes such as Dense, which\\nrepresents a fully connected layer with a specified number of\\nneurons. The following statements create the three-layer\\nnetwork featured in Chapter\\xa08:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(3,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(1))\\nThis network contains an input layer with two neurons, a\\nhidden layer with three neurons, and an output layer with one\\nneuron. Values passed from the hidden layer to the output\\nlayer are transformed by the rectified linear units (ReLU)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 294, 'file_type': 'pdf'}, page_content=\"293\\nactivation function, which, you’ll recall, turns negative\\nnumbers into 0s and helps the model fit to nonlinear\\ndatasets. Observe that you don’t have to add the input layer\\nexplicitly. The input_dim=2 parameter in the first hidden\\nlayer implicitly creates an input layer with two neurons.\\nNOTE\\nrelu is one of several activation functions included in Keras.\\nOthers include tanh, sigmoid, and softmax. You will rarely if ever\\nuse anything other than relu in the hidden layers. Later in this\\nchapter, you’ll see why functions such as sigmoid and softmax are\\nuseful in the output layers of networks that perform\\nclassification rather than regression.\\nOnce all the layers are added, the next step is to call\\ncompile and specify important attributes such as which\\noptimizer and loss function to use during training. Here’s\\nan example:\\n\\nmodel.compile(optimizer='adam',\\nloss='mae',\\nmetrics=['mae'])\\nLet’s walk through the parameters one at a time:\\noptimizer='adam'\\nTells Keras to use the Adam optimizer to adjust\\nweights and biases in each backpropagation pass\\nduring training. Adam is one of eight optimizers\\nbuilt into Keras, and it is among the most advanced.\\nIt employs an adaptive learning rate and is always\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 295, 'file_type': 'pdf'}, page_content=\"294\\nthe one I start with in the absence of a compelling\\nreason to do otherwise.\\nloss='mae'\\nTells Keras to use mean absolute error (MAE) to\\nmeasure loss. This is common for neural networks\\nintended to solve regression problems. Another\\nfrequently used option for regression models is\\nloss='mse' for mean squared error (MSE).\\nmetrics=['mae']\\nTells Keras to capture MAE values as the network is\\ntrained. This information is used after training is\\ncomplete to judge the efficacy of the training.\\nString values such as 'adam' and 'mae' are shortcuts for\\nfunctions built into Keras. For example, optimizer='adam' is\\nequivalent to optimizer=Adam(). The longhand form is useful\\nfor calling the function with nondefault parameter values—\\nfor example, optimizer=Adam(learning_rate=2e-5) to create an\\nAdam optimizer with a custom learning rate. You’ll see an\\nexample of this in Chapter\\xa013 when we fine-tune a model by\\ntraining it with a low learning rate.\\nInside the compile method, Keras creates a TensorFlow object\\ngraph to speed execution. Once the network is compiled, you\\ntrain it by calling fit:\\n\\nhist\\n=\\nmodel.fit(x,\\ny,\\nepochs=100,\\nbatch_size=100,\\nvalidation_split=0.2)\\nThe fit method accepts many parameters. Here are the ones\\nused in this example:\\nx\\nThe dataset’s feature columns.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 296, 'file_type': 'pdf'}, page_content='295\\ny\\nThe dataset’s label column—the one containing the\\nvalues the network will attempt to predict.\\nepochs=100\\nTells Keras to train the network for 100 iterations,\\nor epochs. In each epoch, all of the training data\\npasses through the network one time.\\nbatch_size=100\\nTells Keras to pass 100 training samples through the\\nnetwork before making a backpropagation pass to\\nadjust the weights and biases. Training takes less\\ntime if the batch size is large, but accuracy could\\nsuffer. You typically experiment with different batch\\nsizes to find the right balance between training time\\nand accuracy. Do not assume that lowering the batch\\nsize will improve accuracy. It frequently does, but\\nsometimes does not.\\nvalidation_split=0.2\\nTells Keras that in each epoch, it should train with\\n80% of the rows in the dataset and validate the\\nnetwork’s accuracy with the remaining 20%. If you\\nprefer, you can split the dataset yourself and use\\nthe validation_data parameter to pass the validation\\ndata to fit. Keras doesn’t offer an explicit\\nfunction for splitting a dataset, but you can use\\nScikit’s train_test_split function to do it. One\\ndifference between train_test_split and\\nvalidation_split is that the former splits the data\\nrandomly and includes an option for performing a\\nstratified split. validation_split, by contrast,\\nsimply divides the dataset into two partitions and\\ndoes not attempt to shuffle or stratify. Don’t use\\nvalidation_split on ordered data without shuffling\\nthe data first.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 297, 'file_type': 'pdf'}, page_content=\"296\\nIt might surprise you to learn that if you train the same\\nnetwork on the same dataset several times, the results will\\nbe different each time. By default, weights are initialized\\nwith random values, and different starting points produce\\ndifferent outcomes. Additional randomness baked into the\\ntraining process means the network will train differently\\neven if it’s initialized with the same random weights.\\nRather than fight it, data scientists learn to “embrace the\\nrandomness.” If you work the tutorial in the next section,\\nyour results will differ from mine. They shouldn’t differ by\\na lot, but they will differ.\\nNOTE\\nThe random weights assigned to the connections between neurons\\naren’t perfectly random. Keras includes about a dozen\\ninitializers, each of which initializes parameters in a different\\nway. By default, Dense layers use the Zeroes initializer to\\ninitialize biases and the GlorotUniform initializer to initialize\\nweights. The latter generates random numbers that fall within a\\nuniform distribution whose limits are computed from the network\\ntopology.\\nYou judge the efficacy of training by examining information\\nreturned by the fit method. fit returns a history object\\ncontaining the training and validation metrics specified in\\nthe metrics parameter passed to the compile method. For\\nexample, metrics=['mae'] captures MAE at the end of each\\nepoch. Charting these metrics lets you determine whether you\\ntrained for the right number of epochs. It also lets you know\\nif the network is underfitting or overfitting. Figure\\xa09-1\\nplots MAE over the course of 30 training epochs.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 298, 'file_type': 'pdf'}, page_content='297\\nFigure 9-1. Training and validation accuracy during training\\nThe blue curve in Figure\\xa09-1 reveals how the network fit to\\nthe training data. The orange curve shows how it tested\\nagainst the validation data. Most of the learning was done in\\nthe first 20 epochs, but MAE continued to drop as training\\nprogressed. The validation MAE nearly matched the training\\nMAE at the end, which is an indication that the network\\nisn’t overfitting. You typically don’t care how well the\\nnetwork fits to the training data. You care about the fit to\\nthe validation data because that indicates how the network\\nperforms with data it hasn’t seen before. The greater the\\ngap between the training and validation accuracy, the greater\\nthe likelihood that the network is overfitting.\\nOnce a neural network is trained, you call its predict method\\nto make a prediction:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 299, 'file_type': 'pdf'}, page_content='298\\n\\nprediction\\n=\\nmodel.predict(np.array([[2,\\n2]]))\\nIn this example, the network accepts two floating-point\\nvalues as input and returns a single floating-point value as\\noutput. The value returned by predict is that output.\\nSizing a Neural Network\\nA neural network is characterized by the number of layers\\n(the depth of the network), the number of neurons in each\\nlayer (the widths of the layers), the types of layers (in\\nthis example, Dense layers of fully connected neurons), and\\nthe activation functions used. There are other layer types,\\nmany of which I will introduce in later chapters. Dropout\\nlayers, for example, can increase a network’s ability to\\ngeneralize by randomly dropping connections between layers\\nduring weight updates, while Conv2D layers enable us to build\\nconvolutional neural networks (CNNs) that excel at image\\nprocessing.\\nWhen designing a network, how do you pick the right number of\\nlayers and the right number of neurons for each layer? The\\nshort answer is that the “right” width and depth depends on\\nthe problem you’re trying to solve, the dataset you’re\\ntraining with, and the accuracy you desire. As a rule, you\\nwant the minimum width and depth required to achieve that\\naccuracy, and you get there using a combination of intuition\\nand experimentation. That said, here are a few guidelines to\\nkeep in mind:\\n\\xa0\\nGreater widths and depths give the network more\\ncapacity to “learn” by fitting more tightly to\\nthe training data. They also increase the\\nlikelihood of overfitting. It’s the validation\\nresults that matter, and sometimes loosening the'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 300, 'file_type': 'pdf'}, page_content='299\\nfit to the training data allows the network to\\ngeneralize better. The simplest way to loosen the\\nfit is to reduce the number of neurons.\\nGenerally speaking, you prefer greater width to\\ngreater depth in part to avoid the vanishing\\ngradient problem, which diminishes the impact of\\nadded layers. The ReLU activation function\\nprovides some protection against vanishing\\ngradients, but that protection isn’t absolute.\\nFor an explanation, see “How to Fix the\\nVanishing Gradients Problem Using the ReLU”. In\\naddition, a network with, say, 100 neurons in one\\nlayer trains faster than a network with five\\nlayers of 20 neurons each because the former has\\nfewer weights. Think about it: there are no\\nconnections between neurons in one layer, but\\nthere are 1,600 connections (202 × 4) between\\nfive layers containing 20 neurons each.\\nFewer neurons means less training time. State-of-\\nthe-art neural networks trained with large\\ndatasets sometimes take days or weeks to train on\\nhigh-end GPUs, so training time is important.\\nIn real life, data scientists experiment with various widths\\nand depths to find the right balance between training time,\\naccuracy, and the network’s ability to generalize. For a\\nmultilayer perceptron, you rarely ever need more than two\\nhidden layers, and one is often sufficient. A network with\\none or two hidden layers has the capacity to solve even\\ncomplex nonlinear problems. Two layers with 128 neurons each,\\nfor example, gives you 16,384 (1282) weights that can be\\nadjusted, plus 256 biases. That’s a lot of fitting power. I\\nfrequently start with one or, at most, two layers of 512\\nneurons each and halve the width or depth until the\\nvalidation accuracy drops below an acceptable threshold.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 301, 'file_type': 'pdf'}, page_content=\"300\\nUsing a Neural Network to Predict Taxi Fares\\nLet’s put this knowledge to work building and training a\\nneural network. The problem that we’ll solve is the same one\\npresented in Chapter\\xa02: using data from the New York City\\nTaxi and Limousine Commission to predict taxi fares. We’ll\\nuse a neural network as a regression model to make the\\npredictions.\\nDownload the CSV file containing the dataset if you didn’t\\ndownload it in Chapter\\xa02 and copy it into the Data directory\\nwhere your Jupyter notebooks are hosted. Then use the\\nfollowing code to load the dataset and show the first five\\nrows. It contains about 55,000 rows and is a subset of a much\\nlarger dataset that was recently used in Kaggle’s New York\\nCity Taxi Fare Prediction competition:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/taxi-fares.csv',\\nparse_dates=['pickup_datetime'])\\ndf.head()\\nThe data requires a fair amount of prep work before it’s\\nuseful—something that’s quite common in machine learning\\nand in deep learning too. Use the following statements to\\ntransform the raw dataset into one suitable for training, and\\nrefer to the taxi-fare example in Chapter\\xa02 for a step-by-\\nstep explanation of the transformations applied:\\n\\nfrom\\nmath\\nimport\\nsqrt\\n\\ndf\\n=\\ndf[df['passenger_count']\\n==\\n1]\\ndf\\n=\\ndf.drop(['key',\\n'passenger_count'],\\naxis=1)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 302, 'file_type': 'pdf'}, page_content=\"301\\nfor\\ni,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 dt\\n=\\nrow['pickup_datetime']\\n\\xa0\\xa0\\xa0 df.at[i,\\n'day_of_week']\\n=\\ndt.weekday()\\n\\xa0\\xa0\\xa0 df.at[i,\\n'pickup_time']\\n=\\ndt.hour\\n\\xa0\\xa0\\xa0 x\\n=\\n(row['dropoff_longitude']\\n-\\nrow['pickup_longitude'])\\n*\\n54.6\\n\\xa0\\xa0\\xa0 y\\n=\\n(row['dropoff_latitude']\\n-\\nrow['pickup_latitude'])\\n*\\n69.0\\n\\xa0\\xa0\\xa0 distance\\n=\\nsqrt(x**2\\n+\\ny**2)\\n\\xa0\\xa0\\xa0 df.at[i,\\n'distance']\\n=\\ndistance\\n\\ndf.drop(['pickup_datetime',\\n'pickup_longitude',\\n'pickup_latitude',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'dropoff_longitude',\\n'dropoff_latitude'],\\naxis=1,\\ninplace=True)\\n\\ndf\\n=\\ndf[(df['distance']\\n>\\n1.0)\\n&\\n(df['distance']\\n<\\n10.0)]\\ndf\\n=\\ndf[(df['fare_amount']\\n>\\n0.0)\\n&\\n(df['fare_amount']\\n<\\n50.0)]\\ndf.head()\\nThe resulting dataset contains columns for the day of the\\nweek (0–6, where 0 corresponds to Monday), the hour of the\\nday (0–23), and the distance traveled in miles, and from\\nwhich outliers have been removed:\\nThe next step is to create the neural network. Use the\\nfollowing statements to create a network with an input layer\\nthat accepts three values (day, time, and distance), two\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 303, 'file_type': 'pdf'}, page_content=\"302\\nhidden layers with 512 neurons each, and an output layer with\\na single neuron (the predicted fare amount):\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\\ninput_dim=3))\\nmodel.add(Dense(512,\\nactivation='relu'))\\nmodel.add(Dense(1))\\nmodel.compile(optimizer='adam',\\nloss='mae',\\nmetrics=['mae'])\\nmodel.summary()\\nThe call to summary in the last statement produces a concise\\nsummary of the network topology, including the number of\\ntrainable parameters—weights and biases that can be adjusted\\nto fit the network to a dataset (Figure\\xa09-2). For a given\\nlayer, the parameter count is the product of the number of\\nneurons in that layer and the previous layer (the number of\\nweights connecting the neurons in the two layers) plus the\\nnumber of neurons in the layer (the biases associated with\\nthose neurons). This network is a relatively simple one, and\\nyet it features more than a quarter million knobs and dials\\nthat can be adjusted to fit it to a dataset.\\nNow separate the feature columns from the label column and\\nuse them to train the network. Set validation_split to 0.2 to\\nvalidate the network using 20% of the training data. Train\\nfor 100 epochs and use a batch size of 100. Given that the\\ndataset contains more than 38,000 samples, this means that\\nabout 380 backpropagation passes will be performed in each\\nepoch:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 304, 'file_type': 'pdf'}, page_content=\"303\\n\\nx\\n=\\ndf.drop('fare_amount',\\naxis=1)\\ny\\n=\\ndf['fare_amount']\\n\\nhist\\n=\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=100)\\nFigure 9-2. Trainable parameters in a simple neural network\\nUse the history object returned by fit to plot the training\\nand validation accuracy for each epoch:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nerr\\n=\\nhist.history['mae']\\nval_err\\n=\\nhist.history['val_mae']\\nepochs\\n=\\nrange(1,\\nlen(err)\\n+\\n1)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 305, 'file_type': 'pdf'}, page_content=\"304\\n\\nplt.plot(epochs,\\nerr,\\n'-',\\nlabel='Training MAE')\\nplt.plot(epochs,\\nval_err,\\n':',\\nlabel='Validation MAE')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Mean Absolute Error')\\nplt.legend(loc='upper right')\\nplt.plot()\\nYour results will be slightly different from mine, but they\\nshould look something like this:\\nThe final validation MAE was about 2.25, which means that on\\naverage, a taxi fare predicted by this network should be\\naccurate to within about $2.25.\\nRecall from Chapter\\xa02 that a common accuracy measure for\\nregression models is the coefficient of determination, or R2\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 306, 'file_type': 'pdf'}, page_content='305\\nscore. Keras doesn’t have a function for computing R2\\nscores, but Scikit does. To that end, use the following\\nstatements to compute R2 for the network:\\n\\nfrom\\nsklearn.metrics\\nimport\\nr2_score\\n\\nr2_score(y,\\nmodel.predict(x))\\nAgain, your results will differ from mine but will probably\\nland at around 0.75.\\nFinish up by using the model to predict what it will cost to\\nhire a taxi for a 2-mile trip at 5:00 p.m. on Friday:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nmodel.predict(np.array([[4,\\n17,\\n2.0]]))\\nNow predict the fare amount for a 2-mile trip taken at 5:00\\np.m. one day later (on Saturday):\\n\\nmodel.predict(np.array([[5,\\n17,\\n2.0]]))\\nDoes the model predict a higher or lower fare amount for the\\nsame trip on Saturday afternoon? Do the results make sense\\ngiven that the data comes from New York City cabs?\\nBefore you close out this notebook, use it as a basis for\\nfurther experimentation. Here are a few things you can try in\\norder to gain further insights into neural networks:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 307, 'file_type': 'pdf'}, page_content='306\\nRun the notebook from start to finish a few times\\nand note the differences in R2 scores as well as\\nthe MAE curves. Remember that neural networks are\\ninitialized with random weights each time\\nthey’re created, and additional randomness\\nduring the training process further ensures that\\nthe results will vary from run to run.\\nVary the width of the hidden layers. I used 512\\nneurons in each layer and found that doing so\\nproduced acceptable results. Would 128, 256, or\\n1,024 neurons per layer improve the accuracy? Try\\nit and find out. Since the results will vary\\nslightly from one run to the next, it might be\\nuseful to train the network several times in each\\nconfiguration and average the results.\\nVary the batch size. What effect does that have\\non training time, and why? How about the effect\\non accuracy?\\nFinally, try reducing the network to one hidden layer\\ncontaining just 16 neurons. Train it again and check the R2\\nscore. Does the result surprise you? How many traina\\u2060ble\\nparameters does this network contain?\\nBinary Classification with Neural Networks\\nOne of the common uses for machine learning is binary\\nclassification, which looks at an input and predicts which of\\ntwo possible classes it belongs to. Practical uses include\\nsentiment analysis, spam filtering, and fraud detection. Such\\nmodels are trained with datasets labeled with 1s and 0s\\nrepresenting the two classes, employ popular learning\\nalgorithms such as logistic regression and Naive Bayes, and\\nare frequently built with libraries such as Scikit-Learn.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 308, 'file_type': 'pdf'}, page_content=\"307\\nDeep learning can be used for binary classification too. In\\nfact, building a neural network that acts as a binary\\nclassifier is not much different than building one that acts\\nas a regressor. In the previous section, you built a neural\\nnetwork that solved a regression problem. That network had an\\ninput layer that accepted three values—distance to travel,\\nhour of the day, and day of the week—and output a predicted\\ntaxi fare. Building a neural network that performs binary\\nclassification involves making two simple changes:\\n\\xa0\\nAdd an activation function—specifically, the\\nsigmoid activation function—to the output layer.\\nsigmoid produces a value from 0.0 to 1.0\\nrepresenting the probability that the input\\nbelongs to the positive class. For a reminder of\\nwhat a sigmoid function does, refer to the\\ndiscussion of logistic regression in Chapter\\xa03.\\nChange the loss function to binary_crossentropy,\\nwhich is purpose-built for binary classifiers.\\nAccordingly, change metrics to '[accuracy]' so\\nthat accuracies computed by the loss function are\\ncaptured in the history object returned by fit.\\nHere’s a network designed to perform binary classification\\nrather than regression:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\\ninput_dim=3))\\nmodel.add(Dense(512,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 309, 'file_type': 'pdf'}, page_content=\"308\\nmodel.compile(optimizer='adam',\\nloss='binary_crossentropy',\\nmetrics=\\n['accuracy'])\\nThat’s it. That’s all it takes to create a neural network\\nthat serves as a binary classifier. You still call fit to\\ntrain the network, and you use the returned history object to\\nplot the training and validation accuracy to determine\\nwhether you trained for a sufficient number of epochs and see\\nhow well the network fit to the data.\\nNOTE\\nWhat is binary cross-entropy, and what does it do to help a binary\\nclassifier converge on a solution? During training, the cross-\\nentropy loss function exponentially increases the penalty for\\nwrong outputs to drive the weights and biases more aggressively in\\nthe right direction.\\nLet’s say a sample belongs to the positive class (its label is\\n1), and the network predicts that the probability it’s a 1 is\\n0.9. The cross-entropy loss, also known as log loss, is –log(0.9),\\nwhich is 0.04. But if the network outputs a probability of 0.1 for\\nthe same sample, the error is –log(0.1), which equals 1. What’s\\nsignificant is that if the predicted probability is really wrong,\\nthe penalty is much higher. If the sample is a 1 and the network\\nsays the probability it’s a 1 is a mere 0.0001, the cross-entropy\\nloss is –log(0.0001), or 4. Cross-entropy loss basically pats the\\noptimizer on the back when it’s close to the right answer and\\nslaps it on the hand when it’s not. The worse the prediction, the\\nharder the slap.\\nTo sum up, you build a neural network that performs binary\\nclassification by including a single neuron with sigmoid\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 310, 'file_type': 'pdf'}, page_content='309\\nactivation in the output layer and specifying\\nbinary_crossentropy as the loss function. The output from the\\nnetwork is a probability from 0.0 to 1.0 that the input\\nbelongs to the positive class. Doesn’t get much simpler than\\nthat!\\nMaking Predictions\\nOne of the benefits of a neural network is that it can easily\\nfit nonlinear datasets. You don’t have to worry about trying\\ndifferent learning algorithms as you do with conventional\\nmachine learning models; the network is the learning\\nalgorithm. As an example, consider the dataset in Figure\\xa09-\\n3, in which each data point consists of an x–y coordinate\\npair and belongs to one of two classes.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 311, 'file_type': 'pdf'}, page_content=\"310\\nFigure 9-3. Nonlinear dataset containing two classes\\nThe following code trains a neural network to predict a class\\nbased on a point’s x and y coordinates:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(optimizer='adam',\\nloss='binary_crossentropy',\\nmetrics=\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 312, 'file_type': 'pdf'}, page_content=\"311\\n['accuracy'])\\nhist\\n=\\nmodel.fit(x,\\ny,\\nepochs=40,\\nbatch_size=10,\\nvalidation_split=0.2)\\nThis network contains just one hidden layer with 128 neurons,\\nand yet a plot of the training and validation accuracy\\nreveals that it is remarkably successful in separating the\\nclasses:\\nOnce a binary classifier is trained, you make predictions by\\ncalling its predict method. Thanks to the sigmoid activation\\nfunction, predict returns a number from 0.0 to 1.0\\nrepresenting the probability that the input belongs to the\\npositive class. In this example, purple data points represent\\nthe negative class (0), while red data points represent the\\npositive class (1). Here the network is asked to predict the\\nprobability that a data point at (–0.5, 0.0) belongs to the\\nred class:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 313, 'file_type': 'pdf'}, page_content=\"312\\n\\nmodel.predict(np.array([[-0.5,\\n0.0]]))\\nThe answer is 0.57, which indicates that (–0.5, 0.0) is more\\nlikely to be red than purple. If you simply want to know\\nwhich class the point belongs to, do it this way:\\n\\n(model.predict(np.array([[-0.5,\\n0.0]]))\\n>\\n0.5).astype('int32')\\nThe answer is 1, which corresponds to red. Older versions of\\nKeras included a predict_classes method that did the same\\nwithout the astype cast, but that method was recently\\ndeprecated and removed.\\nTraining a Neural Network to Detect Credit\\nCard Fraud\\nLet’s train a neural network to detect credit card fraud.\\nBegin by downloading a ZIP file containing the dataset if you\\nhaven’t already and copying creditcard.csv from the ZIP file\\ninto your notebooks’ Data subdirectory. It’s the same one\\nused in Chapters 3 and 6. It contains information about\\n284,808 credit card transactions, including the amount of\\neach transaction and a label: 0 for legitimate transactions\\nand 1 for fraudulent transactions. It also contains 28\\ncolumns named V1 through V28 whose meaning has been\\nobfuscated with principal component analysis. The dataset is\\nhighly imbalanced, containing fewer than 500 examples of\\nfraudulent transactions.\\nNow load the dataset:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 314, 'file_type': 'pdf'}, page_content=\"313\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/creditcard.csv')\\ndf.head(10)\\nUse the following statements to drop the Time column, divide\\nthe dataset into features x and labels y, and split the\\ndataset into two datasets: one for training and one for\\ntesting. Rather than allow Keras to do the split for us,\\nwe’ll do it ourselves so that we can later run the test data\\nthrough the network and use a confusion matrix to analyze the\\nresults:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\n=\\ndf.drop(['Time',\\n'Class'],\\naxis=1)\\ny\\n=\\ndf['Class']\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 x,\\ny,\\ntest_size=0.2,\\nstratify=y,\\nrandom_state=0)\\nCreate a neural network for binary classification:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\n\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=29))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\n\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\nmetrics=\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 315, 'file_type': 'pdf'}, page_content=\"314\\n['accuracy'])\\n\\nmodel.summary()\\nThe next step is to train the model. Notice the\\nvalidation_data parameter passed to fit, which uses the test\\ndata split off from the larger dataset to assess the model’s\\naccuracy as training takes place:\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=10,\\nbatch_size=100)\\nNow plot the training and validation accuracy using the per-\\nepoch values in the history object:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\n':',\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 316, 'file_type': 'pdf'}, page_content='315\\nThe result looked like this for me. Remember that your\\nresults will be different thanks to the randomness inherent\\nto training neural networks:\\nOn the surface, the validation accuracy (around 0.9994)\\nappears to be very high. But remember that the dataset is\\nimbalanced. Fraudulent transactions represent less than 0.2%\\nof all the samples, which means that the model could simply\\nguess that every transaction is legitimate and get it right\\nabout 99.8% of the time. Use a confusion matrix to visualize\\nhow the model performs during testing with data it wasn’t\\ntrained with:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\ny_predicted\\n=\\nmodel.predict(x_test)\\n>\\n0.5'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 317, 'file_type': 'pdf'}, page_content=\"316\\nlabels\\n=\\n['Legitimate',\\n'Fraudulent']\\n\\ncmd.from_predictions(y_test,\\ny_predicted,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nHere’s how it turned out for me:\\nYour results will probably vary. Indeed, train the model\\nseveral times and you’ll get different results each time. In\\nthis run, the model correctly identified 56,858 transactions\\nas legitimate while misclassifying legitimate transactions\\njust six times. This means legitimate transactions are\\nclassified correctly about 99.99% of the time. Meanwhile, the\\nmodel caught more than 73% of the fraudulent transactions.\\nThat’s acceptable, because credit card companies would\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 318, 'file_type': 'pdf'}, page_content='317\\nrather allow 100 fraudulent transactions to go through than\\ndecline one legitimate transaction.\\nNOTE\\nData scientists often use three datasets, not two, to train and\\nassess the accuracy of a neural network: a training dataset for\\ntraining, a validation dataset for validating the network (and\\nscoring its progress) as training takes place, and a test dataset\\nfor evaluating the network’s accuracy once training is complete.\\nThe preceding example used the same dataset for validation and\\ntesting—the 20% split off from the original dataset with\\ntrain_test_split. That’s ostensibly fine because validation data is\\nnot used to adjust the network’s weights and biases during\\ntraining. However, if you really want to have confidence in the\\nnetwork’s accuracy, it is never a bad idea to test it with a\\nthird dataset not used for training or validation. In the real\\nworld, the ultimate test of a deep-learning model’s accuracy is\\nhow it performs against data that it has never seen before.\\nA final note regarding this example has to do with an extra\\nparameter you can pass to the fit method that is particularly\\nuseful when dealing with imbalanced datasets. As an\\nexperiment, try replacing the call to fit with the following\\nstatement:\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\nepochs=10,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=100,\\nclass_weight={\\n0:\\n1.0,\\n1:\\n0.01\\n})\\nThen run the notebook again from start to finish. In all\\nlikelihood, the resulting confusion matrix will show zero (or'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 319, 'file_type': 'pdf'}, page_content=\"318\\nat most, one or two) misclassified legitimate transactions,\\nbut the percentage of correctly identified fraudulent\\ntransactions will decrease too. The class_weight parameter in\\nthis example tells the model that you care a lot more about\\nclassifying legitimate samples correctly than correctly\\nidentifying fraudulent samples. You can experiment with\\ndifferent weights for the two classes and find the balance\\nthat best suits the business requirements that prompted you\\nto build the model in the first place.\\nMulticlass Classification with Neural\\nNetworks\\nHere again is a simple binary classifier that accepts two\\ninputs, has a hidden layer with 128 neurons, and outputs a\\nvalue from 0.0 to 1.0 representing the probability that the\\ninput belongs to the positive class:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(optimizer='adam',\\nloss=' binary_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nKey elements include an output layer with one neuron assigned\\nthe sigmoid activation function, and binary_crossentropy as\\nthe loss function. Three simple modifications repurpose this\\nnetwork to do multiclass classification:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 320, 'file_type': 'pdf'}, page_content=\"319\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(4,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nThe changes are as follows:\\n\\xa0\\nThe output layer contains one neuron per class\\nrather than just one neuron. If the dataset\\ncontains four classes, then the output layer has\\nfour neurons. If the dataset contains 10 classes,\\nthen the output layer has 10 neurons. Each neuron\\ncorresponds to one class.\\nThe output layer uses the softmax activation\\nfunction rather than the sigmoid activation\\nfunction. Each neuron in the output layer yields\\na probability for the corresponding class, and\\nthanks to the softmax function, the sum of all\\nthe probabilities is 1.0.\\nThe loss function is\\nsparse_categorical_crossentropy. During training,\\nthis loss function exponentially penalizes error\\nin the probabilities predicted by a multiclass\\nclassifier, just as binary_crossentropy does for\\nbinary classifiers.\\nAfter defining the network, you call fit to train it and\\npredict to make predictions. Since an example is worth a\\nthousand words, let’s fit a neural network to a two-\\ndimensional dataset comprising four classes (Figure\\xa09-4).\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 321, 'file_type': 'pdf'}, page_content=\"320\\nFigure 9-4. Nonlinear dataset containing four classes\\nThe following code trains a neural network to predict a class\\nbased on a point’s x and y coordinates:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(4,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 322, 'file_type': 'pdf'}, page_content='321\\n\\nhist\\n=\\nmodel.fit(x,\\ny,\\nepochs=40,\\nbatch_size=10,\\nvalidation_split=0.2)\\nA plot of the training and validation accuracy reveals that\\nthe network had little trouble separating the classes:\\nYou make predictions by calling the classifier’s predict\\nmethod. For each input, predict returns an array of\\nprobabilities—one per class. The predicted class is the one\\nassigned the highest probability. In this example, purple\\ndata points represent class 0, light blue represent class 1,\\ntaupe represent class 2, and red represent class 3. Here the\\nnetwork is asked to classify a point that lies at (0.2, 0.8):\\n\\nmodel.predict(np.array([[0.2,\\n0.8]]))'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 323, 'file_type': 'pdf'}, page_content='322\\nThe answer is an array of four probabilities corresponding to\\nclasses 0, 1, 2, and 3, in that order:\\n\\n[2.1877741e-02, 5.3804164e-05, 5.0240371e-02, 9.2782807e-01]\\nThe network predicted there’s a 2% chance that (0.2, 0.8)\\ncorresponds to class 0, a 0% chance that it corresponds to\\nclass 1, a 5% chance that it corresponds to class 2, and a\\n93% chance that it corresponds to class 3. Looking at the\\nplot, that seems like a reasonable answer.\\nIf you simply want to know which class the point belongs to,\\nyou can do it this way:\\n\\nnp.argmax(model.predict(np.array([[0.2,\\n0.8]])),\\naxis=1)\\nThe answer is 3, which corresponds to red. Older versions of\\nKeras included a predict_classes method that did the same\\nwithout the call to argmax, but that method has since been\\ndeprecated and removed.\\nNOTE\\nKeras also includes a loss function named categorical_cross\\u200ben\\u2060tropy\\nthat is frequently used for multiclass classification. It works\\nlike sparse_categorical_cross\\u200ben\\u2060tropy, but it requires labels to be\\none-hot-encoded. Rather than pass fit a label column containing\\nvalues from 0 to 3, for example, you pass it four columns\\ncontaining 0s and 1s. Keras provides a utility function named\\nto_categorical to do the encoding. If you use\\nsparse_categorical_crossentropy, however, you can use the label\\ncolumn as is.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 324, 'file_type': 'pdf'}, page_content='323\\nTraining a Neural Network to Recognize Faces\\nChapter\\xa05 documented the steps for training a support vector\\nmachine to recognize faces. Let’s train a neural network to\\ndo the same. We’ll use the same dataset as before: the\\nLabeled Faces in the Wild (LFW) dataset, which contains more\\nthan 13,000 facial images of famous people and is built into\\nScikit as a sample dataset. Recall that of the more than\\n5,000 people represented in the dataset, 1,680 have two or\\nmore facial images, while only 5 have 100 or more. We’ll set\\nthe minimum number of faces per person to 100, which means\\nthat five sets of faces corresponding to five famous people\\nwill be imported.\\nStart by creating a new Jupyter notebook and using the\\nfollowing statements to load the dataset:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\n=\\nfetch_lfw_people(min_faces_per_person=100)\\nimage_count\\n=\\nfaces.images.shape[0]\\nimage_height\\n=\\nfaces.images.shape[1]\\nimage_width\\n=\\nfaces.images.shape[2]\\nclass_count\\n=\\nlen(faces.target_names)\\n\\nfaces\\n=\\nfetch_lfw_people(min_faces_per_person=100)\\nprint(faces.target_names)\\nprint(faces.images.shape)\\nIn total, 1,140 facial images were loaded. Each measures 47\\n× 62 pixels. Use the following code to show the first 24\\nimages in the dataset and the people to whom the faces\\nbelong:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 325, 'file_type': 'pdf'}, page_content=\"324\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n\\nfig,\\nax\\n=\\nplt.subplots(3,\\n8,\\nfigsize=(18,\\n10))\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nCheck the balance in the dataset by generating a histogram\\nshowing how many facial images were imported for each person:\\n\\nfrom\\ncollections\\nimport\\nCounter\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\ncounts\\n=\\nCounter(faces.target)\\nnames\\n=\\n{}\\n\\nfor\\nkey\\nin\\ncounts.keys():\\n\\xa0\\xa0\\xa0 names[faces.target_names[key]]\\n=\\ncounts[key]\\n\\ndf\\n=\\npd.DataFrame.from_dict(names,\\norient='index')\\ndf.plot(kind='bar')\\nThere are far more images of George W. Bush than of anyone\\nelse in the dataset. Classification models are best trained\\nwith balanced datasets. Use the following code to reduce the\\ndataset to 100 images of each person:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nmask\\n=\\nnp.zeros(faces.target.shape,\\ndtype=bool)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 326, 'file_type': 'pdf'}, page_content='325\\n\\nfor\\ntarget\\nin\\nnp.unique(faces.target):\\n\\xa0\\xa0\\xa0 mask[np.where(faces.target\\n==\\ntarget)[0][:100]]\\n=\\n1\\n\\nx_faces\\n=\\nfaces.data[mask]\\ny_faces\\n=\\nfaces.target[mask]\\nx_faces.shape\\nx_faces contains 500 facial images, and y_faces contains the\\nlabels that go with them: 0 for Colin Powell, 1 for Donald\\nRumsfeld, and so on.\\nThe next step is to divide the pixel values by 255. After\\nthat, split the data for training and testing. We’ll set\\naside 20% of the data for testing, let Keras use it to\\nvalidate the model during training, and later use it to\\nassess the results with a confusion matrix:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nface_images\\n=\\nx_faces\\n/\\n255\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(face_images,\\ny_faces,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 train_size=0.8,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y_faces,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nNOTE\\nWhy divide the pixel values by 255? Neural networks frequently\\ntrain better with normalized data, and dividing by 255 is a simple\\nway to normalize pixel values. It’s not uncommon to use Scikit’s\\nStandardScaler class to apply unit variance instead, and sometimes\\na network trains better that way. The only way to find out is to'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 327, 'file_type': 'pdf'}, page_content=\"326\\ntry. In this example, I tried both and found that the results were\\nabout the same either way.\\nCreate a neural network containing one hidden layer with 512\\nneurons. Use sparse_categorical_crossentropy as the loss\\nfunction and softmax as the activation function in the output\\nlayer since this is a multiclass classification task:\\n\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_shape=(image_width\\n*\\nimage_height,)))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary()\\nNow train the network:\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=100,\\nbatch_size=20)\\nPlot the training and validation accuracy:\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 328, 'file_type': 'pdf'}, page_content=\"327\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nFinally, use a confusion matrix to visualize how the network\\nperforms against test data:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\ny_pred\\n=\\nmodel.predict(x_test)\\nfig,\\nax\\n=\\nplt.subplots(figsize=(5,\\n5))\\nax.grid(False)\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=faces.target_names,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHow many times did the model correctly identify George W.\\nBush? How many times did it identify him as someone else?\\nWould the network be just as accurate with 128 neurons in the\\nhidden layer as it is with 512?\\nDropout\\nThe goal\\xa0of any machine learning model is to make accurate\\npredictions. In a perfect world, the gap between training\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 329, 'file_type': 'pdf'}, page_content=\"328\\naccuracy and validation accuracy would be close to 0 in the\\nlater stages of training a neural network. In the real world,\\nit rarely happens that way. Training accuracy for the model\\nin the previous section approached 100%, but validation\\naccuracy probably peaked between 80% and 85%. This means the\\nmodel isn’t generalizing as well as you’d like. It learned\\nthe training data very well, but when presented with data it\\nhadn’t seen before (the validation data), it underperformed.\\nThis may be a sign that the model is overfitting. In the end,\\nit’s not training accuracy that matters; it’s how\\naccurately the model responds to new data.\\nOne way to combat overfitting is to reduce the depth of the\\nnetwork, the width of individual layers, or both. Fewer\\nneurons means fewer trainable parameters, and fewer\\nparameters makes it harder for the network to fit too tightly\\nto the training data.\\nAnother way to guard against overfitting is to introduce\\ndropout to the network. Dropout randomly drops connections\\nbetween layers during training to prevent the network from\\nlearning the training data too well. It’s like reading a\\nbook but skipping every other page in hopes that you’ll\\nlearn high-level concepts without getting bogged down in the\\ndetails. Dropout was introduced in a 2014 paper titled\\n“Dropout: A Simple Way to Prevent Neural Networks from\\nOverfitting”.\\nKeras’s Dropout class makes adding dropout to a network\\ndead-simple. To demonstrate, go back to the example in the\\nprevious section and redefine the network this way:\\n\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nDropout\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 330, 'file_type': 'pdf'}, page_content=\"329\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_shape=(image_width\\n*\\nimage_height,)))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nNow train the network again and plot the training and\\nvalidation accuracy. Here’s how it turned out for me:\\nThe gap didn’t close much (if at all), but sometimes adding\\ndropout in this manner will increase the validation accuracy.\\nThe key statement in this example is model.add(Dropout(0.2)),\\nwhich adds a Dropout layer that randomly drops (ignores) 20%\\nof the connections between the neurons in the hidden layer\\nand the neurons in the output layer in each backpropagation\\npass. You can be more aggressive by dropping more connections\\n—increasing 0.2 to 0.4, for example—but you can also reach\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 331, 'file_type': 'pdf'}, page_content=\"330\\na point of diminishing returns. Note that if you’re super-\\naggressive with the dropout rate (for example, 0.5), you’ll\\nprobably need to train the model for more epochs. Making it\\nharder to learn means taking longer to learn too.\\nIn practice, the only way to know whether dropout will\\nimprove a model’s ability to generalize is to try it. In\\naddition to trying different dropout percentages, you can try\\nintroducing dropout between two or more layers in hopes of\\nfinding a combination that works.\\nSaving and Loading Models\\nIn Chapter\\xa07, you learned how to serialize (save) a trained\\nScikit model and load it in a client app. The same\\nrequirement applies to neural networks: you need a way to\\nsave a trained network and load it later in order to\\noperationalize it.\\nYou can get the weights and biases from a model with Keras’s\\nget_weights method, and you can restore them with\\nset_weights. But saving a trained model so that you can re-\\ncreate it later requires an additional step. Specifically,\\nyou must save the network architecture: the number of and\\ntypes of layers, the number of neurons in each layer, the\\nactivation functions used in each layer, and so on.\\nFortunately, all that requires just one line of code. That\\nline of code differs depending on which of two formats you\\nwant the model saved in:\\n\\nmodel.save('my_model.h5')\\n# Save the model in Keras's H5 format\\nmodel.save('my_model')\\xa0\\xa0\\xa0 # Save the model in TensorFlow's native format\\nLoading a saved model is equally simple:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 332, 'file_type': 'pdf'}, page_content=\"331\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nload_model\\n\\nmodel\\n=\\nload_model('my_model.h5')\\n# Load model saved in H5 format\\nmodel\\n=\\nload_model('my_model')\\n# Load model saved in TensorFlow format\\nSaving the model in H5 format produces a single .h5 file that\\nencapsulates the entire model. Saving it in TensorFlow’s\\nnative format, also known as the SavedModel format, produces\\na series of files and subdirectories containing the\\nserialized model. Google recommends using the latter,\\nalthough it’s still common to see Keras’s H5 format used.\\nThere is no functional difference between the two, but .h5\\nfiles can only be read by Keras apps written in Python, while\\nmodels saved in SavedModel format can be loaded by other\\nframeworks. Apps written in C# with Microsoft’s ML.NET, for\\nexample, can load models saved in SavedModel format\\nregardless of the programming language in which the model was\\ncrafted.\\nNOTE\\nThe H5 format was originally devised so that Keras models could be\\nsaved in a manner independent of the deep-learning framework used\\nas the backend. Keras is still available in a standalone version\\nthat supports backends other than TensorFlow (specifically, CNTK\\nand Theano), but those frameworks have been deprecated—they are\\nno longer being developed—and are rarely used today other than in\\nlegacy models. The version of Keras built into TensorFlow supports\\nonly TensorFlow backends.\\nOnce a saved model is loaded, it acts identically to the\\noriginal. The predictions that it makes, for example, are\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 333, 'file_type': 'pdf'}, page_content='332\\nidentical to the predictions made by the original model. You\\ncan even further train the model by running additional\\ntraining samples through it. This highlights one of the major\\ndifferences between neural networks and traditional machine\\nlearning models built with Scikit. Since the state of a\\nnetwork is defined by its weights and biases and loading a\\nmodel restores the weights and biases, neural networks\\ninherently support incremental training, also known as\\ncontinual learning, so that they can become smarter over\\ntime. Most Scikit models do not because serializing the\\nmodels doesn’t save the internal state accumulated as\\ntraining takes place.\\nTo recap: you can run a million training samples through a\\nneural network, save it, load it, and run another million\\ntraining samples through it and the network picks up right\\nwhere it left off. The results are identical to running 2\\nmillion training samples through the network to begin with\\nsave for minor differences that result from the randomness\\nthat is always inherent to training.\\nKeras Callbacks\\nAs you train a neural network and it achieves peak validation\\naccuracy, the peak is hard to capture. Rather than nicely\\nlevel out in later epochs, the validation accuracy may go\\ndown or oscillate between peaks and valleys. Given the\\nstochastic (random) nature of neural networks, if you mark\\nthe epoch that achieved maximum validation accuracy and train\\nagain for exactly that number of epochs, you won’t get the\\nsame results the second time. How do you train for exactly\\nthe right number of epochs to produce the best (most\\naccurate) network possible?\\nAn elegant solution is Keras’s callbacks API, which lets you\\nwrite callback functions that are called at various points\\nduring training—for example, at the end of each epoch—and\\nthat have the ability to alter and even stop the training'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 334, 'file_type': 'pdf'}, page_content=\"333\\nprocess. Here’s an example that creates a child class named\\nStopCallback that inherits from Keras’s Callback class. The\\nchild class implements the on_epoch_end function that’s\\ncalled at the end of each training epoch and stops training\\nif the validation accuracy reaches 95%:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nCallback\\n\\nclass\\nStopCallback(Callback):\\n\\xa0\\xa0\\xa0 accuracy_threshold\\n=\\nNone\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 def __init__(self,\\nthreshold):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.accuracy_threshold\\n=\\nthreshold\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 def\\non_epoch_end(self,\\nepoch,\\nlogs=None):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(logs.get('val_accuracy')\\n>=\\nself.accuracy_threshold):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.model.stop_training\\n=\\nTrue\\n\\ncallback\\n=\\nStopCallback(0.95)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nmodel.save('best_model.h5')\\nNote the validation accuracy threshold (0.95) passed to\\nStopCallback’s constructor. The call to fit ostensibly\\ntrains the network for 100 epochs, but if the validation\\naccuracy reaches 0.95 before that, training stops in its\\ntracks. The final statement saves the model that achieved\\nthat accuracy.\\non_epoch_end is one of several functions you can implement in\\nclasses that inherit from Callback to receive a callback when\\na predetermined checkpoint is reached in the training\\nprocess. Others include on_epoch_begin, on_train_begin,\\non_train_end, on_train_batch_begin, and on_train_batch_end.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 335, 'file_type': 'pdf'}, page_content=\"334\\nYou’ll find a complete list, along with examples, in\\n“Writing Your Own Callbacks” in the Keras documentation.\\nIn addition to providing a base Callback class from which you\\ncan create your own callback classes, Keras provides several\\ncallback classes of its own. One of them is the EarlyStopping\\nclass, which lets you stop training based on a specified\\ncriterion such as decreasing validation accuracy or\\nincreasing training loss without writing a lot of code. In\\nthe following example, training stops early if the validation\\naccuracy fails to improve for five consecutive epochs\\n(patience=5). When training is halted, the network’s weights\\nand biases are automatically restored to what they were when\\nvalidation accuracy peaked in the final five epochs\\n(restore_best_weights=True):\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nEarlyStopping\\n\\ncallback\\n=\\nEarlyStopping(monitor='val_accuracy',\\npatience=5,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 restore_best_weights=True)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nStopping the training process based on rising training loss\\nrather than decreasing validation accuracy at the end of each\\nepoch requires a minor code change:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nEarlyStopping\\n\\ncallback\\n=\\nEarlyStopping(monitor='loss',\\npatience=5,\\nrestore_best_weights=True)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 336, 'file_type': 'pdf'}, page_content=\"335\\nThe callback that is used perhaps more than any other is\\nModelCheckpoint, which saves a model at specified intervals\\nduring training or, if you set save_best_only to True, saves\\nthe most accurate model. The next example trains a model for\\n100 epochs and saves the one that exhibits the highest\\nvalidation accuracy in best_model.h5:\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nModelCheckpoint\\n\\ncallback\\n=\\nModelCheckpoint(filepath='best_model.h5',\\nmonitor='val_accuracy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 save_best_only=True)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nAnother frequently used callback is TensorBoard, which logs a\\nvariety of information to a specified location in the\\nfilesystem as a model is trained. The following example logs\\nto the logs subdirectory of the current directory:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nTensorBoard\\n\\ncallback\\n=\\nTensorBoard(log_dir='logs',\\nhistogram_freq=1)\\nmodel.fit(x_train,\\ny_train,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nYou can use a tool called TensorBoard to monitor accuracy and\\nloss, changes in the model’s weights and biases, and more\\nwhile training takes place or after it has completed. You can\\nlaunch TensorBoard from a Jupyter notebook and point it to\\nthe logs subdirectory with a command like this one:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 337, 'file_type': 'pdf'}, page_content='336\\n\\n%tensorboard --logdir logs\\nOr you can launch it from a command prompt by executing the\\nsame command without the percent sign. Then point your\\nbrowser to http://localhost:6006 to open the TensorBoard\\nconsole (Figure\\xa09-5). “Get Started with TensorBoard” in\\nthe TensorFlow documentation contains a helpful tutorial on\\nthe basics of TensorBoard. It’s an indispensable tool in the\\nhands of professionals, especially when training complex\\nmodels that require hours, days, or even weeks to fully\\ntrain.\\nFigure 9-5. TensorBoard showing the results of training a neural network\\nOther Keras callback classes include LearningRateScheduler\\nfor adjusting the learning rate at the beginning of each\\nepoch and CSV\\u200bLog\\u2060ger for capturing the results of each'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 338, 'file_type': 'pdf'}, page_content='337\\ntraining epoch in a CSV file. Refer to the callbacks API\\ndocumentation for a complete list. In addition, observe that\\nthe fit method’s callbacks parameter is a Python list, which\\nmeans you can specify multiple callbacks when you train a\\nmodel. You could use one callback to stop training if certain\\nconditions are met, for example, and another callback to log\\ntraining metrics in a CSV file.\\nSummary\\nKeras and TensorFlow are widely used open source frameworks\\nthat facilitate building, training, saving, loading, and\\nconsuming (making predictions with) neural networks. You can\\nbuild neural networks in a variety of programming languages\\nusing native TensorFlow APIs, but Keras abstracts those APIs\\nand makes deep learning much more approachable. Keras apps\\nare written in Python.\\nNeural networks, like traditional machine learning models,\\ncan be used to solve regression problems and classification\\nproblems. A network that performs regression has one neuron\\nin the output layer with no activation function; the output\\nfrom the network is a floating-point number. A network that\\nperforms binary classification also has one neuron in the\\noutput layer, but the sigmoid activation function ensures\\nthat the output is a value from 0.0 to 1.0 representing the\\nprobability that the input represents the positive class. For\\nmulticlass classification, the number of neurons in the\\noutput layer equals the number of classes the network can\\npredict. The softmax activation function transforms the raw\\nvalues assigned to the output neurons into an array of\\nprobabilities for each class.\\nWhen you find that a neural network is fitting too tightly to\\nthe training data, one way to combat overfitting and increase\\nthe network’s ability to generalize is to reduce the\\ncomplexity of the network: reduce the number of layers, the\\nnumber of neurons in individual layers, or both. Another'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 339, 'file_type': 'pdf'}, page_content='338\\napproach is to add dropout to the network. Dropout purposely\\nimpedes a network’s ability to learn the training data by\\nrandomly ignoring a subset of the connections between layers\\nwhen updating weights and biases.\\nKeras’s callbacks API lets you customize the training\\nprocess. By processing the callbacks that occur at the end of\\neach training epoch, for example, you can check the model’s\\naccuracy and halt training if it has reached an acceptable\\nlevel. Keras also includes a simple and easy-to-use API for\\nsaving and loading trained models. This is essential for\\noperationalizing the models that you train—deploying them to\\nproduction and using the predictive powers developed during\\ntraining.\\nThe facial recognition model in this chapter exceeded 80% in\\nvalidation accuracy, but modern deep-learning models often\\nachieve 99% accuracy on the same dataset. It won’t surprise\\nyou to learn that there is more to deep learning than\\nmultilayer perceptrons. We’ll take a deep dive into facial\\nrecognition in Chapter\\xa011, but first we’ll explore a\\ndifferent type of neural network—one that’s particularly\\nadept at solving computer-vision problems. It’s called the\\nconvolutional neural network, and it is the subject of\\nChapter\\xa010.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 340, 'file_type': 'pdf'}, page_content='339\\nChapter 10. Image Classification\\nwith Convolutional Neural\\nNetworks\\nComputer vision is a branch of deep learning in which\\ncomputers discern information from images. Real-world uses\\ninclude identifying objects in photos, removing inappropriate\\nimages from social media sites, counting the cars in line at\\na tollbooth, and recognizing faces in photos. Computer-vision\\nmodels can even be combined with natural language processing\\n(NLP) models to caption photos. I snapped a photo while on\\nvacation and asked Azure’s Computer Vision service to\\ncaption it. The result is shown in Figure\\xa010-1. It’s\\nsomewhat remarkable given that no human intervention was\\nrequired.\\nFigure 10-1. “A body of water with a dock and a building in the\\nbackground”—Azure AI'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 341, 'file_type': 'pdf'}, page_content='340\\nThe field of computer vision has advanced rapidly in recent\\nyears, mostly due to convolutional neural networks, also\\nknown as CNNs or ConvNets. In 2012, an eight-layer CNN called\\nAlexNet outperformed traditional machine learning models\\nentered in the annual ImageNet Large Scale Visual Recognition\\nChallenge (ILSVRC) by achieving an error rate of 15.3% when\\nidentifying objects in photos. In 2015, ResNet-152 featuring\\na whopping 152 layers won the challenge with an error rate of\\njust 3.5%, which exceeds a human’s ability to classify\\nimages featured in the competition.\\nCNNs are magical because they treat images as images rather\\nthan just arrays of pixel values. They use a decades-old\\ntechnology called convolution kernels to extract “features”\\nfrom images, allowing them to recognize the shape of a cat’s\\nhead or the outline of a dog’s tail. Moreover, they are easy\\nto build with Keras and TensorFlow.\\nState-of-the-art CNNs such as ResNet-152 are trained at great\\nexpense with millions of images on GPUs, but there’s a lot\\nyou can do with an ordinary CPU. In this chapter, you’ll\\nlearn what CNNs are and how they work, and you’ll build and\\ntrain a few CNNs of your own. You’ll also learn how to\\nleverage advanced CNNs published for public consumption by\\ncompanies such as Google and Microsoft, and how to use a\\ntechnique called transfer learning to repurpose those CNNs to\\nsolve domain-specific problems.\\nUnderstanding CNNs\\nFigure\\xa010-2 shows the topology of a basic CNN. It begins\\nwith one or more sets of convolution layers and pooling\\nlayers. Convolution layers extract features from images,\\ngenerating transformed images that are commonly referred to\\nas feature maps because they highlight distinguishing\\nfeatures such as shapes and contours. Pooling layers reduce\\nthe feature maps’ size by half so that features can be\\nextracted at various resolutions and are less sensitive to'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 342, 'file_type': 'pdf'}, page_content='341\\nsmall changes in position. Output from the final pooling\\nlayer is flattened to one dimension and input to one or more\\ndense layers for classification. The convolution and pooling\\nlayers are called bottleneck layers since they reduce the\\ndimensionality of images input to them. They also account for\\nthe bulk of the computation time during training.\\nConvolution layers extract features from images by passing\\nconvolution kernels over them—the same technique used by\\nimage editing tools to blur, sharpen, and emboss images. A\\nkernel is simply a matrix of values. It usually measures 3 ×\\n3, but it can be larger. To process an image, you place the\\nkernel in the upper-left corner of the image, multiply the\\nkernel values by the pixel values underneath, and compute a\\nnew value for the center pixel by summing the products, as\\nshown in Figure\\xa010-3. Then you move the kernel one pixel to\\nthe right and repeat the process, continuing row by row and\\ncolumn by column until the entire image has been processed.\\nFigure 10-2. Convolutional neural network'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 343, 'file_type': 'pdf'}, page_content='342\\nFigure 10-3. Processing image pixels with a 3 × 3 convolution kernel\\nFigure\\xa010-4 shows what happens when you apply a 3 × 3\\nkernel to a hot dog image. This particular kernel is called a\\nbottom Sobel kernel, and it’s designed to do edge detection\\nby highlighting edges as if a light were shined from the\\nbottom. The convolution layers of a CNN use kernels like this\\none to extract features that help distinguish one class from\\nanother.\\nFigure 10-4. Processing an image with a bottom Sobel kernel\\nA convolution layer doesn’t use just one kernel to process\\nimages. It uses many—sometimes 100 or more. The kernel\\nvalues aren’t determined ahead of time. They are initialized\\nwith random values and then learned (adjusted) as the CNN is\\ntrained, just as the weights connecting neurons in dense\\nlayers are learned. Each kernel also has a bias associated'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 344, 'file_type': 'pdf'}, page_content='343\\nwith it, just like a neuron in a dense layer. The images in\\nFigure\\xa010-5 were generated by the first convolution layer in\\na trained CNN. You can see how the various convolution\\nkernels allow the network to view the same hot dog image in\\ndifferent ways, and how certain features such as the shape of\\nthe bun and the ribbon of mustard on top are highlighted.\\nFigure 10-5. Images generated by convolution kernels in a CNN\\nPooling layers downsample images to reduce their size. The\\nmost common resizing technique is max pooling, which divides\\nimages into 2 × 2 blocks of pixels and selects the highest\\nof the four values in each block. An alternative is average\\npooling, which averages the values in each block.\\nFigure\\xa010-6 shows how an image contracts as it passes\\nthrough successive pooling layers. The first row came from\\nthe first pooling layer, the second row came from the second\\npooling layer, and so on.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 345, 'file_type': 'pdf'}, page_content='344\\nFigure 10-6. Images generated by pooling layers in a CNN\\nPooling isn’t the only way to downsize an image. While less\\ncommon, reduction can be accomplished without pooling layers\\nby setting a convolution layer’s stride to 2. Stride is the\\nnumber of pixels a convolution kernel moves as it passes over\\nan image. It defaults to 1, but setting it to 2 halves the\\nimage size by ignoring every other row and every other column\\nof pixels.\\nThe dense layers at the end of the network classify features\\nextracted from the bottleneck layers and are referred to as\\nthe CNN’s classification layers. They are no different than\\nthe multilayer perceptrons featured in Chapter\\xa09. For binary\\nclassification, the output layer contains one neuron and uses\\nthe sigmoid activation function. For multiclass\\nclassification, the output layer contains one neuron per\\nclass and uses the softmax activation function.\\nNOTE\\nThere’s no law that says bottleneck layers have to be paired with\\nclassification layers. You could take the feature maps output from\\nthe bottleneck layers and classify them with a support vector\\nmachine rather than a multilayer perceptron. It’s not as far-\\nfetched as it sounds. In Chapter\\xa012, I’ll introduce one well-\\nknown model that does just that.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 346, 'file_type': 'pdf'}, page_content=\"345\\nUsing Keras and TensorFlow to Build CNNs\\nTo simplify building CNNs that classify images, Keras offers\\nthe Conv2D class, which models convolution layers, and the\\nMaxPooling2D class, which implements max pooling layers. The\\nfollowing statements create a CNN with two pairs of\\nconvolution and pooling layers, a flatten layer to reshape\\nthe output into a 1D array for input to a dense layer, a\\ndense layer to classify the features extracted from the\\nbottleneck layers, and a softmax output layer for\\nclassification:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nFlatten,\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Conv2D(32,\\n(3,\\n3),\\nactivation='relu',\\ninput_shape=(28,\\n28,\\n1)))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(64,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(10,\\nactivation='softmax'))\\nThe first parameter passed to the Conv2D function is the\\nnumber of convolution kernels to include in the layer. More\\nkernels means more fitting power, similar to the number of\\nneurons in a dense layer. The second parameter is the\\ndimensions of each kernel. You sometimes get greater accuracy\\nfrom 5 × 5 kernels, but a kernel that size increases\\ntraining time by requiring 25 multiplication operations for\\neach pixel as opposed to nine for a 3 × 3 kernel. The\\ninput_shape parameter in the first layer specifies the size\\nof the images input to the CNN: in this case, one-channel\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 347, 'file_type': 'pdf'}, page_content='346\\n(grayscale) 28 × 28 images. All the images used to train a\\nCNN must be the same size.\\nNOTE\\nConv2D processes images, which are two-dimensional. Keras also\\noffers the Conv1D class for processing 1D data and Conv3D for 3D\\ndata. The former finds use processing text and time-series data.\\nCanonical use cases for Conv3D include analyzing video and 3D\\nmedical images.\\nGiven a set of images with a relatively high degree of\\nseparation between classes, it’s perfectly feasible to train\\na CNN to classify those images on a typical laptop or PC. A\\ngreat example is the MNIST dataset, which contains 60,000\\ntraining images of scanned, handwritten digits, each\\nmeasuring 28 × 28 pixels, plus 10,000 test images.\\nFigure\\xa010-7 shows the first 50 scans in the training set.\\nFigure 10-7. The MNIST digits dataset'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 348, 'file_type': 'pdf'}, page_content=\"347\\nLet’s train a CNN to recognize digits in the MNIST dataset,\\nwhich conveniently is one of several sample datasets built\\ninto Keras. Begin by creating a new Jupyter notebook and\\nusing the following statements to load the dataset, reshape\\nthe 28 × 28 images into 28 × 28 × 1 arrays (28 × 28\\nimages containing a single color channel), and divide the\\npixel values by 255 as a simple form of normalization:\\n\\nfrom\\ntensorflow.keras.datasets\\nimport\\nmnist\\n\\n(train_images,\\ny_train),\\n(test_images,\\ny_test)\\n=\\nmnist.load_data()\\nx_train\\n=\\ntrain_images.reshape(60000,\\n28,\\n28,\\n1)\\n/\\n255\\nx_test\\n=\\ntest_images.reshape(10000,\\n28,\\n28,\\n1)\\n/\\n255\\nNext, define a CNN that accepts 28 × 28 × 1 arrays of pixel\\nvalues as input, contains two pairs of convolution and\\npooling layers, and has a softmax output layer with 10\\nneurons since the dataset contains scans of 10 different\\ndigits:\\n\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nDense,\\nFlatten\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Conv2D(32,\\n(3,\\n3),\\nactivation='relu',\\ninput_shape=(28,\\n28,\\n1)))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(64,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(10,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 349, 'file_type': 'pdf'}, page_content=\"348\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary(line_length=80)\\nFigure\\xa010-8 shows the output from the call to summary on the\\nfinal line. The summary reveals a lot about how this CNN\\nprocesses images. Each pooling layer reduces the image size\\nby half, while each convolution layer reduces the image’s\\nheight and width by two pixels. Why is that? By default, a\\nconvolution kernel doesn’t start with its center cell over\\nthe pixel in the upper-left corner of the image; rather, its\\nupper-left corner is aligned with the image’s upper-left\\ncorner. For a 3 × 3 kernel, there’s a 1-pixel-wide border\\naround the edges that doesn’t survive the convolution. (For\\na 5 × 5 kernel, the border that doesn’t survive is 2 pixels\\nwide.) The term for this is padding, and if you’d like, you\\ncan override the default behavior to push the kernel’s\\ncenter cell right up to the edges of the image. In Keras,\\nthis is accomplished by including a padding='same' parameter\\nin the call to Conv2D.\\nFigure 10-8. Output from the summary method\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 350, 'file_type': 'pdf'}, page_content='349\\nAnother takeaway is that each 28 × 28 image exits the first\\nconvolution layer as a 3D array or tensor measuring 26 × 26\\n× 32: one 26 × 26 feature map for each of the 32 kernels.\\nAfter max pooling, the tensor is reduced to 13 × 13 × 32\\nand input to the second convolution layer, where 64 more\\nkernels filter features from the thirty-two 13 × 13 feature\\nmaps and combine them to produce 64 new feature maps (a\\ntensor measuring 11 × 11 × 64). A final pooling layer\\nreduces that to 5 × 5 × 64. These values are flattened into\\na 1D tensor containing 1,600 values and fed into a dense\\nlayer for classification.\\nNOTE\\nThe big picture here is that the CNN transforms each 28 × 28\\nimage comprising 784 pixel values into an array of 1,600 floating-\\npoint numbers that (hopefully) distinguishes the contents of the\\nimage more clearly than ordinary pixel values do. That’s what\\nbottleneck layers do: they transform matrices of integer pixel\\nvalues into tensors of floating-point numbers that better\\ncharacterize the images input to them. As you’ll see in\\nChapter\\xa013, NLP networks use word embeddings to create dense\\nvector representations of the words in a document. Dense vector\\nrepresentation is a term you’ll encounter a lot in deep learning.\\nIt’s nothing more than arrays of floating-point numbers that do\\nmore to characterize the input than the input data itself.\\nThe output from summary would look exactly the same if the\\nimages input to the network were three-channel color images\\nrather than one-channel grayscale images. Applying a\\nconvolution layer with n kernels to an image produces n\\nfeature maps regardless of image depth, just as applying a\\nconvolution layer featuring n kernels to the feature maps\\noutput by preceding layers produces n new feature maps'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 351, 'file_type': 'pdf'}, page_content=\"350\\nregardless of input depth. Internally, CNNs use tensor dot\\nproducts to produce 2D feature maps from 3D feature maps.\\nPython’s NumPy library includes a function named tensordot\\nfor computing tensor dot products quickly.\\nNow train the network and plot the training and validation\\naccuracy:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=10,\\nbatch_size=50)\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nOnce trained, this simple CNN can achieve 99% accuracy\\nclassifying handwritten digits:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 352, 'file_type': 'pdf'}, page_content='351\\nOne reason it can attain such accuracy is the number of\\ntraining samples—roughly 6,000 per class. (As a test, I\\ntrained the network with just 100 samples of each class and\\ngot 92% accuracy.) Another factor is that a 2 looks very\\ndifferent from, say, an 8. If a person can rather easily\\ndistinguish between the two, then a CNN can too.\\nTraining a CNN to Recognize Arctic Wildlife\\nA basic CNN can easily achieve 99% accuracy on the MNIST\\ndataset. But it isn’t as easy when the problem is more\\nperceptual—for example, when the goal is to determine\\nwhether a photo contains a dog or a cat. One reason is that\\nmost 8s look a lot alike, while dogs and cats come in many\\nvarieties. Another factor is that each digit in the MNIST\\ndataset is carefully cropped to precisely fill the frame,\\nwhereas dogs and cats can appear anywhere in the frame and'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 353, 'file_type': 'pdf'}, page_content='352\\ncan be photographed in different poses and from an infinite\\nnumber of angles.\\nTo demonstrate, let’s train a CNN to distinguish between\\nArctic foxes, polar bears, and walruses. For context, imagine\\nyou’ve been tasked with creating a system that uses AI to\\nexamine pictures snapped by motion-activated cameras deployed\\nin the Arctic to document polar bear activity.\\nStart by downloading a ZIP file containing images for\\ntraining and testing the CNN. Unpack the ZIP file and place\\nits contents in a subdirectory named Wildlife where your\\nJupyter notebooks are hosted. The ZIP file contains folders\\nnamed train, test, and samples. Each folder contains\\nsubfolders named arctic_fox, polar_bear, and walrus. The\\ntraining folders contain 100 images each, while the test\\nfolders contain 40 images each. Figure\\xa010-9 shows some of\\nthe polar bear training images. These are public images that\\nwere downloaded from the internet and cropped and resized to\\n224 × 224 pixels.\\nFigure 10-9. Polar bear images\\nNow create a Jupyter notebook and use the following code to\\ndefine a pair of helper functions—one to load a batch of\\nimages from a specified location in the filesystem and assign\\nthem labels, and another to show the first eight images in a\\nbatch of images:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 354, 'file_type': 'pdf'}, page_content=\"353\\n\\nimport\\nos\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndef\\nload_images_from_path(path,\\nlabel):\\n\\xa0\\xa0\\xa0 images,\\nlabels\\n=\\n[],\\n[]\\n\\n\\xa0\\xa0\\xa0 for\\nfile\\nin\\nos.listdir(path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 img\\n=\\nimage.load_img(os.path.join(path,\\nfile),\\ntarget_size=(224,\\n224,\\n3))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 images.append(image.img_to_array(img))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 labels.append((label))\\n\\n\\xa0\\xa0\\xa0 return\\nimages,\\nlabels\\n\\ndef\\nshow_images(images):\\n\\xa0\\xa0\\xa0 fig,\\naxes\\n=\\nplt.subplots(1,\\n8,\\nfigsize=(20,\\n20),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.imshow(images[i]\\n/\\n255)\\n\\nx_train,\\ny_train,\\nx_test,\\ny_test\\n=\\n[],\\n[],\\n[],\\n[]\\nUse the following statements to load 100 Arctic fox training\\nimages and plot a subset of them:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Wildlife/train/arctic_fox',\\n0)\\nshow_images(images)\\n\\nx_train\\n+=\\nimages\\ny_train\\n+=\\nlabels\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 355, 'file_type': 'pdf'}, page_content=\"354\\nDo the same to load and label the polar bear training images:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Wildlife/train/polar_bear',\\n1)\\nshow_images(images)\\n\\nx_train\\n+=\\nimages\\ny_train\\n+=\\nlabels\\nAnd then the walrus training images:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Wildlife/train/walrus',\\n2)\\nshow_images(images)\\n\\nx_train\\n+=\\nimages\\ny_train\\n+=\\nlabels\\nYou also need to load the images used to validate the CNN.\\nStart with 40 Arctic fox test images:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Wildlife/test/arctic_fox',\\n0)\\nshow_images(images)\\n\\nx_test\\n+=\\nimages\\ny_test\\n+=\\nlabels\\nThen the polar bear test images:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Wildlife/test/polar_bear',\\n1)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 356, 'file_type': 'pdf'}, page_content=\"355\\nshow_images(images)\\n\\nx_test\\n+=\\nimages\\ny_test\\n+=\\nlabels\\nAnd finally the walrus test images:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Wildlife/test/walrus',\\n2)\\nshow_images(images)\\n\\nx_test\\n+=\\nimages\\ny_test\\n+=\\nlabels\\nThe next step is to normalize the training and testing images\\nby dividing their pixel values by 255:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nx_train\\n=\\nnp.array(x_train)\\n/\\n255\\nx_test\\n=\\nnp.array(x_test)\\n/\\n255\\n\\ny_train\\n=\\nnp.array(y_train)\\ny_test\\n=\\nnp.array(y_test)\\nNow it’s time to build a CNN. Since the images measure 224\\n× 224 and we want the final feature maps to compress as much\\ninformation as possible into a small space, we’ll use five\\npairs of convolution and pooling layers to extract features\\nfrom the training images at five resolutions: 224 × 224, 111\\n× 111, 54 × 54, 26 × 26, and 12 × 12. We’ll follow those\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 357, 'file_type': 'pdf'}, page_content=\"356\\nwith a dense layer and a softmax output layer containing\\nthree neurons—one for each of the three classes:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nFlatten,\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Conv2D(32,\\n(3,\\n3),\\nactivation='relu',\\ninput_shape=(224,\\n224,\\n3)))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(64,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(64,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(128,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(128,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary(line_length=80)\\nCall fit to train the model:\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=20)\\nIf you train the model on a CPU, training will probably\\nrequire from 10 to 20 seconds per epoch. (Think of all those\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 358, 'file_type': 'pdf'}, page_content=\"357\\npixel calculations taking place on all those images with all\\nthose convolution kernels.) When training is complete, use\\nthe following statements to plot the training and validation\\naccuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nHere is the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 359, 'file_type': 'pdf'}, page_content='358\\nWere the results what you expected? The validation accuracy\\nis decent, but it’s not state of the art. It probably landed\\nbetween 60% and 70%. Modern CNNs often do 95% or better\\nclassifying images such as these. You might be able to\\nsqueeze more out of this model by stacking convolution layers\\nor increasing the number of kernels, and you might get it to\\ngeneralize slightly better by introducing a dropout layer.\\nBut you won’t reach 95% with this network and this dataset.\\nOne of the reasons modern CNNs can do image classification so\\naccurately is that they’re trained with millions of images.\\nYou don’t need millions of samples of each class, but you\\nprobably need at least an order of magnitude more—if not two\\norders of magnitude more—than the 300 you trained with here.\\nYou could scour the internet for more images, but more images\\nmeans more training time. If the goal is to achieve an\\naccuracy of 95% or more, you’ll quickly get to the point'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 360, 'file_type': 'pdf'}, page_content='359\\nwhere the CNN takes too long to train—or find yourself\\nshopping for an NVIDIA GPU.\\nThat doesn’t mean CNNs aren’t practical for solving\\nbusiness problems. It just means that there’s more to learn.\\nThe next section is the first step in understanding how to\\nattain high levels of accuracy without training a CNN from\\nscratch.\\nPretrained CNNs\\nMicrosoft, Google, and other tech companies use a subset of\\nthe ImageNet dataset containing more than 1 million images to\\ntrain state-of-the-art CNNs to recognize hundreds of objects,\\nincluding Arctic foxes and polar bears. Then they make them\\navailable for public consumption. Called pretrained CNNs,\\nthey are more sophisticated than anything you’re likely to\\ntrain yourself. And if that’s not awesome enough, Keras\\nreduces the process of loading a pretrained CNN to one line\\nof code.\\nKeras provides classes that wrap more than two dozen popular\\npretrained CNNs. The full list is documented on the Keras\\nwebsite. Most of these CNNs are documented in scholarly\\npapers such as “Deep Residual Learning for Image\\nRecognition” and “EfficientNet: Rethinking Model Scaling\\nfor Convolutional Neural Networks”. Some have won\\nprestigious competitions such as the ImageNet Large Scale\\nVisual Recognition Challenge and the COCO Detection\\nChallenge. Among the most notable are the ResNet family of\\nnetworks from Microsoft and the Inception networks from\\nGoogle. Also noteworthy is MobileNet, which trades size for\\naccuracy and is ideal for mobile devices due to its small\\nmemory footprint. You can learn more about it in the Google\\nAI blog.\\nPRETRAINED CNN ARCHITECTURES'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 361, 'file_type': 'pdf'}, page_content='360\\nPretrained CNNs achieve impressive levels of accuracy not\\njust because they’re trained with millions of images but\\nalso because they are deeper and architecturally more\\nadvanced than the CNNs presented thus far. For example,\\nthey use consecutive convolution layers to further refine\\nthe features extracted from each image, and many use\\nbatch normalization to normalize (standardize to unit\\nvariance) values propagated between layers during\\ntraining.\\nBut pretrained CNNs are more sophisticated for other\\nreasons as well. ResNets, for example, pioneered a\\nconcept called residual layers, which add their input to\\ntheir output. This simple innovation helped mitigate the\\nvanishing-gradient problem that causes updates to become\\nincreasingly smaller as the optimizer goes backward\\nthrough the network updating weights and biases, and it\\nmade much deeper networks possible.\\nAnd then there is the Inception family of CNNs, whose\\nchief innovation was the use of 1 × 1 convolution\\nkernels to minimize the computational overhead of larger\\nconvolution kernels by reducing the depth of the feature\\nmaps input to them. Recent versions of Inception have\\nincorporated features of ResNets as well, and a\\nderivative architecture known as Xception improved on\\nInception by introducing depthwise separable\\nconvolutions. If you’d like to learn more, check out “A\\nSimple Guide to the Versions of the Inception Network”\\nand “Xception: Deep Learning with Depthwise Separable\\nConvolutions”.\\nThe following statement instantiates Keras’s MobileNetV2\\nclass and initializes it with the weights, biases, and kernel\\nvalues arrived at when the network was trained on the\\nImageNet dataset:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 362, 'file_type': 'pdf'}, page_content=\"361\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nMobileNetV2\\n\\nmodel\\n=\\nMobileNetV2(weights='imagenet')\\nThe weights='imagenet' parameter tells Keras what parameters\\nto load to re-create the network in its trained state. You\\ncan also pass a path to a file containing custom weights, but\\nimagenet is the only set of predefined weights that are\\ncurrently supported.\\nBefore an image is submitted to a pretrained CNN for\\nclassification, it must be sized to the dimensions the CNN\\nexpects—typically 224 × 224—and preprocessed. Different\\nCNNs expect images to be preprocessed in different ways, so\\nKeras provides a preprocess_input function for each\\npretrained CNN. It also includes utility functions for\\nloading and resizing images. The following statements load an\\nimage from the filesystem and preprocess it for input to the\\nMobileNetV2 network:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.mobilenet\\nimport\\npreprocess_input\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\nx\\n=\\nimage.load_img('arctic_fox.jpg',\\ntarget_size=(224,\\n224))\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\nIn most cases, preprocess_input does all the work that’s\\nneeded, which often involves applying unit variance to pixel\\nvalues and converting RGB images to BGR format. In some\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 363, 'file_type': 'pdf'}, page_content=\"362\\ncases, however, you still need to divide the pixel values by\\n255. ResNet50V2 is one example:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\nx\\n=\\nimage.load_img('arctic_fox.jpg',\\ntarget_size=(224,\\n224))\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n/\\n255\\nOnce an image is preprocessed, making a prediction is as\\nsimple as calling the network’s predict method:\\n\\ny\\n=\\nmodel.predict(x)\\nTo help you interpret the output, Keras also provides a\\nnetwork-specific decode\\u200b_pre\\u2060dictions method. Figure\\xa010-10\\nshows what that method returned for a photo submitted to\\nResNet50V2.\\nFigure 10-10. Output from decode_predictions\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 364, 'file_type': 'pdf'}, page_content=\"363\\nResNet50V2 is 89% sure the photo contains an Arctic fox—\\nwhich, it so happens, it does. MobileNetV2 predicted with 92%\\ncertainty that the photo contains an Arctic fox. Both\\nnetworks were trained on the same dataset, but different\\npretrained CNNs classify images slightly differently.\\nUsing ResNet50V2 to Classify Images\\nLet’s use Keras to load a pretrained CNN and classify a pair\\nof images. Fire up a notebook and use the following\\nstatements to load ResNet50V2:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50V2\\n\\nmodel\\n=\\nResNet50V2(weights='imagenet')\\nmodel.summary()\\nNext, load an Arctic fox image and show it in the notebook:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\nx\\n=\\nimage.load_img('Wildlife/samples/arctic_fox/arctic_fox_140.jpeg',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nNow preprocess the image (remember that for ResNet50V2, you\\nalso have to divide all the pixel values by 255 after calling\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 365, 'file_type': 'pdf'}, page_content=\"364\\nKeras’s preprocess_input method) and pass it to the CNN for\\nclassification:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\ndecode_predictions\\n\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n/\\n255\\n\\ny\\n=\\nmodel.predict(x)\\ndecode_predictions(y)\\nThe output should look like this:\\n\\n[[('n02120079', 'Arctic_fox', 0.9999944),\\n\\xa0 ('n02114548', 'white_wolf', 4.760021e-06),\\n\\xa0 ('n02119789', 'kit_fox', 2.3306782e-07),\\n\\xa0 ('n02442845', 'mink', 1.2460312e-07),\\n\\xa0 ('n02111889', 'Samoyed', 1.1914468e-07)]]\\nResNet50V2 is virtually certain that the image contains an\\nArctic fox. But now load a walrus image:\\n\\nx\\n=\\nimage.load_img('Wildlife/samples/walrus/walrus_143.png',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 366, 'file_type': 'pdf'}, page_content=\"365\\nAsk ResNet50V2 to classify it:\\n\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n/\\n255\\n\\ny\\n=\\nmodel.predict(x)\\ndecode_predictions(y)\\nHere’s the output:\\n\\n[[('n02454379', 'armadillo', 0.63758147),\\n\\xa0 ('n01704323', 'triceratops', 0.16057032),\\n\\xa0 ('n02113978', 'Mexican_hairless', 0.07795086),\\n\\xa0 ('n02398521', 'hippopotamus', 0.022284042),\\n\\xa0 ('n01817953', 'African_grey', 0.016944142)]]\\nResNet50V2 thinks the image is most likely an armadillo, but\\nit’s not even very sure about that. Can you guess why?\\nResNet50V2 was trained with almost 1.3 million images. None\\nof them, however, contained a walrus. The ImageNet 1000 Class\\nList shows a complete list of classes it was trained to\\nrecognize. A pretrained CNN is great when you need it to\\nclassify images using the classes it was trained with, but it\\nis powerless to handle domain-specific tasks that it wasn’t\\ntrained for.\\nBut all is not lost. A technique called transfer learning\\nenables pretrained CNNs to be repurposed to solve domain-\\nspecific problems. The repurposing can be done on an ordinary\\nCPU; no GPU required. Transfer learning sometimes achieves\\n95% accuracy with just a few hundred training images. Once\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 367, 'file_type': 'pdf'}, page_content='366\\nyou learn about it, you’ll have a completely different\\nperspective on the efficacy of using CNNs to solve business\\nproblems.\\nTransfer Learning\\nEarlier, you used a dataset with photos of Arctic foxes,\\npolar bears, and walruses to train a CNN to recognize Artic\\nwildlife. Trained with 300 images—100 for each of the three\\nclasses—the CNN achieved an accuracy of around 60%. That’s\\nnot sufficient for most purposes.\\nOne solution is to train the CNN with tens of thousands of\\nphotos. A better solution—one that can deliver world-class\\naccuracy with the 300 photos you have and doesn’t require\\nexpensive hardware—is transfer learning. In the hands of\\nsoftware developers and engineers, transfer learning makes\\nCNNs a practical solution for a variety of computer-vision\\nproblems. And it requires orders of magnitude less time and\\ncompute power than CNNs trained from scratch. Let’s take a\\nmoment to understand what transfer learning is and how it\\nworks—and then put it to work identifying Arctic wildlife.\\nPretrained CNNs trained on the ImageNet dataset can identify\\nArctic foxes and polar bears, but they can’t identify\\nwalruses because they weren’t trained with walrus images.\\nTransfer learning lets you repurpose pretrained CNNs to\\nidentify objects they weren’t originally trained to\\nidentify. It leverages the intelligence baked into pretrained\\nCNNs, but it repurposes that intelligence to solve new\\nproblems.\\nRecall that a CNN has two groups of layers: bottleneck layers\\ncontaining the convolution and pooling layers that extract\\nfeatures from images at various resolutions, and\\nclassification layers, which classify features output from\\nthe bottleneck layers as belonging to an Arctic fox, a polar\\nbear, or something else. Convolution layers use convolution'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 368, 'file_type': 'pdf'}, page_content='367\\nkernels to extract features, and the values in the\\nconvolution kernels are learned during training. This\\nlearning accounts for the bulk of the training time. When\\nsophisticated CNNs are trained with millions of images, the\\nconvolution kernels become very efficient at extracting\\nfeatures. But that efficiency comes at a cost.\\nThe premise behind transfer learning is shown in Figure\\xa010-\\n11. You load the bottleneck layers of a pretrained CNN, but\\nyou don’t load the classification layers. Instead, you\\nprovide your own, which train orders of magnitude more\\nquickly than an entire CNN. Then you pass the training images\\nthrough the bottleneck layers for feature extraction and\\ntrain the classification layers on the resulting features.\\nThe pretrained CNN might have been trained to extract\\nfeatures from pictures of apples and oranges, but those same\\nlayers are probably pretty good at extracting features from\\nphotos of dogs and cats too. By using the pretrained\\nbottleneck layers to extract features and then using those\\nfeatures to train your own classification layers, you can\\nteach the model that a certain feature extracted from an\\nimage might be indicative of a dog rather than an apple.\\nFigure 10-11. Neural network architecture for transfer learning\\nTransfer learning is relatively simple to implement with\\nKeras and TensorFlow. Recall that the following statement'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 369, 'file_type': 'pdf'}, page_content=\"368\\nloads ResNet50V2 and initializes it with the weights\\n(including kernel values) and biases that were arrived at\\nwhen the network was trained on a subset of the ImageNet\\ndataset:\\n\\nbase_model\\n=\\nResNet50V2(weights='imagenet')\\nTo load ResNet50V2 (or any other pretrained CNN that Keras\\nsupports) without the classification layers, you simply add\\nan include_top=False attribute:\\n\\nbase_model\\n=\\nResNet50V2(weights='imagenet',\\ninclude_top=False)\\nFrom that point, there are two ways to go about transfer\\nlearning. The first involves appending classification layers\\nto the base model’s bottleneck layers and setting each base\\nlayer’s trainable attribute to False so that the weights,\\nbiases, and convolution kernels won’t be updated when the\\nnetwork is trained:\\n\\nfor\\nlayer\\nin\\nbase_model.layers:\\n\\xa0\\xa0\\xa0 layer.trainable\\n=\\nFalse\\n\\nmodel\\n=\\nSequential()\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 370, 'file_type': 'pdf'}, page_content=\"369\\n\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=10,\\nbatch_size=10)\\nThe second technique is to run all the training images\\nthrough the base model for feature extraction, and then run\\nthe features through a separate network containing your\\nclassification layers:\\n\\nfeatures\\n=\\nbase_model.predict(x)\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\n\\nmodel.fit(features,\\ny,\\nvalidation_split=0.2,\\nepochs=10,\\nbatch_size=10)\\nWhich technique is better? The second is faster because the\\ntraining images go through the bottleneck layers for feature\\nextraction just one time rather than once per epoch. It’s\\nthe technique you should use in the absence of a compelling\\nreason to do otherwise. The first technique is slightly\\nslower, but it lends itself to fine-tuning, in which you\\nunfreeze one or more bottleneck layers after training is\\ncomplete and train for a few more epochs with a very low\\nlearning rate. It also facilitates data augmentation, which\\nI’ll introduce in the next section.\\nNOTE\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 371, 'file_type': 'pdf'}, page_content='370\\nFine-tuning is frequently applied to transfer-learning models\\nafter training is complete in an effort to squeeze out an extra\\npercentage point or two of accuracy. We will use fine-tuning in\\nChapter\\xa013 to increase the accuracy of an NLP model that utilizes\\na pretrained neural network.\\nIf you use the first technique to implement transfer\\nlearning, you make predictions by preprocessing the images\\nand passing them to the model’s predict method. For the\\nsecond technique, making predictions is a two-step process.\\nAfter preprocessing the images, you pass them to the base\\nmodel’s predict method, and then you pass the output from\\nthat method to your model’s predict method:\\n\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n/\\n255\\n\\nfeatures\\n=\\nbase_model.predict(x)\\npredictions\\n=\\nmodel.predict(features)\\nAnd with that, transfer learning is complete. All that\\nremains is to put it in practice.\\nUsing Transfer Learning to Identify Arctic\\nWildlife\\nLet’s use transfer learning to solve the same problem that\\nwe attempted to solve earlier with a scratch-built CNN:\\nbuilding a model that determines whether a photo contains an\\nArctic fox, a polar bear, or a walrus.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 372, 'file_type': 'pdf'}, page_content=\"371\\nCreate a Jupyter notebook and use the same code you used\\nearlier to load the training and test images and assign\\nlabels to them: 0 for Arctic foxes, 1 for polar bears, and 2\\nfor walruses. Once that’s done, the next step is to\\npreprocess the images. We’ll use ResNet50V2 as our\\npretrained CNN, so use the ResNet version of preprocess_input\\nto preprocess the pixels. Then divide the pixel values by\\n255:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nx_train\\n=\\npreprocess_input(np.array(x_train))\\n/\\n255\\nx_test\\n=\\npreprocess_input(np.array(x_test))\\n/\\n255\\n\\ny_train\\n=\\nnp.array(y_train)\\ny_test\\n=\\nnp.array(y_test)\\nThe next step is to load ResNet50V2, being careful to load\\nthe bottleneck layers but not the classification layers, and\\nuse it to extract features from the training and test images:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50V2\\n\\nbase_model\\n=\\nResNet50V2(weights='imagenet',\\ninclude_top=False)\\n\\nx_train\\n=\\nbase_model.predict(x_train)\\nx_test\\n=\\nbase_model.predict(x_test)\\nNow train a neural network to classify features extracted\\nfrom the training images:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 373, 'file_type': 'pdf'}, page_content=\"372\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\nHow well did the network train? Plot the training accuracy\\nand validation accuracy for each epoch:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 374, 'file_type': 'pdf'}, page_content=\"373\\nYour results will differ from mine, but I got about 97%\\naccuracy. If you didn’t quite get there, try training the\\nnetwork again:\\nFinally, use a confusion matrix to visualize how well the\\nnetwork distinguishes between classes:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\nfig,\\nax\\n=\\nplt.subplots(figsize=(4,\\n4))\\nax.grid(False)\\n\\ny_pred\\n=\\nmodel.predict(x_test)\\nclass_labels\\n=\\n['arctic fox',\\n'polar bear',\\n'walrus']\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 375, 'file_type': 'pdf'}, page_content=\"374\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=class_labels,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHere’s how it turned out for me:\\nTo see transfer learning at work, load one of the Arctic fox\\nimages from the samples folder. That folder contains wildlife\\nimages with which the model was neither trained nor\\nvalidated:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 376, 'file_type': 'pdf'}, page_content=\"375\\n\\nx\\n=\\nimage.load_img('Wildlife/samples/arctic_fox/arctic_fox_140.jpeg',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nNow preprocess the image, run it through ResNet50V2’s\\nfeature extraction layers, and run the output through the\\nnewly trained classification layers:\\n\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n/\\n255\\n\\ny\\n=\\nbase_model.predict(x)\\npredictions\\n=\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nFor me, the network predicted with almost 100% confidence\\nthat the image contains an Arctic fox:\\n\\narctic fox: 1.0\\npolar bear: 0.0\\nwalrus: 0.0\\nPerhaps that’s not surprising, since ResNet50V2 was trained\\nwith Arctic fox images. But now let’s load a walrus image,\\nwhich, you’ll recall, ResNet50V2 was unable to classify:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 377, 'file_type': 'pdf'}, page_content=\"376\\n\\nx\\n=\\nimage.load_img('Wildlife/samples/walrus/walrus_143.png',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nPreprocess the image and make a prediction:\\n\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n/\\n255\\n\\ny\\n=\\nbase_model.predict(x)\\npredictions\\n=\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nHere’s how it turned out this time:\\n\\narctic fox: 0.0\\npolar bear: 0.0\\nwalrus: 1.0\\nResNet50V2 wasn’t trained to recognize walruses, but your\\nnetwork was. That’s transfer learning in a nutshell. It’s\\nthe deep-learning equivalent of having your cake and eating\\nit too. And it’s the secret sauce that makes CNNs a viable\\ntool for anyone with a laptop and a few hundred training\\nimages.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 378, 'file_type': 'pdf'}, page_content='377\\nThat’s not to say that transfer learning will always get you\\n97% accuracy with 100 images per class. It won’t. If a\\ndataset lacks the information to achieve that level of\\nseparation, neither scratch-built CNNs nor transfer learning\\nwill magically make it happen. That’s always true in machine\\nlearning and AI. You can’t get water from a rock. And you\\ncan’t build an accurate model from data that doesn’t\\nsupport it.\\nData Augmentation\\nThe previous example demonstrated how to use transfer\\nlearning to build a model that, with just 300 training\\nimages, can classify photos of three different types of\\nArctic wildlife with 97% accuracy. One of the benefits of\\ntransfer learning is that it can do more with fewer images.\\nThis feature is also a bug, however. With just 100 or so\\nsamples of each class, there is little diversity among\\nimages. A model might be able to recognize a polar bear if\\nthe bear’s head is perfectly aligned in the center of the\\nphoto. But if the training images don’t include photos with\\nthe bear’s head aligned differently or tilted at different\\nangles, the model might have difficulty classifying the\\nphoto.\\nOne solution is data augmentation. Rather than scare up more\\ntraining images, you can rotate, translate, and scale the\\nimages you have. It doesn’t always increase a CNN’s\\naccuracy, but it frequently does, especially with small\\ndatasets. Keras makes it easy to randomly transform training\\nimages provided to a network. Images are transformed\\ndifferently in each epoch, so if you train for 10 epochs, the\\nnetwork sees 10 different variations of each training image.\\nThis can increase a model’s ability to generalize with\\nlittle impact on training time. Figure\\xa010-12 shows the\\neffect of applying random transforms to a hot dog image. You\\ncan see why presenting the same image to a model in different'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 379, 'file_type': 'pdf'}, page_content='378\\nways might make the model more adept at recognizing hot dogs,\\nregardless of how the hot dog is framed.\\nFigure 10-12. Hot dog image with random transforms applied\\nKeras has built-in support for data augmentation with images.\\nLet’s look at a couple of ways to put image augmentation to\\nwork, and then apply it to the Arctic wildlife model.\\nImage Augmentation with ImageDataGenerator\\nOne way to apply image augmentation when training a model is\\nto use Keras’s ImageDataGenerator class. ImageDataGenerator\\ngenerates batches of training images on the fly, either from\\nimages you’ve loaded (for example, with Keras’s load_img\\nfunction) or from a specified location in the filesystem. The\\nlatter is especially useful when training CNNs with millions\\nof images because it loads images into memory in batches\\nrather than all at once. Regardless of where the images come\\nfrom, however, ImageDataGenerator is happy to apply\\ntransforms as it serves them up.\\nHere’s a simple example that you can try yourself. Use the\\nfollowing code to load an image from your filesystem, wrap an\\nImageDataGenerator around it, and generate 24 versions of the\\nimage:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nfrom\\ntensorflow.keras.preprocessing.image\\nimport\\nImageDataGenerator\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 380, 'file_type': 'pdf'}, page_content=\"379\\n# Load an image\\nx\\n=\\nimage.load_img('Wildlife/train/polar_bear/polar_bear_010.jpeg')\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\n\\n# Wrap an ImageDataGenerator around it\\nidg\\n=\\nImageDataGenerator(rescale=1./255,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 horizontal_flip=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rotation_range=30,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 width_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 height_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 zoom_range=0.2)\\nidg.fit(x)\\n\\n# Generate 24 versions of the image\\ngenerator\\n=\\nidg.flow(x,\\n[0],\\nbatch_size=1,\\nseed=0)\\nfig,\\naxes\\n=\\nplt.subplots(3,\\n8,\\nfigsize=(16,\\n6),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\n\\nfor\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0 img,\\nlabel\\n=\\ngenerator.next()\\n\\xa0\\xa0\\xa0 ax.imshow(img[0])\\nHere’s the result:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 381, 'file_type': 'pdf'}, page_content='380\\nThe parameters passed to ImageDataGenerator tell it how to\\ntransform each image it delivers:\\nrescale=1./255\\nDivides each pixel value by 255\\nhorizontal_flip=True\\nRandomly flips the image horizontally (around the\\nvertical axis)\\nrotation_range=30\\nRandomly rotates the image by –30 to 30 degrees\\nwidth_shift_range=0.2 and height_shift_range=0.2\\nRandomly translates the image by –20% to 20%\\nzoom_range=0.2\\nRandomly scales the image by –20% to 20%\\nThere are other parameters that you can use, such as\\nvertical_flip,\\nshear_range, and brightness_range, but you get\\nthe picture. The flow method used in this example generates\\nimages from the images you pass to fit. The related flow_from\\u200b\\n_direc\\u2060tory method loads images from the filesystem and\\noptionally labels them based on the subdirectories they’re\\nin.\\nThe generator returned by flow can be passed directly to a\\nmodel’s fit method to provide randomly transformed images to\\nthe model as it is trained. Assume that x_train and y_train\\nhold a collection of training images and labels. The\\nfollowing code wraps an ImageDataGenerator around them and\\nuses them to train a model:\\n\\nidg\\n=\\nImageDataGenerator(rescale=1./255,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 382, 'file_type': 'pdf'}, page_content='381\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 horizontal_flip=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rotation_range=30,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 width_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 height_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 zoom_range=0.2)\\n\\nidg.fit(x_train)\\nimage_batch_size\\n=\\n10\\ngenerator\\n=\\nidg.flow(x_train,\\ny_train,\\nbatch_size=image_batch_size,\\nseed=0)\\n\\nmodel.fit(generator,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 steps_per_epoch=len(x_train)\\n//\\nimage_batch_size,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=10)\\nThe steps_per_epoch parameter is key because an\\nImageDataGenerator can provide an infinite number of versions\\nof each image. In this example, the batch_size parameter\\npassed to flow tells the generator to create 10 images in\\neach batch. Dividing the number of images by the image batch\\nsize to calculate steps_per_epoch ensures that in each\\ntraining epoch, the model is provided with one transformed\\nversion of each image in the dataset.\\nNOTE\\nVersions of Keras prior to 2.1 didn’t allow a generator to be\\npassed to the fit method. Instead, they provided a separate method\\nnamed fit_generator. That method is deprecated and will be removed\\nin a future release.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 383, 'file_type': 'pdf'}, page_content=\"382\\nObserve that the call to fit includes a validation_data\\nparameter identifying a separate set of images and labels for\\nvalidating the network during training. You generally don’t\\nwant to augment validation images, so you should avoid using\\nvalidation_split when passing a generator to fit.\\nImage Augmentation with Augmentation Layers\\nYou can use ImageDataGenerator to provide transformed images\\nto a model, but recent versions of Keras provide an\\nalternative in the form of image preprocessing layers and\\nimage augmentation layers. Rather than transform training\\nimages separately, you can integrate the transforms directly\\ninto the model. Here’s an example:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nFlatten,\\nDense\\nfrom\\ntensorflow.keras.layers\\nimport\\nRescaling,\\nRandomFlip,\\nRandomRotation\\nfrom\\ntensorflow.keras.layers\\nimport\\nRandomTranslation,\\nRandomZoom\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Rescaling(1./255))\\nmodel.add(RandomFlip(mode='horizontal'))\\nmodel.add(RandomTranslation(0.2,\\n0.2))\\nmodel.add(RandomRotation(0.2))\\nmodel.add(RandomZoom(0.2))\\nmodel.add(Conv2D(32,\\n(3,\\n3),\\nactivation='relu',\\ninput_shape=(224,\\n224,\\n3)))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(128,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 384, 'file_type': 'pdf'}, page_content='383\\nEach image used to train the CNN has its pixel values divided\\nby 255 and is then randomly flipped, translated, rotated, and\\nscaled. Significantly, the RandomFlip, RandomTranslation,\\nRandomRotation, and RandomZoom layers operate only on training\\nimages. They are inactive when the network is validated or\\nasked to make predictions. Consequently, it’s fine to use\\nvalidation_split when training a model that contains image\\naugmentation layers. The Rescaling layer is active at all\\ntimes, meaning you no longer have to remember to divide pixel\\nvalues by 255 before training the model or submitting an\\nimage for classification.\\nApplying Image Augmentation to Arctic\\nWildlife\\nWould image augmentation make transfer learning even better?\\nThere’s one way to find out.\\nCreate a Jupyter notebook and copy the code that loads the\\ntraining and test images from the transfer learning example.\\nThen use the following statements to prepare the data. Note\\nthat there is no need to divide by 255 this time because a\\nRescaling layer will take care of that:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nx_train\\n=\\npreprocess_input(np.array(x_train))\\nx_test\\n=\\npreprocess_input(np.array(x_test))\\n\\ny_train\\n=\\nnp.array(y_train)\\ny_test\\n=\\nnp.array(y_test)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 385, 'file_type': 'pdf'}, page_content=\"384\\nNow load ResNet50V2 without the classification layers and\\ninitialize it with the ImageNet weights. A key element here\\nis preventing the bottleneck layers from training when the\\nnetwork is trained by setting their trainable attributes to\\nFalse, effectively freezing those layers. Rather than setting\\neach individual layer’s trainable attribute to False, we’ll\\nset trainable to False on the model itself and allow that\\nsetting to be “inherited” by the individual layers:\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50V2\\n\\nbase_model\\n=\\nResNet50V2(weights='imagenet',\\ninclude_top=False)\\nbase_model.trainable\\n=\\nFalse\\nDefine a network that incorporates rescaling and augmentation\\nlayers, ResNet50V2’s bottleneck layers, and dense layers for\\nclassification. Then train the network:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense,\\nRescaling,\\nRandomFlip\\nfrom\\ntensorflow.keras.layers\\nimport\\nRandomRotation,\\nRandomTranslation,\\nRandomZoom\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Rescaling(1./255))\\nmodel.add(RandomFlip(mode='horizontal'))\\nmodel.add(RandomTranslation(0.2,\\n0.2))\\nmodel.add(RandomRotation(0.2))\\nmodel.add(RandomZoom(0.2))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 386, 'file_type': 'pdf'}, page_content=\"385\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\nHow well did the network train? Plot the training accuracy\\nand validation accuracy for each epoch:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nWith a little luck, the accuracy slightly exceeded that of\\nthe model trained without data augmentation:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 387, 'file_type': 'pdf'}, page_content='386\\nNOTE\\nYou may find that the version of the model that uses data\\naugmentation is less accurate than the version that doesn’t. To\\nbe sure, I trained each version 10 times and averaged the results.\\nI found that the data augmentation version delivered, on average,\\nabout 0.5% more accuracy than the version that lacks augmentation.\\nThat’s not a lot, but data scientists frequently go to great\\nlengths to improve accuracy by just a fraction of a percentage\\npoint.\\nUse a confusion matrix to visualize how well the network\\nperformed during testing:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 388, 'file_type': 'pdf'}, page_content=\"387\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\nfig,\\nax\\n=\\nplt.subplots(figsize=(4,\\n4))\\nax.grid(False)\\n\\ny_pred\\n=\\nmodel.predict(x_test)\\nclass_labels\\n=\\n['arctic fox',\\n'polar bear',\\n'walrus']\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=class_labels,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHere’s how it turned out for me:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 389, 'file_type': 'pdf'}, page_content='388\\nData scientists sometimes employ data augmentation even when\\nthey’re training a CNN from scratch rather than employing\\ntransfer learning, especially when the dataset is relatively\\nsmall. It’s a useful tool to know about, and one that could\\nmake a difference when you’re trying to squeeze every last\\nounce of accuracy out of a deep-learning model.\\nGlobal Pooling\\nThe purpose of including a Flatten layer in a CNN is to\\nreshape the 3D tensors containing the final feature maps into\\n1D tensors suitable for input to a Dense layer. But Flatten\\nisn’t the only way to do it. Flattening sometimes leads to'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 390, 'file_type': 'pdf'}, page_content=\"389\\noverfitting by providing too much information to the\\nclassification layers.\\nOne way to combat overfitting is to introduce a Dropout\\nlayer. Another strategy is to reduce the width of the Dense\\nlayer. A third option is to replace the Flatten layer with a\\nGlobalMaxPooling2D layer or a GlobalAverage\\u200bPool\\u2060ing2D layer.\\nThey, too, output 1D tensors, but they generate them in a\\ndifferent way. And that way is less prone to overfitting.\\nTo demonstrate, modify the MNIST dataset example earlier in\\nthis chapter to use a GlobalMaxPooling2D layer rather than a\\nFlatten layer:\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D, \\\\\\n\\xa0\\xa0\\xa0 GlobalMaxPooling2D,\\nDense\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Conv2D(32,\\n(3,\\n3),\\nactivation='relu',\\ninput_shape=(28,\\n28,\\n1)))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(64,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(GlobalMaxPooling2D())\\n# In lieu of Flatten()\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(10,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary(line_length=120)\\nThe summary (Figure\\xa010-13) shows that the output from the\\nGlobalMaxPooling2D layer is a tensor containing 64 values—\\none per feature map emitted by the final MaxPooling2D layer—\\nrather than 5 × 5 × 64, or 1,600, values, as it was for the\\nFlatten layer. Each value is the maximum of the 25 values in\\neach 5 × 5 feature map. Had you used GlobalAveragePooling2D\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 391, 'file_type': 'pdf'}, page_content='390\\ninstead, each value would have been the average of the 25\\nvalues in each feature map.\\nFigure 10-13. Output from the summary method\\nGlobal pooling sometimes increases a CNN’s ability to\\ngeneralize and sometimes does not. For the MNIST dataset, it\\nslightly diminishes accuracy. As is so often the case in\\nmachine learning, the only way to know is to try. And due to\\nthe randomness inherent in training neural networks, it’s\\nalways advisable to train the network several times in each\\nconfiguration and average the results before drawing\\nconclusions.\\nAudio Classification with CNNs\\nImagine that you’re the leader of a group of climate\\nscientists concerned about the planet’s dwindling\\nrainforests. The world loses up to 10 million acres of old-\\ngrowth rainforests each year, much of it due to illegal\\nlogging. Your team plans to convert thousands of discarded\\nsmartphones into solar-powered listening devices and position\\nthem throughout the Amazon to transmit alerts in response to\\nthe sounds of chainsaws and truck engines. You need software\\nthat uses AI to identify such sounds in real time. And you\\nneed it fast, because climate change won’t wait.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 392, 'file_type': 'pdf'}, page_content='391\\nAn effective way to perform audio classification is to\\nconvert audio streams into spectrogram images, which provide\\nvisual representations of spectrums of frequencies as they\\nvary over time, and use CNNs to classify the spectrograms.\\nThe spectrograms in Figure\\xa010-14 were generated from WAV\\nfiles containing chainsaw sounds. Let’s use transfer\\nlearning to create a model that can identify the telltale\\nsounds of logging operations and distinguish them from\\nambient sounds such as wildlife and thunderstorms.\\nFigure 10-14. Spectrograms generated from audio files containing chainsaw\\nsounds\\nNOTE\\nThe tutorial in this section was inspired by the Rainforest\\nConnection, which uses recycled Android phones to monitor\\nrainforests for sounds of illegal activity. A TensorFlow CNN\\nhosted in the cloud analyzes audio from the phones and may one day\\nrun on the phones themselves with an assist from TensorFlow Lite,\\na smaller version of TensorFlow designed for mobile, embedded, and\\nedge devices. For more information, see “The Fight Against\\nIllegal Deforestation with TensorFlow” in the Google AI blog.\\nIt’s just one example of how AI is making the world a better\\nplace.\\nBegin by downloading a ZIP file containing a dataset of\\nrainforest sounds. (Warning: it’s a 666 MB download.) Create\\na subdirectory named Sounds in the directory where your\\nnotebooks are hosted, and copy the contents of the ZIP file\\ninto the subdirectory. Sounds now contains subdirectories'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 393, 'file_type': 'pdf'}, page_content='392\\nnamed background, chainsaw, engine, and storm. Each\\nsubdirectory contains 100 WAV files. The WAV files in the\\nbackground directory contain rainforest background noises\\nonly, while the files in the other subdirectories include the\\nsounds of chainsaws, engines, and thunderstorms overlaid on\\nthe background noises. I generated these files by using a\\nsoundscape synthesis package named Scaper to combine sounds\\nin the public UrbanSound8K dataset with rainforest sounds.\\nPlay a few of the WAV files on your computer to get a feel\\nfor the sounds they contain.\\nNow create a Jupyter notebook and paste the following code\\ninto the first cell:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\nlibrosa.display,\\nos\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndef\\ncreate_spectrogram(audio_file,\\nimage_file):\\n\\xa0\\xa0\\xa0 fig\\n=\\nplt.figure()\\n\\xa0\\xa0\\xa0 ax\\n=\\nfig.add_subplot(1,\\n1,\\n1)\\n\\xa0\\xa0\\xa0 fig.subplots_adjust(left=0,\\nright=1,\\nbottom=0,\\ntop=1)\\n\\n\\xa0\\xa0\\xa0 y,\\nsr\\n=\\nlibrosa.load(audio_file)\\n\\xa0\\xa0\\xa0 ms\\n=\\nlibrosa.feature.melspectrogram(y=y,\\nsr=sr)\\n\\xa0\\xa0\\xa0 log_ms\\n=\\nlibrosa.power_to_db(ms,\\nref=np.max)\\n\\xa0\\xa0\\xa0 librosa.display.specshow(log_ms,\\nsr=sr)\\n\\n\\xa0\\xa0\\xa0 fig.savefig(image_file)\\n\\xa0\\xa0\\xa0 plt.close(fig)\\n\\ndef\\ncreate_pngs_from_wavs(input_path,\\noutput_path):\\n\\xa0\\xa0\\xa0 if\\nnot\\nos.path.exists(output_path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 os.makedirs(output_path)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 394, 'file_type': 'pdf'}, page_content=\"393\\n\\xa0\\xa0\\xa0 dir\\n=\\nos.listdir(input_path)\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nfile\\nin\\nenumerate(dir):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_file\\n=\\nos.path.join(input_path,\\nfile)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_file\\n=\\nos.path.join(output_path,\\nfile.replace('.wav',\\n'.png'))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 create_spectrogram(input_file,\\noutput_file)\\nThis code defines a pair of functions to help convert WAV\\nfiles into spectrogram images. create_spectrogram uses a\\nPython package named Librosa to create a spectrogram image\\nfrom a WAV file. create_pngs_from_wavs converts all the WAV\\nfiles in a specified directory into spectrogram images. You\\nwill need to install Librosa if it isn’t installed already.\\nUse the following statements to create PNG files containing\\nspectrograms from all the WAV files in the Sounds\\ndirectory’s subdirectories:\\n\\ncreate_pngs_from_wavs('Sounds/background',\\n'Spectrograms/background')\\ncreate_pngs_from_wavs('Sounds/chainsaw',\\n'Spectrograms/chainsaw')\\ncreate_pngs_from_wavs('Sounds/engine',\\n'Spectrograms/engine')\\ncreate_pngs_from_wavs('Sounds/storm',\\n'Spectrograms/storm')\\nCheck the Spectrograms directory for subdirectories\\ncontaining spectrograms and confirm that each subdirectory\\ncontains 100 PNG files. Then use the following code to define\\ntwo new helper functions for loading and displaying\\nspectrograms, and declare two Python lists—one to store\\nspectrogram images and another to store class labels:\\n\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\ndef\\nload_images_from_path(path,\\nlabel):\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 395, 'file_type': 'pdf'}, page_content=\"394\\n\\xa0\\xa0\\xa0 images,\\nlabels\\n=\\n[],\\n[]\\n\\n\\xa0\\xa0\\xa0 for\\nfile\\nin\\nos.listdir(path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 images.append(image.img_to_array(image.load_img(os.path.join(path,\\nfile),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224,\\n3))))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 labels.append((label))\\n\\n\\xa0\\xa0\\xa0 return\\nimages,\\nlabels\\n\\ndef\\nshow_images(images):\\n\\xa0\\xa0\\xa0 fig,\\naxes\\n=\\nplt.subplots(1,\\n8,\\nfigsize=(20,\\n20),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.imshow(images[i]\\n/\\n255)\\n\\nx,\\ny\\n=\\n[],\\n[]\\nUse the following statements to load the background\\nspectrogram images, add them to the list named x, and label\\nthem with 0s:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Spectrograms/background',\\n0)\\nshow_images(images)\\n\\nx\\n+=\\nimages\\ny\\n+=\\nlabels\\nRepeat this process to load chainsaw spectrograms from the\\nSpectrograms/chainsaw directory, engine spectrograms from the\\nSpectrograms/engine directory, and thunderstorm spectrograms\\nfrom the Spectrograms/storm directory. Label chainsaw\\nspectrograms with 1s, engine spectrograms with 2s, and\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 396, 'file_type': 'pdf'}, page_content='395\\nthunderstorm spectrograms with 3s. Here are the labels for\\nthe four classes of images:\\n\\n\\xa0Spectrogram type\\xa0 \\xa0Label\\xa0\\n\\xa0Background\\xa0\\n\\xa00\\xa0\\n\\xa0Chainsaw\\xa0\\n\\xa01\\xa0\\n\\xa0Engine\\xa0\\n\\xa02\\xa0\\n\\xa0Storm\\xa0\\n\\xa03\\xa0\\nSince this model may one day run on mobile phones, we’ll use\\nMobileNetV2 as the base network. Use the following code to\\npreprocess the pixels and split the images and labels into\\ntwo datasets—one for training and one for testing:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\nfrom\\ntensorflow.keras.applications.mobilenet\\nimport\\npreprocess_input\\n\\nx\\n=\\npreprocess_input(np.array(x))\\ny\\n=\\nnp.array(y)\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(x,\\ny,\\nstratify=y,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 test_size=0.3,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nCall Keras’s MobileNetV2 function to instantiate MobileNetV2\\nwithout the classification layers. Then run the training data\\nand test data through MobileNetV2 to extract features from\\nthe spectrogram images:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nMobileNetV2'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 397, 'file_type': 'pdf'}, page_content=\"396\\nbase_model\\n=\\nMobileNetV2(weights='imagenet',\\ninclude_top=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_shape=(224,\\n224,\\n3))\\n\\ntrain_features\\n=\\nbase_model.predict(x_train)\\ntest_features\\n=\\nbase_model.predict(x_test)\\nDefine a neural network to classify features extracted by\\nMobileNetV2:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Flatten())\\nmodel.add(Dense(512,\\nactivation='relu'))\\nmodel.add(Dense(4,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nNOTE\\nAs an experiment, I replaced the Flatten layer with a Global\\u200b\\nAvera\\u2060gePooling2D layer. Validation accuracy improved slightly, but\\nthe model didn’t generalize as well when tested with audio\\nextracted from a documentary video. This underscores an important\\npoint from Chapter\\xa09: you can have full trust and confidence in a\\nmodel only when it’s tested with data it has never seen before—\\npreferably data that comes from a different source.\\nTrain the network with the features:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 398, 'file_type': 'pdf'}, page_content=\"397\\n\\nhist\\n=\\nmodel.fit(train_features,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(test_features,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\nPlot the training and validation accuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe validation accuracy should reach 95% or higher:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 399, 'file_type': 'pdf'}, page_content=\"398\\nRun the test images through the network and use a confusion\\nmatrix to assess the results:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\nfig,\\nax\\n=\\nplt.subplots(figsize=(4,\\n4))\\nax.grid(False)\\n\\ny_pred\\n=\\nmodel.predict(test_features)\\nclass_labels\\n=\\n['background',\\n'chainsaw',\\n'engine',\\n'storm']\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=class_labels,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 400, 'file_type': 'pdf'}, page_content='399\\nThe network is reasonably adept at identifying clips that\\ndon’t contain the sounds of chainsaws or engines. It\\nsometimes confuses chainsaw sounds and engine sounds. That’s\\nOK, because the presence of either might indicate illicit\\nactivity in a rainforest:\\nThe Sounds directory has a subdirectory named samples\\ncontaining WAV files with which the CNN was neither trained\\nnor validated. The WAV files bear no relation to the samples\\nused for training and testing; they come from a YouTube video\\ndocumenting Brazil’s efforts to curb illegal logging. Let’s\\nuse the model you just trained to analyze these files for\\nsounds of logging activity.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 401, 'file_type': 'pdf'}, page_content=\"400\\nStart by creating a spectrogram from the first sample WAV\\nfile, which contains audio of loggers cutting down trees in\\nthe Amazon:\\n\\ncreate_spectrogram('Sounds/samples/sample1.wav',\\n'Spectrograms/sample1.png')\\n\\nx\\n=\\nimage.load_img('Spectrograms/sample1.png',\\ntarget_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nPreprocess the spectrogram image, pass it to MobileNetV2 for\\nfeature extraction, and classify the features:\\n\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n\\ny\\n=\\nbase_model.predict(x)\\npredictions\\n=\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nNow create a spectrogram from a WAV file that features the\\nsound of a logging truck rumbling through the rainforest:\\n\\ncreate_spectrogram('Sounds/samples/sample2.wav',\\n'Spectrograms/sample2.png')\\n\\nx\\n=\\nimage.load_img('Spectrograms/sample2.png',\\ntarget_size=(224,\\n224))\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 402, 'file_type': 'pdf'}, page_content=\"401\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nPreprocess the image, pass it to MobileNetV2 for feature\\nextraction, and classify the features:\\n\\nx\\n=\\nimage.img_to_array(x)\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\nx\\n=\\npreprocess_input(x)\\n\\ny\\n=\\nbase_model.predict(x)\\npredictions\\n=\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nIf the network got either of the samples wrong, try training\\nit again. Remember that a neural network will train\\ndifferently every time, in part because Keras initializes the\\nweights with small random values. In the real world, data\\nscientists often train a neural network several times and\\naverage the results to quantify its accuracy.\\nSummary\\nConvolutional neural networks excel at image classification\\nbecause they use convolution kernels to extract features from\\nimages at different resolutions—features intended to\\naccentuate differences between classes. Convolution layers\\nuse convolution kernels to extract features, and pooling\\nlayers reduce the size of the feature maps output from the\\nconvolution layers. Output from these layers is input to\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 403, 'file_type': 'pdf'}, page_content='402\\nfully connected layers for classification. Keras provides\\nimplementations of convolution and pooling layers in classes\\nsuch as Conv2D and MaxPooling2D.\\nTraining a CNN from scratch when there is a relatively high\\ndegree of separation between classes—for example, the MNIST\\ndataset—is feasible on an ordinary laptop or PC. Training a\\nCNN to solve a more perceptual problem requires more training\\nimages and commensurately more compute power. Transfer\\nlearning is a practical alternative to training CNNs from\\nscratch. It uses the intelligence already present in the\\nbottleneck layers of pretrained CNNs to extract features from\\nimages, and then uses its own classification layers to\\ninterpret the results.\\nData augmentation can increase the accuracy of a CNN trained\\nwith a relatively small number of images and is especially\\nuseful with transfer learning. Augmentation involves applying\\nrandom transforms such as translations and rotations to the\\ntraining images. You can transform images before inputting\\nthem to the network with Keras’s ImageDataGenerator class,\\nor you can build the transforms into the network with layers\\nsuch as RandomRotation and RandomTranslation. Layers that\\ntransform images are active at training time but inactive\\nwhen the network makes predictions.\\nCNNs are applicable to a wide variety of computer-vision\\nproblems and are almost single-handedly responsible for the\\nrapid advancements made in that field in the past decade.\\nThey play an important role in modern facial recognition\\nsystems too. Want to know more? Detecting and identifying\\nfaces in photographs is the subject of the next chapter.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 404, 'file_type': 'pdf'}, page_content='403\\nChapter 11. Face Detection and\\nRecognition\\nNot long ago, I boarded a flight to Europe and was surprised\\nthat I didn’t have to show my passport. I passed in front of\\na camera and was promptly welcomed aboard the flight. It was\\npart of an early pilot for Delta Air Lines’ effort to push\\nforward with facial recognition and offer a touchless curb-\\nto-gate travel experience.\\nFacial recognition is everywhere. It’s one of the most\\ncommon, and sometimes controversial, applications for AI.\\nMeta, formerly known as Facebook, uses it to tag friends in\\nphotos—at least it did until it killed the feature due to\\nprivacy concerns. Apple uses it to allow users to unlock\\ntheir iPhones, while Microsoft uses it to unlock Windows PCs.\\nUber uses it to confirm the identity of its drivers. Used\\nproperly, facial recognition has vast potential to make the\\nworld a better, safer, and more secure place.\\nSuppose you want to build a system that identifies people in\\nphotos or video frames. Perhaps it’s part of a security\\nsystem that restricts access to college dorms to students and\\nstaff who are authorized to enter. Or perhaps you’re writing\\nan app that searches your hard disk for photos of people you\\nknow. (“Show me all the photos of me and my daughter.”)\\nBuilding systems such as these requires algorithms or models\\ncapable of:\\n\\xa0\\nFinding faces in photos or video frames, a\\nprocess known as face detection\\nIdentifying the faces detected, a process known\\nas facial recognition or face identification'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 405, 'file_type': 'pdf'}, page_content='404\\nNumerous well-known algorithms exist for finding and\\nidentifying faces in photos. Some rely on deep learning—in\\nparticular, convolutional neural networks—while some do not.\\nFacial recognition, after all, predated the explosion of deep\\nlearning by decades. But deep learning has supercharged the\\nscience of facial recognition and made it more practical than\\never before.\\nThis chapter begins by introducing two popular face detection\\nmethods. Then it moves on to facial recognition and\\nintroduces transfer learning as a means for recognizing\\nfaces. It concludes with a tutorial in which you put the\\npieces together and build a facial recognition system of your\\nown. Sound like fun? Then let’s get started.\\nFace Detection\\nThe sections that follow introduce two widely used algorithms\\nfor face detection—one that relies on machine learning and\\nanother that uses deep learning—as well as libraries that\\nimplement them. The goal is to be able to find all the faces\\nin a photo or video frame like the one in Figure\\xa011-1.\\nAfterward, I’ll present an easy-to-use function that you can\\ncall to extract all the facial images from a photo and save\\nthem to disk or submit them to a facial recognition model.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 406, 'file_type': 'pdf'}, page_content='405\\nFigure 11-1. Face detection\\nFace Detection with Viola-Jones\\nOne of the fastest and most popular algorithms for detecting\\nfaces in photos stems from a paper published in 2001 titled\\n“Rapid Object Detection Using a Boosted Cascade of Simple\\nFeatures”. Sometimes known as Viola-Jones (the authors of\\nthe paper), the algorithm keys on the relative intensities of\\nadjacent blocks of pixels. For example, the average pixel\\nintensity in a rectangle around the eyes is typically darker\\nthan the average pixel intensity in a rectangle immediately\\nbelow that area. Similarly, the bridge of the nose is usually\\nlighter than the region around the eyes, so two dark\\nrectangles with a bright rectangle in the middle might\\nrepresent two eyes and a nose. The presence of many such\\nHaar-like features in a frame at the right locations is an\\nindicator that the frame contains a face (Figure\\xa011-2).'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 407, 'file_type': 'pdf'}, page_content='406\\nFigure 11-2. Face detection using Haar-like features\\nViola-Jones works by sliding windows of various sizes over an\\nimage looking for frames with Haar-like features in the right\\nplaces. At each stop, the pixels in the window are scaled to\\na specified size (typically 24 × 24), and features are\\nextracted and fed into a binary classifier that returns\\npositive indicating the frame contains a face or negative\\nindicating it does not. Then the window slides to the next\\nlocation and the detection regimen begins again.\\nThe key to Viola-Jones’s performance is the binary\\nclassifier. A frame that is 24 pixels wide and 24 pixels high\\ncontains more than 160,000 combinations of rectangles\\nrepresenting potential Haar-like features. Rather than\\ncompute values for every combination, Viola-Jones computes\\nonly those that the classifier requires. Furthermore, how\\nmany features the classifier requires depends on the content\\nof the frame. The classifier is actually several binary\\nclassifiers arranged in stages. The first stage might require\\njust one feature. The second stage might require 10, the\\nthird might require 20, and so on. Features are extracted and\\npassed to stage n only if stage n – 1 returns positive,\\ngiving rise to the term cascade classifier.\\nFigure\\xa011-3 depicts a three-stage cascade classifier. Each\\nstage is carefully tuned to achieve a 100% detection rate\\nusing a limited number of features even if the false-positive\\nrate is high. In the first stage, one feature determines\\nwhether the frame contains a face. A positive response means\\nthe frame might contain a face; a negative response means'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 408, 'file_type': 'pdf'}, page_content='407\\nthat it most certainly doesn’t, in which case no further\\nchecks are performed. If stage 1 returns positive, however,\\n10 other features are extracted and passed to stage 2. A\\nframe is judged to contain a face only if all stages return\\npositive, yielding a cumulative false-positive rate near\\nzero. In machine learning, this is a design pattern known as\\nhigh recall then precision because while individual stages\\nare tuned for high recall, the cumulative effect is one of\\nhigh precision.\\nFigure 11-3. Face detection using a cascade classifier\\nOne benefit of this architecture is that frames lacking faces\\ntend to fall out fast because they evoke a negative response\\nearly in the cascade. Because most frames don’t contain\\nfaces, the algorithm runs very quickly until it encounters a\\nframe that does. In testing with a 38-stage classifier\\ntrained on 6,061 features from 4,916 facial images, Viola and\\nJones found that, on average, just 10 features were extracted\\nfrom each frame.\\nThe efficacy of Viola-Jones depends on the cascade\\nclassifier, which is essentially a machine learning model\\ntrained with facial and nonfacial images. Training is slow,\\nbut predictions are fast. In some respects, Viola-Jones acts\\nlike a CNN handcrafted to extract the minimum number of\\nfeatures needed to determine whether a frame contains a face.\\nTo speed feature extraction, Viola-Jones uses a clever\\nmathematical trick called integral images to rapidly compute\\nthe difference in intensity between two blocks of pixels. The\\nresult is a system that can identify bounding boxes'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 409, 'file_type': 'pdf'}, page_content=\"408\\nsurrounding faces in an image with a relatively high degree\\nof accuracy, and it can do so quickly enough to detect faces\\nin live video frames.\\nUsing the OpenCV Implementation of Viola-\\nJones\\nOpenCV is a popular open source computer-vision library\\nthat’s free for commercial use. It provides an\\nimplementation of Viola-Jones in its CascadeClassifier class,\\nalong with an XML file containing a cascade classifier\\ntrained to detect faces. The following statements use Cascade\\u200b\\nClassi\\u2060fier in a Jupyter notebook to detect faces in an image\\nand draw rectangles around the faces. You can use an image of\\nyour own or download the one featured in my example from\\nGitHub:\\n\\nimport\\ncv2\\nfrom\\ncv2\\nimport\\nCascadeClassifier\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\nimage\\n=\\nplt.imread('Data/Amsterdam.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\n\\nmodel\\n=\\nCascadeClassifier(cv2.data.haarcascades\\n+\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'haarcascade_frontalface_default.xml')\\nfaces\\n=\\nmodel.detectMultiScale(image)\\n\\nfor\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\n=\\nface\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 410, 'file_type': 'pdf'}, page_content=\"409\\n\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\nHere’s the output with a photo of a mother and her daughter\\ntaken in Amsterdam a few years ago:\\nCascadeClassifier detected the two faces in the photo, but it\\nalso suffered a number of false positives. One way to\\nmitigate that is to use the minNeighbors parameter. It\\ndefaults to 3, but higher values make CascadeClassifier more\\nselective. With minNeighbors=20, detectMultiScale finds just\\nthe faces of the two people:\\n\\nfaces\\n=\\nmodel.detectMultiScale(image,\\nminNeighbors=20)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 411, 'file_type': 'pdf'}, page_content='410\\nHere is the output:\\nAs detectMultiScale analyzes an image, it typically detects a\\nface multiple times, each defined by a bounding box that’s\\naligned slightly differently. The minNeighbors parameter\\nspecifies the minimum number of times a face must be detected\\nto be reported as a face. Higher values deliver higher\\nprecision (fewer false positives), but at the cost of lower\\nrecall, which means some faces might not be detected.\\nCascadeClassifier frequently requires tuning in this manner\\nto strike the right balance between finding too many faces\\nand finding too few. With that in mind, it is among the\\nfastest face detection algorithms in existence. It can also\\nbe used to detect objects other than faces by loading XML\\nfiles containing other pretrained classifiers. In OpenCV’s\\nGitHub repo, you’ll find XML files for detecting silverware\\nand other objects using Haar-like features, and XML files'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 412, 'file_type': 'pdf'}, page_content='411\\nthat detect objects using a different type of discriminator\\ncalled local binary patterns.\\nFace Detection with Convolutional Neural\\nNetworks\\nWhile more computationally expensive, deep-learning methods\\noften do a better job of detecting faces in images than\\nViola-Jones. In particular, multitask cascaded convolutional\\nneural networks, or MTCNNs, have proven adept at face\\ndetection in a variety of benchmarks. They also identify\\nfacial landmarks such as the eyes, the nose, and the mouth.\\nFigure\\xa011-4 is adapted from a diagram in the 2016 paper\\ntitled “Joint Face Detection and Alignment Using Multitask\\nCascaded Convolutional Networks” that proposed MTCNNs. An\\nMTCNN uses three CNNs arranged in a series to detect faces.\\nThe first one, called the Proposal Network, or P-Net, is a\\nshallow CNN that searches the image at various resolutions\\nlooking for features indicative of faces. Rectangles\\nidentified by P-Net are combined to form candidate face\\nrectangles and are input to the Refine Network, or R-Net,\\nwhich is a deeper CNN that examines each rectangle more\\nclosely and rejects those that lack faces. Finally, output\\nfrom R-Net is input to the Output Network (O-Net), which\\nfurther filters candidate rectangles and identifies facial\\nlandmarks. MTCNNs are multitask CNNs because they produce\\nthree outputs each—a classification output indicating the\\nconfidence level that the rectangle contains a face, and two\\nregression outputs locating the face and facial landmarks—\\nrather than just one. And they’re cascaded like Viola-Jones\\nclassifiers to quickly rule out frames that don’t contain\\nfaces.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 413, 'file_type': 'pdf'}, page_content='412\\nFigure 11-4. Multitask cascaded convolutional neural network\\nA handy MTCNN implementation is available in the Python\\npackage named MTCNN. The following statements use it to\\ndetect faces in the same photo featured in the previous\\nexample:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\nfrom\\nmtcnn.mtcnn\\nimport\\nMTCNN\\n%matplotlib\\ninline'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 414, 'file_type': 'pdf'}, page_content=\"413\\nimage\\n=\\nplt.imread('Data/Amsterdam.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\n\\ndetector\\n=\\nMTCNN()\\nfaces\\n=\\ndetector.detect_faces(image)\\n\\nfor\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\n=\\nface['box']\\n\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\nHere’s the result:\\nThe MTCNN detected not only the faces of the two people but\\nalso the face of a statue reflected in the door behind them.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 415, 'file_type': 'pdf'}, page_content=\"414\\nHere’s what detect_faces actually returned—a list\\ncontaining three dictionaries, each corresponding to one of\\nthe faces in the photo:\\n[\\n\\xa0 {\\n\\xa0\\xa0\\xa0 'box': [723, 248, 204, 258],\\n\\xa0\\xa0\\xa0 'confidence': 0.9997798800468445,\\n\\xa0\\xa0\\xa0 'keypoints': {\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'left_eye': (765, 341),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'right_eye': (858, 343),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'nose': (800, 408),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_left': (770, 432),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_right': (864, 433)\\n\\xa0\\xa0\\xa0 }\\n\\xa0 },\\n\\xa0 {\\n\\xa0\\xa0\\xa0 'box': [538, 258, 183, 232],\\n\\xa0\\xa0\\xa0 'confidence': 0.9997591376304626,\\n\\xa0\\xa0\\xa0 'keypoints': {\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'left_eye': (601, 353),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'right_eye': (685, 344),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'nose': (662, 394),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_left': (614, 433),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_right': (689, 424)\\n\\xa0\\xa0\\xa0 }\\n\\xa0 },\\n\\xa0 {\\n\\xa0\\xa0\\xa0 'box': [1099, 84, 40, 41],\\n\\xa0\\xa0\\xa0 'confidence': 0.8863282203674316,\\n\\xa0\\xa0\\xa0 'keypoints': {\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'left_eye': (1108, 101),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'right_eye': (1123, 96),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'nose': (1116, 102),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_left': (1114, 115),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_right': (1127, 111)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 416, 'file_type': 'pdf'}, page_content=\"415\\n\\xa0\\xa0\\xa0 }\\n\\xa0 }\\n]\\nYou can eliminate the face in the reflection in either of two\\nways: by ignoring faces with a confidence level below a\\ncertain threshold, or by passing a min_face_size parameter to\\nthe MTCNN function so that detect_faces ignores faces smaller\\nthan a specified size. Here’s a modified for loop that does\\nthe former:\\n\\nfor\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0 if\\nface['confidence']\\n>\\n0.9:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\n=\\nface['box']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\nAnd here’s the result:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 417, 'file_type': 'pdf'}, page_content='416\\nThe facial rectangles in Figure\\xa011-1 were generated using\\nMTCNN’s default settings—that is, without any filtering\\nbased on confidence levels or face sizes. Generally speaking,\\nit does a better job out of the box than CascadeClassifier at\\ndetecting faces.\\nExtracting Faces from Photos\\nOnce you know how to find faces in photos, it’s a simple\\nmatter to extract facial images in order to train a model or\\nsubmit them to a trained model for identification.\\nExample\\xa011-1 presents a Python function that accepts a path\\nto an image file and returns a list of facial images. By\\ndefault, it crops facial images so that they’re square\\n(perfect for passing them to a CNN), but you can disable\\ncropping by passing the function a crop=False parameter. You'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 418, 'file_type': 'pdf'}, page_content='417\\ncan also specify a minimum confidence level with a\\nmin_confidence parameter, which defaults to 0.9.\\nExample 11-1. Function for extracting facial images from a\\nphoto\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\nPIL\\nimport\\nImage,\\nImageOps\\nfrom\\nmtcnn.mtcnn\\nimport\\nMTCNN\\n\\ndef\\nextract_faces(input_file,\\nmin_confidence=0.9,\\ncrop=True):\\n\\xa0\\xa0\\xa0 # Load the image and orient it correctly\\n\\xa0\\xa0\\xa0 pil_image\\n=\\nImage.open(input_file)\\n\\xa0\\xa0\\xa0 exif\\n=\\npil_image.getexif()\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 for\\nk\\nin\\nexif.keys():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nk\\n!=\\n0x0112:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 exif[k]\\n=\\nNone\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 del\\nexif[k]\\n\\n\\xa0\\xa0\\xa0 pil_image.info[\"exif\"]\\n=\\nexif.tobytes()\\n\\xa0\\xa0\\xa0 pil_image\\n=\\nImageOps.exif_transpose(pil_image)\\n\\xa0\\xa0\\xa0 image\\n=\\nnp.array(pil_image)\\n\\n\\xa0\\xa0\\xa0 # Find the faces in the image\\n\\xa0\\xa0\\xa0 detector\\n=\\nMTCNN()\\n\\xa0\\xa0\\xa0 faces\\n=\\ndetector.detect_faces(image)\\n\\xa0\\xa0\\xa0 faces\\n=\\n[face\\nfor\\nface\\nin\\nfaces\\nif\\nface[\\'confidence\\']\\n>=\\nmin_confidence]\\n\\xa0\\xa0\\xa0 results\\n=\\n[]\\n\\n\\xa0\\xa0\\xa0 for\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x1,\\ny1,\\nw,\\nh\\n=\\nface[\\'box\\']\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(crop):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Compute crop coordinates\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nw\\n>\\nh:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 419, 'file_type': 'pdf'}, page_content=\"418\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x1\\n=\\nx1\\n+\\n((w\\n-\\nh)\\n//\\n2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 w\\n=\\nh\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 elif\\nh\\n>\\nw:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y1\\n=\\ny1\\n+\\n((h\\n-\\nw)\\n//\\n2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 h\\n=\\nw\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Extract the facial image and add it to the list\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x2\\n=\\nx1\\n+\\nw\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y2\\n=\\ny1\\n+\\nh\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 results.append(Image.fromarray(image[y1:y2,\\nx1:x2]))\\n\\n\\xa0\\xa0\\xa0 # Return all the facial images\\n\\xa0\\xa0\\xa0 return\\nresults\\nI passed the photo in Figure\\xa011-5 to the function, and it\\nreturned the faces underneath. The items returned from\\nextract_faces are Python Imaging Library (PIL) images, so you\\ncan resize them or save them to disk with a single line of\\ncode. Here’s a code snippet that extracts all the faces from\\na photo, resizes them to 224 × 224 pixels, and saves the\\nresized images:\\nfaces\\n=\\nextract_faces('PATH_TO_IMAGE_FILE')\\n\\nfor\\ni,\\nface\\nin\\nenumerate(faces):\\n\\xa0\\xa0\\xa0 face.resize((224,\\n224)).save(f'face{i}.jpg')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 420, 'file_type': 'pdf'}, page_content='419\\nFigure 11-5. Facial images extracted from a photo\\nWith extract_faces to lend a hand, it’s a relatively simple\\nmatter to generate a set of facial images for training a CNN\\nfrom a batch of photos on your hard disk, or to extract faces\\nfrom a photo and submit them to a CNN for identification.\\nFacial Recognition\\nNow that you know how to detect faces in photos, the next\\nstep is to learn how to identify them. Several algorithms for\\nrecognizing faces in photos have been developed over the\\nyears. Some rely on biometrics, such as the distance between\\nthe eyes or the texture of the skin, while others take a more\\nholistic approach by treating facial identification as a\\npattern recognition problem. State-of-the-art models today'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 421, 'file_type': 'pdf'}, page_content='420\\nfrequently rely on deep convolutional neural networks.\\nOne\\xa0of the primary benchmarks for facial recognition models\\nis the Labeled Faces in the Wild (LFW) dataset pictured in\\nFigure\\xa011-6, which contains more than 13,000 facial images\\nof more than 5,000 people collected from the web. Deep-\\nlearning models such as MobiFace and FaceNet routinely\\nachieve greater than 99% accuracy on the dataset. This equals\\nor exceeds a human’s ability to identify faces in LFW\\nphotos.\\nFigure 11-6. The Labeled Faces in the Wild dataset\\nChapter\\xa05 presented a support vector machine (SVM) that\\nachieved 85% accuracy using a subset of 500 images—100 each\\nof five famous people—from the dataset. Chapter\\xa09 tackled\\nthe same problem with a neural network, with similar results.\\nThese models merely scratch the surface of what modern facial\\nrecognition can accomplish. Let’s apply CNNs and transfer\\nlearning to the same LFW subset and see if they can do better\\nat recognizing faces in photos. Along the way, you’ll learn\\na valuable lesson about pretrained CNNs and the specificity\\nof the weights that are generated when those CNNs are\\ntrained.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 422, 'file_type': 'pdf'}, page_content='421\\nApplying Transfer Learning to Facial\\nRecognition\\nThe first step in exploring CNN-based facial recognition is\\nto load the LFW dataset. This time, we’ll load full-size\\ncolor images and crop them to 128 × 128 pixels. Here’s the\\ncode:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\n=\\nfetch_lfw_people(min_faces_per_person=100,\\nresize=1.0,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 slice_=(slice(60,\\n188),\\nslice(60,\\n188)),\\ncolor=True)\\nclass_count\\n=\\nlen(faces.target_names)\\n\\nprint(faces.target_names)\\nprint(faces.images.shape)\\nBecause we set min_faces_per_person to 100, a total of 1,140\\nfacial images corresponding to five people were loaded. Use\\nthe following statements to show the first several images and\\nthe labels that go with them:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\nfig,\\nax\\n=\\nplt.subplots(3,\\n6,\\nfigsize=(18,\\n10))\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i]\\n/\\n255)\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 423, 'file_type': 'pdf'}, page_content='422\\nThe dataset is imbalanced, containing almost as many photos\\nof George W. Bush as of everyone else combined. Use the\\nfollowing code to reduce the dataset to 100 images of each\\nperson, for a total of 500 facial images:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nmask\\n=\\nnp.zeros(faces.target.shape,\\ndtype=bool)\\n\\nfor\\ntarget\\nin\\nnp.unique(faces.target):\\n\\xa0\\xa0\\xa0 mask[np.where(faces.target\\n==\\ntarget)[0][:100]]\\n=\\n1\\n\\nx_faces\\n=\\nfaces.data[mask]\\ny_faces\\n=\\nfaces.target[mask]\\nx_faces\\n=\\nnp.reshape(x_faces,\\n(x_faces.shape[0],\\nfaces.images.shape[1],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 faces.images.shape[2],\\nfaces.images.shape[3]))\\nx_faces.shape\\nNow preprocess the pixel values for input to a pretrained\\nResNet50 CNN and use Scikit-Learn’s train_test_split\\nfunction to split the dataset, yielding 400 training samples\\nand 100 test samples:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nface_images\\n=\\npreprocess_input(np.array(x_faces))\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 face_images,\\ny_faces,\\ntrain_size=0.8,\\nstratify=y_faces,\\n\\xa0\\xa0\\xa0 random_state=0)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 424, 'file_type': 'pdf'}, page_content=\"423\\nIf you wanted, you could divide the preprocessed pixel values\\nby 255 and train a CNN from scratch right now with this data.\\nHere’s how you’d go about it (in case you care to give it a\\ntry):\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nConv2D,\\nMaxPooling2D\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Conv2D(32,\\n(3,\\n3),\\nactivation='relu',\\ninput_shape=\\n(x_train.shape[1:])))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(64,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Conv2D(64,\\n(3,\\n3),\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\n2))\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\n\\nmodel.fit(x_train\\n/\\n255,\\ny_train,\\nvalidation_data=(x_test\\n/\\n255,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=20,\\nbatch_size=10)\\nI did it and then plotted the training and validation\\naccuracy:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 425, 'file_type': 'pdf'}, page_content='424\\nThe validation accuracy is better than that of an SVM or a\\nconventional neural network, but it’s nowhere near what\\nmodern CNNs achieve on the LFW dataset. So clearly there is a\\nbetter way.\\nThat better way, of course, is transfer learning, which we\\ncovered in Chapter\\xa010. ResNet50 was trained with more than 1\\nmillion images from the ImageNet dataset, so it should be\\nadept at extracting features from photos—more so than a\\nhandcrafted CNN trained with 400 images. Let’s see if\\nthat’s the case. Use the following statements to load\\nResNet50’s feature extraction layers, initialize them with\\nthe ImageNet weights, and freeze them so that the weights\\naren’t adjusted during training:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 426, 'file_type': 'pdf'}, page_content=\"425\\n\\nbase_model\\n=\\nResNet50(weights='imagenet',\\ninclude_top=False)\\nbase_model.trainable\\n=\\nFalse\\nNow add classification layers to the base model and include a\\nResizing layer to resize images input to the network to the\\nsize that ResNet50 expects:\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense,\\nResizing\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Resizing(224,\\n224))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nTrain the model and plot the training and validation\\naccuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 427, 'file_type': 'pdf'}, page_content=\"426\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nResults will vary, but my run produced a validation accuracy\\naround 94%:\\nThis is an improvement over a CNN trained from scratch, and\\nit’s an indication that ResNet50 does a better job of\\nextracting features from facial images. But it’s still not\\nstate of the art. Is it possible to do even better?\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 428, 'file_type': 'pdf'}, page_content=\"427\\nBoosting Transfer Learning with Task-Specific\\nWeights\\nInitialized with ImageNet weights, ResNet50 does a credible\\njob of feature extraction. Those weights were arrived at when\\nResNet50 was trained on more than 1 million photos of objects\\nranging from basketballs to butterflies. It was not, however,\\ntrained with facial images. Would it be better at extracting\\nfeatures from facial images if it were trained with facial\\nimages?\\nIn 2017, a group of researchers at the University of\\nOxford’s Visual Geometry Group published a paper titled\\n“VGGFace2: A Dataset for Recognising Faces Across Pose and\\nAge”. After assembling a dataset comprising several million\\nfacial images, they trained two variations of ResNet50 with\\nit and published the results. They also published the\\nweights, which are wrapped in a handy Python library named\\nKeras-vggface. That library includes a class named VGGFace\\nthat encapsulates ResNet50 with TensorFlow-compatible\\nweights. Out of the box, the VGGFace model is capable of\\nrecognizing the faces of thousands of celebrities ranging\\nfrom Brie Larson to Jennifer Aniston. But its real value lies\\nin using transfer learning to repurpose it to recognize faces\\nit wasn’t trained to recognize before.\\nTo simplify matters, I installed Keras-vggface, created an\\ninstance of VGGFace without the classification layers,\\ninitialized the weights, and saved the model to an H5 file\\nnamed vggface.h5. Download that file and drop it into your\\nnotebooks’ Data subdirectory. Then use the following code to\\ncreate an instance of VGGFace built on top of ResNet50, and\\nadd custom classification layers:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nload_model\\n\\nbase_model\\n=\\nload_model('Data/vggface.h5')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 429, 'file_type': 'pdf'}, page_content=\"428\\nbase_model.trainable\\n=\\nFalse\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Resizing(224,\\n224))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nNext, train the model and plot the training and validation\\naccuracy:\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe results are spectacular:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 430, 'file_type': 'pdf'}, page_content=\"429\\nTo be sure, run the test data through the network and use a\\nconfusion matrix to assess the results:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\ny_pred\\n=\\nmodel.predict(x_test)\\nfig,\\nax\\n=\\nplt.subplots(figsize=(5,\\n5))\\nax.grid(False)\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=faces.target_names,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nBecause VGGFace was tuned to extract features from facial\\nimages, it achieves a perfect score on the 100 test images.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 431, 'file_type': 'pdf'}, page_content='430\\nThat’s not to say that it will never fail to recognize a\\nface. It does indicate that, on the dataset you trained it\\nwith, it is remarkably adept at extracting features from\\nfacial images:\\nAnd therein lies an important lesson. CNNs that are trained\\nin task-specific ways frequently provide a better base for\\ntransfer learning than CNNs trained in a more generic\\nfashion. If the goal is to perform facial recognition,\\nyou’ll almost always do better with a CNN trained with\\nfacial images than a CNN trained with photos of thousands of'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 432, 'file_type': 'pdf'}, page_content='431\\ndissimilar objects. For a neural network, it’s all about the\\nweights.\\nArcFace\\nVGGFace isn’t the only pretrained CNN that excels at\\nextracting features from facial images. Another is ArcFace,\\nwhich was introduced in a 2019 paper titled “ArcFace:\\nAdditive Angular Margin Loss for Deep Face Recognition”. A\\nhandy implementation is available in a Python package named\\nArcface.\\nEach facial image submitted to ArcFace is transformed into a\\ndense vector of 512 values known as a face embedding. The\\ncode for creating an embedding is simple:\\n\\nfrom\\narcface\\nimport\\nArcFace\\n\\naf\\n=\\nArcFace.ArcFace()\\nembedding\\n=\\naf.calc_emb(image)\\nEmbeddings can be used to train machine learning models, and\\nthey can be used to make predictions with those models.\\nThanks to the loss function named in the title of the paper,\\nembeddings created by ArcFace often do a better job of\\ncapturing the uniqueness of a face than embeddings generated\\nby conventional CNNs.\\nAnother use for the embeddings created by ArcFace is face\\nverification, which compares two facial images and computes\\nthe probability that they represent the same person. The\\nfollowing statements generate embeddings for two facial\\nimages and use the cosine_similarity function introduced in\\nChapter\\xa04 to quantify the similarity between the two:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 433, 'file_type': 'pdf'}, page_content='432\\n\\naf\\n=\\nArcFace.ArcFace()\\nface_emb1\\n=\\naf.calc_emb(image1)\\nface_emb2\\n=\\naf.calc_emb(image2)\\nsim\\n=\\ncosine_similarity([face_emb1,\\nface_emb2])[0][1]\\nThe result is a value from 0.0 to 1.0, with higher values\\nreflecting greater similarity between the faces.\\nPutting It All Together: Detecting and\\nRecognizing Faces in Photos\\nAny time a model scores perfectly in testing, you should be\\nskeptical. No model is perfect, and even if it achieves 100%\\naccuracy against a test dataset, it won’t duplicate that in\\nthe wild. Given that VGGFace was trained with images of some\\nof the same famous people found in the LFW dataset, is it\\npossible that it’s biased toward those people? That transfer\\nlearning with VGGFace wouldn’t do as well if trained with\\nimages of ordinary people? And how would it perform with just\\na handful of training images?\\nTo answer these questions, let’s build a notebook that\\ntrains a facial recognition model based on VGGFace, uses an\\nMTCNN to detect faces in photos, and uses the model to\\nidentify the faces it detects. The dataset you’ll use\\ncontains eight pictures each of three ordinary people in\\nslightly different poses, at ages up to 20 years apart, with\\nand without glasses (Figure\\xa011-7). These images were\\nextracted from photos using the extract_faces function in\\nExample\\xa011-1 and resized to 224 × 224.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 434, 'file_type': 'pdf'}, page_content='433\\nFigure 11-7. Photos for training a facial recognition model\\nBegin by downloading a ZIP file containing the facial images\\nand copying the contents of the ZIP file into a subdirectory\\nnamed Faces where your notebooks are hosted. The ZIP file\\ncontains four folders: one named Jeff, one named Lori, one\\nnamed Abby, and one named Samples that contains uncropped\\nphotos for testing.\\nNow create a new notebook and run the following code in the\\nfirst cell to define helper functions for loading and\\ndisplaying facial images from the subdirectories you copied\\nthem to, and declare a pair of Python lists to hold the\\nimages and labels:\\n\\nimport\\nos\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndef\\nload_images_from_path(path,\\nlabel):\\n\\xa0\\xa0\\xa0 images,\\nlabels\\n=\\n[],\\n[]\\n\\n\\xa0\\xa0\\xa0 for\\nfile\\nin\\nos.listdir(path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 images.append(image.img_to_array(image.load_img(os.path.join(path,\\nfile),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224,\\n3))))'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 435, 'file_type': 'pdf'}, page_content=\"434\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 labels.append((label))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 return\\nimages,\\nlabels\\n\\ndef\\nshow_images(images):\\n\\xa0\\xa0\\xa0 fig,\\naxes\\n=\\nplt.subplots(1,\\n8,\\nfigsize=(20,\\n20),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.imshow(images[i]\\n/\\n255)\\n\\nx,\\ny\\n=\\n[],\\n[]\\nNext, load the images of Jeff and label them with 0s:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Faces/Jeff',\\n0)\\nshow_images(images)\\n\\xa0\\xa0\\xa0 \\nx\\n+=\\nimages\\ny\\n+=\\nlabels\\nLoad the images of Lori and label them with 1s:\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Faces/Lori',\\n1)\\nshow_images(images)\\n\\xa0\\xa0\\xa0 \\nx\\n+=\\nimages\\ny\\n+=\\nlabels\\nLoad the images of Abby and label them with 2s:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 436, 'file_type': 'pdf'}, page_content=\"435\\n\\nimages,\\nlabels\\n=\\nload_images_from_path('Faces/Abby',\\n2)\\nshow_images(images)\\n\\xa0\\xa0\\xa0 \\nx\\n+=\\nimages\\ny\\n+=\\nlabels\\nFinally, preprocess the pixels for the ResNet50 version of\\nVGGFace and split the data fifty-fifty so that the network\\nwill be trained with four randomly selected images of each\\nperson and validated with the same number of images:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nfaces\\n=\\npreprocess_input(np.array(x))\\nlabels\\n=\\nnp.array(y)\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\n=\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 faces,\\nlabels,\\ntrain_size=0.5,\\nstratify=labels,\\n\\xa0\\xa0\\xa0 random_state=0)\\nThe next step is to load the saved VGGFace model and freeze\\nthe bottleneck layers. If you didn’t download vggface.h5\\nearlier, download it now and drop it into your notebooks’\\nData subdirectory. Then execute the following code:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nload_model\\n\\nbase_model\\n=\\nload_model('Data/vggface.h5')\\nbase_model.trainable\\n=\\nFalse\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 437, 'file_type': 'pdf'}, page_content=\"436\\nNow define a network that uses transfer learning with VGGFace\\nto identify faces. The Resizing layer ensures that each image\\nmeasures exactly 224 × 224 pixels. The Dense layer contains\\njust eight neurons because the training dataset is small and\\nwe don’t want the network to fit too tightly to it:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense,\\nResizing\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Resizing(224,\\n224))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(8,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nTrain the model:\\n\\nhist\\n=\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=2,\\nepochs=10)\\nPlot the training and validation accuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval_acc\\n=\\nhist.history['val_accuracy']\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 438, 'file_type': 'pdf'}, page_content=\"437\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\n':',\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nHopefully you got something like this:\\nNow comes the fun part: using an MTCNN to detect the faces in\\na photo and the trained model to identify those faces. First\\nmake sure the MTCNN package is installed in your environment.\\nThen define a pair of helper functions—one that retrieves a\\nface from a specified location in an image (get_face), and\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 439, 'file_type': 'pdf'}, page_content=\"438\\nanother that loads a photo and annotates faces in the photo\\nwith names and confidence levels (label_faces):\\n\\nfrom\\nmtcnn.mtcnn\\nimport\\nMTCNN\\nfrom\\nPIL\\nimport\\nImage,\\nImageOps\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\n\\ndef\\nget_face(image,\\nface):\\n\\xa0\\xa0\\xa0 x1,\\ny1,\\nw,\\nh\\n=\\nface['box']\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 if\\nw\\n>\\nh:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x1\\n=\\nx1\\n+\\n((w\\n-\\nh)\\n//\\n2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 w\\n=\\nh\\n\\xa0\\xa0\\xa0 elif\\nh\\n>\\nw:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y1\\n=\\ny1\\n+\\n((h\\n-\\nw)\\n//\\n2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 h\\n=\\nw\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 x2\\n=\\nx1\\n+\\nh\\n\\xa0\\xa0\\xa0 y2\\n=\\ny1\\n+\\nw\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 return\\nimage[y1:y2,\\nx1:x2]\\n\\ndef\\nlabel_faces(path,\\nmodel,\\nnames,\\nface_threshold=0.9,\\nprediction_threshold=0.9,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 show_outline=True,\\nsize=(12,\\n8)):\\n\\xa0\\xa0\\xa0 # Load the image and orient it correctly\\n\\xa0\\xa0\\xa0 pil_image\\n=\\nImage.open(path)\\n\\xa0\\xa0\\xa0 exif\\n=\\npil_image.getexif()\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 for\\nk\\nin\\nexif.keys():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nk\\n!=\\n0x0112:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 exif[k]\\n=\\nNone\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 del\\nexif[k]\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 440, 'file_type': 'pdf'}, page_content='439\\n\\xa0\\xa0\\xa0 pil_image.info[\"exif\"]\\n=\\nexif.tobytes()\\n\\xa0\\xa0\\xa0 pil_image\\n=\\nImageOps.exif_transpose(pil_image)\\n\\xa0\\xa0\\xa0 np_image\\n=\\nnp.array(pil_image)\\n\\n\\xa0\\xa0\\xa0 fig,\\nax\\n=\\nplt.subplots(figsize=size,\\nsubplot_kw={\\'xticks\\':\\n[],\\n\\'yticks\\':\\n[]})\\n\\xa0\\xa0\\xa0 ax.imshow(np_image)\\n\\n\\xa0\\xa0\\xa0 detector\\n=\\nMTCNN()\\n\\xa0\\xa0\\xa0 faces\\n=\\ndetector.detect_faces(np_image)\\n\\xa0\\xa0\\xa0 faces\\n=\\n[face\\nfor\\nface\\nin\\nfaces\\nif\\nface[\\'confidence\\']\\n>\\nface_threshold]\\n\\n\\xa0\\xa0\\xa0 for\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\n=\\nface[\\'box\\']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Use the model to identify the face\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 face_image\\n=\\nget_face(np_image,\\nface)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 face_image\\n=\\nimage.array_to_img(face_image)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 face_image\\n=\\npreprocess_input(np.array(face_image))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 predictions\\n=\\nmodel.predict(np.expand_dims(face_image,\\naxis=0))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 confidence\\n=\\nnp.max(predictions)\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(confidence\\n>\\nprediction_threshold):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Optionally draw a box around the face\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nshow_outline:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor=\\'red\\',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Label the face\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index\\n=\\nint(np.argmax(predictions))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n= f\\'{names[index]} ({confidence:.1%})\\'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.text(x\\n+\\n(w\\n/\\n2),\\ny,\\ntext,\\ncolor=\\'white\\',\\nbackgroundcolor=\\'red\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ha=\\'center\\',\\nva=\\'bottom\\',\\nfontweight=\\'bold\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 bbox=dict(color=\\'red\\'))'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 441, 'file_type': 'pdf'}, page_content=\"440\\nNow pass the first sample image in the Samples folder to\\nlabel_faces:\\n\\nlabels\\n=\\n['Jeff',\\n'Lori',\\n'Abby']\\nlabel_faces('Faces/Samples/Sample-1.jpg',\\nmodel,\\nlabels)\\nThe output should look like this, although your percentages\\nmight be different:\\nTry it again, but this time with a different photo:\\n\\nlabel_faces('Faces/Samples/Sample-2.jpg',\\nmodel,\\nlabels)\\nHere’s the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 442, 'file_type': 'pdf'}, page_content=\"441\\nFinally, submit a photo containing all three individuals that\\nthe model was trained with:\\n\\nlabel_faces('Faces/Samples/Sample-3.jpg',\\nmodel,\\nlabels)\\nTrained with just 12 facial images—four of each person—the\\nmodel does a credible job of identifying faces in photos. Of\\ncourse, you could generate a dataset of your own by passing\\nphotos of friends and family members through the function in\\nExample\\xa011-1 and training the model with the resulting\\nimages.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 443, 'file_type': 'pdf'}, page_content='442\\nHandling Unknown Faces: Closed-Set Versus\\nOpen-Set Classification\\nNow for some bad news. A VGGFace facial recognition model is\\nadept at identifying faces it was trained with, but it\\ndoesn’t know what to do when it encounters a face it wasn’t\\ntrained with. Try it: pass in a photo of yourself. The model\\nwill probably identify you as Jeff, Lori, or Abby, and it\\nmight do so with a high level of confidence. It literally\\ndoesn’t know what it doesn’t know. This is especially true\\nwhen the dataset is small and the network is given room to\\noverfit.\\nThe reason why has nothing to do with VGGFace. It has\\neverything to do with the fact that a neural network with a\\nsoftmax output layer is a closed-set classifier, meaning it\\nclassifies any sample presented to it for predictions as one\\nof the classes it was trained with. (Remember that softmax\\nensures that the sum of the probabilities for all classes is\\n1.0.) The alternative is an open-set classifier (Figure\\xa011-\\n8), which has the ability to say “this sample doesn’t\\nbelong to any of the classes I was trained with.”\\nFigure 11-8. Closed-set versus open-set classification\\nThere is not a one-size-fits-all solution for building open-\\nset classifiers in the deep-learning community today. A 2016\\npaper titled “Towards Open Set Deep Networks” proposed one\\nsolution in the form of openmax output layers, which replace\\nsoftmax output layers and “estimate the probability of an'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 444, 'file_type': 'pdf'}, page_content='443\\ninput being from an unknown class.” Essentially, if the\\nnetwork is trained with 10 classes, the openmax output layer\\nadds an 11th output representing the unknown class. It works\\nby taking the activations from the final classification layer\\nand adjusting them using a Weibull distribution rather than\\nnormalizing the probabilities as softmax does.\\nAnother potential solution was put forth in a 2018 paper\\ntitled “Reducing Network Agnostophobia” from researchers at\\nthe University of Colorado. It proposed replacing cross-\\nentropy loss with a new loss function called entropic open-\\nset loss that drives softmax scores for unknown classes\\ntoward a uniform probability distribution. Using this\\ntechnique, you could more reliably detect samples belonging\\nto an unknown class using probability thresholds paired with\\nconventional softmax output layers. For a great summary of\\nthe problems posed by open-set classification in deep\\nlearning and an overview of openmax and entropic open-set\\nloss, see “Does a Neural Network Know When It Doesn’t\\nKnow?” by Tivadar Danka.\\nYet another solution is to use ArcFace to verify each face\\nthe model identifies by comparing an embedding generated from\\nthat face to a reference embedding for the same person. You\\ncould reject the model’s conclusion if cosine similarity\\nfalls below a predetermined threshold.\\nA more naive approach is to prevent the network from learning\\nthe training data too well in hopes that unknown classes will\\nyield lower softmax probabilities. That’s why I included\\njust eight neurons in the classification layer in the\\nprevious example. (You could go even further by introducing a\\ndropout layer.) It works up to a point, but it isn’t\\nperfect. The label_faces function has a default prediction\\nthreshold of 0.9, meaning it labels a face only if the model\\nclassifies it with at least 90% confidence. You could set\\nprediction_threshold to 0.99 to rule out more unknown faces,\\nbut at the cost of failing to identify more known faces.\\nTuning in this manner to strike the right balance between'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 445, 'file_type': 'pdf'}, page_content='444\\nrecognizing known faces while ignoring unknowns is an\\ninevitable part of readying a facial recognition model for\\nproduction.\\nSummary\\nBuilding an end-to-end facial recognition system requires a\\nmeans for detecting faces in photos as well as a means for\\nclassifying (identifying) those faces. One way to detect\\nfaces is the Viola-Jones algorithm, for which the OpenCV\\nlibrary provides a convenient implementation. An alternative\\nthat relies on deep learning is a multitask cascaded\\nconvolutional neural network, or MTCNN. The Python package\\nnamed MTCNN contains a ready-to-use MTCNN implementation.\\nViola-Jones is faster and more suitable for real-time\\napplications (for example, identifying faces in a live webcam\\nfeed), but MTCNNs are generally more accurate and incur fewer\\nfalse positives.\\nDeep learning can be applied to the task of facial\\nrecognition by employing convolutional neural networks.\\nTransfer learning with a pretrained CNN such as ResNet50 can\\nidentify faces with a relatively high degree of accuracy, but\\ntransfer learning with a CNN that was trained with millions\\nof facial images delivers unparalleled accuracy. The primary\\nreason is that the CNN’s bottleneck layers are optimized for\\nextracting features from facial images.\\nWith a highly optimized set of weights, a neural network can\\ndo almost anything. Generic weights will suffice when nothing\\nbetter is available, but given a task-specific set of weights\\nto start with, facial recognition via transfer learning can\\nachieve human-like accuracy.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 446, 'file_type': 'pdf'}, page_content='445\\nChapter 12. Object Detection\\nThe previous chapter introduced two popular algorithms for\\ndetecting faces in photographs: Viola-Jones, which relies on\\nmachine learning, and MTCNNs, which rely on deep learning.\\nFace detection is a special case of object detection, in\\nwhich computers detect and identify objects in images.\\nIdentifying an object is an image classification problem,\\nsomething at which CNNs excel. But finding objects to\\nidentify poses a different challenge.\\nObject detection is challenging because objects aren’t\\nassumed to be perfectly cropped and aligned as they are for\\nimage classification tasks. Nor are they limited to one per\\nimage. Figure\\xa012-1 shows what a self-driving car might see\\nas it scans video frames from a forward-pointing camera. A\\nCNN trained to do conventional image classification using\\ncarefully prepared training images is powerless to help. It\\nmight be able to classify the image as one of a city street,\\nbut it can’t determine that the image contains cars, people,\\nand traffic lights, much less pinpoint their locations.\\nObject detection has grown in speed and accuracy in recent\\nyears, and state-of-the-art methods rely on deep learning. In\\nparticular, they employ CNNs, which were introduced in\\nChapter\\xa010. Let’s discuss how CNNs do object detection and\\nidentification and try our hand at it using cutting-edge\\nobject detection models.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 447, 'file_type': 'pdf'}, page_content='446\\nFigure 12-1. Detecting and identifying objects in a scene\\nR-CNNs\\nOne way to apply deep learning to the task of object\\ndetection is to use region-based CNNs, also known as region\\nCNNs or simply R-CNNs. The first R-CNN was introduced in a\\n2014 paper titled “Rich Feature Hierarchies for Accurate\\nObject Detection and Semantic Segmentation”. The model\\ndescribed in the paper comprises three stages. The first\\nstage scans the image and identifies up to 2,000 bounding\\nboxes representing regions of interest—regions that might\\ncontain objects. The second stage is a deep CNN that extracts\\nfeatures from regions of interest. The third is a support\\nvector machine that classifies the features. The output is a\\ncollection of bounding boxes with class labels and confidence\\nscores. An algorithm called non-maximum suppression (NMS)\\nfilters the output and selects the best bounding box for each\\nobject.\\nNMS is a crucial element of virtually all modern object\\ndetection systems. A detector invariably emits several\\nbounding boxes for each object. If a photo contains one\\ninstance of a given class—for example, one zebra—NMS'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 448, 'file_type': 'pdf'}, page_content='447\\nselects the bounding box with the highest confidence score.\\nIf the photo contains two zebras (Figure\\xa012-2), NMS divides\\nthe bounding boxes into two groups and selects the box with\\nthe highest confidence score in each group. It groups boxes\\nbased on the amount of overlap between them. Overlap is\\ncomputed by dividing the area of intersection between two\\nboxes by the area formed by the union of the boxes. If the\\nresulting intersection-over-union (IoU) score is greater than\\na predetermined threshold (typically 0.5), NMS assigns the\\nboxes to the same group. Otherwise, it assigns them to\\nseparate groups.\\nWhen two objects of the same class have little or no overlap,\\nNMS easily separates the two. When two instances of the same\\nclass overlap significantly (picture one zebra standing\\nbehind the other), the IoU threshold might have to be\\nadjusted to achieve separation. IoU threshold is a\\nhyperparameter that can be tuned to strike the right balance\\nbetween being overly aggressive at separating overlapping\\nobjects and not aggressive enough.\\nFigure 12-2. Non-maximum suppression\\nThe first stage of most R-CNN implementations uses an\\nalgorithm called selective search to identify regions of\\ninterest by keying on similarities in color, texture, shape,\\nand size. Figure\\xa012-3 shows the first 500 bounding boxes\\ngenerated when OpenCV’s implementation of selective search'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 449, 'file_type': 'pdf'}, page_content='448\\nexamines an image. Submitting a targeted list of regions to\\nthe CNN is faster than a brute-force sliding-window approach\\nthat inputs the contents of the window to the CNN at every\\nstop.\\nFigure 12-3. Bounding boxes generated by selective search\\nEven with selective search narrowing the list of candidate\\nregions input to stage 2, an R-CNN can’t do object detection\\nin real time. Why? Because the CNN individually processes the\\n2,000 or so regions of interest identified in stage 1. These\\nregions invariably overlap, so the CNN processes the same\\npixels multiple times.\\nA 2015 paper titled “Fast R-CNN” addressed this by\\nproposing a modified architecture in which the entire image\\npasses through the CNN one time (Figure\\xa012-4). Selective\\nsearch or a similar algorithm identifies regions of interest\\nin the image, and those regions are projected onto the\\nfeature map generated by the CNN. A region of interest (ROI)\\npooling layer then uses a form of max pooling to reduce the\\nfeatures in each region of interest to a fixed-length vector,\\nindependent of the region’s size and shape. (By contrast, R-\\nCNN scales each region to an image of predetermined size'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 450, 'file_type': 'pdf'}, page_content='449\\nbefore submitting it to the CNN, which is substantially more\\nexpensive than ROI pooling.) Classification of the feature\\nvectors is performed by fully connected layers rather than\\nSVMs, and the output is split to include both a softmax\\nclassifier and a bounding-box regressor. NMS filters the\\nbounding boxes down to the ones that matter. The result is a\\nsystem that trains an order of magnitude faster than R-CNN,\\nmakes predictions two orders of magnitude faster, and is\\nslightly more accurate than R-CNN.\\nFigure 12-4. Fast R-CNN architecture\\nNOTE\\nROI pooling reduces any region of interest to a feature vector of\\na specified size, regardless of the region’s height and width.\\nImagine that you have an 8 × 16 region of a feature map and you\\nwant to reduce it to 4 × 4. You can divide the 8 × 16 region\\ninto a 4 × 4 grid, with each cell in the grid measuring 2 × 4.\\nYou can then take the maximum of the eight values in each cell and\\nplug them into the 4 × 4 grid. That’s ROI pooling. It’s simple,\\nfast, and effective. And it works with regions of any size and\\naspect ratio.\\nA 2016 paper titled “Faster R-CNN: Towards Real-Time Object\\nDetection with Region Proposal Networks” further boosted\\nperformance by replacing selective search with a region'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 451, 'file_type': 'pdf'}, page_content='450\\nproposal network, or RPN. The RPN is a shallow CNN that\\nshares layers with the main CNN (Figure\\xa012-5). To generate\\nregion proposals, it slides a window over the feature map\\ngenerated by the last shared layer. At each stop in the\\nwindow’s travel, the RPN evaluates n candidate regions\\ncalled anchors or anchor boxes and computes an objectness\\nscore for each based on IoU scores with ground-truth boxes.\\nObjectness scores and anchor boxes are input to fully\\nconnected layers for classification (does the anchor contain\\nan object, and if so, what type?) and regression. The output\\nfrom these layers ultimately determines the regions of\\ninterest projected onto the feature map generated by the main\\nCNN and forwarded to the ROI pooling layer.\\nFigure 12-5. Faster R-CNN architecture\\nFaster R-CNN can perform 10 times faster than Fast R-CNN and\\ndo object detection in near real time. It’s typically more\\naccurate than Fast R-CNNs too, thanks to the RPN’s superior\\nability to identify candidate regions. Selective search is\\nstatic, but an RPN gets smarter as the network is trained.\\nRPNs also execute in parallel to the main CNN, which means\\nregion proposals have little impact on performance.\\nMask R-CNN\\nThe next chapter in the R-CNN story came in 2017 in a paper\\ntitled “Mask R-CNN”. Mask R-CNNs extend Faster R-CNNs by\\nadding instance segmentation, which identifies the shapes of\\nobjects detected in an image using segmentation masks like'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 452, 'file_type': 'pdf'}, page_content='451\\nthe ones in Figure\\xa012-6. Performance impact is minimal\\nbecause instance segmentation is performed in parallel with\\nregion evaluation. The benefit of Mask R-CNNs is that they\\nprovide more detail about the objects they detect. For\\nexample, you can tell whether a person’s arms are extended—\\nsomething you can’t discern from a simple bounding box. They\\nare also slightly more accurate than Faster R-CNNs because\\nthey replace ROI pooling with ROI alignment, which discards\\nless information when generating feature vectors whose\\nboundaries don’t perfectly align with the boundaries of the\\nregions they represent.\\nFigure 12-6. Bounding boxes and segmentation masks generated by a Mask R-CNN\\nZoom uses instance segmentation to display custom backgrounds\\nbehind you in video feeds. Instance segmentation also powers\\nAI-based image editing tools that crop figures from the\\nforeground and place them on different backgrounds. Let’s\\ndemonstrate using a pretrained Mask R-CNN.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 453, 'file_type': 'pdf'}, page_content='452\\nStart by downloading a ZIP file containing the files needed\\nfor this exercise. The files are:\\nadam.jpg\\nA photo of a young man wearing a backpack\\nmaui.jpg\\nA photo taken poolside in Hawaii\\nmask.py\\nContains helper functions for preprocessing images\\nbefore they’re input to a Mask R-CNN and\\npostprocessing functions for interpreting the results\\nMaskRCNN-12-int8.onnx\\nContains a pretrained Mask R-CNN saved in ONNX format\\nPlace mask.py in the directory where your Jupyter notebooks\\nare hosted, and the remaining files in your notebooks’ Data\\nsubdirectory.\\nThe critical file in this ensemble is MaskRCNN-12-int8.onnx.\\nChapter\\xa07 introduced Open Neural Network Exchange (ONNX) as\\na means for loading Scikit-Learn models written in Python and\\nconsuming them in other languages. But ONNX does more than\\nbridge the language gap to Scikit. It’s a neutral format\\nthat enables neural networks written with one deep-learning\\nframework to be exported to others. For example, you can save\\na PyTorch model to a .onnx file, and then convert the .onnx\\nfile to a TensorFlow model and use it as if it had been\\nwritten with TensorFlow from the outset. Or you can install\\nan ONNX runtime and load and call the model without\\nconverting it to TensorFlow. Chapter\\xa07 demonstrated how to\\nsave a Scikit model in ONNX format and use the Python ONNX\\nruntime to load the model and call it from Python. It also\\nused the .NET ONNX runtime to load the model and call it from'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 454, 'file_type': 'pdf'}, page_content=\"453\\nC#. As noted in Chapter\\xa07, ONNX runtimes are available for a\\nvariety of platforms and programming languages.\\nMaskRCNN-12-int8.onnx contains a sophisticated Mask R-CNN\\nimplementation from Facebook Research. It uses a ResNet50\\nbackbone to extract features from the images input to it, and\\nit includes a branch that computes instance segmentation\\nmasks in parallel with bounding box computations. It was\\ntrained with more than 200,000 images from the COCO dataset.\\nThose images contain more than 1.5 million objects that fall\\ninto 80 categories ranging from cats and dogs to people,\\nbicycles, and cars. And because the trained model was\\npublished in the ONNX model zoo on GitHub, you can load it\\nand pass images to it with just a few lines of code.\\nThe first step in using the model in Python is to make sure\\nthe Python ONNX runtime is installed in your environment. For\\nthis exercise, make sure OpenCV is installed too. Then create\\na Jupyter notebook and run the following code in the first\\ncell to load and display adam.jpg:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nPIL\\nimport\\nImage\\n%matplotlib\\ninline\\n\\nimage\\n=\\nImage.open('Data/adam.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\nHere’s the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 455, 'file_type': 'pdf'}, page_content=\"454\\nThe goal is to run this image through Mask R-CNN, detect the\\nobjects in it, and annotate the image accordingly. To that\\nend, use the following code to preprocess the image pixels\\nthe way the model expects, load Mask R-CNN from the ONNX\\nfile, and submit the preprocessed image to it:\\n\\nfrom\\nmask\\nimport\\n*\\nimport\\nonnxruntime\\nas\\nrt\\n\\nimage_data\\n=\\npreprocess(image)\\nsession\\n=\\nrt.InferenceSession('Data/MaskRCNN-12-int8.onnx')\\ninput_name\\n=\\nsession.get_inputs()[0].name\\nresult\\n=\\nsession.run(None,\\n{\\ninput_name:\\nimage_data\\n})\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 456, 'file_type': 'pdf'}, page_content='455\\nThe value returned by the run method is an array of four\\narrays. The first array contains bounding boxes for the\\nobjects detected in the image. In this example, it detected a\\ntotal of 13 objects:\\n\\narray([[\\n377.50537,\\xa0 174.36874,\\xa0 801.177\\xa0 ,\\xa0 787.7125\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n428.15994,\\xa0 344.70105,\\xa0 699.99097,\\xa0 599.8499\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n757.1859\\n,\\xa0 529.4938\\n,\\xa0 814.2776\\n,\\xa0 658.8041\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n432.57123,\\xa0 351.229\\xa0 ,\\xa0 672.2242\\n,\\xa0 484.63702],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n435.53653,\\xa0 357.34235,\\xa0 494.73557,\\xa0 516.4939\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n672.56714,\\xa0 516.2991\\n,\\xa0 822.82153,\\xa0 663.68726],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n608.1904\\n,\\xa0 361.0429\\n,\\xa0 686.1207\\n,\\xa0 552.70105],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n629.84894,\\xa0 355.3375\\n,\\xa0 799.4963\\n,\\xa0 718.9458\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n437.99078,\\xa0 331.19318,\\xa0 709.4969\\n,\\xa0 748.2785\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n438.44028,\\xa0 348.14404,\\xa0 579.8056\\n,\\xa0 546.0588\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [1152.0518\\n,\\xa0 203.89978,\\n1163.7583\\n,\\xa0 223.2485\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [1151.8087\\n,\\xa0 132.45256,\\n1164.0013\\n,\\xa0 150.6876\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [1151.8087\\n,\\xa0 404.0875\\n,\\n1164.0013\\n,\\xa0 423.058\\xa0 ],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n416.03802,\\xa0 396.8123\\n,\\xa0 694.8975\\n,\\xa0 511.93896],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n683.35236,\\xa0 765.0078\\n,\\xa0 701.3403\\n,\\xa0 784.47546]],\\ndtype=float32)\\nThe second array is an array of integer class identifiers\\ncorresponding to classes in the COCO dataset: 1 for a person,\\n2 for a bicycle, and so on. These identify the objects in the\\nimage:\\n\\narray([\\n1,\\n25,\\n27,\\n25,\\n25,\\n27,\\n25,\\n27,\\n27,\\n25,\\xa0 1,\\xa0 1,\\xa0 1,\\n25,\\n75],\\n\\xa0\\xa0\\xa0\\xa0\\xa0 dtype=int64)\\nThe third array contains confidence scores for the 13\\nobjects. They range from a high of 99.9% for the person in\\nthe image to a low of 5.5% for the logo on his cap, which the'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 457, 'file_type': 'pdf'}, page_content='456\\nmodel thinks might be a clock. Observe that only two objects\\nscored a confidence level of 70% or higher:\\narray([0.99911565,\\n0.8009716\\n,\\n0.4624603\\n,\\n0.19400069,\\n0.17042953,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.11972303,\\n0.11626374,\\n0.10352837,\\n0.08243025,\\n0.08089489,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.07266886,\\n0.07266886,\\n0.07266886,\\n0.06278796,\\n0.0549276\\n],\\n\\xa0\\xa0\\xa0\\xa0\\xa0 dtype=float32)\\nFinally, the fourth array contains segmentation masks for\\neach object. Each mask is a 28 × 28 array of floating-point\\nvalues sometimes referred to as a soft mask due to the soft\\nedges. Figure\\xa012-7 shows the mask for the person detected in\\nthe image. When mapped back to the image’s original size and\\naspect ratio, it does a reasonable job of defining which\\npixels correspond to that person.\\nFigure 12-7. A 28 × 28 soft mask corresponding to the person in the\\npreceding photo\\nThe next step is to grab the bounding boxes, predicted class\\nlabels, confidence scores, and segmentation masks and pass\\nthem to the annotate_image function to visualize the results:\\n\\nboxes\\n=\\nresult[0]\\xa0 # Bounding boxes\\nlabels\\n=\\nresult[1]\\n# Class labels'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 458, 'file_type': 'pdf'}, page_content='457\\nscores\\n=\\nresult[2]\\n# Confidence scores\\nmasks\\n=\\nresult[3]\\xa0 # Segmentation masks\\n\\nannotate_image(image,\\nboxes,\\nlabels,\\nscores,\\nmasks)\\nHere’s the result:\\nBy default, annotate_image ignores objects identified with\\nless than 70% confidence. You can override that by including\\na min_confidence parameter in the call. The model detected\\ntwo objects in the image with a confidence that equaled or\\nexceeded 70%: a person and a backpack. annotate_image draws\\nthe bounding boxes and annotates them with class names and\\nconfidence levels. It also shades the objects by overlaying\\nthem with partially transparent pixels. These are the\\nsegmentation masks returned by run.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 459, 'file_type': 'pdf'}, page_content=\"458\\nannotate_image is one of several helper functions found in\\nmask.py. Another is preprocess, which preps each image in the\\nway Mask R-CNN expects by resizing the image, making sure the\\nwidth and height are multiples of 32, converting the image to\\nBGR format, and normalizing the pixel values. I brought both\\nof these functions over from the model’s GitHub page with\\nsome minor modifications. Since Mask R-CNNs are often used to\\ncrop objects from images, I also added a change_background\\nfunction that extracts all the objects detected in an image\\nwith a specified confidence level (default = 0.7) and\\ncomposites them onto a new background. To see for yourself,\\nrun the following statements in the notebook’s next cell:\\n\\nfg_image\\n=\\nImage.open('Data/adam.jpg')\\nbg_image\\n=\\nImage.open('Data/maui.jpg')\\n\\nchange_background(session,\\nfg_image,\\nbg_image)\\nThe output should look like this:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 460, 'file_type': 'pdf'}, page_content='459\\nchange_background submits the foreground image (the one\\npassed in the function’s second parameter) to the model and\\ncopies all the pixels inside the segmentation masks to the\\nbackground image. It works best when the foreground and\\nbackground images are the same size, but it will resize the\\nbackground image if needed to match the foreground image. The\\nbackground will be distorted if its aspect ratio differs from\\nthat of the foreground image.\\nThe fact that segmentation masks produced by Mask R-CNN\\nmeasure just 28 × 28 explains the “tearing” around the\\npixels copied from the foreground image: the resolution of\\nthe mask is lower than that of the person in the photo. You\\ncould modify Mask R-CNN to use higher-resolution masks, but\\nyou’d have to retrain the model from scratch. That’s a\\ntime-consuming endeavor with a model as complex as this one\\nand a dataset as large as COCO, even if you do the training\\non GPUs.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 461, 'file_type': 'pdf'}, page_content='460\\nYOLO\\nWhile the R-CNN family of object detection systems delivers\\nunparalleled accuracy, it leaves something to be desired when\\nit comes to real-time object detection of the type required\\nby, say, self-driving cars. A paper titled “You Only Look\\nOnce: Unified, Real-Time Object Detection” published in 2015\\nproposed an alternative to R-CNNs known as YOLO (You Only\\nLook Once) that revolutionized the way engineers think about\\nobject detection. From the paper’s introduction:\\nHumans glance at an image and instantly know what objects\\nare in the image, where they are, and how they interact.\\nThe human visual system is fast and accurate, allowing us\\nto perform complex tasks like driving with little\\nconscious thought. Fast, accurate algorithms for object\\ndetection would allow computers to drive cars without\\nspecialized sensors, enable assistive devices to convey\\nreal-time scene information to human users, and unlock\\nthe potential for general purpose, responsive robotic\\nsystems.\\nCurrent detection systems repurpose classifiers to\\nperform detection. To detect an object, these systems\\ntake a classifier for that object and evaluate it at\\nvarious locations and scales in a test image. Systems\\nlike deformable parts models (DPM) use a sliding window\\napproach where the classifier is run at evenly spaced\\nlocations over the entire image.\\nMore recent approaches like R-CNN use region proposal\\nmethods to first generate potential bounding boxes in an\\nimage and then run a classifier on these proposed boxes.\\nAfter classification, post-processing is used to refine\\nthe bounding boxes, eliminate duplicate detections, and\\nrescore the boxes based on other objects in the scene.\\nThese complex pipelines are slow and hard to optimize\\nbecause each individual component must be trained\\nseparately.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 462, 'file_type': 'pdf'}, page_content='461\\nWe reframe object detection as a single regression\\nproblem, straight from image pixels to bounding box\\ncoordinates and class probabilities. Using our system,\\nyou only look once (YOLO) at an image to predict what\\nobjects are present and where they are.\\nYOLO systems are characterized by their performance:\\nOur base network runs at 45 frames per second with no\\nbatch processing on a Titan X GPU and a fast version runs\\nat more than 150 fps. This means we can process streaming\\nvideo in real-time with less than 25 milliseconds of\\nlatency. Furthermore, YOLO achieves more than twice the\\nmean average precision of other real-time systems.\\nAt a high level, YOLO works by dividing feature maps into\\ngrids of cells and evaluating each cell for the presence of\\nan object, as shown in Figure\\xa012-8. Using the anchor-box\\nconcept borrowed from Faster R-CNN, YOLO analyzes bounding\\nboxes of various shapes and sizes around each cell and\\nassigns classes and probabilities to the boxes. One CNN\\nhandles everything, including feature extraction,\\nclassification, and regression designating the sizes and\\nlocations of the bounding boxes, and an image goes through\\nthe CNN just once. At the end, NMS reduces the number of\\nbounding boxes to one per object, and each bounding box is\\nattributed with a class as well as a confidence level—the\\nprobability that the box actually contains an object of the\\nspecified class.\\nFigure 12-8. YOLO’s approach to analyzing an image'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 463, 'file_type': 'pdf'}, page_content='462\\nThere are currently seven versions of YOLO referred to as\\nYOLOv1 through YOLOv7. Each new version improves on the\\nprevious version in terms of accuracy and performance. There\\nare also variations such as PP-YOLO and YOLO9000. YOLOv3 was\\nthe last version that YOLO creator Joseph Redmon contributed\\nto and is considered a reference implementation of sorts. By\\nextracting feature maps from certain layers of the CNN,\\nYOLOv3 analyzes the image using a 13 × 13 grid, a 26 × 26\\ngrid, and a 52 × 52 grid in an effort to detect objects of\\nvarious sizes. It uses anchors to predict nine bounding boxes\\nper cell. YOLO’s primary weakness is that it has difficulty\\ndetecting very small objects that are close together,\\nalthough YOLOv3 improved on YOLOv1 and YOLOv2 in this regard.\\nMore information about YOLO can be found on its creator’s\\nwebsite. A separate article titled “Digging Deep into YOLO\\nV3” offers a deep dive into the YOLOv3 architecture.\\nYOLOv3 and Keras\\nYOLO was originally written using a deep-learning framework\\ncalled Darknet, but it can be implemented with other\\nframeworks as well. The keras-yolo3 project on GitHub\\ncontains a Keras implementation of YOLOv3 that can be trained\\nfrom scratch or initialized with predefined weights and used\\nfor inference—that is, to make predictions. This version\\naccepts 416 × 416 images. To simplify usage, I created a\\nfile named yolov3.py containing helper classes and helper\\nfunctions. It is a modified version of a file that’s\\navailable in the keras-yolo3 project. I removed elements that\\nweren’t needed for making predictions, rewrote some of the\\ncode for improved utility and performance, and added a little\\ncode of my own, but most of the credit goes to Huynh Ngoc\\nAnh, whose GitHub ID is experiencor. With yolov3.py and a set\\nof weights to lend a hand, you can load YOLOv3 and make\\npredictions with just a few lines of code.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 464, 'file_type': 'pdf'}, page_content=\"463\\nTo see YOLOv3 in action, begin by downloading a ZIP file\\ncontaining the files you need. Open the ZIP file and place\\nyolov3.py in the directory with your notebooks. Place the\\nother files in your notebooks’ Data subdirectory. Next,\\ndownload yolov3.weights, which contains the weights arrived\\nat when YOLOv3 was trained on the COCO dataset, and place it\\nin the Data subdirectory as well.\\nNow create a new Jupyter notebook and paste the following\\ncode into the first cell:\\n\\nfrom\\nyolov3\\nimport\\n*\\n\\nmodel\\n=\\nmake_yolov3_model()\\nweight_reader\\n=\\nWeightReader('Data/yolov3.weights')\\nweight_reader.load_weights(model)\\nmodel.summary()\\nRun the code to import the helper classes and functions in\\nyolov3.py, create the model, and initialize it with the COCO\\nweights. Then use the following statements to load a photo of\\na couple biking the city wall surrounding Xian, China:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\nimage\\n=\\nplt.imread('Data/xian.jpg')\\nwidth,\\nheight\\n=\\nimage.shape[1],\\nimage.shape[0]\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 465, 'file_type': 'pdf'}, page_content=\"464\\nNow let’s see if YOLOv3 can detect objects in the image. Use\\nthe following code to load the image again, resize it to 416\\n× 416, preprocess the pixels, and submit the resulting image\\nto the model for prediction:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.preprocessing.image\\nimport\\nload_img,\\nimg_to_array\\n\\nx\\n=\\nload_img('Data/xian.jpg',\\ntarget_size=(YOLO3.width,\\nYOLO3.height))\\nx\\n=\\nimg_to_array(x)\\n/\\n255\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\ny\\n=\\nmodel.predict(x)\\npredict returns arrays containing information about objects\\ndetected in the image at three resolutions (that is, with\\ngrid cells measuring 8, 16, and 32 pixels square), but the\\narrays need to be decoded into bounding boxes and the boxes\\nfiltered with NMS. To help, yolov3.py contains a function\\ncalled decode_predictions (inspired by Keras’s\\ndecode_predictions function) to do the post-processing. decode\\u200b\\n_pre\\u2060dic\\u2060tions requires the width and height of the original\\nimage as input so that it can scale the bounding boxes to\\nmatch the original image dimensions. The return value is a\\nlist of BoundingBox objects, each containing the pixel\\ncoordinates of a box surrounding an object detected in the\\nscene, along with a label identifying the object and a\\nconfidence value from 0.0 to 1.0.\\nThe next step, then, is to pass the predictions returned by\\npredict to decode\\u200b_pre\\u2060dic\\u2060tions and list the bounding boxes:\\n\\nboxes\\n=\\ndecode_predictions(y,\\nwidth,\\nheight)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 466, 'file_type': 'pdf'}, page_content=\"465\\nfor\\nbox\\nin\\nboxes:\\n\\xa0\\xa0\\xa0 print(f'({box.xmin}, {box.ymin}), ({box.xmax}, {box.ymax}), '\\n+\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 f'{box.label}, {box.score}')\\nHere’s the output:\\n\\n(692, 232), (1303, 1490), person, 0.9970048069953918\\n(1314, 327), (1920, 1496), person, 0.9957388639450073\\n(716, 786), (1277, 1634), bicycle, 0.9924144744873047\\n(1210, 845), (2397, 1600), bicycle, 0.9957170486450195\\nThe model detected four objects in the image: two people and\\ntwo bikes. The labels come from the COCO dataset. There are\\n80 in all, and they’re built into the YOLO3 class in\\nyolov3.py. Use the following command to list them and see all\\nthe different types of objects the model can detect:\\n\\nYOLO3.labels\\nyolov3.py also contains a helper function named\\nannotate_image that loads an image from the filesystem and\\ndraws the bounding boxes returned by decode_predictions as\\nwell as labels and confidence values. Use the following\\nstatement to visualize what the model found in the image:\\n\\nannotate_image('Data/xian.jpg',\\nboxes)\\nThe output should look like this:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 467, 'file_type': 'pdf'}, page_content=\"466\\nNow let’s try it with another image—this time, a photo of a\\nyoung woman and the family dog. First show the image and save\\nits width and height:\\n\\nimage\\n=\\nplt.imread('Data/abby-lady.jpg')\\nwidth,\\nheight\\n=\\nimage.shape[1],\\nimage.shape[0]\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\nPreprocess the image and pass it to the model’s predict\\nmethod:\\n\\nx\\n=\\nload_img('Data/abby-lady.jpg',\\ntarget_size=(YOLO3.width,\\nYOLO3.height))\\nx\\n=\\nimg_to_array(x)\\n/\\n255\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 468, 'file_type': 'pdf'}, page_content=\"467\\nx\\n=\\nnp.expand_dims(x,\\naxis=0)\\ny\\n=\\nmodel.predict(x)\\nShow the image again, this time annotated with the objects\\ndetected in it:\\n\\nboxes\\n=\\ndecode_predictions(y,\\nwidth,\\nheight)\\nannotate_image('Data/abby-lady.jpg',\\nboxes)\\nHere is the output:\\nThe model detected the girl and her laptop, but it didn’t\\ndetect the dog even though the COCO training images included\\ndogs. Under the hood, it did detect the dog, but with less\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 469, 'file_type': 'pdf'}, page_content=\"468\\nthan 90% confidence. The model’s predict method typically\\nreturns information about thousands of bounding boxes, most\\nof which can be ignored because the confidence levels are so\\nlow. By default, decode_predictions ignores bounding boxes\\nwith confidence scores less than 0.9, but you can override\\nthat by including a min_score parameter in the call. Use the\\nfollowing statements to decode the predictions and visualize\\nthem again, this time with a minimum confidence level of 55%:\\n\\nboxes\\n=\\ndecode_predictions(y,\\nwidth,\\nheight,\\nmin_score=0.55)\\nannotate_image('Data/abby-lady.jpg',\\nboxes)\\nWith the confidence threshold lowered to 0.55, the model not\\nonly detected the dog, but also the sofa:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 470, 'file_type': 'pdf'}, page_content='469\\nBecause the model was trained on the COCO dataset, it can\\ndetect lots of other objects too, including traffic lights,\\nstop signs, various types of food and animals, and even\\nbottles and wine glasses. Try it with some images of your own\\nto experience state-of-the-art object detection firsthand.\\nCustom Object Detection\\nAn object detection model trained on the COCO dataset can\\nreadily detect and identify 80 different types of objects.\\nBut what if you need an object detection model that\\nidentifies other objects? What if, for example, you need a\\nmodel that detects license plates on cars, packages left on\\nyour front porch, or cancer cells in tissue samples? All of\\nthese are ways in which object detection is employed in\\nindustry today. And none of them rely on models trained\\nsolely with COCO images.\\nTraining an object detection model from scratch is a heavy\\nlift even for researchers at Microsoft, Facebook, and Google.\\nYou can short-circuit the process a bit by starting with the\\nCOCO weights and incrementally training the network with new\\nclasses, but even that is compute and time intensive.\\nThere is an easier way. The premise underlying this entire\\nbook is that you’re a software developer or engineer, not a\\nPhD data scientist. You want to use machine learning (or deep\\nlearning) to add value to your business. You want results,\\nnot equations on a blackboard. You may or may not own a GPU,\\nbut you have a job to do, and in this case, that job involves\\ncustom object detection.\\nThe “easier way” is Azure Cognitive Services—specifically,\\na member of the Cognitive Services family called the Custom\\nVision service. I will formally introduce Azure Cognitive\\nServices in Chapter\\xa014, but if your goal is to build a\\ncustom object detector, now’s the right time for your first\\nforay into the world of AI as a service. In my opinion,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 471, 'file_type': 'pdf'}, page_content='470\\nnothing makes crafting custom object detection models easier\\nthan the Custom Vision service. Little expertise is required,\\nand at the end, you’re left with a trained model that you\\ncan easily consume in Python or C# or other programming\\nlanguages such as Swift.\\nAzure’s Custom Vision service is one of the best-kept\\nsecrets in AI. Let’s use it to train a custom object\\ndetection model—and then don our marine biologist cap and\\nput the model to work.\\nTraining a Custom Object Detection Model with\\nthe Custom Vision Service\\nThe Custom Vision service is one of more than 20 services and\\nAPIs that make up Azure Cognitive Services. Cognitive\\nServices enables you to build intelligence into apps without\\nrequiring deep expertise in machine learning and AI. The\\nCustom Vision service lets you build image classification\\nmodels and object detection models and train them in the\\ncloud on GPUs. When training is complete, you can either\\ndeploy the models as web services and call them using REST\\nAPIs, or download them in various formats, including\\nTensorFlow and Core ML, and consume them locally. Included in\\nthe download are sample source code files demonstrating how\\nto consume the model.\\nTo use the Custom Vision service, you need a Microsoft\\naccount and an Azure subscription. If you don’t have a\\nMicrosoft account, you can create one for free. If you don’t\\nhave an Azure subscription, you can create a free trial\\nsubscription. You have to provide a credit card or debit\\ncard, but you get $200 in free credits to use for 30 days and\\naccess to many free service tiers. Even if you have a paid\\nsubscription or let your free trial roll over into a paid\\nsubscription, you won’t necessarily incur any charges for\\nusing the Custom Vision service—especially if you export the\\nmodels you create.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 472, 'file_type': 'pdf'}, page_content='471\\nNOTE\\nIf you’d prefer not to create an Azure subscription, you can skip\\nthe remainder of this section and pick up again with the next\\nsection, where we use the trained model to detect objects. All the\\nfiles you need in order to utilize the model are present in the\\nChapter 12 folder of this book’s GitHub repo.\\nReady to build a custom object detection model? How about one\\nthat spots sea turtles in the wild? Start by downloading a\\ndataset of sea turtle images. I used Bing image search to\\nfind these photos and limited the search to images that are\\nfree to be shared and used commercially. Copy the images from\\nthe ZIP file to a convenient location on your hard disk.\\nNext, navigate to the Custom Vision portal in your browser\\nand log in with your Microsoft account. Click “+ NEW\\nPROJECT,” and in the “Create new project” dialog, click\\n“create new” to create a new Azure resource for the project\\n(Figure\\xa012-9).\\nFigure 12-9. Creating a new Azure resource in a Custom Vision service\\nproject'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 473, 'file_type': 'pdf'}, page_content='472\\nEnter a name such as “turtle-detector” for the resource and\\nselect your Azure subscription, as shown in Figure\\xa012-10.\\nSelect an existing Azure resource group for the new resource\\nor click “create new” and create a new one. Make sure\\nCognitiveServi\\u2060ces is selected as the resource type and either\\naccept the default Azure location or choose one that’s\\ncloser to you. Finally, select F0 as the pricing tier if\\nit’s available, or S1 if it’s not. (F0 is a free tier and\\nis generally limited to one per Azure subscription.) Then\\nclick the “Create resource” button.\\nFigure 12-10. Creating a new Cognitive Services resource'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 474, 'file_type': 'pdf'}, page_content='473\\nNOTE\\nIn Azure, a resource group is a collection of related resources\\nthat serve a common purpose and share a common lifetime. Deleting\\na resource group deletes all the resources in that group. In\\naddition, you can easily get billing information for the group as\\na whole rather than add up the costs of the individual resources.\\nIn the “Create new project” dialog, enter a name for the\\nproject and select the resource that you just created\\n(Figure\\xa012-11). Select Object Detection as the project type\\nand “General (compact)” as the domain. Selecting one of the\\n“compact” domains is essential if the goal is to train a\\nmodel that can be exported and consumed locally. Make sure\\n“Basic platforms” is selected under Export Capabilities,\\nand then click “Create project.”'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 475, 'file_type': 'pdf'}, page_content='474\\nFigure 12-11. Creating a project containing an object detector'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 476, 'file_type': 'pdf'}, page_content='475\\nNow that the Custom Vision project has been created, the next\\nstep is to upload the images that you’ll use to train the\\nmodel and annotate them with class names and bounding boxes.\\nTo begin, click “Add images.” Then navigate to the folder\\nwhere your sea turtle images are stored and upload all 50.\\nAfter the upload is complete, click the first image to open\\nit in the image detail editor. Use your mouse to draw a box\\naround the sea turtle, as shown in Figure\\xa012-12. Then type\\nsea-turtle into the tag editor that pops up, and click the\\nplus sign to the right of it. Now click the right arrow at\\nthe far right to move to the next image, and repeat this\\nprocess for the remaining training images. You don’t have to\\nenter the tag name again; you simply select it from a list of\\nexisting tags. In addition, you usually don’t have to draw\\nboxes around the sea turtles. Hover your cursor over a sea\\nturtle and the editor will use a little AI of its own to\\nsuggest a bounding box.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 477, 'file_type': 'pdf'}, page_content='476\\nFigure 12-12. Identifying sea turtles in training images\\nNOTE\\nThe most tedious part of training a custom object detection model\\nis identifying objects and bounding boxes in all the training\\nimages. The Custom Vision service helps by providing an\\nintelligent editor to minimize the amount of clicking and dragging\\nrequired. Still, imagine doing this for hundreds of thousands of\\nimages!\\nIn cases in which a photo contains two or more sea turtles,\\nbe sure to tag them all, as shown in Figure\\xa012-13. It’s\\nfine if the bounding boxes overlap. Objects frequently do\\noverlap in the real world, after all. Data scientists refer\\nto overlapping bounding boxes as occlusions.\\nFigure 12-13. Sea turtles with overlapping bounding boxes'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 478, 'file_type': 'pdf'}, page_content='477\\nAfter you’ve tagged all 50 training images, click the green\\nTrain button in the upper-right corner of the page. When\\nasked to choose a training type (Figure\\xa012-14), select Quick\\nTraining to conserve time and money, and then click the Train\\nbutton in the lower-right corner of the dialog to commence\\ntraining. The Advanced Training option usually produces a\\nmore accurate model, but it takes longer and charges you for\\nup to the number of hours you budget. Advanced training might\\nnot be an option if you selected the free F0 tier when you\\ncreated the model. F0 provides one hour of training per month\\nat no charge.\\nFigure 12-14. Choosing a training option\\nQuick training usually takes 5 to 10 minutes. When training\\nis complete, you’ll see a summary like the one in'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 479, 'file_type': 'pdf'}, page_content='478\\nFigure\\xa012-15. You already know what precision and recall\\nare. The third metric, mean average precision (mAP), is a\\ncommon metric for gauging the accuracy of object detection\\nmodels. It reflects the model’s ability to classify objects\\nand identify the objects’ bounding boxes. The higher the\\npercentage, the better. If the number isn’t sufficiently\\nhigh, then you have two choices: train with more images, or\\ntry advanced training rather than quick training (or both).\\nState-of-the-art object detection models are usually trained\\nwith thousands of examples of each class.\\nFigure 12-15. Training results'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 480, 'file_type': 'pdf'}, page_content='479\\nNOTE\\nAs an experiment, I trained the same model using the advanced\\ntraining option and budgeted one hour for training. Training took\\nless than 30 minutes and mAP reached 97.2%. At the time of this\\nwriting, advanced training costs $10 per hour, so the cost to my\\nAzure subscription was less than $5. The Custom Vision pricing\\npage has more information on pricing.\\nYou can test the model’s accuracy by clicking the Quick Test\\nbutton at the top of the page and selecting a photo or two\\nthat the model wasn’t trained with. Or you can click Publish\\nin the upper-left corner of the page and deploy the model as\\na web service. But our goal is to download the model and run\\nit locally. To that end, click Export at the top of the page.\\nChoose TensorFlow as the export type and then click the\\nExport button. Wait for the export to be prepared, and then\\nclick the Download button (Figure\\xa012-16).\\nFigure 12-16. Downloading the trained TensorFlow model'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 481, 'file_type': 'pdf'}, page_content='480\\nWhen the download completes, open the downloaded ZIP file.\\nCopy the following files from the ZIP file to the directory\\nwhere your Jupyter notebooks are hosted:\\nlabels.txt\\nContains the label you created (sea-turtle) when you\\nlabeled the images.\\nmodel.pb\\nContains the trained and serialized TensorFlow model.\\nobject_detection.py\\nFound in the ZIP file’s python folder, this file\\ncontains helper functions for utilizing the model in\\nPython apps.\\nOnce these files are downloaded, delete the project in the\\nCustom Vision portal unless you want to go back and refine\\nthe model by training it with more images (or granting it\\nmore training time). To ensure that no additional charges to\\nyour Azure subscription will be incurred, delete the Azure\\nresource group containing the model. You can delete resource\\ngroups through the Azure portal.\\nNote that when preparing a model with the Custom Vision\\nservice, you aren’t limited to one class of object. You can\\nupload images with dozens of different classes and tag them\\naccordingly in the image detail editor. In that case,\\nlabels.txt will contain all the labels rather than just one.\\nUsing the Exported Model\\nNow you’re ready to put the model to work. Begin by creating\\na new Jupyter notebook and pasting the following code into\\nthe first cell. These statements define a class named\\nSeaTurtleDetector that represents a TensorFlow model and a\\nfunction named annotate_image that annotates an image to show'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 482, 'file_type': 'pdf'}, page_content='481\\nwhat the model detected. The SeaTurtleDetector class is a\\nchild of the ObjectDetection class implemented in\\nobject_detection.py. It has a predict method that you can\\ncall to detect objects in an image:\\n\\nimport\\nsys\\nimport\\ntensorflow\\nas\\ntf\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\nfrom\\nobject_detection\\nimport\\nObjectDetection\\nfrom\\nPIL\\nimport\\nImage\\nimport\\nnumpy\\nas\\nnp\\n\\nclass\\nSeaTurtleDetector(ObjectDetection):\\n\\xa0\\xa0\\xa0 def __init__(self,\\ngraph_def,\\nlabels):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 super(SeaTurtleDetector,\\nself).__init__(labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.graph\\n=\\ntf.compat.v1.Graph()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\nself.graph.as_default():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_data\\n=\\ntf.compat.v1.placeholder(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tf.float32,\\n[1,\\nNone,\\nNone,\\n3],\\nname=\\'Placeholder\\')\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tf.import_graph_def(graph_def,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_map={\"Placeholder:0\":\\ninput_data},\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 name=\"\")\\n\\n\\xa0\\xa0\\xa0 def\\npredict(self,\\npreprocessed_image):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 inputs\\n=\\nnp.array(preprocessed_image,\\ndtype=float)[:,\\n:,\\n(2,\\n1,\\n0)]\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\ntf.compat.v1.Session(graph=self.graph)\\nas\\nsess:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_tensor\\n=\\nsess.graph.get_tensor_by_name(\\'model_outputs:0\\')\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 outputs\\n=\\nsess.run(output_tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\'Placeholder:0\\':\\ninputs[np.newaxis,\\n...]})\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\noutputs[0]\\n\\ndef\\nannotate_image(image,\\npredictions,\\nmin_score=0.7,\\nfigsize=(12,\\n8)):\\n\\xa0\\xa0\\xa0 fig,\\nax\\n=\\nplt.subplots(figsize=figsize,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 483, 'file_type': 'pdf'}, page_content=\"482\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\n\\xa0\\xa0\\xa0 img_width,\\nimg_height\\n=\\nimage.size\\n\\xa0\\xa0\\xa0 ax.imshow(image)\\n\\n\\xa0\\xa0\\xa0 for\\np\\nin\\npredictions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 score\\n=\\np['probability']\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nscore\\n>=\\nmin_score:\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x1\\n=\\np['boundingBox']['left']\\n*\\nimg_width\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y1\\n=\\np['boundingBox']['top']\\n*\\nimg_height\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 width\\n=\\np['boundingBox']['width']\\n*\\nimg_width\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 height\\n=\\np['boundingBox']['height']\\n*\\nimg_height\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 label\\n=\\np['tagName']\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x1,\\ny1),\\nwidth,\\nheight,\\nfill=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 color='red',\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 label\\n= f'{label} ({score:.0%})'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.text(x1\\n+\\n(width\\n/\\n2),\\ny1,\\nlabel,\\ncolor='white',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 backgroundcolor='red',\\nha='center',\\nva='bottom',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 fontweight='bold',\\nbbox=dict(color='red'))\\nNext, use the following statements to load the serialized\\nmodel in model.pb and the label in labels.txt and initialize\\nan object detection model:\\n\\n# Load the serialized model graph\\ngraph_def\\n=\\ntf.compat.v1.GraphDef()\\nwith\\ntf.io.gfile.GFile('model.pb',\\n'rb')\\nas\\nf:\\n\\xa0\\xa0\\xa0 graph_def.ParseFromString(f.read())\\n\\n# Load the labels\\nwith\\nopen('labels.txt',\\n'r')\\nas\\nf:\\n\\xa0\\xa0\\xa0 labels\\n=\\n[l.strip()\\nfor\\nl\\nin\\nf.readlines()]\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 484, 'file_type': 'pdf'}, page_content=\"483\\n# Create the model\\nmodel\\n=\\nSeaTurtleDetector(graph_def,\\nlabels)\\nFinally, go out to the internet and download a sea turtle\\nphoto. Then load the photo into your notebook and see if the\\nmodel detects the sea turtle in it:\\n\\n%matplotlib\\ninline\\n\\nimage\\n=\\nImage.open('PATH_TO_IMAGE_FILE')\\npredictions\\n=\\nmodel.predict_image(image)\\nannotate_image(image,\\npredictions)\\nDid the model get it right? It did for me:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 485, 'file_type': 'pdf'}, page_content='484\\nKeep in mind that state-of-the-art accuracy in object\\ndetection is a product of training with lots of labeled\\nimages. The 50 you trained with here are a good start, but to\\ntrain a model to a sufficient level of accuracy for real-\\nworld use, you’ll probably need a minimum of 10 times more.\\nThe good news is that once you’ve collected the images, the\\nCustom Vision service makes the process of labeling the\\nimages and training the model relatively easy.\\nSummary\\nObject detection represents the tip of the spear in the world\\nof computer vision. It’s how self-driving cars “see”\\nobjects in front of them, and it’s the technology underlying\\nnumerous real-world applications. State-of-the-art object\\ndetection is accomplished with deep learning using specially\\ncrafted CNNs that do more than mere image classification. You\\ncan use pretrained models such as YOLO and Mask R-CNN to\\ndetect objects in images and video frames. With Mask R-CNN,\\nyou can even generate segmentation masks revealing additional\\ninformation about those objects.\\nCustom object detection requires you to train models like\\nthese with images of your own carefully labeled to denote\\nobjects and bounding boxes. Azure’s Custom Vision service\\nsimplifies the process of training custom object detection\\nmodels, and allows the models you train to be hosted in the\\ncloud and called using REST APIs or downloaded and consumed\\nlocally. It’s one of several services that comprise Azure\\nCognitive Services, and it’s the right tool for the job when\\nthe job calls for detecting objects that pretrained models\\nweren’t trained to detect.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 486, 'file_type': 'pdf'}, page_content='485\\nChapter 13. Natural Language\\nProcessing\\nIt’s not difficult to use Scikit-Learn to build machine\\nlearning models that analyze text for sentiment, identify\\nspam, and classify text in other ways. But today, state-of-\\nthe-art text classification is most often performed with\\nneural networks. You already know how to build neural\\nnetworks that accept numbers and images as input. Let’s\\nbuild on that to learn how to construct deep-learning models\\nthat process text—a segment of deep learning known as\\nnatural language processing, or NLP for short.\\nNLP encompasses a variety of activities including text\\nclassification, named-entity recognition, keyword extraction,\\nquestion answering, and language translation. The accuracy of\\nNLP models has improved in recent years for a variety of\\nreasons, not the least of which are newer and better ways of\\nconverting words and sentences into dense vector\\nrepresentations that incorporate meaning, and a relatively\\nnew neural network architecture called the transformer that\\ncan zero in on the most meaningful words and even\\ndifferentiate between different meanings of the same word.\\nOne element that virtually all neural networks that process\\ntext have in common is an embedding layer, which uses word\\nembeddings to transform arrays, or sequences, of scalar\\nvalues representing words into arrays of floating-point\\nnumbers called word vectors. These vectors encode information\\nabout the meanings of words and the relationships between\\nthem. Output from an embedding layer can be input to a\\nclassification layer, or it can be input to other types of\\nneural network layers to tease more meaning from it before\\nsubjecting it to further processing.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 487, 'file_type': 'pdf'}, page_content=\"486\\nTransformers? Embedding layers? Word vectors? There’s a lot\\nto unpack here, but once you wrap your head around a few\\nbasic concepts, neural networks that process language are\\npure magic. Let’s dive in. We’ll start by learning how to\\nprepare text for processing by a deep-learning model and how\\nto create word embeddings. Then we’ll put that knowledge to\\nwork building neural networks that classify text, translate\\ntext from one language to another, and more—all classic\\napplications of NLP.\\nText Preparation\\nChapter\\xa04 introduced Scikit-Learn’s CountVectorizer class,\\nwhich converts rows of text into rows of word counts that a\\nmachine learning model can consume. Count\\u200bVec\\u2060torizer also\\nconverts characters to lowercase, removes numbers and\\npunctuation symbols, and optionally removes stop words—\\ncommon words such as and and the that are likely to have\\nlittle influence on the outcome.\\nText must be cleaned and vectorized before it’s used to\\ntrain a neural network too, but vectorization is typically\\nperformed differently. Rather than create a table of word\\ncounts, you create a table of sequences containing tokens\\nrepresenting individual words. Tokens are often indices into\\na dictionary, or vocabulary, built from the corpus of words\\nin the dataset. To help, Keras provides the Tokenizer class,\\nwhich you can think of as the deep-learning equivalent of\\nCount\\u200bVec\\u2060torizer. Here’s an example that uses Tokenizer to\\ncreate sequences from four lines of text:\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\n\\nlines\\n=\\n[\\n\\xa0\\xa0\\xa0 'The quick brown fox',\\n\\xa0\\xa0\\xa0 'Jumps over $$$ the lazy brown dog',\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 488, 'file_type': 'pdf'}, page_content=\"487\\n\\xa0\\xa0\\xa0 'Who jumps high into the blue sky after counting 123',\\n\\xa0\\xa0\\xa0 'And quickly returns to earth'\\n]\\n\\ntokenizer\\n=\\nTokenizer()\\ntokenizer.fit_on_texts(lines)\\nsequences\\n=\\ntokenizer.texts_to_sequences(lines)\\nfit_on_texts creates a dictionary containing all the words in\\nthe input text. texts_to_sequences returns a list of\\nsequences, which are simply arrays of indices into the\\ndictionary:\\n\\n[[1, 4, 2, 5],\\n[3, 6, 1, 7, 2, 8],\\n[9, 3, 10, 11, 1, 12, 13, 14, 15, 16],\\n[17, 18, 19, 20, 21]]\\nThe word brown appears in lines 1 and 2 and is represented by\\nthe index 2. Therefore, 2 appears in both sequences.\\nSimilarly, a 3 representing the word jumps appears in\\nsequences 2 and 3. The index 0 isn’t used to denote words;\\nit’s reserved to serve as padding. More on this in a moment.\\nYou can use Tokenizer’s sequences_to_texts method to reverse\\nthe process and convert the sequences back into text:\\n\\n['the quick brown fox',\\n'jumps over the lazy brown dog',\\n'who jumps high into the blue sky after counting 123',\\n'and quickly returns to earth']\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 489, 'file_type': 'pdf'}, page_content=\"488\\nOne revelation that comes from this is that Tokenizer\\nconverts text to lowercase and removes symbols, but it\\ndoesn’t remove stop words or numbers. If you want to remove\\nstop words, you can use a separate library such as the\\nNatural Language Toolkit (NLTK). You can also remove words\\ncontaining numbers while you’re at it:\\n\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\nfrom\\nnltk.tokenize\\nimport\\nword_tokenize\\nfrom\\nnltk.corpus\\nimport\\nstopwords\\n\\nlines\\n=\\n[\\n\\xa0\\xa0\\xa0 'The quick brown fox',\\n\\xa0\\xa0\\xa0 'Jumps over $$$ the lazy brown dog',\\n\\xa0\\xa0\\xa0 'Who jumps high into the blue sky after counting 123',\\n\\xa0\\xa0\\xa0 'And quickly returns to earth'\\n]\\n\\ndef\\nremove_stop_words(text):\\n\\xa0\\xa0\\xa0 text\\n=\\nword_tokenize(text.lower())\\n\\xa0\\xa0\\xa0 stop_words\\n=\\nset(stopwords.words('english'))\\n\\xa0\\xa0\\xa0 text\\n=\\n[word\\nfor\\nword\\nin\\ntext\\nif\\nword.isalpha()\\nand\\nnot\\nword\\nin\\nstop_words]\\n\\xa0\\xa0\\xa0 return\\n' '.join(text)\\n\\nlines\\n=\\nlist(map(remove_stop_words,\\nlines))\\n\\ntokenizer\\n=\\nTokenizer()\\ntokenizer.fit_on_texts(lines)\\ntokenizer.texts_to_sequences(lines)\\nThe resulting sequences look like this:\\n\\n[[3, 1, 4],\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 490, 'file_type': 'pdf'}, page_content=\"489\\n[2, 5, 1, 6],\\n[2, 7, 8, 9, 10],\\n[11, 12, 13]]\\nwhich, converted back to text, are as follows:\\n\\n['quick brown fox',\\n'jumps lazy brown dog',\\n'jumps high blue sky counting',\\n'quickly returns earth']\\nThe sequences range from three to five values in length, but\\na neural network expects all sequences to be the same length.\\nKeras’s pad_sequences function performs this final step,\\ntruncating sequences longer than the specified length and\\npadding sequences shorter than the specified length with 0s:\\n\\nfrom\\ntensorflow.keras.preprocessing.sequence\\nimport\\npad_sequences\\n\\npadded_sequences\\n=\\npad_sequences(sequences,\\nmaxlen=4)\\nThe resulting padded sequences look like this:\\n\\narray([[\\n0,\\xa0 3,\\xa0 1,\\xa0 4],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n2,\\xa0 5,\\xa0 1,\\xa0 6],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n7,\\xa0 8,\\xa0 9,\\n10],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [\\n0,\\n11,\\n12,\\n13]])\\nConverting these sequences back to text yields this:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 491, 'file_type': 'pdf'}, page_content='490\\n\\n[\\'quick brown fox\\',\\n\\'jumps lazy brown dog\\',\\n\\'high blue sky counting\\',\\n\\'quickly returns earth\\']\\nBy default, pad_sequences pads and truncates on the left, but\\nyou can include a padding=\\'post\\' parameter if you prefer to\\npad and truncate on the right. Padding on the right is\\nsometimes important when using neural networks to translate\\ntext.\\nNOTE\\nRemoving stop words frequently has little or no effect on text\\nclassification tasks. If you simply want to remove numbers from\\ntext input to a neural network without removing stop words, create\\na Tokenizer this way:\\n\\ntokenizer\\n=\\nTokenizer(\\n\\xa0\\xa0\\xa0 filters=\\'!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_` \\' \\\\\\n\\xa0\\xa0\\xa0 \\'{|}~\\\\t\\\\n0123456789\\')\\nThe filters parameter tells Tokenizer what characters to remove. It\\ndefaults to \\'!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_`{|}~\\\\t\\\\n\\'. This code simply\\nadds 0–9 to the list. Note that Tokenizer does not remove\\napostrophes by default, but you can remove them by adding an\\napostrophe to the filters list.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 492, 'file_type': 'pdf'}, page_content='491\\nIt’s important that text input to a neural network for\\npredictions be tokenized and padded in the same way as text\\ninput to the model for training. If you’re thinking it sure\\nwould be nice not to have to do the tokenization and\\nsequencing manually, there is a way around it. Rather than\\nwrite a lot of code, you can include a TextVectorization\\nlayer in the model. I’ll demonstrate how momentarily. But\\nfirst, you need to learn about embedding layers.\\nWord Embeddings\\nOnce text is tokenized and converted into padded sequences,\\nit is ready for training a neural network. But you probably\\nwon’t get very far training on the raw padded sequences.\\nOne of the crucial elements of a neural network that\\nprocesses text is an embedding layer whose job is to convert\\npadded sequences of word tokens into arrays of word vectors,\\nwhich represent each word with an array (vector) of floating-\\npoint numbers rather than a single integer. Each word in the\\ninput text is represented by a vector in the embedding layer,\\nand as the network is trained, vectors representing\\nindividual words are adjusted to reflect their relationship\\nto one another. If you’re building a sentiment analysis\\nmodel and words such as excellent and amazing have similar\\nconnotations, then the vectors representing those words in\\nthe embedding space should be relatively close together so\\nthat phrases such as “excellent service” and “amazing\\nservice” score similarly.\\nImplementing an embedding layer by hand is a complex\\nundertaking (especially the training aspect), so Keras offers\\nthe Embedding class. With Keras, creating a trainable\\nembedding layer requires just one line of code:\\n\\nEmbedding(input_dim=10000,\\noutput_dim=32,\\ninput_length=100)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 493, 'file_type': 'pdf'}, page_content='492\\nIn order, the three parameters passed to the Embedding\\nfunction are:\\n\\xa0\\nThe vocabulary size, or the number of words in\\nthe vocabulary built by Tokenizer\\nThe number of dimensions m in the embedding space\\nThe length n of each padded sequence\\nYou pick the number of dimensions m, and each word gets\\nencoded in the embedding space as an m-dimensional vector.\\nMore dimensions provide more fitting power, but also increase\\ntraining time. In practice, m is usually a number from 32 to\\n512.\\nThe vectors that represent individual words in an embedding\\nlayer are learned during training, just as the weights\\nconnecting neurons in adjacent dense layers are learned. If\\nthe number of training samples is sufficiently high, training\\nthe network usually creates effective vector representations\\nof all the words. However, if you have only a few hundred\\ntraining samples, the embedding layer might not have enough\\ninformation to properly vectorize the corpus of text.\\nIn that case, you can elect to initialize the embedding layer\\nwith pretrained word embeddings rather than rely on it to\\nlearn the word embeddings on its own. Several popular\\npretrained word embeddings exist in the public domain,\\nincluding the GloVe word vectors developed by Stanford and\\nGoogle’s own Word2Vec. Pretrained embeddings tend to model\\nsemantic relationships between words, recognizing, for\\nexample, that king and queen are related terms while stairs\\nand zebra are not. While that can be beneficial, a network\\ntrained to classify text usually performs better when word\\nembeddings are learned from the training data because such\\nembeddings are task specific. For an example showing how to\\nuse pretrained embeddings, see “Using Pre-trained Word\\nEmbeddings” by the author of Keras.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 494, 'file_type': 'pdf'}, page_content='493\\nText Classification\\nFigure\\xa013-1 shows the baseline architecture for a neural\\nnetwork that classifies text. Tokenized text sequences are\\ninput to the embedding layer. The output from the embedding\\nlayer is a 2D matrix of floating-point values measuring m by\\nn, where m is the number of dimensions in the embedding space\\nand n is the sequence length. The Flatten layer following the\\nembedding layer “flattens” the 2D output into a 1D array\\nsuitable for input to a dense layer, and the dense layer\\nclassifies the values emitted from the flatten layer. You can\\nexperiment with different dimensions in the embedding layer\\nand different widths of the dense layer to maximize accuracy.\\nYou can also add more dense layers if needed.\\nFigure 13-1. Neural network for classifying text\\nA neural network like the one in Figure\\xa013-1 can be\\nimplemented this way:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\\n\\nmodel\\n=\\nSequential()\\n\\nmodel.add(Embedding(10000,\\n32,\\ninput_length=100))\\n\\nmodel.add(Flatten())'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 495, 'file_type': 'pdf'}, page_content=\"494\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(optimizer='adam',\\nloss='binary_crossentropy',\\nmetrics=\\n['accuracy'])\\nOne application for a neural network that classifies text is\\nspam filtering. Chapter\\xa04 demonstrated how to use Scikit to\\nbuild a machine learning model that separates spam from\\nlegitimate emails. Let’s build an equivalent deep-learning\\nmodel with Keras and TensorFlow. We’ll use the same dataset\\nwe used before: one containing 1,000 emails, half of which\\nare spam (indicated by 1s in the label column) and half of\\nwhich are not (indicated by 0s in the label column).\\nBegin by downloading the dataset and copying it into the Data\\nsubdirectory where your Jupyter notebooks are hosted. Then\\nuse the following statements to load the dataset and shuffle\\nthe rows to distribute positive and negative samples\\nthroughout. Shuffling is important because rather than use\\ntrain_test_split to create a validation dataset, we’ll use\\nfit’s validation_split parameter. It doesn’t shuffle the\\ndata as train_test_split does:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/ham-spam.csv')\\ndf\\n=\\ndf.sample(frac=1,\\nrandom_state=0)\\ndf.head()\\nUse the following statements to remove any duplicate rows\\nfrom the dataset and check for balance:\\n\\ndf\\n=\\ndf.drop_duplicates()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 496, 'file_type': 'pdf'}, page_content=\"495\\ndf.groupby('IsSpam').describe()\\nNext, extract the emails from the DataFrame’s Text column\\nand labels from the IsSpam column. Then use Keras’s\\nTokenizer class to tokenize the text and convert it into\\nsequences, and pad_sequences to produce sequences of equal\\nlength. There’s no need to remove stop words because doing\\nso doesn’t impact the outcome:\\n\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\nfrom\\ntensorflow.keras.preprocessing.sequence\\nimport\\npad_sequences\\n\\nx\\n=\\ndf['Text']\\ny\\n=\\ndf['IsSpam']\\n\\nmax_words\\n=\\n10000\\n# Limit the vocabulary to the 10,000 most common words\\nmax_length\\n=\\n500\\n\\ntokenizer\\n=\\nTokenizer(num_words=max_words)\\ntokenizer.fit_on_texts(x)\\nsequences\\n=\\ntokenizer.texts_to_sequences(x)\\nx\\n=\\npad_sequences(sequences,\\nmaxlen=max_length)\\nDefine a binary classification model that contains an\\nembedding layer with 32 dimensions, a flatten layer to\\nflatten output from the embedding layer, a dense layer for\\nclassification, and an output layer with a single neuron and\\nsigmoid activation:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 497, 'file_type': 'pdf'}, page_content=\"496\\nmodel\\n=\\nSequential()\\n\\nmodel.add(Embedding(max_words,\\n32,\\ninput_length=max_length))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\nmetrics=\\n['accuracy'])\\n\\nmodel.summary()\\nTrain the network and allow Keras to use 20% of the training\\nsamples for validation:\\n\\nhist\\n=\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=5,\\nbatch_size=20)\\nUse the history object returned by fit to plot the training\\nand validation accuracy in each epoch:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\n':',\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 498, 'file_type': 'pdf'}, page_content=\"497\\nplt.legend(loc='lower right')\\nplt.plot()\\nHopefully, the network achieved a validation accuracy\\nexceeding 95%. If it didn’t, train it again. Here’s how it\\nturned out for me:\\nOnce you’re satisfied with the accuracy, use the following\\nstatements to compute the probability that an email regarding\\na code review is spam:\\n\\ntext\\n=\\n'Can you attend a code review on Tuesday? ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Need to make sure the logic is rock solid.'\\n\\nsequence\\n=\\ntokenizer.texts_to_sequences([text])\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 499, 'file_type': 'pdf'}, page_content=\"498\\npadded_sequence\\n=\\npad_sequences(sequence,\\nmaxlen=max_length)\\nmodel.predict(padded_sequence)[0][0]\\nThen do the same for another email:\\n\\ntext\\n=\\n'Why pay more for expensive meds when ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'you can order them online and save $$$?'\\n\\nsequence\\n=\\ntokenizer.texts_to_sequences([text])\\npadded_sequence\\n=\\npad_sequences(sequence,\\nmaxlen=max_length)\\nmodel.predict(padded_sequence)[0][0]\\nWhat did the network predict for the first email? What about\\nthe second? Do you agree with the predictions? Remember that\\na number close to 0.0 indicates that the email is not spam,\\nwhile a number close to 1.0 indicates that it is.\\nAutomating Text Vectorization\\nRather than run Tokenizer and pad_sequences manually, you can\\npreface an embedding layer with a TextVectorization layer.\\nHere’s an example:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nimport\\ntensorflow\\nas\\ntf\\n\\nmodel\\n=\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 500, 'file_type': 'pdf'}, page_content=\"499\\nmodel.add(Embedding(max_words,\\n32,\\ninput_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary()\\nNote that the input layer (an instance of InputLayer) is now\\nexplicitly defined, and it’s configured to accept string\\ninput. In addition, before training the model, the\\nTextVectorization layer must be fit to the input data by\\ncalling adapt:\\n\\nmodel.layers[0].adapt(x)\\nNow you no longer need to preprocess the training text, and\\nyou can pass raw text strings to predict:\\n\\ntext\\n=\\n'Why pay more for expensive meds when ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'you can order them online and save $$$?'\\nmodel.predict([text])[0][0]\\nTextVectorization doesn’t remove stop words, so if you want\\nthem removed, you can do that separately or use the\\nTextVectorization function’s standardize parameter to\\nidentify a callback function that does it for you.\\nUsing TextVectorization in a Sentiment\\nAnalysis Model\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 501, 'file_type': 'pdf'}, page_content='500\\nTo demonstrate how TextVectorization layers simplify text\\nprocessing, let’s use it to build a binary classifier that\\nperforms sentiment analysis. We’ll use the same dataset we\\nused in Chapter\\xa04: the IMDB reviews dataset containing\\n25,000 positive reviews and 25,000 negative reviews.\\nDownload the dataset if you haven’t already and place it in\\nyour Jupyter notebooks’ Data subdirectory. Then create a new\\nnotebook and use the following statements to load and shuffle\\nthe dataset:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv(\\'Data/reviews.csv\\',\\nencoding=\"ISO-8859-1\")\\ndf\\n=\\ndf.sample(frac=1,\\nrandom_state=0)\\ndf.head()\\nRemove duplicate rows and check for balance:\\n\\ndf\\n=\\ndf.drop_duplicates()\\ndf.groupby(\\'Sentiment\\').describe()\\nNow create the model and include a TextVectorization layer to\\npreprocess input text:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\\nimport\\ntensorflow\\nas\\ntf\\n\\nmax_words\\n=\\n20000'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 502, 'file_type': 'pdf'}, page_content=\"501\\nmax_length\\n=\\n500\\n\\nmodel\\n=\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\\nmodel.add(Embedding(max_words,\\n32,\\ninput_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary()\\nExtract the reviews from the DataFrame’s Text column and the\\nlabels (0 for negative sentiment, 1 for positive) from the\\nSentiment column, and use the former to fit the\\nTextVectorization layer to the text. Then train the model:\\n\\nx\\n=\\ndf['Text']\\ny\\n=\\ndf['Sentiment']\\nmodel.layers[0].adapt(x)\\n\\nhist\\n=\\nmodel.fit(x,\\ny,\\nvalidation_split=0.5,\\nepochs=5,\\nbatch_size=250)\\nWhen training is complete, plot the training and validation\\naccuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 503, 'file_type': 'pdf'}, page_content=\"502\\n\\nacc\\n=\\nhist.history['accuracy']\\nval\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\n':',\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe model fits to the training text extremely well, but\\nvalidation accuracy usually peaks between 85% and 90%:\\nUse the model to score a positive comment for sentiment:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 504, 'file_type': 'pdf'}, page_content=\"503\\n\\ntext\\n=\\n'Excellent food and fantastic service!'\\nmodel.predict([text])[0][0]\\nNow do the same for a negative comment:\\n\\ntext\\n=\\n'The long lines and poor customer service really turned me off.'\\nmodel.predict([text])[0][0]\\nObserve how much simpler the code is. Operationalizing the\\nmodel is simpler too, because you no longer need a Tokenizer\\nfit to the training data to prepare text submitted to the\\nmodel for predictions. Be aware, however, that a model with a\\nText\\u200bVec\\u2060torization layer can’t be saved in Keras’s H5\\nformat. It can be saved in TensorFlow’s SavedModel format.\\nThe following statement saves the model in the saved_model\\nsubdirectory of the current directory:\\n\\nmodel.save('saved_model')\\nOnce the model is reloaded, you can pass text directly to it\\nfor making predictions.\\nFactoring Word Order into Predictions\\nBoth of the models you just built are bag-of-words models\\nthat ignore word order. Such models are common and are often\\nmore accurate than other types of models. But that’s not\\nalways the case. The relative position of the words in a\\nsentence sometimes has meaning. Credit and card should\\nprobably influence a spam classifier one way if they appear\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 505, 'file_type': 'pdf'}, page_content='504\\nfar apart in a sentence and another way if they appear\\ntogether.\\nOne way to improve—or at least attempt to improve—on a\\nsimple bag-of-words model is to use n-grams as described in\\nChapter\\xa04. An n-gram is a collection of n words appearing in\\nconsecutive order. Keras’s TextVectorization class features\\nan ngrams parameter that makes applying n-grams easy. The\\nfollowing statement creates a TextVectorization layer that\\nconsiders word pairs as well as individual words:\\n\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ngrams=2))\\nOne limitation of n-grams is that they only consider words\\nthat are directly adjacent to each other. A slightly more\\nrobust way to factor word position into a classification task\\nis to replace dense layers with Conv1D and MaxPooling1D\\nlayers, turning the network into a convolutional neural\\nnetwork. CNNs are most often used to classify images, but\\none-dimensional convolution layers play well with text\\nsequences. Here’s the network presented in the previous\\nexample recast as a CNN:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv1D,\\nMaxPooling1D,\\nGlobalMaxPooling1D\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nEmbedding\\nimport\\ntensorflow\\nas\\ntf\\n\\nmodel\\n=\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 506, 'file_type': 'pdf'}, page_content=\"505\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\\nmodel.add(Embedding(max_words,\\n32,\\ninput_length=max_length))\\nmodel.add(Conv1D(32,\\n7,\\nactivation='relu'))\\nmodel.add(MaxPooling1D(5))\\nmodel.add(Conv1D(32,\\n7,\\nactivation='relu'))\\nmodel.add(GlobalMaxPooling1D())\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nRather than process individual words, the Conv1D layers in\\nthis example extract features from groups of vectors\\nrepresenting words (seven in the first layer and seven more\\nin the second), just as Conv2D layers extract features from\\nblocks of pixels. The MaxPooling1D layer condenses the output\\nfrom the first Conv1D layer to reveal higher-level structure\\nin input sequences, similar to the way reducing the\\nresolution of an image tends to draw out macro features such\\nas the shape of a person’s body while minimizing or\\nfiltering out altogether lesser features such as the shape of\\na person’s eyes. A simple CNN like this one sometimes\\nclassifies text more accurately than bag-of-words models, and\\nsometimes does not. As is so often the case in machine\\nlearning, the only way to know is to try.\\nRecurrent Neural Networks (RNNs)\\nYet another way to factor word position into a classifier is\\nto include recurrent layers in the network. Recurrent layers\\nwere originally invented to process time-series data—for\\nexample, to look at weather data for the past five days and\\npredict what tomorrow’s high temperature will be. If you\\nsimply took all the weather data for those five days and\\ntreated each day independently, trends evident in the data\\nwould be lost. A recurrent layer, however, might detect those\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 507, 'file_type': 'pdf'}, page_content='506\\ntrends and factor them into its output. A sequence of vectors\\noutput by an embedding layer qualifies as a time series\\nbecause words in a phrase are ordered consecutively and words\\nused early in a phrase could inform how words that occur\\nlater are interpreted.\\nFigure\\xa013-2 illustrates how a recurrent layer transforms\\nword embeddings into a vector that’s influenced by word\\norder. In this example, sequences are input to an embedding\\nlayer, which transforms each word (token) in the sequence\\ninto a vector of floating-point numbers. These word\\nembeddings are input to a recurrent layer, yielding another\\noutput vector. To compute that vector, cells in the recurrent\\nlayer loop over the embeddings comprising the sequence. The\\ninput to iteration n + 1 of the loop is the current embedding\\nvector and the output from iteration n—the so-called hidden\\nstate. The output from the recurrent layer is the output from\\nthe final iteration of the loop. The result is different than\\nit would have been had each embedding vector been processed\\nindependently because each iteration uses information from\\nthe previous iteration to compute an output. Context from a\\nword early in a sequence can carry over to words that occur\\nlater on.\\nFigure 13-2. Processing word embeddings with a recurrent layer'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 508, 'file_type': 'pdf'}, page_content='507\\nIt’s not difficult to build a very simple recurrent layer by\\nhand that works reasonably well with short sequences (say,\\nfour or five words), but longer sequences will suffer from a\\nvanishing-gradient effect that means words far apart will\\nexert little influence over one another. One solution is a\\nrecurrent layer composed of Long Short-Term Memory (LSTM)\\ncells, which were introduced in a 1997 paper titled,\\nappropriately enough, “Long Short-Term Memory”.\\nLSTM cells are miniature neural networks in their own right.\\nAs the cells loop over the words in a sequence, they learn\\n(just as the weights connecting neurons in dense layers are\\nlearned) which words are important and lend that information\\nprecedence in subsequent iterations through the loop. A dense\\nlayer doesn’t recognize that there’s a connection between\\nblue and sky in the phrase “I like blue, for on a clear and\\nsunny day, it is the color of the sky.” An LSTM layer does\\nand can factor that into its output. Its power diminishes,\\nhowever, as words grow farther apart.\\nKeras provides a handy implementation of LSTM layers in its\\nLSTM class. It also provides a GRU class implementing gated\\nrecurrent unit (GRU) layers, which are simplified LSTM layers\\nthat train faster and often yield results that equal or\\nexceed those of LSTM layers. Here’s how our spam classifier\\nwould look if it were modified to use LSTM:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nEmbedding,\\nLSTM\\nimport\\ntensorflow\\nas\\ntf\\n\\nmodel\\n=\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\\nmodel.add(Embedding(max_words,\\n32,\\ninput_length=max_length))'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 509, 'file_type': 'pdf'}, page_content=\"508\\nmodel.add(LSTM(32))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nLSTMs are compute intensive, especially when dealing with\\nlong sequences. If you try this code, you’ll find that the\\nmodel takes much longer to train. You’ll also find that\\nvalidation accuracy improves little if at all. This goes back\\nto the fact that bag-of-words models, while simple, also tend\\nto be effective at classifying text.\\nLSTM layers can be stacked, just like dense layers. The trick\\nis to include a return_sequences=True attribute in all LSTM\\nlayers except the last so that the previous layers return all\\nthe vectors generated (all the hidden state) rather than just\\nthe final vector. Google Translate once used two stacks of\\nLSTMs eight layers deep to encode phrases in one language and\\ndecode them into another.\\nUsing Pretrained Models to Classify Text\\nIf training your own sentiment analysis model doesn’t yield\\nthe accuracy you require, you can always turn to a pretrained\\nmodel. Just as there are pretrained computer-vision models\\ntrained with millions of labeled images, there are pretrained\\nsentiment analysis models available that were trained with\\nmillions of labeled text samples. Keras doesn’t provide a\\nconvenient wrapper around these models as it does for\\npretrained CNNs, but they are relatively easy to consume\\nnonetheless.\\nMany such models are available from Hugging Face, an AI-\\nfocused company whose goal is to advance and democratize AI.\\nHugging Face originally concentrated on NLP models but has\\nsince expanded its library to include other types of models,\\nincluding image classification models and object detection\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 510, 'file_type': 'pdf'}, page_content=\"509\\nmodels. Currently, Hugging Face hosts more than 400 sentiment\\nanalysis models trained on different types of input ranging\\nfrom tweets to product reviews in a variety of languages. It\\neven offers models that analyze text for joy, anger,\\nsurprise, and other emotions.\\nAll of these models are free for you to use. Care to give it\\na try? Start by installing Hugging Face’s Transformers\\npackage in your Python environment (for example, pip install\\ntransformers). Then fire up a Jupyter notebook and use the\\nfollowing code to load Hugging Face’s default sentiment\\nanalysis model:\\n\\nfrom\\ntransformers\\nimport\\npipeline\\n\\nmodel\\n=\\npipeline('sentiment-analysis')\\nYou’ll incur a short delay while the model is downloaded for\\nthe first time. Once the download completes, score a sentence\\nfor sentiment:\\n\\nmodel('The long lines and poor customer service really turned me off')\\nHere’s the result:\\n\\n[{'label': 'NEGATIVE', 'score': 0.9995430707931519}]\\nTry it with a positive comment:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 511, 'file_type': 'pdf'}, page_content=\"510\\n\\nmodel('Great food and excellent service!')\\nHere’s the result:\\n\\n[{'label': 'POSITIVE', 'score': 0.9998843669891357}]\\nIn the return value, label indicates whether the sentiment is\\npositive or negative, and score reveals the model’s\\nconfidence in the label.\\nIt’s just as easy to analyze a text string for emotion by\\nloading a different pretrained model. To demonstrate, try\\nthis:\\n\\nmodel\\n=\\npipeline('text-classification',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 model='bhadresh-savani/distilbert-base-uncased-emotion',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return_all_scores=True)\\n\\nmodel('The long lines and poor customer service really turned me off')\\nHere’s what it returned for me:\\n\\n[[{'label': 'sadness', 'score': 0.10837080329656601},\\n\\xa0 {'label': 'joy', 'score': 0.002373947761952877},\\n\\xa0 {'label': 'love', 'score': 0.0006029471987858415},\\n\\xa0 {'label': 'anger', 'score': 0.8861245512962341},\\n\\xa0 {'label': 'fear', 'score': 0.0019340706057846546},\\n\\xa0 {'label': 'surprise', 'score': 0.0005936296074651182}]]\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 512, 'file_type': 'pdf'}, page_content='511\\nIt doesn’t get much easier than that! And as you’ll see in\\na moment, pretrained Hugging Face models lend themselves to\\nmuch more than just text classification.\\nNeural Machine Translation\\nIn the universe of natural language processing, text\\nclassification is a relatively simple task. At the opposite\\nend of the spectrum lies neural machine translation (NMT),\\nwhich uses deep learning to translate text from one language\\nto another. NMT has proven superior to the rules-based\\nmachine translation (RBMT) and statistical machine\\ntranslation (SMT) systems that predated the explosion of deep\\nlearning and today is the basis for virtually all state-of-\\nthe-art text translation services.\\nThe gist of text classification is that you transform an\\ninput sequence into a vector characterizing the sequence, and\\nthen you input the vector to a classifier. There are several\\nways to generate that vector. You can reshape the 2D output\\nfrom an embedding layer into a 1D vector, or you can feed\\nthat output into a recurrent layer or convolution layer in\\nhopes of generating a vector that is more context aware.\\nWhichever route you choose, the goal is simple: convert a\\nstring of text into an array of floating-point numbers that\\nuniquely describes it and use a sigmoid or softmax output\\nlayer to classify it.\\nNMT is basically an extension of text classification. You\\nstart by converting a text sequence into a vector. But rather\\nthan classify the vector, you use it to generate a new\\nsequence. One way to do that is with an LSTM encoder-decoder.\\nLSTM Encoder-Decoders\\nUntil a few years ago, most NMT models, including the one\\nunderlying Google Translate, were LSTM-based sequence-to-'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 513, 'file_type': 'pdf'}, page_content='512\\nsequence models similar to the one in Figure\\xa013-3. In such\\nmodels, one or more LSTM layers encode a tokenized input\\nsequence representing the phrase to be translated into a\\nvector. A second set of recurrent layers uses that vector as\\ninput and decodes it into a tokenized phrase in another\\nlanguage. The model accepts sequences as input and returns\\nsequences as output, hence the term sequence-to-sequence\\nmodel. A softmax output layer at the end outputs a set of\\nprobabilities for each token in the output sequence. If the\\nmaximum output phrase length that’s supported is 20 tokens,\\nfor example, and the vocabulary of the output language\\ncontains 20,000 words, then the output is 20 sets (one per\\ntoken) of 20,000 probabilities. The word selected for each\\noutput token is the word assigned the highest probability.\\nFigure 13-3. LSTM-based encoder-decoder for neural machine translation\\nLSTM-based sequence-to-sequence models are relatively easy to\\nbuild with Keras and TensorFlow. This book’s GitHub repo\\ncontains a notebook that uses 50,000 samples to train an LSTM\\nmodel to translate English to French. The model is defined\\nthis way:\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Embedding(en_vocab_size,\\n256,\\ninput_length=en_max_len,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 514, 'file_type': 'pdf'}, page_content=\"513\\nmask_zero=True))\\nmodel.add(LSTM(256))\\nmodel.add(RepeatVector(fr_max_len))\\nmodel.add(LSTM(256,\\nreturn_sequences=True))\\nmodel.add(Dropout(0.4))\\nmodel.add(TimeDistributed(Dense(fr_vocab_size,\\nactivation='softmax')))\\nmodel.compile(loss='sparse_categorical_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])model.summary(line_length=100)\\nThe first layer is an embedding layer that converts word\\ntokens into vectors containing 256 floating-point values. The\\nmask_zero=True parameter indicates that zeros in the input\\nsequences denote padding so that the next layer can ignore\\nthem. (There is no need to translate those tokens, after\\nall.) Next is an LSTM layer that encodes English phrases\\ninput to the model. A second LSTM layer decodes the phrases\\ninto dense vectors representing the French equivalents. In\\nbetween lies a RepeatVector layer that reshapes the output\\nfrom the first LSTM layer for input to the second by\\nrepeating the output a specified number of times. The final\\nlayer is a softmax classification layer that outputs\\nprobabilities for each word in the French vocabulary. The\\nTimeDistributed wrapper ensures that the model outputs a set\\nof probabilities for each token in the output rather than\\njust one set for the entire sequence.\\nAfter 34 epochs of training, the model translates 10 test\\nphrases this way:\\n\\nits fall now => cest maintenant maintenant\\nim losing => je suis en train\\nit was quite funny => cetait fut amusant amusant\\nthats not unusual => ce nest pas inhabituel\\ni think ill do that => je pense que je le\\ntom looks different => tom a lair different\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 515, 'file_type': 'pdf'}, page_content='514\\nits worth a try => ca vaut le coup\\nfortune smiled on him => la la lui a souri\\nlets hit the road => taillons la\\ni love winning => jadore gagner\\nThe model isn’t perfect, in part due to the limited size of\\nthe training set. Real NMT models are trained with hundreds\\nof millions or even billions of phrases. But is this one\\ntruly representative of the models used for state-of-the-art\\ntext translation? Figure\\xa013-4 is adapted from an image in a\\n2016 paper written by Google engineers documenting the\\narchitecture of Google Translate. The architecture maps\\nclosely to that of the model just presented. It’s deeper,\\nwith eight LSTM layers in the encoder and eight in the\\ndecoder. It also employs residual connections between layers\\nto support the network’s greater depth.\\nFigure 13-4. Google Translate circa 2016\\nOne difference between our model and the model described in\\nthe paper is the block labeled “Attention” between the'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 516, 'file_type': 'pdf'}, page_content='515\\nencoder and decoder. In deep learning, attention is a\\nmechanism for focusing a model’s attention on the parts of a\\nphrase that are most important, recognizing that one word can\\nhave different meanings in different contexts and that the\\nmeaning of a word sometimes depends on what’s around it.\\nAttention was introduced to deep learning in a seminal 2014\\npaper titled “Neural Machine Translation by Jointly Learning\\nto Align and Translate”, but it wasn’t until a few years\\nlater that attention took center stage as a way to replace\\nLSTM layers rather than supplement them. Enter perhaps the\\nmost significant contribution to the field of NMT, and to NLP\\noverall, to date: the transformer model.\\nTransformer Encoder-Decoders\\nA landmark 2017 paper titled “Attention Is All You Need”\\nchanged the way data scientists approach NMT and other neural\\ntext processing tasks. It proposed a better way to perform\\nsequence-to-sequence processing based on transformer models\\nthat eschew recurrent layers and use attention mechanisms to\\nmodel the context in which words are used. Today transformer\\nmodels have almost entirely replaced LSTM-based models.\\nFigure\\xa013-5 is adapted from an image in the aforementioned\\npaper. On the left is the encoder, which takes text sequences\\nas input and generates dense vector representations of those\\nsequences. On the right is the decoder, which transforms\\ndense vector representations of input sequences into output\\nsequences. At a high level, a transformer model uses the same\\nencoder-decoder architecture as an LSTM-based model. The\\ndifference lies in how it does the encoding and decoding.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 517, 'file_type': 'pdf'}, page_content='Positional\\nencoding\\n\\nMulti-head\\nattention\\n\\nOutput probabilities\\n\\nLinear\\n\\nMulti-head\\nattention\\n\\nMasked\\n\\nattention\\n\\nmulti-head\\n\\nNx\\n\\nPositional\\nencoding\\n\\nOutput\\nembedding.\\nInputs Outputs (shifted right)\\n\\n516'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 518, 'file_type': 'pdf'}, page_content='517\\nThe chief innovation introduced by the transformer model is\\nthe use of multi-head attention (MHA) layers in place of LSTM\\nlayers. MHA layers embody the concept of self-attention,\\nwhich enables a model to analyze an input sequence and focus\\non the words that are most important as well as the context\\nin which the words are used. In the sentence “We took a walk\\nin the park,” for example, the word park has a different\\nmeaning than it does in “Where did you park the car?” An\\nembedding layer stores one vector representation for park,\\nbut in a transformer model, the MHA layer modifies the vector\\noutput by the embedding layer so that park is represented by\\ntwo different vectors in the two sentences. Not surprisingly,\\nthe values used to make embedding vectors context aware are\\nlearned during training.\\nNOTE\\nHow does self-attention work? The gist is that an MHA layer uses\\ndot products to compute similarity scores for every word pair in a\\nsequence. After normalizing the scores, it uses them to compute\\nweighted versions of each word embedding in the sequence. Then it\\nmodifies them again using weights learned during training.\\nThe “multi” in multi-head attention denotes the fact that an MHA\\nlayer learns several sets of weights rather than just one, not\\nunlike a convolution layer in a CNN. This gives MHA the ability to\\ndiscern context in long sequences where words might have multiple\\nrelationships to one another.\\nMHA also provides additional context regarding words that\\nrefer to other words. An embedding layer represents the\\npronoun it with a single vector, but an MHA layer helps the\\nmodel understand that in the sentence “I love my car because\\nit is fast,” it refers to a car. It also adds weight to the'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 519, 'file_type': 'pdf'}, page_content='518\\nword fast because it’s crucial to the meaning of the\\nsentence. Without it, it’s not clear why you love your car.\\nUnlike LSTM layers, MHA layers’ ability to model\\nrelationships among the words in an input sequence is\\nindependent of sequence length. MHA layers also support\\nparallel workloads and therefore train faster on multiple\\nGPUs. They do not, however, encode information regarding the\\npositions of the words in a phrase. To compensate, a\\ntransformer uses simple vector addition to add information\\ndenoting a word’s position in a sequence to each vector\\noutput from the embedding layer. This is referred to as\\npositional encoding or positional embedding. It’s denoted by\\nthe plus signs labeled “Positional encoding” in Figure\\xa013-\\n5.\\nTransformers aren’t limited to neural machine translation;\\nthey’re used in virtually all aspects of NLP today. The\\nencoder half of a transformer outputs dense vector\\nrepresentations of the sequences input to it. Text\\nclassification can be performed by using a transformer rather\\nthan a standalone embedding layer to encode input sequences.\\nModels architected this way frequently outperform bag-of-\\nwords models, particularly if the ratio of samples to sample\\nlength (the number of training samples divided by the average\\nlength of each sample) exceeds 1,500. This so-called “golden\\nconstant” was discovered by a team of researchers at Google\\nand documented in a tutorial on text classification.\\nBuilding a Transformer-Based NMT Model\\nKeras provides some, but not all, of the building blocks that\\ncomprise an end-to-end transformer. It provides a handy\\nimplementation of self-attention layers in its\\nMultiHeadAttention class, for example, but it doesn’t\\nimplement positional embedding. However, a separate package\\nnamed KerasNLP does. Among others, it includes the following\\nclasses representing layers in a transformer-based network:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 520, 'file_type': 'pdf'}, page_content=\"519\\nTransformerEncoder\\nRepresents a transformer encoder\\nTransformerDecoder\\nRepresents a transformer decoder\\nTokenAndPositionEmbedding\\nImplements an embedding layer that supports\\npositional embedding\\nWith these classes, transformer-based NMT models are\\nrelatively easy to build. You can demonstrate by building an\\nEnglish-to-French translator. Start by installing KerasNLP if\\nit isn’t already installed. Then download en-fr.txt, a data\\nfile that contains 50,000 English phrases and their French\\nequivalents, and drop it into the Data subdirectory where\\nyour Jupyter notebooks are hosted. The file en-fr.txt is a\\nsubset of a larger file containing more than 190,000 phrases\\nand their corresponding translations compiled as part of the\\nTatoeba project. The file is tab delimited. Each line\\ncontains an English phrase, the equivalent French phrase, and\\nan attribution identifying where the translation came from.\\nWe don’t need the attributions, so load the dataset into a\\nDataFrame, remove the attribution column, and shuffle and\\nreindex the rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\n=\\npd.read_csv('Data/en-fr.txt',\\nnames=['en',\\n'fr',\\n'attr'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 usecols=['en',\\n'fr'],\\nsep='\\\\t')\\ndf\\n=\\ndf.sample(frac=1,\\nrandom_state=42)\\ndf\\n=\\ndf.reset_index(drop=True)\\ndf.head()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 521, 'file_type': 'pdf'}, page_content=\"520\\nHere’s the output:\\nThe dataset needs to be cleaned before it’s used to train a\\nmodel. Use the following statements to remove numbers and\\npunctuation symbols, convert words with Unicode characters\\nsuch as où into their ASCII equivalents (ou), convert\\ncharacters to lowercase, and insert [start] and [end] tokens\\nat the beginning and end of each French phrase:\\n\\nimport\\nre\\nfrom\\nunicodedata\\nimport\\nnormalize\\n\\ndef\\nclean_text(text):\\n\\xa0\\xa0\\xa0 text\\n=\\nnormalize('NFD',\\ntext.lower())\\n\\xa0\\xa0\\xa0 text\\n=\\nre.sub('[^A-Za-z ]+',\\n'',\\ntext)\\n\\xa0\\xa0\\xa0 return\\ntext\\n\\ndef\\nclean_and_prepare_text(text):\\n\\xa0\\xa0\\xa0 text\\n=\\n'[start] '\\n+\\nclean_text(text)\\n+\\n' [end]'\\n\\xa0\\xa0\\xa0 return\\ntext\\n\\ndf['en']\\n=\\ndf['en'].apply(lambda\\nrow:\\nclean_text(row))\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 522, 'file_type': 'pdf'}, page_content=\"521\\ndf['fr']\\n=\\ndf['fr'].apply(lambda\\nrow:\\nclean_and_prepare_text(row))\\ndf.head()\\nThe output looks a little cleaner afterward:\\nThe next step is to scan the dataset and determine the\\nmaximum length of the English phrases and of the French\\nphrases. These lengths will determine the lengths of the\\nsequences input to and output from the model:\\n\\nen\\n=\\ndf['en']\\nfr\\n=\\ndf['fr']\\n\\nen_max_len\\n=\\nmax(len(line.split())\\nfor\\nline\\nin\\nen)\\nfr_max_len\\n=\\nmax(len(line.split())\\nfor\\nline\\nin\\nfr)\\nsequence_len\\n=\\nmax(en_max_len,\\nfr_max_len)\\n\\nprint(f'Max phrase length (English): {en_max_len}')\\nprint(f'Max phrase length (French): {fr_max_len}')\\nprint(f'Sequence length: {sequence_len}')\\nIn this example, the longest English phrase contains seven\\nwords, while the longest French phrase contains 16 (including\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 523, 'file_type': 'pdf'}, page_content='522\\nthe [start] and [end] tokens). The model will be able to\\ntranslate English phrases up to seven words in length into\\nFrench phrases up to 14 words in length.\\nNow fit one Tokenizer to the English phrases and another\\nTokenizer to their French equivalents, and generate padded\\nsequences from all the phrases. Note the filters parameter\\npassed to the French tokenizer. It configures the tokenizer\\nto remove all the punctuation characters it normally removes\\nexcept for the square brackets used to delimit [start] and\\n[end] tokens:\\n\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\nfrom\\ntensorflow.keras.preprocessing.sequence\\nimport\\npad_sequences\\n\\nen_tokenizer\\n=\\nTokenizer()\\nen_tokenizer.fit_on_texts(en)\\nen_sequences\\n=\\nen_tokenizer.texts_to_sequences(en)\\nen_x\\n=\\npad_sequences(en_sequences,\\nmaxlen=sequence_len,\\npadding=\\'post\\')\\n\\nfr_tokenizer\\n=\\nTokenizer(filters=\\'!\"#$%&()*+,-./:;<=>?@\\\\\\\\^_`{|}~\\\\t\\\\n\\')\\nfr_tokenizer.fit_on_texts(fr)\\nfr_sequences\\n=\\nfr_tokenizer.texts_to_sequences(fr)\\nfr_y\\n=\\npad_sequences(fr_sequences,\\nmaxlen=sequence_len\\n+\\n1,\\npadding=\\'post\\')\\nNext, compute the vocabulary size for each language from the\\nTokenizer instances:\\n\\nen_vocab_size\\n=\\nlen(en_tokenizer.word_index)\\n+\\n1\\nfr_vocab_size\\n=\\nlen(fr_tokenizer.word_index)\\n+\\n1\\n\\nprint(f\\'Vocabulary size (English): {en_vocab_size}\\')\\nprint(f\\'Vocabulary size (French): {fr_vocab_size}\\')'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 524, 'file_type': 'pdf'}, page_content=\"523\\nThe output reveals that the English vocabulary contains 6,033\\nwords, while the French vocabulary contains 12,197. These\\nvalues will be used to size the model’s two embedding\\nlayers. The latter will also be used to size the output\\nlayer.\\nFinally, create the features and the labels the model will be\\ntrained with. The features are the padded English sequences\\nand the padded French sequences minus the [end] tokens. The\\nlabels are the padded French sequences minus the [start]\\ntokens. Package the features in a dictionary so that they can\\nbe input to a model that accepts multiple inputs:\\n\\ninputs\\n=\\n{\\n'encoder_input':\\nen_x,\\n'decoder_input':\\nfr_y[:,\\n:-1]\\n}\\noutputs\\n=\\nfr_y[:,\\n1:]\\nNow let’s define a model. This time, we’ll use Keras’s\\nfunctional API rather than its sequential API. It’s\\nnecessary because this model has two inputs: one that accepts\\na tokenized English phrase and another that accepts a\\ntoke\\u2060nized French phrase. We’ll also seed the random-number\\ngenerators used by Keras and TensorFlow to get repeatable\\nresults, at least on CPU. This is a departure from all the\\nother examples in this book, but it ensures that when you\\ntrain the model, you get the same results that I did. Here’s\\nthe code:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\ntensorflow\\nas\\ntf\\nfrom\\ntensorflow.keras\\nimport\\nModel\\nfrom\\ntensorflow.keras.layers\\nimport\\nInput,\\nDense,\\nDropout\\nfrom\\nkeras_nlp.layers\\nimport\\nTokenAndPositionEmbedding,\\nTransformerEncoder\\nfrom\\nkeras_nlp.layers\\nimport\\nTransformerDecoder\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 525, 'file_type': 'pdf'}, page_content=\"524\\n\\nnp.random.seed(42)\\ntf.random.set_seed(42)\\n\\nnum_heads\\n=\\n8\\nembed_dim\\n=\\n256\\n\\nencoder_input\\n=\\nInput(shape=(None,),\\ndtype='int64',\\nname='encoder_input')\\nx\\n=\\nTokenAndPositionEmbedding(en_vocab_size,\\nsequence_len,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 embed_dim)(encoder_input)\\nencoder_output\\n=\\nTransformerEncoder(embed_dim,\\nnum_heads)(x)\\nencoded_seq_input\\n=\\nInput(shape=(None,\\nembed_dim))\\n\\ndecoder_input\\n=\\nInput(shape=(None,),\\ndtype='int64',\\nname='decoder_input')\\nx\\n=\\nTokenAndPositionEmbedding(fr_vocab_size,\\nsequence_len,\\nembed_dim,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 mask_zero=True)(decoder_input)\\nx\\n=\\nTransformerDecoder(embed_dim,\\nnum_heads)(x,\\nencoded_seq_input)\\nx\\n=\\nDropout(0.4)(x)\\n\\ndecoder_output\\n=\\nDense(fr_vocab_size,\\nactivation='softmax')(x)\\ndecoder\\n=\\nModel([decoder_input,\\nencoded_seq_input],\\ndecoder_output)\\ndecoder_output\\n=\\ndecoder([decoder_input,\\nencoder_output])\\n\\nmodel\\n=\\nModel([encoder_input,\\ndecoder_input],\\ndecoder_output)\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary(line_length=100)\\nTHE KERAS FUNCTIONAL API\\nThe functional API is a richer version of Keras’s\\nsequential API. Among other things, it lets you create\\nmodels with multiple inputs or outputs and models with\\nshared layers like the ones in Faster R-CNN’s region\\nproposal network. Here’s a simple binary classifier\\ndefined with the sequential API:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 526, 'file_type': 'pdf'}, page_content=\"525\\n\\nmodel\\n=\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=3))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nHere’s the same network created with the functional API:\\n\\ninput\\n=\\nInput(shape=(3,))\\nhidden\\n=\\nDense(128,\\nactivation='relu')(input)\\noutput\\n=\\nDense(1,\\nactivation='sigmoid')(hidden)\\nmodel\\n=\\nModel(inputs=input,\\noutputs=output)\\nWhen you create the model, you specify the inputs and\\noutputs, and since the inputs and outputs parameters\\naccept Python lists, it’s a simple matter to create a\\nmodel with multiple inputs and outputs. For a concise\\nintroduction to the functional API and examples\\ndemonstrating advanced uses, including multiple inputs\\nand outputs and shared layers, see “How to Use the Keras\\nFunctional API for Deep Learning” by Jason Brownlee.\\nThe model is designed to operate iteratively. To translate\\ntext, you first pass an English phrase to the English input\\nand the word “[start]” to the French input. Then you append\\nthe next French word the model predicts to the previous\\nFrench input and call the model again, and you repeat this\\nprocess until the entire phrase has been translated—that is,\\nuntil the next word predicted by the model is “[end].”\\nFigure\\xa013-6 diagrams the model’s architecture. The model\\nincludes two embedding layers: one for English sequences and\\none for French sequences. Both convert word tokens into\\nvectors of 256 floating point values each, and both are\\ninstances of KerasNLP’s TokenAndPositionEmbedding class,\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 527, 'file_type': 'pdf'}, page_content=\"526\\nwhich adds positional information to word embeddings. Output\\nfrom the English embedding layer passes through an encoder\\n(an instance of TransformerEncoder) before being input along\\nwith the output from the French embedding layer to the\\ndecoder, which is an instance of TransformerDecoder. The\\ndecoder outputs a vector representing the next step in the\\ntranslation, and a softmax output layer converts that vector\\ninto a set of probabilities—one for each word in the French\\nvocabulary—identifying the next token. During training, the\\nmask_zero=True parameter passed to the French embedding layer\\nlimits the model to making predictions based on the tokens\\npreceding the one that’s being predicted. In other words,\\ngiven a set of French tokens numbered 0 through n, the model\\nis trained to predict what token n + 1 will be without\\npeeking at n + 1 in the training text.\\nFigure 13-6. Transformer-based NMT architecture\\nNow call fit to train the model, and use an EarlyStopping\\ncallback to end training if the validation accuracy fails to\\nimprove for three consecutive epochs:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nEarlyStopping\\n\\ncallback\\n=\\nEarlyStopping(monitor='val_accuracy',\\npatience=3,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 restore_best_weights=True)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 528, 'file_type': 'pdf'}, page_content=\"527\\n\\nhist\\n=\\nmodel.fit(inputs,\\noutputs,\\nepochs=50,\\nvalidation_split=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nTraining typically requires two to three minutes per epoch on\\nCPU. If you don’t have a GPU and training is too slow, I\\nrecommend running the code in Google Colab. (Be sure to go to\\n“Notebook settings” in the Edit menu and select GPU as the\\nhardware accelerator.) When training is complete, plot the\\nper-epoch training and validation accuracy and observe how\\nthe latter steadily increases until it levels off:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\n':',\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe output should look like this:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 529, 'file_type': 'pdf'}, page_content='528\\nThe plot reveals that at the end of 14 epochs, the model was\\nabout 75% accurate in translating English samples in the\\nvalidation data to French. This isn’t a very robust measure\\nof accuracy because it literally compares each word in the\\npredicted text to each word in the target text and ignores\\nthe fact that a missing or misplaced article such as le\\n(French for the) doesn’t necessarily imply a poor\\ntranslation. The accuracy of NMT models is typically measured\\nwith bilingual evaluation understudy (BLEU) scores. BLEU\\nscores are rather easily computed after the training is\\ncomplete using packages such as NLTK, but during training,\\nvalidation accuracy is a reasonable metric for judging when\\nto halt training.\\nCan the model really translate English to French? Use the\\nfollowing code to define a function that accepts an English\\nphrase and returns a French phrase. Then call it on 10 of the\\nphrases used to validate the model during training and see'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 530, 'file_type': 'pdf'}, page_content=\"529\\nfor yourself. Note that one call to translate_text\\nprecipitates multiple calls to the model. To translate\\n“hello world,” for example, translate_text calls the model\\nwith the inputs “hello world” and “[start].” Assuming the\\nmodel predicts that salut is the next word, translate_text\\ninvokes it again with the inputs “hello world” and\\n“[start] salut.” It repeats this cycle until the next word\\npredicted by the model is “[end]” denoting the end of the\\ntranslation.\\n\\ndef\\ntranslate_text(text,\\nmodel,\\nen_tokenizer,\\nfr_tokenizer,\\nfr_index_lookup,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 sequence_len):\\n\\xa0\\xa0\\xa0 input_sequence\\n=\\nen_tokenizer.texts_to_sequences([text])\\n\\xa0\\xa0\\xa0 padded_input_sequence\\n=\\npad_sequences(input_sequence,\\nmaxlen=sequence_len,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 padding='post')\\n\\xa0\\xa0\\xa0 decoded_text\\n=\\n'[start]'\\n\\n\\xa0\\xa0\\xa0 for\\ni\\nin\\nrange(sequence_len):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_sequence\\n=\\nfr_tokenizer.texts_to_sequences([decoded_text])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 padded_target_sequence\\n=\\npad_sequences(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_sequence,\\nmaxlen=sequence_len,\\npadding='post')[:,\\n:-1]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 prediction\\n=\\nmodel([padded_input_sequence,\\npadded_target_sequence])\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 idx\\n=\\nnp.argmax(prediction[0,\\ni,\\n:])\\n-\\n1\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 token\\n=\\nfr_index_lookup[idx]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 decoded_text\\n+=\\n' '\\n+\\ntoken\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\ntoken\\n==\\n'[end]':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 break\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 return\\ndecoded_text[8:-6]\\n# Remove [start] and [end] tokens\\n\\nfr_vocab\\n=\\nfr_tokenizer.word_index\\nfr_index_lookup\\n=\\ndict(zip(range(len(fr_vocab)),\\nfr_vocab))\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 531, 'file_type': 'pdf'}, page_content=\"530\\ntexts\\n=\\nen[40000:40010].values\\n\\nfor\\ntext\\nin\\ntexts:\\n\\xa0\\xa0\\xa0 translated\\n=\\ntranslate_text(text,\\nmodel,\\nen_tokenizer,\\nfr_tokenizer,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 fr_index_lookup,\\nsequence_len)\\n\\xa0\\xa0\\xa0 print(f'{text} => {translated}')\\nHere’s the output:\\n\\nits fall now => cest desormais tombe\\nim losing => je suis en train de perdre\\nit was quite funny => ce fut assez amusant\\nthats not unusual => ce nest pas inhabituel\\ni think ill do that => je pense que je ferai ca\\ntom looks different => tom a lair different\\nits worth a try => ca vaut le coup dessayer\\nfortune smiled on him => la chance lui souri\\nlets hit the road => cassonsnous\\ni love winning => jadore gagner\\nIf you don’t speak French, use Google Translate to translate\\nsome of the French phrases to English. According to Google,\\nfor example, “la chance lui souri” translates to “Luck\\nsmiled on him,” while “ce nest pas inhabituel” translates\\nto “it’s not unusual.” The model isn’t perfect, but it’s\\nnot bad, either. The vocabulary you used is small, so you\\ncan’t input just any old phrase and expect the model to\\ntranslate it. But simple phrases that use words in the\\ntraining text translate reasonably well.\\nFinish up by using the translate_text function to see how the\\nmodel translates “Hello world” into French:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 532, 'file_type': 'pdf'}, page_content=\"531\\n\\ntranslate_text('Hello world',\\nmodel,\\nen_tokenizer,\\nfr_tokenizer,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 fr_index_lookup,\\nsequence_len)\\nI haven’t had French lessons since high school, but even I\\nknow that “Salut le monde” is a reasonable translation of\\n“Hello world.”\\nUsing Pretrained Models to Translate Text\\nEngineers at Microsoft, Google, and Facebook have the\\nresources to collect millions of text translation samples and\\nthe hardware to train sophisticated transformer models on\\nthem, but you and I do not. The good news is that this\\nneedn’t stop us from writing software that translates text\\nfrom one language to another. Hugging Face has published\\nseveral pretrained transformer models that do a fine job of\\ntext translation. Leveraging those models in Python is\\nsimplicity itself.\\nHere’s an example that translates English to French:\\n\\nfrom\\ntransformers\\nimport\\npipeline\\n\\ntranslator\\n=\\npipeline('translation_en_to_fr')\\ntranslation\\n=\\ntranslator('Programming is fun!')\\n[0]['translation_text']\\nprint(translation)\\nThe same syntax can be used to translate English to German\\nand English to Romanian too. Simply replace\\ntranslation_en_to_fr with translation_en_to_de or\\ntranslation_en_to_ro when creating the pipeline.\\nFor other languages, you use a slightly more verbose syntax\\nto load a transformer and a corresponding tokenizer. The\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 533, 'file_type': 'pdf'}, page_content=\"532\\nfollowing example translates Dutch to English:\\n\\nfrom\\ntransformers\\nimport\\nAutoTokenizer,\\nAutoModelForSeq2SeqLM\\n\\n# Initialize the tokenizer\\ntokenizer\\n=\\nAutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-nl-en')\\n\\n# Initialize the model\\nmodel\\n=\\nAutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-nl-en')\\n\\n# Tokenize the input text\\ntext\\n=\\n'Hallo vrienden, hoe gaat het vandaag?'\\ntokenized_text\\n=\\ntokenizer.prepare_seq2seq_batch([text],\\nreturn_tensors='pt')\\n\\n# Perform translation and decode the output\\ntranslation\\n=\\nmodel.generate(**tokenized_text)\\ntranslated_text\\n=\\ntokenizer.batch_decode(\\n\\xa0\\xa0\\xa0 translation,\\nskip_special_tokens=True)[0]\\nprint(translated_text)\\nYou’ll find an exhaustive list of Hugging Face translators\\nand tokenizers on the organization’s website. There are\\nhundreds of them covering dozens of languages.\\nBidirectional Encoder Representations from\\nTransformers (BERT)\\nThe introduction of transformers in 2017 laid the groundwork\\nfor another landmark innovation in the NLP space:\\nBidirectional Encoder Representations from Transformers, or\\nBERT for short. Introduced by Google researchers in a 2018\\npaper titled “BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding”, BERT advanced the\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 534, 'file_type': 'pdf'}, page_content='533\\nstate of the art by providing pretrained transformers that\\ncan be fine-tuned for a variety of NLP tasks.\\nBERT was instilled with language understanding by training it\\nwith more than 2.5 billion words from Wikipedia articles and\\n800 million words from Google Books. Training required four\\ndays on 64 tensor processing units (TPUs). Fine-tuning is\\naccomplished by further training the pretrained model with\\ntask-specific samples and a reduced learning rate\\n(Figure\\xa013-7). It’s a relatively simple matter, for\\nexample, to fine-tune BERT to perform sentiment analysis and\\noutscore bag-of-words models for accuracy. BERT’s value lies\\nin the fact that it possesses an innate understanding of the\\nlanguages it was trained with and can be refined to perform\\ndomain-specific tasks.\\nAside from the fact that it was trained with a huge volume of\\nsamples, the key to BERT’s ability to understand human\\nlanguages is an innovation known as masked language modeling,\\nor MLM for short. The big idea behind MLM is that a model has\\na better chance of predicting what word should fill in the\\nblank in the phrase “Every good ____ does fine” than it has\\nat predicting the next word in the phrase “Every good\\n____.” The answer could be boy, as in “Every good boy does\\nfine,” or it could be turn, as in “Every good turn deserves\\nanother.” Unidirectional models look at the text to the left\\nor the text to the right and attempt to predict what the next\\nword should be. MLM, on the other hand, uses text on the left\\nand right to inform its decisions. That’s why BERT is a\\n“bidirectional” transformer.\\nWhen BERT models are pretrained, a specified percentage of\\nthe word tokens in each sequence—usually 15%—are randomly\\nremoved or “masked” so that the model can learn to predict\\nthem from the words around them. In addition, BERT models are\\nusually pretrained to do next-sentence prediction, which\\nmakes them more adept at certain NLP tasks such as answering\\nquestions.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 535, 'file_type': 'pdf'}, page_content='534\\nBERT has been called the “Swiss Army knife” of NLP. Google\\nuses it to improve search results and predict text as you\\ntype into a Gmail or Google Doc. Dozens of variations have\\nbeen published, including DistilBERT, which retains 97% of\\nthe accuracy of the original model while weighing in 40%\\nsmaller and running 60% faster. Also available are variations\\nof BERT already fine-tuned for specific tasks such as\\nquestion answering. Such models can be further refined using\\ndomain-specific datasets, or they can be used as is.\\nFigure 13-7. Bidirectional Encoder Representations from Transformers (BERT)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 536, 'file_type': 'pdf'}, page_content='535\\nBuilding a BERT-Based Question Answering\\nSystem\\nHugging Face’s transformers package contains several\\npretrained BERT models already fine-tuned for specific tasks.\\nOne example is the minilm-uncased-squad2 model, which was\\ntrained with Stanford’s SQuAD 2.0 dataset to answer\\nquestions by extracting text from documents. To get a feel\\nfor what models like this one can accomplish, let’s use it\\nto build a simple question-answering system.\\nFirst, some context. When you ask Google a question like the\\none in Figure\\xa013-8, it queries a database containing\\nbillions of web pages to identify ones that might contain an\\nanswer. Then it uses a BERT-based NLP model to extract\\nanswers from the pages it identified and rank them based on\\nconfidence levels.\\nFigure 13-8. Google question answering\\nLet’s load a pretrained BERT model already fine-tuned for\\nquestion answering and use it to extract answers from\\npassages of text in this book. The model we’ll use is a\\nversion of the MiniLM model introduced in a 2020 paper titled\\n“MiniLM: Deep Self-Attention Distillation for Task-Agnostic'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 537, 'file_type': 'pdf'}, page_content='536\\nCompression of Pre-trained Transformers”. This version was\\nfine-tuned on SQuAD 2.0, which contains more than 100,000\\nquestions generated by humans paired with answers culled from\\nWikipedia articles, plus 50,000 questions which lack answers.\\nThe MiniLM architecture enables reading comprehension gained\\nfrom one dataset to be applied to other datasets with little\\nor no retraining.\\nNOTE\\nMy eighth-grade history teacher, Mr. Aird, roomed with Charles\\nLindbergh one summer in the early 1920s. Apparently the world’s\\nmost famous aviator was something of a daredevil in college, and\\nI’ll never forget something Mr. Aird said about him. “In 1927,\\nwhen I learned that Charles had flown solo across the Atlantic, I\\nwasn’t surprised. I have never met a person with less regard for\\nhis own life than Charles Lindbergh.” 😊\\nBegin by creating a new Jupyter notebook and using the\\nfollowing statements to load a pretrained MiniLM model from\\nthe Hugging Face hub and a tokenizer to tokenize text input\\nto the model. (BERT uses a special tokenization format called\\nWordPiece that is slightly different from the one Keras’s\\nTokenizer class and TextVectorization layer use. Fortunately,\\nHugging Face has a solution for that too.) Then compose a\\npipeline from them. The first time you run this code, you’ll\\nexperience a momentary delay while the pretrained weights are\\ndownloaded. After that, the weights will be cached and\\nloading will be fast:\\n\\nfrom\\ntransformers\\nimport\\nAutoTokenizer,\\nTFAutoModelForQuestionAnswering,\\npipeline'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 538, 'file_type': 'pdf'}, page_content=\"537\\n\\nid\\n=\\n'deepset/minilm-uncased-squad2'\\ntokenizer\\n=\\nAutoTokenizer.from_pretrained(id)\\nmodel\\n=\\nTFAutoModelForQuestionAnswering.from_pretrained(id,\\nfrom_pt=True)\\npipe\\n=\\npipeline('question-answering',\\nmodel=model,\\ntokenizer=tokenizer)\\nHugging Face stores weights for this particular model in\\nPyTorch format. The from_pt=True parameter converts the\\nweights to TensorFlow format. It’s not trivial to convert\\nneural network weights from one format to another, but the\\nHugging Face library reduces it to a simple function\\nparameter.\\nNow use the pipeline to answer a question by extracting text\\nfrom a paragraph:\\n\\nquestion\\n=\\n'What does NLP stand for?'\\n\\ncontext\\n=\\n'Natural Language Processing, or NLP, encompasses a variety of ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'activities, including text classification, keyword and topic ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'extraction, text summarization, and language translation. The ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'accuracy of NLP models has improved in recent years for a variety '\\n\\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'of reasons, not the least of which are newer and better ways of ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'converting words and sentences into dense vector representations ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'that incorporate context, and a relatively new neural network ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'architecture called the transformer that can zero in on the most ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'meaningful words and even differentiate between multiple meanings '\\n\\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'of the same word.'\\n\\npipe(question=question,\\ncontext=context)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 539, 'file_type': 'pdf'}, page_content=\"538\\nIs the answer accurate? A human could easily read the\\nparagraph and come up with the same answer, but the fact that\\na deep-learning model can do it indicates that the model\\ndisplays some level of reading comprehension. Observe that\\nthe output contains the answer to the question as well as a\\nconfidence score and the starting and ending indices of the\\nanswer in the paragraph:\\n\\n{'score': 0.9793193340301514,\\n'start': 0,\\n'end': 27,\\n'answer': 'Natural Language Processing'}\\nNow try it again with a different question and context:\\n\\nquestion\\n=\\n'When was TensorFlow released?'\\n\\ncontext\\n=\\n'Machine learning isn\\\\'t hard when you have a properly ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'engineered dataset to work with. The reason it\\\\'s not ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'hard is libraries such as Scikit-Learn and ML.NET, which ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'reduce complex learning algorithms to a few lines of code. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Deep learning isn\\\\'t difficult, either, thanks to libraries ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'such as the Microsoft Cognitive Toolkit (CNTK), Theano, and ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'PyTorch. But the library that most of the world has settled ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'on for building neural networks is TensorFlow, an open source ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'framework created by Google that was released under the ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Apache License 2.0 in 2015.'\\n\\npipe(question=question,\\ncontext=context)['answer']\\nThis time, the output is the answer provided by the model\\nrather than the dictionary containing the answer. Once again,\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 540, 'file_type': 'pdf'}, page_content=\"539\\nis the answer reasonable?\\nRepeat this process with another question and context from\\nwhich to extract an answer:\\n\\nquestion\\n=\\n'Is Keras part of TensorFlow?'\\n\\ncontext\\n=\\n'The learning curve for TensorFlow is rather steep. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Another library, named Keras, provides a simplified ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Python interface to TensorFlow and has emerged as the ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Scikit of deep learning. Keras is all about neural networks. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'It began life as a standalone project in 2015 but was ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'integrated into TensorFlow in 2019. Any code that you write ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'using TensorFlow\\\\'s built-in Keras module ultimately executes ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'in (and is optimized for) TensorFlow. Even Google recommends ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'using the Keras API.'\\n\\npipe(question=question,\\ncontext=context)['answer']\\nPerform one final test using the same context as before but a\\ndifferent question:\\n\\nquestion\\n=\\n'Is it better to use Keras or TensorFlow to build neural networks?'\\npipe(question=question,\\ncontext=context)['answer']\\nThe questions posed here were hand-selected to highlight the\\nmodel’s capabilities. It’s not difficult to come up with\\nquestions that the model can’t answer. Nevertheless, you\\nhave proven the principle that a pretrained BERT model fine-\\ntuned on SQuAD 2.0 can answer straightforward questions from\\npassages of text presented to it.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 541, 'file_type': 'pdf'}, page_content='540\\nSophisticated question-answering systems employ a retriever-\\nreader architecture in which the retriever searches a data\\nstore for relevant documents—ones that might contain an\\nanswer to a question—and the reader extracts answers from\\nthe documents. The reader is often a BERT instance similar to\\nthe one shown earlier. The retriever may be one from the open\\nsource Haystack library published by Deepset, a German\\ncompany focused on NLP solutions. Haystack retrievers\\ninterface with a wide range of document stores including\\nElasticsearch stores, which are highly scala\\u2060ble. If you’d\\nlike to learn more or build a retriever-reader system of your\\nown, I recommend reading Chapter 7 of Natural Language\\nProcessing with Transformers by Lewis Tunstall, Leandro von\\nWerra, and Thomas Wolf (O’Reilly).\\nFine-Tuning BERT to Perform Sentiment\\nAnalysis\\nState-of-the-art sentiment analysis can be accomplished by\\nfine-tuning pretrained BERT models on sentiment analysis\\ndatasets. Let’s fine-tune BERT and see if we can create a\\nsentiment analysis model that’s more accurate than the bag-\\nof-words model presented earlier in this chapter. If your\\ncomputer isn’t equipped with a GPU, I highly recommend\\nrunning this example in Google Colab. Even on a GPU, it can\\ntake an hour or so to run.\\nIf you run this code locally, make sure Hugging Face’s\\nDatasets package is installed. Then create a new Jupyter\\nnotebook. If you use Colab instead, create a new notebook and\\nrun the following commands in the first cell to install the\\nnecessary packages in the Colab environment:\\n\\n!pip install transformers\\n!pip install datasets'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 542, 'file_type': 'pdf'}, page_content=\"541\\nNext, use the following statements to load the IMDB dataset\\nfrom the Datasets package. This is an alternative to loading\\nit from a CSV file. Since we’re using Hugging Face models,\\nwe may as well load the data from Hugging Face too. Plus, if\\nyou’re using Colab, this prevents you from having to upload\\na CSV to the Colab environment. Note that the dataset might\\ntake a few minutes to load the first time:\\n\\nfrom\\ndatasets\\nimport\\nload_dataset\\n\\nimdb\\n=\\nload_dataset('imdb')\\nimdb\\nThe value returned by load_dataset is a dictionary containing\\nthree Hugging Face datasets. Here’s the output from the\\nfinal statement:\\n\\nDatasetDict({\\n\\xa0\\xa0\\xa0 train:\\nDataset({\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 features:\\n['text',\\n'label'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_rows:\\n25000\\n\\xa0\\xa0\\xa0 })\\n\\xa0\\xa0\\xa0 test:\\nDataset({\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 features:\\n['text',\\n'label'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_rows:\\n25000\\n\\xa0\\xa0\\xa0 })\\n\\xa0\\xa0\\xa0 unsupervised:\\nDataset({\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 features:\\n['text',\\n'label'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_rows:\\n50000\\n\\xa0\\xa0\\xa0 })\\n})\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 543, 'file_type': 'pdf'}, page_content=\"542\\nimdb['train'] contains 25,000 samples for training, while\\nimdb['test'] contains 25,000 samples for testing. Movie\\nreviews are stored in the text column of each dataset. Labels\\nare stored in the label column.\\nNext up is tokenizing the input using a BERT WordPiece\\ntokenizer:\\n\\nfrom\\ntransformers\\nimport\\nAutoTokenizer\\n\\ntokenizer\\n=\\nAutoTokenizer.from_pretrained('distilbert-base-uncased')\\n\\ndef\\ntokenize(samples):\\n\\xa0\\xa0\\xa0 return\\ntokenizer(samples['text'],\\ntruncation=True)\\n\\ntokenized_imdb\\n=\\nimdb.map(tokenize,\\nbatched=True)\\nNow that the reviews are tokenized, they need to be converted\\ninto TensorFlow datasets with Hugging Face’s\\nDataset.to_tf_dataset method. The collating function passed\\nto the method dynamically pads the sequences so that they’re\\nall the same length. You can also ask the tokenizer to do the\\npadding, but padding performed that way is static and\\nrequires more memory:\\n\\nfrom\\ntransformers\\nimport\\nDataCollatorWithPadding\\n\\ndata_collator\\n=\\nDataCollatorWithPadding(tokenizer=tokenizer,\\nreturn_tensors='tf')\\n\\ntrain_data\\n=\\ntokenized_imdb['train'].to_tf_dataset(\\n\\xa0\\xa0\\xa0 columns=['attention_mask',\\n'input_ids',\\n'label'],\\n\\xa0\\xa0\\xa0 shuffle=True,\\nbatch_size=16,\\ncollate_fn=data_collator\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 544, 'file_type': 'pdf'}, page_content=\"543\\n)\\n\\nvalidation_data\\n=\\ntokenized_imdb['test'].to_tf_dataset(\\n\\xa0\\xa0\\xa0 columns=['attention_mask',\\n'input_ids',\\n'label'],\\n\\xa0\\xa0\\xa0 shuffle=False,\\nbatch_size=16,\\ncollate_fn=data_collator\\n)\\nNow you’re ready to fine-tune. Call fit on the model as\\nusual, but set the optimizer’s learning rate (the multiplier\\nused to adjust weights and biases during backpropagation) to\\n0.00002, which is a fraction of Adam’s default learning rate\\nof 0.001:\\n\\nfrom\\ntensorflow.keras.optimizers\\nimport\\nAdam\\nfrom\\ntransformers\\nimport\\nTFAutoModelForSequenceClassification\\n\\nmodel\\n=\\nTFAutoModelForSequenceClassification.from_pretrained(\\n\\xa0\\xa0\\xa0 'distilbert-base-uncased',\\nnum_labels=2)\\nmodel.compile(Adam(learning_rate=2e-5),\\nmetrics=['accuracy'])\\nhist\\n=\\nmodel.fit(train_data,\\nvalidation_data=validation_data,\\nepochs=3)\\nPlot the training and validation accuracy to see where the\\nlatter topped out:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\\n\\nacc\\n=\\nhist.history['accuracy']\\nval\\n=\\nhist.history['val_accuracy']\\nepochs\\n=\\nrange(1,\\nlen(acc)\\n+\\n1)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 545, 'file_type': 'pdf'}, page_content=\"544\\n\\nplt.plot(epochs,\\nacc,\\n'-',\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\n':',\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nHere’s how it turned out for me:\\nWith a little luck, validation accuracy topped out at around\\n93%—a few points better than the equivalent bag-of-words\\nmodel. Just imagine what you could do if you trained the\\nmodel with more than 25,000 reviews. One of Hugging Face’s\\npretrained sentiment analysis models—the twitter-roberta-\\nbase model—was trained with 58 million tweets. Not\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 546, 'file_type': 'pdf'}, page_content=\"545\\nsurprisingly, it does a wonderful job of scoring text for\\nsentiment.\\nFinish up by defining an analyze_text function that returns a\\nsentiment score and using it to score a positive review for\\nsentiment. The model returns an object wrapping a tensor\\ncontaining unnormalized sentiment scores (one for negative\\nand one for positive), but you can use TensorFlow’s softmax\\nfunction to normalize them to values from 0.0 to 1.0:\\n\\nimport\\ntensorflow\\nas\\ntf\\n\\ndef\\nanalyze_text(text,\\ntokenizer,\\nmodel):\\n\\xa0\\xa0\\xa0 tokenized_text\\n=\\ntokenizer(text,\\npadding=True,\\ntruncation=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return_tensors='tf')\\n\\xa0\\xa0\\xa0 prediction\\n=\\nmodel(tokenized_text)\\n\\xa0\\xa0\\xa0 return\\ntf.nn.softmax(prediction[0]).numpy()[0][1]\\n\\nanalyze_text('Great food and excellent service!',\\ntokenizer,\\nmodel)\\nTry it again with a negative review:\\n\\nanalyze_text('The long lines and poor customer service really turned me off.',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tokenizer,\\nmodel)\\nFine-tuning isn’t cheap, but it isn’t nearly as expensive\\nas training a sophisticated transformer from scratch. The\\nfact that you could train a sentiment analysis model to be\\nthis accurate in about an hour of GPU time is a tribute to\\nthe power of pretrained BERT models, and to the Google\\nengineers who created them.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 547, 'file_type': 'pdf'}, page_content='546\\nSummary\\nNatural language processing, or NLP, is an area of deep\\nlearning that encompasses text classification, question\\nanswering, text translation, and other tasks that require\\ncomputers to process textual data. A key element of every NLP\\nmodel is an embedding layer, which represents words with\\narrays of floating-point numbers that model the relationships\\nbetween words. The vectors for excellent and amazing in\\nembedding space are close together, for example, while the\\nvectors for butterfly and basketball are far apart since the\\nwords have no semantic relationship. Word embeddings are\\nlearned as a model is trained.\\nText input to an embedding layer must first be tokenized and\\nturned into sequences of equal length. Keras’s Tokenizer\\nclass does most of the work. Rather than tokenize and\\nsequence text separately, you can include a TextVectorization\\nlayer in a model to do the tokenization and padding\\nautomatically.\\nOne way to classify text is to use a traditional dense layer\\nto classify the vectors output from an embedding layer. An\\nalternative is to use convolution layers or recurrent layers\\nto tease information regarding word position from the\\nembedding vectors and classify the output from those layers.\\nThe use of deep learning to translate text to other languages\\nis known as neural machine translation, or NMT. Until\\nrecently, state-of-the-art NMT was performed using LSTM-based\\nencoder-decoder models. Today those models have largely given\\nway to transformer models that use neural attention to focus\\non the words in a phrase that are most meaningful and model\\nword context. A transformer knows that the word train has\\ndifferent meanings in “meet me at the train station” and\\n“it’s time to train a model,” and it factors word order\\ninto its calculations.\\nBERT is a sophisticated transformer model installed with\\nlanguage understanding when engineers at Google trained it'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 548, 'file_type': 'pdf'}, page_content='547\\nwith billions of words. It can be fine-tuned for specific\\ntasks such as question answering and sentiment analysis, and\\nseveral fine-tuned versions have been published for the\\npublic to use. These models can sometimes be used as is.\\nOther times, they can be further refined and adapted to\\ndomain-specific tasks. Because fine-tuning requires orders of\\nmagnitude less data and compute power than training BERT from\\nscratch, pretrained (and pre–fine-tuned) BERT models have\\nproven a boon to NLP.\\nSophisticated NLP models are trained with millions of words\\nor phrases, requiring a substantial investment in data\\ncollection and hardware for training. Companies such as\\nHugging Face publish pretrained models that you can leverage\\nin your code. This is a growing trend in AI: publishing\\nmodels that are already trained to solve common problems.\\nYou’ll still build models to solve problems that are domain\\nspecific, but many tasks—especially those involving NLP—are\\nnot consigned to a particular domain.\\nDownloading pretrained models isn’t the only way to leverage\\nsophisticated AI to solve business problems. Companies such\\nas Microsoft and Amazon train deep-learning models of their\\nown and make them publicly available using REST APIs.\\nMicrosoft calls its suite of APIs Azure Cognitive Services.\\nYou’ll learn all about them in Chapter\\xa014.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 549, 'file_type': 'pdf'}, page_content='548\\nChapter 14. Azure Cognitive\\nServices\\nAs a child growing up in the 1960s, I idolized the Apollo\\nastronauts. Swaggering out to the launch pad and riding\\nflame-breathing rockets into space, they were my superheroes.\\nBut the group I really wanted to emulate—the people I wanted\\nto be—were the engineers in Mission Control. Seated in front\\nof their CRT screens in white shirts and black ties, chatting\\nwith the astronauts and poised to spring into action at the\\nfirst sign of trouble, they were the epitome of cool. They\\nused computers less powerful than today’s smartphones to put\\nmen on the moon—a scientific achievement that is unsurpassed\\nto this day.\\nThanks to deep learning, computers today can perform feats of\\nmagic that the engineers in Mission Control could only have\\ndreamed of. They can recognize objects in images, translate\\ntext and speech to other languages, identify people in video\\nfeeds, turn art into words and words into art, and more. But\\nstate-of-the-art deep-learning models are too complex—and\\ntoo costly—for the average engineer or software developer to\\nbuild. Microsoft reportedly spent hundreds of thousands of\\ndollars training the ResNet model that won the 2015 ImageNet\\nLarge Scale Visual Recognition Challenge. Creating that model\\nrequired a great deal of expertise, massive amounts of GPU\\ntime, and millions of images.\\nA welcome trend in AI today is AI as a service. Microsoft,\\nAmazon, Google, and other tech giants employ professional\\ndata scientists who build sophisticated deep-learning models.\\nThey train them at their own expense and make them available\\nto anyone who wishes to use them by means of REST APIs. If\\nyou can write code to send an HTTP request over the internet,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 550, 'file_type': 'pdf'}, page_content='549\\nyou can leverage these APIs to infuse AI into your apps\\nwithout taking time off from your day job to earn a PhD in\\ndeep learning.\\nMicrosoft calls its suite of AI services Azure Cognitive\\nServices. Amazon uses the name AWS AI Services. Both offer a\\nrich assortment of APIs served by deep-learning models on the\\nbackend—models that are continually refined so that they\\nbecome smarter over time. Need to caption photos uploaded to\\na website? Microsoft’s Computer Vision service can do that;\\nso can Amazon’s Rekognition service. How about building a\\nscreen reader featuring a lifelike human voice to help the\\nhearing impaired? Amazon Polly can handle that, as can Azure\\nCognitive Services’ Speech service and Google’s text-to-\\nspeech API. These are just a few examples of the actions\\ncognitive services can perform, often with just a few lines\\nof code.\\nYou saw Azure Cognitive Services at work in Chapter\\xa012 when\\nyou used the Custom Vision service to train a custom object\\ndetection model. It’s one of several services that comprise\\nthe Cognitive Services family. This chapter introduces others\\nand demonstrates how to use them and how to build solutions\\nwith them. The focus on Azure Cognitive Services isn’t meant\\nto imply that they’re better than their counterparts from\\nAmazon and Google. I’m simply more familiar with them\\nbecause I’ve worked closely with Microsoft for more than two\\ndecades. Once you learn how to call Azure Cognitive Services\\nAPIs, it’s a simple matter to apply that knowledge to\\ncognitive services from other vendors.\\nReady to make some magic happen? Let’s get started.\\nIntroducing Azure Cognitive Services\\nThe lineup of services that comprise Azure Cognitive Services\\nchanges from time to time as new ones are added and old ones\\nare deprecated or matriculated into other Microsoft product'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 551, 'file_type': 'pdf'}, page_content='550\\nlines. Figure\\xa014-1 shows the services that are currently\\noffered and divides them into four categories: vision,\\nlanguage, speech, and decision.\\nFigure 14-1. Azure Cognitive Services\\nVision services bring deep neural networks to bear on\\ncomputer-vision problems. The Custom Vision service lets you\\nbuild custom image classification and object detection\\nmodels. The Face service, which Uber uses to verify the\\nidentities of its drivers, supports state-of-the-art facial\\nrecognition systems. The Computer Vision service exposes a\\nrich API featuring a plethora of ways to analyze images and\\nextract information from them. One application for it is\\ncaptioning photos and generating keywords characterizing\\ntheir content. Figure\\xa014-2 shows a web app I built called\\nIntellipix. Intellipix captions images uploaded by users and\\nstores keywords describing them in a database so that users\\ncan easily pull up all images containing castles, for\\nexample, or photos with water in the foreground or\\nbackground.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 552, 'file_type': 'pdf'}, page_content='551\\nFigure 14-2. Intellipix showing a computer-generated image caption\\nThe language category includes services backed by deep-\\nlearning models trained to perform natural language\\nprocessing (NLP). The Language service provides APIs for\\nsentiment analysis, named-entity recognition, question\\nanswering, key-phrase extraction, language understanding, and\\nmore. Among its many uses is building chatbots that respond\\nintelligently to queries regarding a company’s product or\\nservice offerings. There’s also a Translator service that\\ntranslates text between more than 100 languages and dialects.\\nVolkswagen uses it to translate onscreen instructions in cars\\nto match the language in the owner’s manual and to ensure\\nthe quality of the documentation that they produce in more\\nthan 40 languages.\\nNOTE'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 553, 'file_type': 'pdf'}, page_content='552\\nThe Language service is a unified service that subsumes the\\nfeatures of several older services, including the Text Analytics\\nservice, the Language Understanding service, and QnA Maker.\\nMicrosoft will continue to support the older services for some\\ntime, but new development should target the Language service.\\nThe Speech service provides APIs for converting text to\\nspeech and speech to text. KPMG uses the latter to transcribe\\nrecorded calls and claims that doing so has saved its\\ncustomers millions of dollars in compliance costs. Airbus\\nuses it to build voice-enabled apps for pilot training. The\\nSpeech service also includes a speaker recognition API that\\ncan identify a person’s voice, and a speech translation API\\nthat can translate speech to dozens of other languages in\\nreal time.\\nDecision services include Anomaly Detector, which identifies\\nanomalies in live or recorded data streams and can aggregate\\ninputs from hundreds of disparate sources, such as\\ntemperature sensors and pressure gauges, and model\\nrelationships between them. (See Chapter\\xa06 for an\\nintroduction to anomaly detection.) Airbus uses Anomaly\\nDetector to analyze stresses and strains in aircraft; Siemens\\nuses it to test medical devices for flaws as the final step\\nin production. The Content Moderator service uses AI\\noptionally supplemented by human intervention to flag\\noffensive content in images and videos, profane text, and\\nother inappropriate (and potentially libelous) content. Last\\nbut not least is the Personalizer service, which is perhaps\\nbest described as a recommender system on steroids that\\nprovides personalized content and experiences to end users.\\nKeys and Endpoints'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 554, 'file_type': 'pdf'}, page_content='553\\nAzure Cognitive Services expose REST APIs that are called by\\ntransmitting HTTPS requests over the internet. The APIs are\\nlanguage agnostic: they can be called from any programming\\nlanguage that can put a call on the wire. That includes\\nPython, Java, C#, C, C++, JavaScript, Swift, Go, and\\nvirtually all other modern programming languages. Calls can\\nalso be placed with tools such as Postman and the Linux curl\\ncommand.\\nMost Azure Cognitive Services are free up to a point, but\\nthey’re not altogether free. For example, you can submit\\n5,000 text samples per month to the Language service for\\nsentiment analysis without incurring any costs. More than\\nthat, however, and Azure has to know whose Azure subscription\\nto charge. Consequently, before calling an Azure Cognitive\\nService, you need:\\n\\xa0\\nAn endpoint for the service—the URL that’s the\\ntarget of calls\\nA subscription key for the service, or some other\\nmeans of authenticating calls\\nYou can acquire both from the Azure Portal. As an example,\\nsuppose you wish to perform sentiment analysis on a\\ncollection of tweets. You first open the Azure Portal in your\\nbrowser and log in with your Microsoft account. You then\\ncreate a Cognitive Services Language resource and specify to\\nwhom the Azure subscription costs should be charged, the\\nAzure region in which the service should be located, and a\\npricing tier, as shown in Figure\\xa014-3. A free tier is\\ngenerally available if it doesn’t already exist for the same\\nservice under the same subscription.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 555, 'file_type': 'pdf'}, page_content='554\\nFigure 14-3. Creating a Language resource in the Azure Portal\\nOnce the Language resource is created, you open it in the\\nAzure Portal and click Keys and Endpoint in the menu on the\\nleft side of the page. From there, you can retrieve a\\nsubscription key and endpoint for the service (Figure\\xa014-4).\\nThe key is a string of letters and numbers that uniquely\\nidentifies an Azure subscription. Treat it with care because\\nit can cost you money if it gets out. It’s considered a best\\npractice to rotate the key (replace it with a new one)\\nperiodically in case it falls into the wrong hands. That’s\\nwhy the portal includes Regenerate Key buttons. Why does the\\nportal provide two keys? So you have a valid key to use after\\nregenerating the other one. With only one key, you’d have to\\nregenerate it and then race to modify any apps that use it,\\nand there would be a dead period in which calls placed by\\nthose apps would fail.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 556, 'file_type': 'pdf'}, page_content='555\\nFigure 14-4. Retrieving a key and an endpoint for an Azure Cognitive\\nServices resource\\nThe subscription key in this example is a single-service\\nsubscription key because it works only with the Language\\nresource that you created. If you create another Cognitive\\nServices resource—for example, one for the Computer Vision\\nservice—you get a separate key for it. So that you can avoid\\nmanaging multiple keys for multiple services, most Azure\\nCognitive Services support multiservice keys that work with a\\nrange of services. To get a multiservice key, simply create a\\nCognitive Services resource rather than a service-specific\\nresource and use the key and endpoint that the portal\\nprovides.\\nSubscription keys aren’t the only way to authenticate calls\\nto Azure Cognitive Services. Most Cognitive Services support\\nAzure Active Directory (AAD) authentication, which enhances\\nsecurity by combining AAD identities with role-based access\\ncontrol (RBAC). AAD authentication is particularly compelling\\nfor apps that rely on Cognitive Services APIs and are\\nthemselves hosted in Azure. For more information, refer to'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 557, 'file_type': 'pdf'}, page_content=\"556\\nthe article “Authenticate Requests to Azure Cognitive\\nServices”.\\nCalling Azure Cognitive Services APIs\\nOnce you have a subscription key and endpoint, you’re ready\\nto roll. Here’s a snippet of Python code that uses the\\nLanguage service to evaluate “Programming is fun, but the\\nhours are long” for sentiment. KEY is a placeholder for the\\nLanguage resource’s subscription key, while ENDPOINT is a\\nplaceholder for the corresponding endpoint. The import\\nstatement imports the Python Requests package, which\\nsimplifies the HTTP request-response protocol:\\n\\nimport\\nrequests\\n\\ninput\\n=\\n{\\n'documents':\\n[\\n\\xa0\\xa0\\xa0 {\\n'id':\\n'1000',\\n'text':\\n'Programming is fun, but the hours are long'\\n}]\\n}\\n\\nheaders\\n=\\n{\\n\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Key':\\nKEY,\\n\\xa0\\xa0\\xa0 'Content-type':\\n'application/json'\\n}\\n\\nuri\\n=\\nENDPOINT\\n+\\n'text/analytics/v3.0/sentiment'\\nresponse\\n=\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\nresults\\n=\\nresponse.json()\\n\\nfor\\nresult\\nin\\nresults['documents']:\\n\\xa0\\xa0\\xa0 print(result['confidenceScores'])\\nThe result is as follows:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 558, 'file_type': 'pdf'}, page_content=\"557\\n\\n{'positive': 0.94, 'neutral': 0.05, 'negative': 0.01}\\nThe service returns three scores: one for positive sentiment,\\none for neutral sentiment, and one for negative sentiment.\\nObserve that you can submit multiple text samples in one call\\nto the API because the documents item in the dictionary\\npassed to the call is a Python list.\\nIt might not be obvious from the code, but input must be JSON\\nencoded and sent in the body of the request. The output comes\\nback as JSON too. The Requests package simplifies JSON\\nencoding and decoding. requests.post encodes the input and\\ntransmits a POST request to the designated endpoint, while\\nresponse.json converts the JSON returned in the response into\\na Python dictionary.\\nTo simplify matters, and to insulate you from changes to the\\nunderlying APIs as they evolve over time, Microsoft offers\\nfree software development kits (SDKs) for most Cognitive\\nServices. The Python package named Azure-ai-textanalytics is\\nthe Python SDK for sentiment analysis and other text\\nanalytics and is formally known as the Azure Cognitive\\nServices Text Analytics client library for Python. It can be\\ninstalled just like any other Python package—for example,\\nwith a pip install command. Here’s the previous sample\\nrewritten to use the SDK:\\n\\nfrom\\nazure.core.credentials\\nimport\\nAzureKeyCredential\\nfrom\\nazure.ai.textanalytics\\nimport\\nTextAnalyticsClient\\n\\nclient\\n=\\nTextAnalyticsClient(ENDPOINT,\\nAzureKeyCredential(KEY))\\ninput\\n=\\n[{\\n'id':\\n'1000',\\n'text':\\n'Programming is fun, but the hours are long'\\n}]\\nresponse\\n=\\nclient.analyze_sentiment(input)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 559, 'file_type': 'pdf'}, page_content=\"558\\nfor\\nresult\\nin\\nresponse:\\n\\xa0\\xa0\\xa0 print(result.confidence_scores)\\nThe output is exactly the same, but the JSON is hidden away,\\nand you don’t have to know what magic string to append to\\nthe endpoint because analyze_sentiment does it for you. That\\nmethod belongs to the SDK’s Text\\u200bAna\\u2060lyticsClient class, and\\nit’s one of several methods available for analyzing text.\\nAnother benefit of using the SDKs is more robust error\\nhandling. Calls can and sometimes do fail, and a well-written\\napp responds gracefully to such failures. When a call to\\nAzure Cognitive Services fails, code in the SDK throws an\\nexception that you can catch in your code. The following\\nexample responds to errors by printing the error message\\ncontained in the exception object:\\n\\nfrom\\nazure.core.credentials\\nimport\\nAzureKeyCredential\\nfrom\\nazure.ai.textanalytics\\nimport\\nTextAnalyticsClient\\nfrom\\nazure.core.exceptions\\nimport\\nAzureError\\n\\ntry:\\n\\xa0\\xa0\\xa0 client\\n=\\nTextAnalyticsClient(ENDPOINT,\\nAzureKeyCredential(KEY))\\n\\xa0\\xa0\\xa0 input\\n=\\n[{\\n'id':\\n'1000',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'text':\\n'Programming is fun, but the hours are long'\\n}]\\n\\xa0\\xa0\\xa0 response\\n=\\nclient.analyze_sentiment(input)\\n\\n\\xa0\\xa0\\xa0 for\\nresult\\nin\\nresponse:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(result.confidence_scores)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\nexcept\\nAzureError\\nas\\ne:\\n\\xa0\\xa0\\xa0 print(e.message)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 560, 'file_type': 'pdf'}, page_content='559\\nAzureError is defined in Azure-core, which is a library\\nshared by all Azure Cognitive Services Python SDKs. It’s the\\nparent class for other exception classes that correspond to\\nspecific errors. If, for example, you pass a subscription key\\nthat’s invalid (perhaps because the corresponding\\nsubscription is no longer active or payment is past due), a\\nClientAuthenticationError exception occurs. You can include as\\nmany exception handlers as you’d like to respond to specific\\ntypes of errors. The preceding example uses AzureError as a\\ncatchall.\\nCognitive Services SDKs are available for a variety of\\nprogramming languages,\\xa0including Python, Java, and C#.\\nHere’s the same sample written in C# using the Azure\\nCognitive Services Text Analytics client library for .NET,\\nwhich comes in the form of a NuGet package named\\nAzure.AI.TextAnalyt\\u2060ics:\\n\\nusing\\nAzure;\\nusing\\nAzure.AI.TextAnalytics;\\nusing\\nSystem;\\n\\ntry\\n{\\n\\xa0\\xa0\\xa0 var\\nclient\\n=\\nnew\\nTextAnalyticsClient(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 new\\nUri(ENDPOINT),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 new\\nAzureKeyCredential(KEY)\\n\\xa0\\xa0\\xa0 );\\n\\n\\xa0\\xa0\\xa0 var\\nresponse\\n=\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 client.AnalyzeSentiment(\"Programming is fun, but the hours are long\");\\n\\n\\xa0\\xa0\\xa0 var\\nsentiment\\n=\\nresponse.Value.ConfidenceScores.Positive;\\n\\xa0\\xa0\\xa0 Console.WriteLine(sentiment);\\n}'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 561, 'file_type': 'pdf'}, page_content='560\\ncatch\\n(Exception\\nex)\\n{\\n\\xa0\\xa0\\xa0 Console.WriteLine(ex.Message);\\n}\\nThe output from the code is 0.94, which is the same positive-\\nsentiment score output by the Python examples. Different\\nlanguage, different SDK, but same result, and all with very\\nlittle effort. That’s what Azure Cognitive Services are all\\nabout.\\nAzure Cognitive Services Containers\\nCognitive Services vastly simplify the process of—and lower\\nthe skills barrier for—infusing AI into the apps that you\\nwrite. But they have drawbacks too:\\n\\xa0\\nBecause Azure Cognitive Services run in the\\ncloud, an app that uses them requires an internet\\nconnection.\\nCalls that travel over the internet incur higher\\nlatencies than calls performed locally.\\nAzure Cognitive Services APIs evolve over time\\nand sometimes introduce breaking changes, which\\nmeans code that worked just fine yesterday could\\nbehave differently or be inoperative tomorrow.\\nThe deep-learning models on the backend are\\ncontinually refined and improved, with the result\\nthat the sentiment score for “programming is\\nfun, but the hours are long” could be 0.94 today\\nand 0.85 tomorrow.\\nChanges to the APIs are rarely abrupt. Microsoft usually\\nwarns its customers months in advance before making breaking'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 562, 'file_type': 'pdf'}, page_content='561\\nchanges to an API, and in many cases, you can specify the\\nversion of the service or API you wish to use and insulate\\nyourself from future changes. These issues might not be a big\\ndeal to small firms, but for enterprises, which value\\nstability and reliability above all else, they can be deal\\nbreakers.\\nMicrosoft has addressed these issues by making most Azure\\nCognitive Services available in Docker containers. A\\ncontainerized service can run on premises or in the cloud,\\nand it locks in API versions and the models that back them.\\nNo changes occur unless you update a container image or\\nreplace it with a newer version. You can learn more and view\\nthe latest list of services that are available in\\ncontainerized form in “What Are Azure Cognitive Services\\nContainers?”. Containerized services are also a solution to\\nsecurity and privacy policies that require data to stay on\\npremises. Microsoft doesn’t store data passed to Cognitive\\nServices, but the data does leave your company’s domain.\\nContainerized services are not a way to do an end run around\\nthe billing department and use Azure Cognitive Services for\\nfree. From the documentation:\\nThe Cognitive Services containers are required to submit\\nmetering information for billing purposes. Failure to\\nallow-list various network channels that the Cognitive\\nServices containers rely on will prevent the container\\nfrom working.\\nContainers send encrypted usage information back to Microsoft\\nthrough port 443, and Microsoft uses that information to bill\\nan Azure subscription. Consequently, containers have to be\\nconnected to the internet even if they’re hosted on\\npremises.\\nThere is one exception. If you want to use Azure Cognitive\\nServices in apps that can’t be connected to the internet for\\ntechnical or compliance reasons, disconnected containers can\\nrun absent an internet connection, and they don’t transmit'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 563, 'file_type': 'pdf'}, page_content='562\\nmetering information to Microsoft. Not just anyone can use\\nthem, however. You have to submit an application to Microsoft\\nfor approval, and one of the requirements is that your\\norganization have an Enterprise Agreement (EA) in place with\\nMicrosoft. See “Use Docker Containers in Disconnected\\nEnvironments” for more information and for an up-to-date\\nlist of Azure Cognitive Services that are available for use\\nin disconnected scenarios.\\nThe Computer Vision Service\\nBy now you’re probably ready to stop talking about Azure\\nCognitive Services and write some code. So am I. Let’s start\\nwith perhaps the most feature-rich cognitive service of all.\\nThe Computer Vision service is one of three vision services\\nin Azure Cognitive Services. It exposes a set of APIs that\\nsupport a variety of tasks, including:\\n\\xa0\\nCaptioning images\\nGenerating tags describing the contents of an\\nimage\\nIdentifying objects (and their bounding boxes) in\\nimages\\nDetecting sensitive or inappropriate content in\\nimages\\nExtracting text from images\\nDetecting faces in images (a subset of the Face\\nservice)\\nGenerating “smart thumbnail” images by using AI\\nto identify the subject of a photo and creating a\\nthumbnail version that’s centered on the subject\\nPerforming spatial analysis on video streams'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 564, 'file_type': 'pdf'}, page_content='563\\nSpatial analysis is the latest addition to the Computer\\nVision service. Among other things, it can track people in\\nlive video feeds, measure distances between them in real\\ntime, and determine who’s wearing a mask (and who is not),\\nas shown in Figure\\xa014-5. It’s currently in public preview,\\nand you can read all about it in “What Is Spatial\\nAnalysis?”.\\nFigure 14-5. Using spatial analysis to verify social distancing and masking\\n(images © Microsoft; used with permission)\\nThe best way to get acquainted with the Computer Vision\\nservice is to call a few of its APIs. First install the Azure\\nCognitive Services Computer Vision SDK for Python. (It’s in\\na Python package named Azure-cognitiveservices-vision-\\ncomputervision.) Next, download a ZIP file containing a few\\nsample images and copy the images into the Data subdirectory\\nwhere your Jupyter notebooks are hosted. Use the Azure Portal\\nto create a Computer Vision resource and obtain a key and an\\nendpoint. Then create a new notebook and run the following\\ncode in the first cell after replacing KEY with the key and\\nENDPOINT with the endpoint:\\n\\nfrom\\nazure.cognitiveservices.vision.computervision\\nimport\\nComputerVisionClient\\nfrom\\nmsrest.authentication\\nimport\\nCognitiveServicesCredentials\\n\\nclient\\n=\\nComputerVisionClient(ENDPOINT,\\nCognitiveServicesCredentials(KEY))'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 565, 'file_type': 'pdf'}, page_content=\"564\\nThe ComputerVisionClient class implements several methods\\nthat you can call to invoke Computer Vision APIs. Most of\\nthose methods come in two versions: one that accepts an image\\nURL and one that accepts an actual image. The describe_image\\nmethod, for example, accepts an image URL, while the\\ndescribe_image_in_stream method accepts a stream containing\\nthe image. My examples use the in_stream methods, but you can\\neasily modify them to pass image URLs rather than images.\\nNow load one of the sample images you copied from the ZIP\\nfile and use describe_image_in_stream to caption it:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n\\nimage\\n=\\nplt.imread('Data/dubai.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\n\\nwith\\nopen('Data/dubai.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\n=\\nclient.describe_image_in_stream(image)\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 for\\ncaption\\nin\\nresult.captions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f'{caption.text} ({caption.confidence:.1%})')\\nConfirm that the output is as follows:\\n\\nA man riding a sand dune (53.8%)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 566, 'file_type': 'pdf'}, page_content=\"565\\ndescribe_image_in_stream returns zero or more captions, each\\nwith a score reflecting the Computer Vision service’s\\nconfidence that the caption is accurate. In this example, it\\nreturned just one caption with a confidence of 53.8%.\\nIn addition to captioning images, the Computer Vision service\\ncan generate a list of keywords (“tags”) describing an\\nimage’s content. One use for such tags is to make an image\\ndatabase searchable. Use the tag_image_in_stream method to\\ntag the image captioned in the previous example:\\n\\nwith\\nopen('Data/dubai.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\n=\\nclient.tag_image_in_stream(image)\\n\\n\\xa0\\xa0\\xa0 for\\ntag\\nin\\nresult.tags:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f'{tag.name} ({tag.confidence:.1%})')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 567, 'file_type': 'pdf'}, page_content=\"566\\nHere are the tags and the confidence levels assigned to them:\\n\\ndune (99.5%)\\nsky (99.2%)\\noutdoor (98.7%)\\nclothing (98.2%)\\ndesert (98.1%)\\nsand (97.9%)\\naeolian landform (96.9%)\\nperson (96.1%)\\nsinging sand (95.8%)\\nerg (94.0%)\\nsahara (93.6%)\\nnature (93.4%)\\nfootwear (90.9%)\\nlandscape (88.0%)\\nsand dune (83.5%)\\nground (77.5%)\\nThe Computer Vision service can also detect objects in\\nimages. The following statements load an image and show all\\nthe objects that were detected along with bounding boxes and\\nconfidence scores:\\n\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\n\\ndef\\nannotate_object(name,\\nconfidence,\\nbbox,\\nmin_confidence=0.5):\\n\\xa0\\xa0\\xa0 if\\n(confidence\\n>\\nmin_confidence):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\n=\\nbbox.x,\\nbbox.y,\\nbbox.w,\\nbbox.h\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n= f'{name} ({confidence:.1%})'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.text(x\\n+\\n(w\\n/\\n2),\\ny,\\ntext,\\ncolor='white',\\nbackgroundcolor='red',\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 568, 'file_type': 'pdf'}, page_content=\"567\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ha='center',\\nva='bottom',\\nfontweight='bold',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 bbox=dict(color='red'))\\n\\nimage\\n=\\nplt.imread('Data/xian.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\nwith\\nopen('Data/xian.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\n=\\nclient.detect_objects_in_stream(image)\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 for\\nobject\\nin\\nresult.objects:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 annotate_object(object.object_property,\\nobject.confidence,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 object.rectangle)\\ndetect_objects_in_stream detected the people and the bicycles\\nin the photo, as well as a few other items:\\nCan the Computer Vision service detect faces in photos too?\\nIt certainly can. It can also provide information about age\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 569, 'file_type': 'pdf'}, page_content=\"568\\nand gender. The following example annotates the faces in a\\nphoto with labels denoting age and gender. The\\nComputerVisionClient class lacks a dedicated method for\\ndetecting faces, but you can call the general-purpose\\nanalyze_image_in_stream method with a visual\\u200b_features\\nparameter requesting facial info:\\n\\nfrom\\nazure.cognitiveservices.vision.computervision.models \\\\\\nimport\\nVisualFeatureTypes\\n\\ndef\\nannotate_face(face):\\n\\xa0\\xa0\\xa0 x,\\ny\\n=\\nface.face_rectangle.left,\\nface.face_rectangle.top\\n\\xa0\\xa0\\xa0 w,\\nh\\n=\\nface.face_rectangle.width,\\nface.face_rectangle.height\\n\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\xa0\\xa0\\xa0 text\\n= f'{face.gender} ({face.age})'\\n\\xa0\\xa0\\xa0 ax.text(x\\n+\\n(w\\n/\\n2),\\ny,\\ntext,\\ncolor='white',\\nbackgroundcolor='red',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ha='center',\\nva='bottom',\\nfontweight='bold',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 bbox=dict(color='red'))\\n\\nimage\\n=\\nplt.imread('Data/amsterdam.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\n\\nwith\\nopen('Data/amsterdam.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\n=\\nclient.analyze_image_in_stream(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 image,\\nvisual_features=[VisualFeatureTypes.faces])\\n\\n\\xa0\\xa0\\xa0 for\\nface\\nin\\nresult.faces:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 annotate_face(face)\\nHere’s the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 570, 'file_type': 'pdf'}, page_content='569\\nNOTE\\nThe Face service can detect faces and identify ages, genders,\\nemotions, facial landmarks, and more. It also has methods for\\ncomparing facial images, identifying faces, and building facial\\nrecognition systems. Its API is a superset of the Computer Vision\\nservice’s face API. As a result of Microsoft’s Responsible AI\\ninitiative, however, the Face service is no longer available to\\nthe general public. For more information about the Face service\\nand how to apply for access to it, refer to “What Is the Azure\\nFace Service?”.\\nSuppose you’re the proprietor of a public website that\\naccepts photo uploads and you’d like to reject photos'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 571, 'file_type': 'pdf'}, page_content=\"570\\ncontaining inappropriate content. Called as follows,\\nanalyze_image_in_stream scores a photo for adultness (does\\nthe photo contain nudity?), raciness (does it contain bare\\nskin?), and goriness (does it contain blood and gore?) on a\\nscale of 0.0 to 1.0. Here’s how it responds to a photo of a\\nyoung girl cliff-jumping in her bathing suit in Hawaii:\\n\\nimage\\n=\\nplt.imread('Data/maui.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\n\\nwith\\nopen('Data/maui.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\n=\\nclient.analyze_image_in_stream(image,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 visual_features=[VisualFeatureTypes.adult])\\n\\n\\xa0\\xa0\\xa0 print(f'Adultness: {result.adult.adult_score}')\\n\\xa0\\xa0\\xa0 print(f'Raciness: {result.adult.racy_score}')\\n\\xa0\\xa0\\xa0 print(f'Goriness: {result.adult.gore_score}')\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 print(f'Is adult: {result.adult.is_adult_content}')\\n\\xa0\\xa0\\xa0 print(f'Is racy: {result.adult.is_racy_content}')\\n\\xa0\\xa0\\xa0 print(f'Is gory: {result.adult.is_gory_content}')\\nHere’s the output:\\n\\nAdultness: 0.02214685082435608\\nRaciness: 0.4205135107040405\\nGoriness: 0.0016634463099762797\\nIs adult: False\\nIs racy: False\\nIs gory: False\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 572, 'file_type': 'pdf'}, page_content=\"571\\nThe bikini in the photo yielded a moderate raciness score,\\nbut one that’s below the threshold of 0.5 required for\\nis_racy_content to be True. Based on the other scores it\\nreturned, the Computer Vision service believes that the photo\\nis neither “adult” nor gory.\\nYet another capability that the Computer Vision service lends\\nto application developers is using AI to extract text from\\nphotos. It’s perfect for digitizing printed documents.\\nHere’s an example:\\n\\ndef\\ndraw_box(bbox):\\n\\xa0\\xa0\\xa0 vals\\n=\\nbbox.split(',')\\n\\xa0\\xa0\\xa0 x\\n=\\nint(vals[0])\\n\\xa0\\xa0\\xa0 y\\n=\\nint(vals[1])\\n\\xa0\\xa0\\xa0 w\\n=\\nint(vals[2])\\n\\xa0\\xa0\\xa0 h\\n=\\nint(vals[3])\\n\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 573, 'file_type': 'pdf'}, page_content=\"572\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\nimage\\n=\\nplt.imread('Data/1040-es.jpg')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={'xticks':\\n[],\\n'yticks':\\n[]})\\nax.imshow(image)\\n\\nwith\\nopen('Data/1040-es.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\n=\\nclient.recognize_printed_text_in_stream(image)\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 for\\nregion\\nin\\nresult.regions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nline\\nin\\nregion.lines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n=\\n' '.join([word.text\\nfor\\nword\\nin\\nline.words])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 draw_box(line.bounding_box)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(text)\\nHere’s the output, minus all the lines of text output by the\\nprint statement:\\nThe recognize_printed_text_in_stream method only recognizes\\nprinted text. A related method named read_in_stream\\nrecognizes handwritten text too. Let’s see how it performs\\non the same scanned document:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 574, 'file_type': 'pdf'}, page_content='573\\n\\nimport\\ntime\\nfrom\\nazure.cognitiveservices.vision.computervision.models \\\\\\nimport\\nOperationStatusCodes\\n\\ndef\\ndraw_box(bbox):\\n\\xa0\\xa0\\xa0 x,\\ny\\xa0 =\\nbbox[0],\\nbbox[1]\\n\\xa0\\xa0\\xa0 w\\n=\\nbbox[4]\\n-\\nx\\n\\xa0\\xa0\\xa0 h\\n=\\nbbox[5]\\n-\\ny\\n\\xa0\\xa0\\xa0 rect\\n=\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor=\\'red\\',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\nimage\\n=\\nplt.imread(\\'Data/1040-es.jpg\\')\\nfig,\\nax\\n=\\nplt.subplots(figsize=(12,\\n8),\\nsubplot_kw={\\'xticks\\':\\n[],\\n\\'yticks\\':\\n[]})\\nax.imshow(image)\\n\\nwith\\nopen(\\'Data/1040-es.jpg\\',\\nmode=\\'rb\\')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 response\\n=\\nclient.read_in_stream(image,\\nraw=True)\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 location\\n=\\nresponse.headers[\"Operation-Location\"]\\n\\xa0\\xa0\\xa0 opid\\n=\\nlocation[len(location)\\n-\\n36:]\\n\\xa0\\xa0\\xa0 results\\n=\\nclient.get_read_result(opid)\\n\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 while\\nresults.status\\n==\\nOperationStatusCodes.running:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 results\\n=\\nclient.get_read_result(opid)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 time.sleep(1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 if\\nresults.status\\n==\\nOperationStatusCodes.succeeded:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nresult\\nin\\nresults.analyze_result.read_results:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nline\\nin\\nresult.lines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 draw_box(line.bounding_box)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(line.text)\\nThis example is a little more involved because read_in_stream\\nreturns before the call has completed. We therefore loop'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 575, 'file_type': 'pdf'}, page_content=\"574\\nuntil the call completes and then retrieve the results. The\\ncall to sleep in each iteration of the while loop allows the\\nuser interface to remain responsive while waiting for the\\ncall to complete. The results are worth the wait:\\nPerhaps you’d prefer to extract only handwritten text from a\\ndocument. You can do that too, because for each line of text\\nit detects, read_in_stream includes a style attribute equal\\nto “handwriting” for handwritten text. To demonstrate,\\nreplace the final two lines in the previous example with\\nthese:\\n\\nif\\n(line.appearance.style.name\\n==\\n'handwriting'):\\n\\xa0\\xa0\\xa0 draw_box(line.bounding_box)\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 print(line.text)\\nThis time, only handwritten text is highlighted:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 576, 'file_type': 'pdf'}, page_content='575\\nread_in_stream does a commendable job of converting the\\nhandwritten text into Python strings too:\\n\\n$ 100\\nJeff\\nProsise\\n111-22-0000\\n1313 Mockingbird Lane\\nOak Ridge\\nTN\\n37830\\nA final note regarding the Computer Vision service is that it\\ndoesn’t accept images larger than 4 MB. Anything larger\\nproduces a ComputerVisionErrorResponse\\u200bExcep\\u2060tion. You can catch\\nthese exceptions (or AzureError exceptions, which are higher\\nup the food chain) and recover gracefully, or you can check\\nan image’s size before submitting it and downsize it if\\nnecessary.\\nThe Language Service'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 577, 'file_type': 'pdf'}, page_content=\"576\\nThe Computer Vision service employs deep-learning models\\nsimilar to the ones you learned about in Chapters 10, 11, and\\n12. The Language and Translator services use models like the\\nones in Chapter\\xa013. The latter two services embody NLP:\\nsentiment analysis, neural machine translation, question\\nanswering, and more. The Text\\u200bAnaly\\u2060ticsClient class in the\\nPython text analytics SDK provides a convenient interface to\\nmany of the Language service’s features. You’ve already\\nseen it used for sentiment analysis. Here’s another example\\nthat applies sentiment analysis to multiple text samples with\\none round trip to the Language service:\\n\\nfrom\\nazure.core.credentials\\nimport\\nAzureKeyCredential\\nfrom\\nazure.ai.textanalytics\\nimport\\nTextAnalyticsClient\\n\\nclient\\n=\\nTextAnalyticsClient(ENDPOINT,\\nAzureKeyCredential(KEY))\\n\\ninput\\n=\\n[\\n\\xa0\\xa0\\xa0 {\\n'id':\\n'1000',\\n'text':\\n'Programming is fun, but the hours are long'\\n},\\n\\xa0\\xa0\\xa0 {\\n'id':\\n'1001',\\n'text':\\n'Great food and excellent service'\\n},\\n\\xa0\\xa0\\xa0 {\\n'id':\\n'1002',\\n'text':\\n'The product worked as advertised but is ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'overpriced'\\n},\\n\\xa0\\xa0\\xa0 {\\n'id':\\n'1003',\\n'text':\\n'Moving to the cloud was the best decision ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'we ever made'\\n},\\n\\xa0\\xa0\\xa0 {\\n'id':\\n'1004',\\n'text':\\n'Programming is so fun I\\\\'d do it for free. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Don\\\\'t tell my boss!'\\n}\\n]\\nresponse\\n=\\nclient.analyze_sentiment(input)\\n\\nfor\\nresult\\nin\\nresponse:\\n\\xa0\\xa0\\xa0 text\\n=\\n''.join([x.text\\nfor\\nx\\nin\\nresult.sentences])\\n\\xa0\\xa0\\xa0 print(f'{text} => {result.confidence_scores.positive}')\\nHere’s the output:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 578, 'file_type': 'pdf'}, page_content=\"577\\n\\nProgramming is fun, but the hours are long => 0.94\\nGreat food and excellent service => 1.0\\nThe product worked as advertised but is overpriced => 0.0\\nMoving to the cloud was the best decision we ever made => 1.0\\nProgramming is so fun I'd do it for free. Don't tell my boss! => 1.0\\nLet’s say you’re writing an app that collects tweets\\nreferencing your company and analyzes them for sentiment. The\\nidea is that if sentiment turns negative, you can give the\\nmarketing department a heads-up. It’s faster and more\\nefficient to place one call to Azure Cognitive Services and\\nanalyze 100 tweets than to make 100 calls analyzing one tweet\\nat a time.\\nSentiment analysis is one of several operations supported by\\nthe TextAnalytic\\u2060s\\u200bCli\\u2060ent class. Another is named-entity\\nrecognition. Suppose you’re building a system that sorts and\\nprioritizes support tickets received by your company’s help\\ndesk. The recognize_entities method extracts entities such as\\npeople, places, organizations, dates and times, and\\nquantities from input text. It reveals the entity types as\\nwell:\\n\\ndocuments\\n=\\n[\\n\\xa0\\xa0\\xa0 'My printer isn\\\\'t working. Can someone from IT come to my office ' \\\\\\n\\xa0\\xa0\\xa0 'and have a look?'\\n]\\n\\nresults\\n=\\nclient.recognize_entities(documents)\\n\\nfor\\nresult\\nin\\nresults:\\n\\xa0\\xa0\\xa0 for\\nentity\\nin\\nresult.entities:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f'{entity.text} ({entity.category})')\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 579, 'file_type': 'pdf'}, page_content=\"578\\nThe output from this example is as follows:\\n\\nprinter (Product)\\nIT (Skill)\\noffice (Location)\\nA related method named recognize_pii_entities extracts\\npersonally identifiable information (PII) entities such as\\nbank account info, Social Security numbers, and credit card\\nnumbers from text input to it, while the extract_key_phrases\\nmethod extracts key phrases:\\n\\ndocuments\\n=\\n[\\n\\xa0\\xa0\\xa0 'Natural Language Processing, or NLP, encompasses a variety of ' \\\\\\n\\xa0\\xa0\\xa0 'activities including text classification, keyword extraction,' \\\\\\n\\xa0\\xa0\\xa0 'named-entity recognition, question answering, and language '\\\\\\n\\xa0\\xa0\\xa0 'translation.'\\n]\\n\\nresults\\n=\\nclient.extract_key_phrases(documents)\\n\\nfor\\nresult\\nin\\nresults:\\n\\xa0\\xa0\\xa0 for\\nphrase\\nin\\nresult.key_phrases:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(phrase)\\nHere’s the output:\\n\\nNatural Language Processing\\nlanguage translation\\ntext classification\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 580, 'file_type': 'pdf'}, page_content='579\\nkeyword extraction\\nquestion answering\\nNLP\\nvariety\\nactivities\\nrecognition\\nThis example is a simple one given that the text is so brief,\\nbut you could pass in hundreds of large documents and use the\\nresults to get a snapshot of each document’s content, or\\ngroup documents that contain similar keywords.\\nTextAnalyticsClient provides a wrapper around text analytics\\nAPIs. Other Python SDKs unlock additional features of the\\nLanguage service. For example, the question-answering SDK\\nprovides APIs for answering questions from manuals, FAQs,\\nblog posts, and other documents that you provide. For more\\ninformation, and to see this aspect of the Language service\\nin action, refer to the article titled “Azure Cognitive\\nLanguage Services Question Answering Client Library for\\nPython”.\\nThe Translator Service\\nThe Translator service uses state-of-the-art neural machine\\ntranslation to translate text between dozens of languages. It\\ncan also identify written languages. Suppose your objective\\nis to translate into English questions written in other\\nlanguages and submitted through your company’s website.\\nFirst you need to determine whether the source language is\\nEnglish. If it’s not, you want to translate it so that you\\ncan respond to the customer’s request.\\nThe following code analyzes a text sample and shows the\\nlanguage it’s written in. Microsoft doesn’t currently offer\\na Python SDK for the Translator service, but you can use\\nPython’s Requests package to simplify calls. To demonstrate,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 581, 'file_type': 'pdf'}, page_content=\"580\\ncreate a Translator resource in the Azure Portal and grab the\\nsubscription key (the one labeled “Text Translation”),\\nendpoint, and region. Then run the following code, replacing\\nKEY and ENDPOINT with the key and endpoint and REGION with\\nthe Azure region you selected (for example, southcentralus):\\n\\nimport\\nrequests\\n\\ninput\\n=\\n[{\\n'text':\\n'Quand votre nouveau livre sera-t-il disponible?'\\n}]\\n\\nheaders\\n=\\n{\\n\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Key':\\nKEY,\\n\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Region':\\nREGION,\\n\\xa0\\xa0\\xa0 'Content-type':\\n'application/json'\\n}\\n\\nuri\\n=\\nENDPOINT\\n+\\n'detect?api-version=3.0&to=en'\\nresponse\\n=\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\nresults\\n=\\nresponse.json()\\n\\nprint(results[0]['language'])\\nThe output is fr for French. Now that you’ve determined the\\nsource language isn’t English, you can translate it this\\nway:\\n\\nuri\\n=\\nENDPOINT\\n+\\n'translate?api-version=3.0&from=fr&to=en'\\nresponse\\n=\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\nresults\\n=\\nresponse.json()\\n\\nprint(results[0]['translations'][0]['text'])\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 582, 'file_type': 'pdf'}, page_content=\"581\\nThe translated text is:\\n\\nWhen will your new book be available?\\nYou can omit from=fr from the URL and allow the Translator\\nservice to detect the language for you. You can also detect\\nthe language and translate the text with one call. If you\\ncall the translate endpoint without a from parameter, the\\nreturn value includes a detectedLanguage item that identifies\\nthe language in the source text:\\n\\n[{'detectedLanguage': {'language': 'fr', 'score': 1.0}, 'translations':\\n[{'text':\\n'When will your new book be available?', 'to': 'en'}]}]\\nIf passed a list containing multiple text samples, the\\nTranslator service will translate all of them in one call. It\\nalso supports transliteration: translating text to other\\nalphabets. To see for yourself, use Google Translate to\\ntranslate “When will your new book be available” to Thai or\\nHindi. Then paste the Thai or Hindi text over the French text\\nin the previous example and run it again. Be sure to also\\nchange from=fr to from=th or from=hi or simply remove the from\\nparameter altogether.\\nTIP\\nWhen you create a Translator resource, you have a choice of\\ncreating a global resource or one tied to a specific Azure region.\\nThe preceding examples assume that you created it as a regional\\nresource. If you created a global Translator resource instead, you\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 583, 'file_type': 'pdf'}, page_content='582\\ncan set the Ocp-Apim-Subscription-Region header to global or omit the\\nheader altogether.\\nWhen you obtained an endpoint for the Translator service, did\\nyou notice that the Azure Portal offered two endpoints—one\\nfor “Text Translation” and another for “Document\\nTranslation?” That’s because the Translator service\\nfeatures a second API for translating entire documents,\\nincluding PDFs. There’s even a Python SDK to help out. The\\nonly catch is that documents must first be uploaded to Azure\\nblob storage. For examples showing the document translation\\nAPI in action, see “Azure Document Translation Client\\nLibrary for Python”. The API is asynchronous and can process\\nbatches of documents in one call, so it’s ideal not just for\\ntranslating individual documents, but for translating large\\nvolumes of documents at scale.\\nThe Speech Service\\nOne of the more challenging tasks for deep-learning models is\\nprocessing human speech. Azure Cognitive Services includes a\\nSpeech service that converts text to speech, speech to text,\\nand more. It’s even capable of captioning recorded videos\\nand live video streams and filtering out profanity as it\\ndoes. A Python SDK simplifies the code you write and makes it\\nremarkably easy to incorporate speech into your apps.\\nTo demonstrate, install the package named Azure-\\ncognitiveservices-speech containing the Python Speech SDK.\\nUse the Azure Portal to create a Cognitive Services Speech\\nresource and make note of the subscription key and service\\nregion. Then create a Jupyter notebook and run the following\\ncode in the first cell after replacing KEY with the\\nsubscription key and REGION with the region you selected:'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 584, 'file_type': 'pdf'}, page_content=\"583\\n\\nfrom\\nazure.cognitiveservices\\nimport\\nspeech\\n\\nspeech_config\\n=\\nspeech.SpeechConfig(KEY,\\nREGION)\\nspeech_config.speech_recognition_language\\n=\\n'en-US'\\nNow run the following statements, and when prompted, speak\\ninto your microphone. This sample creates a SpeechRecognizer\\nobject and uses its recognize_once_async method to convert up\\nto 30 seconds of live audio from your PC’s default\\nmicrophone into text. Observe that the text doesn’t appear\\nuntil you’ve finished speaking:\\n\\nrecognizer\\n=\\nspeech.SpeechRecognizer(speech_config)\\n\\nprint('Speak into your microphone')\\nresult\\n=\\nrecognizer.recognize_once_async().get()\\n\\nif\\nresult.reason\\n==\\nspeech.ResultReason.RecognizedSpeech:\\n\\xa0\\xa0\\xa0 print(result.text)\\nIt couldn’t be much simpler than that. How about converting\\ntext to speech? Here’s an example that uses the SDK’s\\nSpeechSynthesizer class to vocalize a sentence. The\\nsynthesized voice belongs to an English speaker named Jenny\\n(en-US-JennyNeural), and it’s one of more than 300 neural\\nvoices you can choose from:\\n\\nspeech_config.speech_synthesis_voice_name\\n=\\n'en-US-JennyNeural'\\nsynthesizer\\n=\\nspeech.SpeechSynthesizer(speech_config)\\nsynthesizer.speak_text_async('When will your new book be published?').get()\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 585, 'file_type': 'pdf'}, page_content=\"584\\nAll of the “speakers” are multilingual. If you ask a French\\nspeaker—for example, fr-FR-CelesteNeural—to synthesize an\\nEnglish sentence, the vocalization will feature a French\\naccent.\\nYou can combine a TranslationRecognizer object with a\\nSpeechSynthesizer object to translate speech in real time.\\nThe following example takes spoken English as input and plays\\nit back in French using the voice of a native French speaker:\\n\\nspeech_config.speech_synthesis_voice_name\\n=\\n'fr-FR-YvetteNeural'\\nsynthesizer\\n=\\nspeech.SpeechSynthesizer(speech_config)\\n\\ntranslation_config\\n=\\nspeech.translation.SpeechTranslationConfig(KEY,\\nREGION)\\ntranslation_config.speech_recognition_language\\n=\\n'en-US'\\ntranslation_config.add_target_language('fr')\\n\\nrecognizer\\n=\\nspeech.translation.TranslationRecognizer(translation_config)\\n\\nprint('Speak into your microphone')\\nresult\\n=\\nrecognizer.recognize_once_async().get()\\n\\nif\\nresult.reason\\n==\\nspeech.ResultReason.TranslatedSpeech:\\n\\xa0\\xa0\\xa0 text\\n=\\nresult.translations['fr']\\n\\xa0\\xa0\\xa0 synthesizer.speak_text_async(text).get()\\nThese samples use your PC’s default microphone for voice\\ninput and default speakers for output. You can specify other\\nsources of input and output by passing an Audio\\u200bCon\\u2060fig object\\nto the methods that create SpeechRecognizer,\\nSpeechSynthesizer, and TranslationRecognizer objects. Among\\nthe options this enables is using a file or stream rather\\nthan a microphone as the source of input.\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 586, 'file_type': 'pdf'}, page_content='585\\nPutting It All Together: Contoso Travel\\nImagine you’re a web developer and your client is Contoso\\nTravel. To motivate its customers to stay in touch, the\\ntravel agency wants its website to include a service that\\ntranslates signage. The idea is that a customer exploring a\\nfaraway land can snap a picture of a sign, upload it to the\\nsite, and see a translation in the language of their choice.\\nNo typing, no forms to fill out—just “Here’s a picture,\\ntell me what it says.”\\nA few years ago, such a website would have been unthinkable\\nfor most small businesses. Today it’s within the\\ncapabilities of anyone who can sling a few lines of code.\\nLet’s use Python’s Flask framework to build a website that\\nmakes your client happy. We’ll use Azure Cognitive Services\\nto do the heavy lifting of extracting and translating text,\\nand we’ll end up with the product shown in Figure\\xa014-6.\\nBegin by making sure the required packages are installed,\\nincluding Flask, Requests, and Azure-cognitiveservices-\\nvision-computervision. Then create a project directory in the\\nlocation of your choice on your hard disk. Name it Contoso or\\nanything else you’d like. Next, download a ZIP file\\ncontaining a starter kit for the website and copy its\\ncontents into the project directory. Take a moment to examine\\nthe files that you copied. These files comprise a website\\nwritten in Python and Flask. They include the following:\\napp.py\\nHolds the Python code that drives the site\\ntemplates/index.xhtml\\nContains the site’s home page\\nstatic/main.css\\nContains CSS to dress up the home page\\nstatic/banner.jpg'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 587, 'file_type': 'pdf'}, page_content='586\\nContains the website’s banner\\nstatic/placeholder.jpg\\nContains a placeholder image for photos that have yet\\nto be uploaded\\nFigure 14-6. The Contoso Travel website translating a road sign\\nThe site contains a single page named index.xhtml that’s\\ndisplayed when a user navigates to the site. The code that\\ndisplays it lives in app.py, and it includes logic for\\nuploading photos. You will modify app.py to extract and\\ntranslate text from the photos that users upload.\\nNOTE\\nDoes “Contoso” sound familiar? It’s the company name used in\\ncountless Microsoft samples and tutorials. There’s a story behind'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 588, 'file_type': 'pdf'}, page_content='587\\nwhy it’s so popular. When you write content for Microsoft,\\nyou’re not allowed to make up company names because doing so is\\nlikely to get Microsoft sued. Contoso is one of several fictitious\\ncompany names Microsoft has trademarked over the years. Others\\nthat might be familiar include Northwind Traders, Fabrikam, and\\nAdventureWorks.\\nOpen a command prompt or terminal window and cd to the\\nproject directory. If you’re running Windows, use the\\nfollowing command to create an environment variable named\\nFLASK_ENV that tells Flask to run in development mode:\\n\\nset FLASK_ENV=development\\nIf you’re running Linux or macOS, use this command instead:\\n\\nexport FLASK_ENV=development\\nRunning Flask in development mode is helpful when you’re\\ndeveloping a website because Flask automatically reloads any\\nfiles that change while the site is running. If you let Flask\\ndefault to production mode and change the contents of an HTML\\nfile or other asset, you have to restart Flask for the\\nchanges to appear in your browser.\\nNow use the following command to start Flask:\\n\\nflask run'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 589, 'file_type': 'pdf'}, page_content=\"588\\nOpen a browser and go to http://localhost:5000. When Contoso\\nTravel’s home page appears, click the Upload Photo button\\nand select a JPG or PNG file on your computer. Confirm that\\nthe photo uploads without errors. At this point, no\\nprocessing is performed on the photo, so the only visual clue\\nthat the upload succeeded is that the photo appears in the\\nweb page.\\nNext, use the Azure Portal to create a Computer Vision\\nresource and a Translator resource if you haven’t already.\\nOpen app.py in your favorite code editor and find the\\nfollowing statements near the top of the file:\\n\\n# Define Cognitive Services variables\\nvision_key\\n=\\n'VISION_KEY'\\nvision_endpoint\\n=\\n'VISION_ENDPOINT'\\ntranslator_key\\n=\\n'TRANSLATOR_KEY'\\ntranslator_endpoint\\n=\\n'TRANSLATOR_ENDPOINT'\\ntranslator_region\\n=\\n'TRANSLATOR_REGION'\\nReplace VISION_KEY and TRANSLATOR_KEY with the services’\\nsubscription keys, VISION_ENDPOINT and TRANSLATOR_ENDPOINT\\nwith the endpoints, and TRANSLA\\u2060TOR\\u200b_REGION with the region\\nselected for the Translator service. For Translator, use the\\nendpoint labeled “Text Translation.” Be sure to leave the\\ntick marks delimiting the strings intact.\\nNow add the following function for extracting text from a\\nphoto to app.py, placing it right after the comment that\\nreads “Function that extracts text from images” near the\\nbottom of the file:\\n\\ndef\\nextract_text(endpoint,\\nkey,\\nimage):\\n\\xa0\\xa0\\xa0 try:\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 590, 'file_type': 'pdf'}, page_content=\"589\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 client\\n=\\nComputerVisionClient(endpoint,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 CognitiveServicesCredentials(key))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 result\\n=\\nclient.recognize_printed_text_in_stream(image)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lines\\n=\\n[]\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nregion\\nin\\nresult.regions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nline\\nin\\nregion.lines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n=\\n' '.join([word.text\\nfor\\nword\\nin\\nline.words])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lines.append(text)\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nlen(lines)\\n==\\n0:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lines.append('Photo contains no text to translate')\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\nlines\\n\\n\\xa0\\xa0\\xa0 except\\nComputerVisionErrorResponseException\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n['Error calling the Computer Vision service: '\\n+\\ne.message]\\n\\n\\xa0\\xa0\\xa0 except\\nException\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n['Error calling the Computer Vision service']\\nThen add the following function for translating text. Place\\nit after the comment that reads “Function that translates\\ntext into a specified language” near the bottom of app.py:\\n\\ndef\\ntranslate_text(endpoint,\\nregion,\\nkey,\\nlines,\\nlanguage):\\n\\xa0\\xa0\\xa0 try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 headers\\n=\\n{\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Key':\\nkey,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Region':\\nregion,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Content-type':\\n'application/json'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 }\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input\\n=\\n[]\"), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 591, 'file_type': 'pdf'}, page_content='590\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nline\\nin\\nlines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input.append({\\n\"text\":\\nline\\n})\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 uri\\n=\\nendpoint\\n+\\n\\'translate?api-version=3.0&to=\\'\\n+\\nlanguage\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 response\\n=\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 response.raise_for_status()\\n# Raise exception if call failed\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 results\\n=\\nresponse.json()\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 translated_lines\\n=\\n[]\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nresult\\nin\\nresults:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\ntranslated_line\\nin\\nresult[\"translations\"]:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 translated_lines.append(translated_line[\"text\"])\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\ntranslated_lines\\n\\n\\xa0\\xa0\\xa0 except\\nrequests.exceptions.HTTPError\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n[\\'Error calling the Translator service: \\'\\n+\\ne.strerror]\\n\\n\\xa0\\xa0\\xa0 except\\nException\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n[\\'Error calling the Translator service\\']\\nFind the function named index in app.py. This is the function\\ncalled when the home page is requested or a photo is\\nuploaded. Under the comment that reads “Use the Computer\\nVision service to extract text from the image,” add the\\nfollowing line of code to call extract_text with the photo\\nthat the user just uploaded:\\n\\nlines\\n=\\nextract_text(vision_endpoint,\\nvision_key,\\nimage)\\nUnder the comment that reads “Use the Translator service to\\ntranslate text extracted from the image,” add a line of code'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 592, 'file_type': 'pdf'}, page_content='591\\nto translate the text returned by extract_text to the\\nspecified language:\\n\\ntranslated_lines\\n=\\ntranslate_text(translator_endpoint,\\ntranslator_region,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 translator_key,\\nlines,\\nlanguage)\\nAdd the following statements immediately after the comment\\nthat reads “Flash the translated text”:\\n\\nfor\\ntranslated_line\\nin\\ntranslated_lines:\\n\\xa0\\xa0\\xa0 flash(translated_line)\\nThese statements use Flask’s message-flashing support to\\ndisplay the strings in translated_lines in a modal dialog.\\nFinish up by saving your changes to app.py. Return to the\\nbrowser in which the website is running and refresh the page.\\n(If you closed it, open a new browser instance and navigate\\nto http://localhost:5000.) Select a language from the drop-\\ndown list at the top of the page. Then upload a photo\\ncontaining a road sign or any other image with text in it.\\nWas the text extracted and translated? The app isn’t\\nperfect, but it should work as expected most of the time. If\\nyou’re not satisfied, try replacing calls to\\nrecognize_printed_text_in_stream with calls to read_in_stream.\\nThe latter is more aggressive at finding text in photos.\\nWhile you’re at it, you could also use the Translator\\nservice to translate error messages into the language the\\nuser selected or the Speech service to vocalize translated\\ntext. With Azure Cognitive Services lending a hand, the only\\nlimit is your imagination.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 593, 'file_type': 'pdf'}, page_content='592\\nSummary\\nAzure Cognitive Services lower the bar for incorporating\\nsophisticated deep-learning models into the apps you write by\\nproviding REST APIs that any app with an internet connection\\ncan call. Containerized versions of most services allow them\\nto be hosted locally or on premises and also provide an\\noption for running in disconnected scenarios. One of the\\nbenefits of containers is locking in a particular version of\\nthe models you’re using or the APIs that access them so that\\nchanges to Azure Cognitive Services won’t affect your apps.\\nThe range of services offered by Azure Cognitive Services\\nincludes vision services for extracting information from\\nimages and video streams, language services for translating\\ntext into other languages and performing other NLP tasks, and\\nspeech services for incorporating speech into your apps. Free\\nSDKs are available for most services. They simplify the code\\nyou write, and they’re available for a variety of popular\\nprogramming languages including Python, Java, and C#.\\nMachine learning and deep learning are making the world a\\nbetter place one model at a time. Soon both will be\\nconsidered part of the essential skill set of every engineer\\nand software developer. Are you up to the task? My hope is\\nthat this book gives you the confidence to say yes and the\\ntools to make it happen. It’s too late to monitor Apollo\\nmissions in Mission Control, but countless other frontiers\\nare waiting to be explored.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 594, 'file_type': 'pdf'}, page_content='593\\nIndex\\nA\\n\\xa0\\naccuracy measures\\nfor classification models, Classification Models,\\nAccuracy Measures for Classification Models-Accuracy\\nMeasures for Classification Models\\nconfusion matrix, Accuracy Measures for\\nClassification Models, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nfor neural network predictions, Training a Neural\\nNetwork to Detect Credit Card Fraud-Training a Neural\\nNetwork to Detect Credit Card Fraud, Training a CNN\\nto Recognize Arctic Wildlife\\nfor regression models, Accuracy Measures for\\nRegression Models-Accuracy Measures for Regression\\nModels\\ntraining versus validation accuracy, Dropout\\nvalidation accuracy (see validation accuracy)\\naccuracy score, Accuracy Measures for Classification\\nModels, Accuracy Measures for Classification Models,\\nClassifying Passengers Who Sailed on the Titanic\\nactivation functions, Understanding Neural Networks\\nReLU, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow\\nsigmoid, Building Neural Networks with Keras and\\nTensorFlow, Binary Classification with Neural'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 595, 'file_type': 'pdf'}, page_content='594\\nNetworks-Making Predictions, Understanding CNNs\\nsoftmax (see softmax activation function)\\ntanh, Building Neural Networks with Keras and\\nTensorFlow\\nAdam optimizer, Building Neural Networks with Keras and\\nTensorFlow\\nadaptive learning rate algorithms, Training Neural\\nNetworks\\nadditive modeling, Gradient-Boosting Machines\\nadditive smoothing, Naive Bayes\\nagglomerative clustering, Segmenting Customers Using More\\nThan Two Dimensions\\nAlexNet, Image Classification with Convolutional Neural\\nNetworks\\nalgorithm, Machine Learning\\nAlphaGo, Machine Learning Versus Artificial Intelligence\\nanalyze_image_in_stream method, The Computer Vision\\nService\\nanalyze_sentiment, Calling Azure Cognitive Services APIs\\nanchors and anchor boxes, R-CNNs\\nannotate_image function, Mask R-CNN\\nanomaly detection, Anomaly Detection-Multivariate Anomaly\\nDetection\\nbearing failure prediction, Using PCA to Predict\\nBearing Failure-Using PCA to Predict Bearing Failure\\ncredit card fraud detection, Using PCA to Detect\\nCredit Card Fraud-Using PCA to Detect Credit Card\\nFraud\\nmultivariate, Multivariate Anomaly Detection'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 596, 'file_type': 'pdf'}, page_content='595\\nAnomaly Detector, Introducing Azure Cognitive Services\\nanonymizing data, Anonymizing Data-Anonymizing Data\\nArcFace, ArcFace, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\narctic wildlife recognition, Accuracy Measures for\\nClassification Models, Training a CNN to Recognize Arctic\\nWildlife-Training a CNN to Recognize Arctic Wildlife,\\nUsing Transfer Learning to Identify Arctic Wildlife-Using\\nTransfer Learning to Identify Arctic Wildlife, Applying\\nImage Augmentation to Arctic Wildlife-Applying Image\\nAugmentation to Arctic Wildlife\\narea under the curve (AUC), Accuracy Measures for\\nClassification Models\\nargmax function, Multiclass Classification with Neural\\nNetworks\\nartificial intelligence (AI)\\nhistory of, Machine Learning Versus Artificial\\nIntelligence\\nversus machine learning (ML), Machine Learning Versus\\nArtificial Intelligence-Machine Learning Versus\\nArtificial Intelligence\\nattention mechanisms, Transformer Encoder-Decoders-\\nTransformer Encoder-Decoders\\nAUC (area under the curve), Accuracy Measures for\\nClassification Models\\naudio classification with CNNs, Audio Classification with\\nCNNs-Audio Classification with CNNs\\nAudioConfig object, The Speech Service\\naugmentation layers, Image Augmentation with Augmentation\\nLayers\\naverage pooling layer, Understanding CNNs'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 597, 'file_type': 'pdf'}, page_content='596\\nAWS AI Services, Azure Cognitive Services\\nAzure Cognitive Services APIs, Custom Object Detection,\\nAzure Cognitive Services-Summary\\ncalling APIs, Calling Azure Cognitive Services APIs-\\nCalling Azure Cognitive Services APIs\\nComputer Vision service, Image Classification with\\nConvolutional Neural Networks, Introducing Azure\\nCognitive Services, The Computer Vision Service-The\\nComputer Vision Service\\ncontainers, Azure Cognitive Services Containers-Azure\\nCognitive Services Containers\\nContoso Travel exercise, Putting It All Together:\\nContoso Travel-Putting It All Together: Contoso\\nTravel\\nDecision services, Introducing Azure Cognitive\\nServices\\nkeys and endpoints, Keys and Endpoints-Keys and\\nEndpoints\\nLanguage service, Introducing Azure Cognitive\\nServices, The Language Service-The Language Service\\nSDKs available, Calling Azure Cognitive Services\\nAPIs-Calling Azure Cognitive Services APIs\\nSpeech service, Introducing Azure Cognitive Services,\\nThe Speech Service-The Speech Service\\nsubscription setup, Training a Custom Object\\nDetection Model with the Custom Vision Service-\\nTraining a Custom Object Detection Model with the\\nCustom Vision Service\\nTranslator service, The Translator Service-The\\nTranslator Service\\nAzure Portal, Keys and Endpoints'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 598, 'file_type': 'pdf'}, page_content='597\\nAzureError, Calling Azure Cognitive Services APIs\\nB\\n\\xa0\\nbackpropagation, Training Neural Networks-Training Neural\\nNetworks, Building Neural Networks with Keras and\\nTensorFlow, Dropout\\nbag of words, Text Classification\\nbatch normalization, Pretrained CNNs\\nBayes’ theorem, Naive Bayes\\nbearing failure prediction, anomaly detection, Using PCA\\nto Predict Bearing Failure-Using PCA to Predict Bearing\\nFailure\\nBERT (bidirectional encoder representations from\\ntransformers), Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\nbuilding a question answering system, Building a\\nBERT-Based Question Answering System-Building a BERT-\\nBased Question Answering System\\nsentiment analysis, Fine-Tuning BERT to Perform\\nSentiment Analysis-Fine-Tuning BERT to Perform\\nSentiment Analysis\\nbiases, neural networks, Understanding Neural Networks,\\nUnderstanding Neural Networks, Using a Neural Network to\\nPredict Taxi Fares\\nbidirectional encoder representations from transformers\\n(BERT), Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\nbilingual evaluation understudy (BLEU) scores, Building a\\nTransformer-Based NMT Model'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 599, 'file_type': 'pdf'}, page_content='598\\nbinary classification models, Binary Classification-\\nDetecting Credit Card Fraud\\ncredit card fraud detection, Detecting Credit Card\\nFraud-Detecting Credit Card Fraud\\nlogistic regression (see logistic regression)\\nwith neural networks, Binary Classification with\\nNeural Networks-Training a Neural Network to Detect\\nCredit Card Fraud\\nTitanic passengers, Classifying Passengers Who Sailed\\non the Titanic-Classifying Passengers Who Sailed on\\nthe Titanic\\nbinary classifiers, in Viola-Jones face detection, Face\\nDetection with Viola-Jones\\nbinary trees, Decision Trees\\nbinary_crossentropy function, Binary Classification with\\nNeural Networks\\nBLEU (bilingual evaluation understudy) scores, Building a\\nTransformer-Based NMT Model\\nBoltzmann machines, Machine Learning Versus Artificial\\nIntelligence\\nboosting, Gradient-Boosting Machines-Gradient-Boosting\\nMachines\\nbottleneck layers, Understanding CNNs, Understanding\\nCNNs, Using Keras and TensorFlow to Build CNNs, Transfer\\nLearning-Transfer Learning\\nbottom Sobel kernel, Understanding CNNs\\nbounding boxes, object detection, R-CNNs\\nROI pooling, R-CNNs\\nRPN, Mask R-CNN, Training a Custom Object Detection\\nModel with the Custom Vision Service'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 600, 'file_type': 'pdf'}, page_content='599\\nViola-Jones, Using the OpenCV Implementation of\\nViola-Jones\\nYOLO, YOLO, YOLOv3 and Keras\\nbreast cancer dataset, Data Normalization, Anonymizing\\nData\\nC\\n\\xa0\\nC parameter, SVMs, How Support Vector Machines Work,\\nHyperparameter Tuning-Hyperparameter Tuning\\nC#\\nbuilding models with ML.NET, Building ML Models in C#\\nwith ML.NET-Saving and Loading ML.NET Models\\nusing ONNX to work with Python, Using ONNX to Bridge\\nthe Language Gap-Using ONNX to Bridge the Language\\nGap\\nCalifornia Housing Prices dataset, Accuracy Measures for\\nRegression Models\\nCallback class, Keras Callbacks\\ncallbacks, Keras, Keras Callbacks-Keras Callbacks\\nCART (Classification and Regression Tree) algorithm,\\nDecision Trees\\ncascade classifiers, Face Detection with Viola-Jones-\\nUsing the OpenCV Implementation of Viola-Jones\\nCascadeClassifier class, Using the OpenCV Implementation\\nof Viola-Jones-Using the OpenCV Implementation of Viola-\\nJones\\ncategorical data, Categorical Data-Categorical Data\\ncategorical values, Categorical Data'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 601, 'file_type': 'pdf'}, page_content='600\\ncategorical_crossentropy function, Multiclass\\nClassification with Neural Networks\\ncentroid, cluster, Unsupervised Learning with k-Means\\nClustering\\nClassification and Regression Tree (CART) algorithm,\\nDecision Trees\\nclassification boundary, Supervised Learning\\nclassification models, Supervised Learning-Supervised\\nLearning, Classification Models-Summary\\naccuracy measures for, Classification Models,\\nAccuracy Measures for Classification Models-Accuracy\\nMeasures for Classification Models\\naudio classification, Audio Classification with CNNs-\\nAudio Classification with CNNs\\nbinary classifiers, Binary Classification-Detecting\\nCredit Card Fraud, Binary Classification with Neural\\nNetworks-Training a Neural Network to Detect Credit\\nCard Fraud\\nCNNs, Understanding CNNs, Understanding CNNs,\\nTransfer Learning-Transfer Learning\\ndigit recognition model, Building a Digit Recognition\\nModel-Building a Digit Recognition Model\\nGBMs, Gradient-Boosting Machines-Gradient-Boosting\\nMachines\\nimage (see images, classifying and generating)\\nand k-nearest neighbors, k-Nearest Neighbors, Using\\nk-Nearest Neighbors to Classify Flowers-Using k-\\nNearest Neighbors to Classify Flowers\\nlogistic regression (see logistic regression)\\nMLPs for, Understanding CNNs'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 602, 'file_type': 'pdf'}, page_content='601\\nmulticlass, Supervised Learning, Using k-Nearest\\nNeighbors to Classify Flowers-Using k-Nearest\\nNeighbors to Classify Flowers, Classification Models,\\nMulticlass Classification-Multiclass Classification,\\nMulticlass Classification with Neural Networks-\\nMulticlass Classification with Neural Networks\\nmultilabel, Classification Models\\nSVMs (see support vector machines)\\ntext (see text classification and processing)\\ncleaning of text for vectorization, Preparing Text for\\nClassification\\nclosed-set versus open-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set Classification-\\nHandling Unknown Faces: Closed-Set Versus Open-Set\\nClassification\\nclustering algorithms, Unsupervised Learning with k-Means\\nClustering\\nagglomerative clustering, Segmenting Customers Using\\nMore Than Two Dimensions\\nDBSCAN, Segmenting Customers Using More Than Two\\nDimensions\\nk-means (see k-means clustering)\\nlabel encoding, Segmenting Customers Using More Than\\nTwo Dimensions-Segmenting Customers Using More Than\\nTwo Dimensions, Categorical Data-Categorical Data\\nCNNs (see convolutional neural networks)\\nCOCO dataset, Mask R-CNN, YOLOv3 and Keras\\ncoefficient of determination (R²), Accuracy Measures for\\nRegression Models, Using a Neural Network to Predict Taxi\\nFares'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 603, 'file_type': 'pdf'}, page_content='602\\ncollaborative recommender system, Recommender Systems\\nComputer Vision service, Image Classification with\\nConvolutional Neural Networks, Introducing Azure\\nCognitive Services, The Computer Vision Service-The\\nComputer Vision Service\\nComputerVisionClient class, The Computer Vision Service\\nconditional probability, Naive Bayes\\nconfidence value, object detection, YOLOv3 and Keras\\nconfusion matrix, Accuracy Measures for Classification\\nModels, Using SVMs for Facial Recognition-Using SVMs for\\nFacial Recognition\\nConfusionMatrixDisplay class, Accuracy Measures for\\nClassification Models, Spam Filtering\\nconfusion_matrix function, Accuracy Measures for\\nClassification Models\\nconsuming Python model from C# client, Consuming a Python\\nModel from a C# Client-Consuming a Python Model from a C#\\nClient\\ncontainer image, Containerizing a Machine Learning Model\\ncontainer registry, Containerizing a Machine Learning\\nModel\\ncontainerizing an ML model, Operationalizing Machine\\nLearning Models, Containerizing a Machine Learning Model-\\nContainerizing a Machine Learning Model, Azure Cognitive\\nServices Containers-Azure Cognitive Services Containers\\nContent Moderator service, Introducing Azure Cognitive\\nServices\\ncontent-based recommender system, Recommender Systems\\ncontinual learning, Saving and Loading Models'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 604, 'file_type': 'pdf'}, page_content='603\\nContoso Travel exercise, Putting It All Together: Contoso\\nTravel-Putting It All Together: Contoso Travel\\nConv1D class, Factoring Word Order into Predictions\\nConv2D layers, Sizing a Neural Network, Using Keras and\\nTensorFlow to Build CNNs-Using Keras and TensorFlow to\\nBuild CNNs\\nconvolutional kernels (kernels), Understanding CNNs-\\nUnderstanding CNNs\\nconvolutional neural networks (CNNs), Understanding\\nNeural Networks, Image Classification with Convolutional\\nNeural Networks-Summary\\narchitectures, Pretrained CNNs\\narctic wildlife recognition, Training a CNN to\\nRecognize Arctic Wildlife-Training a CNN to Recognize\\nArctic Wildlife, Using Transfer Learning to Identify\\nArctic Wildlife-Using Transfer Learning to Identify\\nArctic Wildlife\\naudio classification, Audio Classification with CNNs-\\nAudio Classification with CNNs\\nbuilding with Keras and TensorFlow, Using Keras and\\nTensorFlow to Build CNNs-Using Keras and TensorFlow\\nto Build CNNs\\nconvolution layers, Understanding CNNs-Understanding\\nCNNs\\ndata augmentation, Data Augmentation-Applying Image\\nAugmentation to Arctic Wildlife\\nface detection, Face Detection with Convolutional\\nNeural Networks-Face Detection with Convolutional\\nNeural Networks, Putting It All Together: Detecting\\nand Recognizing Faces in Photos-Putting It All\\nTogether: Detecting and Recognizing Faces in Photos'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 605, 'file_type': 'pdf'}, page_content='604\\nfacial recognition, Applying Transfer Learning to\\nFacial Recognition-Putting It All Together: Detecting\\nand Recognizing Faces in Photos\\nGANs, Understanding Neural Networks\\nglobal pooling, Global Pooling-Global Pooling\\nobject detection with R-CNNs, R-CNNs-Mask R-CNN\\npretrained models, Pretrained CNNs-Using ResNet50V2\\nto Classify Images\\ncorr method, Using Regression to Predict Taxi Fares-Using\\nRegression to Predict Taxi Fares\\ncosine similarity, Cosine Similarity-Building a Movie\\nRecommendation System, ArcFace\\nCountVectorizer class, Preparing Text for Classification-\\nPreparing Text for Classification, Sentiment Analysis-\\nSentiment Analysis, Spam Filtering, Recommender Systems,\\nConsuming a Python Model from a Python Client, Text\\nPreparation\\ncovariance matrix, Understanding Principal Component\\nAnalysis\\nCover’s theorem, How Support Vector Machines Work\\ncredit card fraud detection\\nanomaly detection, Using PCA to Detect Credit Card\\nFraud-Using PCA to Detect Credit Card Fraud\\nbinary classification, Detecting Credit Card Fraud-\\nDetecting Credit Card Fraud\\nwith neural network, Training a Neural Network to\\nDetect Credit Card Fraud-Training a Neural Network to\\nDetect Credit Card Fraud\\nPCA, Anonymizing Data, Using PCA to Detect Credit\\nCard Fraud-Using PCA to Detect Credit Card Fraud'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 606, 'file_type': 'pdf'}, page_content='605\\ncross-validation, Accuracy Measures for Regression\\nModels-Accuracy Measures for Regression Models, Logistic\\nRegression, Classifying Passengers Who Sailed on the\\nTitanic\\ncross_val_score function, Accuracy Measures for\\nRegression Models\\nCSVLogger class, Keras Callbacks\\nCustom Vision service, Training a Custom Object Detection\\nModel with the Custom Vision Service-Training a Custom\\nObject Detection Model with the Custom Vision Service,\\nIntroducing Azure Cognitive Services\\ncustomer segmentation, Applying k-Means Clustering to\\nCustomer Data-Segmenting Customers Using More Than Two\\nDimensions\\ncustomers dataset, Applying k-Means Clustering to\\nCustomer Data\\nD\\n\\xa0\\nDarknet, YOLOv3 and Keras\\ndata\\noverfitting of, Decision Trees, Hyperparameter\\nTuning, Building Neural Networks with Keras and\\nTensorFlow\\npreprocessing of (see preprocessing data)\\nshuffling of, Building Neural Networks with Keras and\\nTensorFlow, Text Classification\\ntest set, Using k-Nearest Neighbors to Classify\\nFlowers, Accuracy Measures for Regression Models,\\nAccuracy Measures for Regression Models\\ntime series, Recurrent Neural Networks (RNNs)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 607, 'file_type': 'pdf'}, page_content='606\\ntraining set, Using k-Nearest Neighbors to Classify\\nFlowers\\nunderfitting of, Hyperparameter Tuning, Building\\nNeural Networks with Keras and TensorFlow\\nvisualizing high-dimensional, Visualizing High-\\nDimensional Data-Visualizing High-Dimensional Data\\ndata augmentation, Data Augmentation-Applying Image\\nAugmentation to Arctic Wildlife\\ndata cleaning, Using k-Nearest Neighbors to Classify\\nFlowers, Preparing Text for Classification, Building a\\nTransformer-Based NMT Model\\ndata normalization, Data Normalization-Data Normalization\\nDataFrame, Categorical Data\\nDataset.to_tf_dataset method, Fine-Tuning BERT to Perform\\nSentiment Analysis\\nDatasets library, Hugging Face, Fine-Tuning BERT to\\nPerform Sentiment Analysis\\nDBSCAN (density-based spatial clustering of applications\\nwith noise), Segmenting Customers Using More Than Two\\nDimensions\\ndecision boundaries, How Support Vector Machines Work,\\nKernels, Kernel Tricks-Hyperparameter Tuning\\nDecision services, Introducing Azure Cognitive Services\\ndecision tree stumps, Gradient-Boosting Machines\\ndecision trees, Decision Trees-Decision Trees\\nCART training algorithm, Decision Trees\\nfor classification, Logistic Regression\\nGBMs, Gradient-Boosting Machines-Gradient-Boosting\\nMachines'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 608, 'file_type': 'pdf'}, page_content='607\\nand normalization, Data Normalization\\nrandom forests, Random Forests-Random Forests,\\nDetecting Credit Card Fraud, Using PCA to Detect\\nCredit Card Fraud\\nfor regression, Logistic Regression\\nDecisionTreeClassifier class, Decision Trees\\nDecisionTreeRegressor class, Decision Trees\\ndecode_predictions function, YOLOv3 and Keras\\ndeep learning, Machine Learning Versus Artificial\\nIntelligence, Deep Learning-Summary\\n(see also neural networks)\\nDeepset, Building a BERT-Based Question Answering System\\nDense layers, Building Neural Networks with Keras and\\nTensorFlow, Sizing a Neural Network\\ndense vector representation, Using Keras and TensorFlow\\nto Build CNNs\\ndensity-based spatial clustering of applications with\\nnoise (DBSCAN), Segmenting Customers Using More Than Two\\nDimensions\\ndependent decision trees, Gradient-Boosting Machines\\ndepthwise separable convolutions, Pretrained CNNs\\ndescribe_image method, The Computer Vision Service\\ndescribe_image_in_stream method, The Computer Vision\\nService\\ndetectMultiScale, Using the OpenCV Implementation of\\nViola-Jones\\ndigit recognition model, Building a Digit Recognition\\nModel-Building a Digit Recognition Model'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 609, 'file_type': 'pdf'}, page_content='608\\ndimensionality reduction\\nand bottleneck layers, Understanding CNNs\\nclassification layers, CNNs, Understanding CNNs,\\nUnderstanding CNNs\\nIsomap, Visualizing High-Dimensional Data\\nPCA (see principal component analysis)\\npooling layers, Understanding CNNs\\nt-SNE, Linear Regression, Visualizing High-\\nDimensional Data\\ndisconnected containers, Azure Cognitive Services, Azure\\nCognitive Services Containers\\nDistilBERT model, Bidirectional Encoder Representations\\nfrom Transformers (BERT)\\ndocker build command, Containerizing a Machine Learning\\nModel\\nDocker container, Operationalizing Machine Learning\\nModels, Containerizing a Machine Learning Model\\ndocument translation, The Translator Service\\ndot product, Using Keras and TensorFlow to Build CNNs\\nDropout layers, Sizing a Neural Network, Dropout-Dropout,\\nGlobal Pooling\\nE\\n\\xa0\\nEarlyStopping class, Keras Callbacks, Building a\\nTransformer-Based NMT Model\\neigenvectors and eigenvalues, Understanding Principal\\nComponent Analysis'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 610, 'file_type': 'pdf'}, page_content='609\\nElasticsearch, Building a BERT-Based Question Answering\\nSystem\\nelbow method to plot inertias, Unsupervised Learning with\\nk-Means Clustering\\nEmbedding class, Word Embeddings\\nembedding layer, Natural Language Processing, Word\\nEmbeddings-Text Classification\\nencoder–decoder models, LSTM Encoder-Decoders-\\nTransformer Encoder-Decoders\\nensemble learning, random forests, Random Forests-Random\\nForests, Detecting Credit Card Fraud, Using PCA to Detect\\nCredit Card Fraud\\nepochs, Building Neural Networks with Keras and\\nTensorFlow\\nestimators, Pipelining, Consuming a Python Model from a\\nPython Client\\nEuclidean distance, k-Nearest Neighbors\\nExcel, adding ML capabilities to, Adding Machine Learning\\nCapabilities to Excel-Adding Machine Learning\\nCapabilities to Excel\\nexpert systems, Machine Learning Versus Artificial\\nIntelligence\\nexplained variance, plotting, Understanding Principal\\nComponent Analysis-Understanding Principal Component\\nAnalysis\\nextracting faces from photos, Extracting Faces from\\nPhotos-Extracting Faces from Photos\\nextract_key_phrases method, The Language Service\\nF'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 611, 'file_type': 'pdf'}, page_content='610\\nF1 score, Accuracy Measures for Classification Models\\nface detection, Face Detection and Recognition-Extracting\\nFaces from Photos\\nwith CNNs, Face Detection with Convolutional Neural\\nNetworks-Face Detection with Convolutional Neural\\nNetworks, Putting It All Together: Detecting and\\nRecognizing Faces in Photos-Putting It All Together:\\nDetecting and Recognizing Faces in Photos\\nComputer Vision service, The Computer Vision Service\\nextracting faces from photos, Extracting Faces from\\nPhotos-Extracting Faces from Photos\\nwith Viola-Jones, Face Detection with Viola-Jones-\\nUsing the OpenCV Implementation of Viola-Jones\\nface embedding, ArcFace\\nFace service, Introducing Azure Cognitive Services, The\\nComputer Vision Service\\nface verification, ArcFace\\nFaceNet, Facial Recognition\\nfacial recognition, Facial Recognition-Handling Unknown\\nFaces: Closed-Set Versus Open-Set Classification\\nclosed-set versus open-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set\\nClassification-Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nwith CNNs, Applying Transfer Learning to Facial\\nRecognition-Putting It All Together: Detecting and\\nRecognizing Faces in Photos\\nneural networks, Training a Neural Network to\\nRecognize Faces-Training a Neural Network to\\nRecognize Faces\\nand privacy, Face Detection and Recognition'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 612, 'file_type': 'pdf'}, page_content='611\\nwith SVMs, Using SVMs for Facial Recognition-Using\\nSVMs for Facial Recognition\\nwith transfer learning, Applying Transfer Learning to\\nFacial Recognition-Boosting Transfer Learning with\\nTask-Specific Weights\\nfalse positive rate (FPR), Accuracy Measures for\\nClassification Models\\nFast R-CNN, R-CNNs\\nFaster R-CNN, R-CNNs\\nfeature columns, What Is Machine Learning?\\nfeature maps, Understanding CNNs\\nfiltering noise in images, Filtering Noise-Filtering\\nNoise\\nfit method\\nin Keras, Building Neural Networks with Keras and\\nTensorFlow, Keras Callbacks, Image Augmentation with\\nImageDataGenerator\\nin Scikit-Learn, Using k-Nearest Neighbors to\\nClassify Flowers, Accuracy Measures for Regression\\nModels\\nfit_on_texts method, Text Preparation\\nFlask framework, Operationalizing Machine Learning\\nModels, Putting It All Together: Contoso Travel\\nFlatten layer, Global Pooling, Text Classification\\nflow method, Image Augmentation with ImageDataGenerator\\nflow_from_directory method, Image Augmentation with\\nImageDataGenerator\\nfolds, Accuracy Measures for Regression Models'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 613, 'file_type': 'pdf'}, page_content='612\\nFPR (false positive rate), Accuracy Measures for\\nClassification Models\\nfully connected layers, Understanding Neural Networks\\nfunctional API, Keras, Neural Networks, Building a\\nTransformer-Based NMT Model-Building a Transformer-Based\\nNMT Model\\nG\\n\\xa0\\ngamma parameter, SVM kernels, Hyperparameter Tuning-\\nHyperparameter Tuning\\nGANs (generative adversarial networks), Understanding\\nNeural Networks\\ngated recurrent unit (GRU) layer, Recurrent Neural\\nNetworks (RNNs)\\nGBDTs (gradient-boosted decision trees), Gradient-\\nBoosting Machines-Gradient-Boosting Machines\\nGBMs (gradient-boosting machines), Gradient-Boosting\\nMachines-Gradient-Boosting Machines\\ngenerative adversarial networks (GANs), Understanding\\nNeural Networks\\nget_weights method, Saving and Loading Models\\nGini impurity, Decision Trees\\nglobal minimum, Training Neural Networks\\nglobal pooling, Global Pooling-Global Pooling\\nGlobalAveragePooling2D layer, Global Pooling, Audio\\nClassification with CNNs\\nGlobalMaxPooling2D layer, Global Pooling\\nGlorotUniform initializer, Building Neural Networks with\\nKeras and TensorFlow'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 614, 'file_type': 'pdf'}, page_content='613\\nGloVe word vectors, Word Embeddings\\nGPUs (graphics processing units), Machine Learning Versus\\nArtificial Intelligence, Deep Learning, Training a CNN to\\nRecognize Arctic Wildlife\\ngradient descent, Training Neural Networks\\ngradient-boosted decision trees (GBDTs), Gradient-\\nBoosting Machines-Gradient-Boosting Machines\\ngradient-boosting machines (GBMs), Gradient-Boosting\\nMachines-Gradient-Boosting Machines\\nGradientBoostingClassifier class, Gradient-Boosting\\nMachines-Gradient-Boosting Machines, Detecting Credit\\nCard Fraud\\nGradientBoostingRegressor class, Gradient-Boosting\\nMachines-Gradient-Boosting Machines, Using Regression to\\nPredict Taxi Fares\\ngraphics processing units (GPUs), Machine Learning Versus\\nArtificial Intelligence, Deep Learning, Training a CNN to\\nRecognize Arctic Wildlife\\nGridSearchCV, Hyperparameter Tuning, Pipelining, Using\\nSVMs for Facial Recognition-Using SVMs for Facial\\nRecognition\\nGRU (gated recurrent unit) layer, Recurrent Neural\\nNetworks (RNNs)\\nGRU class, Recurrent Neural Networks (RNNs)\\nH\\n\\xa0\\nH2O framework, Building ML Models in C# with ML.NET,\\nSentiment Analysis with ML.NET\\nH5 file format, Saving and Loading Models\\nHaar-like features, Face Detection with Viola-Jones'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 615, 'file_type': 'pdf'}, page_content='614\\nhandwritten text\\nbuilding digit recognition model, Building a Digit\\nRecognition Model-Building a Digit Recognition Model\\nextracting from image, The Computer Vision Service-\\nThe Computer Vision Service\\nHashingVectorizer class, Preparing Text for\\nClassification, Preparing Text for Classification,\\nConsuming a Python Model from a Python Client\\nHaystack library, Building a BERT-Based Question\\nAnswering System\\nhidden layers, Understanding Neural Networks,\\nUnderstanding Neural Networks, Building Neural Networks\\nwith Keras and TensorFlow\\nhidden state, layer, Recurrent Neural Networks (RNNs)\\nhigh recall then precision pattern, Face Detection with\\nViola-Jones\\nhigh-dimensional data visualization, Visualizing High-\\nDimensional Data-Visualizing High-Dimensional Data\\nHugging Face, Using Pretrained Models to Classify Text,\\nUsing Pretrained Models to Translate Text, Building a\\nBERT-Based Question Answering System-Fine-Tuning BERT to\\nPerform Sentiment Analysis\\nhyperparameter tuning, Hyperparameter Tuning-\\nHyperparameter Tuning\\nI\\n\\xa0\\nIDataView, Building ML Models in C# with ML.NET,\\nSentiment Analysis with ML.NET\\nImageDataGenerator, Image Augmentation with\\nImageDataGenerator-Image Augmentation with'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 616, 'file_type': 'pdf'}, page_content='615\\nImageDataGenerator\\nImageNet dataset, Pretrained CNNs\\nimages, classifying and generating, What Is Machine\\nLearning?-What Is Machine Learning?\\narctic wildlife recognition, Applying Image\\nAugmentation to Arctic Wildlife-Applying Image\\nAugmentation to Arctic Wildlife\\nwith augmentation layers, Image Augmentation with\\nAugmentation Layers\\ncaption generating, Introducing Azure Cognitive\\nServices, The Computer Vision Service-The Computer\\nVision Service\\nCNNs (see convolutional neural networks)\\nextracting text from photos, The Computer Vision\\nService\\nface detection (see face detection)\\nfacial recognition (see facial recognition)\\ngenerating with GANs, Understanding Neural Networks\\nwith k-nearest neighbors, Using k-Nearest Neighbors\\nto Classify Flowers-Using k-Nearest Neighbors to\\nClassify Flowers\\nobject detection (see object detection)\\ntuning hyperparameters, Hyperparameter Tuning-\\nHyperparameter Tuning\\nIMDB movie dataset, Sentiment Analysis, Using\\nTextVectorization in a Sentiment Analysis Model, Fine-\\nTuning BERT to Perform Sentiment Analysis\\nimpurity measures, Decision Trees\\nimputing missing values, Classifying Passengers Who\\nSailed on the Titanic'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 617, 'file_type': 'pdf'}, page_content='616\\nInception, Pretrained CNNs\\nincremental training, Saving and Loading Models\\ninertias, plotting, Unsupervised Learning with k-Means\\nClustering\\nInferenceSession method, ONNX, Using ONNX to Bridge the\\nLanguage Gap\\nInput class, ML.NET, Sentiment Analysis with ML.NET\\ninput layer, neural network, Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow\\ninstance segmentation, Mask R-CNN-Mask R-CNN\\nintegral images, Face Detection with Viola-Jones\\nIntellipix, Introducing Azure Cognitive Services\\nintersection-over-union (IoU) score, R-CNNs\\ninverse_transform method, Preparing Text for\\nClassification, Understanding Principal Component\\nAnalysis\\nIris dataset, Using k-Nearest Neighbors to Classify\\nFlowers, Linear Regression\\nisolation forest, Anomaly Detection\\nIsomap, Visualizing High-Dimensional Data\\nJ\\n\\xa0\\nJSON encoding or decoding, Calling Azure Cognitive\\nServices APIs\\nJupyter, Running the Book’s Code Samples\\nK'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 618, 'file_type': 'pdf'}, page_content='617\\nk-fold cross-validation, Accuracy Measures for Regression\\nModels-Accuracy Measures for Regression Models\\nk-means clustering, Unsupervised Learning with k-Means\\nClustering-Segmenting Customers Using More Than Two\\nDimensions\\ncustomer data, applying to, Applying k-Means\\nClustering to Customer Data-Segmenting Customers\\nUsing More Than Two Dimensions\\nsegmenting customer data, Segmenting Customers Using\\nMore Than Two Dimensions-Segmenting Customers Using\\nMore Than Two Dimensions\\nsupervised learning, Supervised Learning-Using k-\\nNearest Neighbors to Classify Flowers\\nk-nearest neighbors, k-Nearest Neighbors-Using k-Nearest\\nNeighbors to Classify Flowers, Data Normalization\\nKeras API, Training Neural Networks, Neural Networks\\nbuilding neural networks with, Building Neural\\nNetworks with Keras and TensorFlow-Using a Neural\\nNetwork to Predict Taxi Fares\\ncallbacks, Keras Callbacks-Keras Callbacks\\nCNNs, Using Keras and TensorFlow to Build CNNs-Using\\nKeras and TensorFlow to Build CNNs\\nfunctional API, Neural Networks, Building a\\nTransformer-Based NMT Model-Building a Transformer-\\nBased NMT Model\\nLSTM-based models, LSTM Encoder-Decoders-LSTM\\nEncoder-Decoders\\nMobileNetV2, Pretrained CNNs\\npretrained CNN models, Pretrained CNNs-Using\\nResNet50V2 to Classify Images'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 619, 'file_type': 'pdf'}, page_content='618\\nsaving models, Saving and Loading Models, Using\\nTextVectorization in a Sentiment Analysis Model\\nsequential API, Neural Networks-Building Neural\\nNetworks with Keras and TensorFlow\\ntensorflow.keras.callbacks.CSVLogger, Keras Callbacks\\ntensorflow.keras.callbacks.EarlyStopping, Keras\\nCallbacks, Building a Transformer-Based NMT Model\\ntensorflow.keras.callbacks.LearningRateScheduler,\\nKeras Callbacks\\ntensorflow.keras.callbacks.ModelCheckpoint, Keras\\nCallbacks\\ntensorflow.keras.callbacks.TensorBoard, Keras\\nCallbacks\\ntensorflow.keras.layers.Conv1D, Factoring Word Order\\ninto Predictions\\ntensorflow.keras.layers.Conv2D, Sizing a Neural\\nNetwork, Using Keras and TensorFlow to Build CNNs-\\nUsing Keras and TensorFlow to Build CNNs\\ntensorflow.keras.layers.Dense, Building Neural\\nNetworks with Keras and TensorFlow, Using a Neural\\nNetwork to Predict Taxi Fares\\ntensorflow.keras.layers.Dropout, Sizing a Neural\\nNetwork, Dropout-Dropout, Global Pooling\\ntensorflow.keras.layers.Embedding, Word Embeddings\\ntensorflow.keras.layers.Flatten, Global Pooling, Text\\nClassification\\ntensorflow.keras.layers.GlobalAveragePooling2D,\\nGlobal Pooling, Audio Classification with CNNs\\ntensorflow.keras.layers.GlobalMaxPooling2D, Global\\nPooling-Global Pooling'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 620, 'file_type': 'pdf'}, page_content='619\\ntensorflow.keras.layers.GRU, Recurrent Neural\\nNetworks (RNNs)\\ntensorflow.keras.layers.Input, Automating Text\\nVectorization\\ntensorflow.keras.layers.LSTM, Recurrent Neural\\nNetworks (RNNs)\\ntensorflow.keras.layers.MaxPooling1D, Factoring Word\\nOrder into Predictions\\ntensorflow.keras.layers.MaxPooling2D, Using Keras and\\nTensorFlow to Build CNNs-Using Keras and TensorFlow\\nto Build CNNs\\ntensorflow.keras.layers.MultiHeadAttention, Building\\na Transformer-Based NMT Model\\ntensorflow.keras.layers.Rescaling, Image Augmentation\\nwith Augmentation Layers\\ntensorflow.keras.layers.TextVectorization, Text\\nPreparation, Automating Text Vectorization-Using\\nTextVectorization in a Sentiment Analysis Model\\ntensorflow.keras.losses.binary_crossentropy, Binary\\nClassification with Neural Networks\\ntensorflow.keras.losses.categorical_crossentropy,\\nMulticlass Classification with Neural Networks\\ntensorflow.keras.losses.sparse_categorical_crossentro\\npy, Multiclass Classification with Neural Networks,\\nTraining a Neural Network to Recognize Faces\\ntensorflow.keras.models.load_model, Saving and\\nLoading Models\\ntensorflow.keras.models.Sequential, Building Neural\\nNetworks with Keras and TensorFlow, Using a Neural\\nNetwork to Predict Taxi Fares'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 621, 'file_type': 'pdf'}, page_content='620\\ntensorflow.keras.preprocessing.image.ImageDataGenerat\\nor, Image Augmentation with ImageDataGenerator-Image\\nAugmentation with ImageDataGenerator\\ntensorflow.keras.preprocessing.sequence.pad_sequences\\n, Text Preparation\\ntensorflow.keras.preprocessing.text.Tokenizer, Text\\nPreparation-Text Preparation, Automating Text\\nVectorization, Building a Transformer-Based NMT Model\\ntensorflow.keras.utils.to_categorical, Multiclass\\nClassification with Neural Networks\\ntransformer-based NMT model, Building a Transformer-\\nBased NMT Model-Building a Transformer-Based NMT\\nModel\\nand YOLO, YOLOv3 and Keras-YOLOv3 and Keras\\nKerasNLP, Building a Transformer-Based NMT Model-Building\\na Transformer-Based NMT Model\\nkernel tricks, Support Vector Machines, Kernel Tricks-\\nKernel Tricks\\nkernels (convolution kernels), Understanding CNNs-\\nUnderstanding CNNs\\nkernels, SVM, Kernels-Kernel Tricks\\nkeyword extraction, Preparing Text for Classification\\nKMeans class, Unsupervised Learning with k-Means\\nClustering\\nKNeighborsClassifier class, Using k-Nearest Neighbors to\\nClassify Flowers-Using k-Nearest Neighbors to Classify\\nFlowers\\nKNeighborsRegressor class, Using k-Nearest Neighbors to\\nClassify Flowers\\nKubernetes, Containerizing a Machine Learning Model'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 622, 'file_type': 'pdf'}, page_content='621\\nL\\n\\xa0\\nL-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–\\nShanno) algorithm, Logistic Regression\\nlabel column, What Is Machine Learning?\\nlabeled data, Supervised Versus Unsupervised Learning,\\nSupervised Learning\\nLabeled Faces in the Wild (LFW) dataset, What Is Machine\\nLearning?, Using SVMs for Facial Recognition,\\nUnderstanding Principal Component Analysis, Training a\\nNeural Network to Recognize Faces, Facial Recognition\\nLabelEncoder class, Categorical Data\\nlabels\\nand classification models, Classification Models\\nencoding in clustering, Segmenting Customers Using\\nMore Than Two Dimensions-Segmenting Customers Using\\nMore Than Two Dimensions, Categorical Data-\\nCategorical Data\\nsentiment analysis dataset as labeled, Sentiment\\nAnalysis\\nunlabeled dataset with PCA, Anomaly Detection, Using\\nPCA to Predict Bearing Failure\\nlanguage models (see natural language processing)\\nLanguage service, Introducing Azure Cognitive Services,\\nThe Language Service-The Language Service\\nLaplace smoothing, Naive Bayes\\nLasso class, Linear Regression\\nlazy learning algorithm, Using k-Nearest Neighbors to\\nClassify Flowers'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 623, 'file_type': 'pdf'}, page_content='622\\nleaf node, decision tree, Decision Trees\\nlearning algorithm, Machine Learning\\nlearning rates, Gradient-Boosting Machines, Training\\nNeural Networks\\nLearningRateScheduler class, Keras Callbacks\\nlemmatizing, Preparing Text for Classification\\nLFW (Labeled Faces in the Wild) dataset, What Is Machine\\nLearning?, Using SVMs for Facial Recognition,\\nUnderstanding Principal Component Analysis, Training a\\nNeural Network to Recognize Faces, Facial Recognition\\nLibrosa package, Audio Classification with CNNs\\nLimited-memory Broyden–Fletcher–Goldfarb–Shanno (L-\\nBFGS) algorithm, Logistic Regression\\nlinear kernel, Kernels, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nlinear regression, Linear Regression-Linear Regression,\\nUnderstanding Neural Networks\\nLinearRegression class, Linear Regression, Using\\nRegression to Predict Taxi Fares\\nLinearSVC class, Using SVMs for Facial Recognition\\nLoadFromTextFile method, ML.NET, Sentiment Analysis with\\nML.NET\\nload_iris function, Using k-Nearest Neighbors to Classify\\nFlowers\\nlocal outlier factor (LOF), Anomaly Detection\\nlogistic function, Logistic Regression\\nlogistic regression, Logistic Regression-Logistic\\nRegression'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 624, 'file_type': 'pdf'}, page_content='623\\ncredit card fraud illustration, Detecting Credit Card\\nFraud-Detecting Credit Card Fraud\\ndigit recognition model, Building a Digit Recognition\\nModel-Building a Digit Recognition Model\\nmultinomial, Multiclass Classification\\nsentiment analysis, Sentiment Analysis-Sentiment\\nAnalysis\\nTitanic passenger survival example, Classifying\\nPassengers Who Sailed on the Titanic-Classifying\\nPassengers Who Sailed on the Titanic\\nLogisticRegression class, Logistic Regression\\nLogisticRegressionCV class, Logistic Regression\\nlogit function, Logistic Regression\\nlong short-term memory (LSTM) cell, Recurrent Neural\\nNetworks (RNNs)\\nloss functions, Training Neural Networks, Multiclass\\nClassification with Neural Networks\\nloss landscape, Training Neural Networks\\nloss parameter, Building Neural Networks with Keras and\\nTensorFlow\\nLSTM (long short-term memory) cell, Recurrent Neural\\nNetworks (RNNs)\\nLSTM class, Recurrent Neural Networks (RNNs)\\nLSTM encoder-decoders, LSTM Encoder-Decoders-LSTM\\nEncoder-Decoders\\nM\\n\\xa0\\nmachine learning (ML), Machine Learning-Summary'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 625, 'file_type': 'pdf'}, page_content='624\\nversus artificial intelligence, Machine Learning\\nVersus Artificial Intelligence-Machine Learning\\nVersus Artificial Intelligence\\noperationalizing (see operationalizing ML models)\\nprogramming resources, Running the Book’s Code\\nSamples\\nspam filter example, What Is Machine Learning?\\nsupervised models (see supervised learning models)\\nand testing data, Using k-Nearest Neighbors to\\nClassify Flowers\\ntypes of learning, Supervised Versus Unsupervised\\nLearning-Using k-Nearest Neighbors to Classify\\nFlowers\\nunsupervised models (see unsupervised learning\\nmodels)\\nMAE (mean absolute error), Building Neural Networks with\\nKeras and TensorFlow, Building Neural Networks with Keras\\nand TensorFlow\\nmake_blobs function, Unsupervised Learning with k-Means\\nClustering\\nmake_pipeline function, Pipelining, Consuming a Python\\nModel from a Python Client\\nManhattan distance, Using k-Nearest Neighbors to Classify\\nFlowers\\nmAP (mean Average Precision), Training a Custom Object\\nDetection Model with the Custom Vision Service\\nMask R-CNN, Mask R-CNN-Mask R-CNN\\nmasked language modeling, Bidirectional Encoder\\nRepresentations from Transformers (BERT)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 626, 'file_type': 'pdf'}, page_content='625\\nMatplotlib, Unsupervised Learning with k-Means\\nClustering, Visualizing High-Dimensional Data\\nmax pooling layer, Understanding CNNs, Using Keras and\\nTensorFlow to Build CNNs\\nMaxPooling1D class, Factoring Word Order into Predictions\\nMaxPooling2D class, Using Keras and TensorFlow to Build\\nCNNs\\nmax_df parameter, Preparing Text for Classification\\nMBGD (mini-batch gradient descent), Training Neural\\nNetworks\\nmean absolute error (MAE), Building Neural Networks with\\nKeras and TensorFlow, Building Neural Networks with Keras\\nand TensorFlow\\nmean Average Precision (mAP), Training a Custom Object\\nDetection Model with the Custom Vision Service\\nmean squared error (MSE), Linear Regression, Building\\nNeural Networks with Keras and TensorFlow\\nmetrics parameter, Building Neural Networks with Keras\\nand TensorFlow\\nMHA (multi-head attention), Transformer Encoder-Decoders\\nMicrosoft.ML.OnnxRuntime, Using ONNX to Bridge the\\nLanguage Gap\\nmini-batch gradient descent (MBGD), Training Neural\\nNetworks\\nMiniLM models, Building a BERT-Based Question Answering\\nSystem-Building a BERT-Based Question Answering System\\nMinkowski distance, Using k-Nearest Neighbors to Classify\\nFlowers\\nMinMaxScaler, Data Normalization, Data Normalization'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 627, 'file_type': 'pdf'}, page_content='626\\nminNeighbors parameter, Using the OpenCV Implementation\\nof Viola-Jones\\nmin_df parameter, Preparing Text for Classification\\nML (see machine learning)\\nML Operations (MLOps), Versioning Pickle Files\\nML.NET, building models in C#, Building ML Models in C#\\nwith ML.NET-Saving and Loading ML.NET Models\\nMLContext class, Sentiment Analysis with ML.NET\\nMLPClassifier class, Understanding Neural Networks\\nMLPRegressor class, Understanding Neural Networks\\nMLPs (multilayer perceptrons), Understanding Neural\\nNetworks, Understanding CNNs\\nMNIST dataset, Using Keras and TensorFlow to Build CNNs,\\nTraining a CNN to Recognize Arctic Wildlife, Global\\nPooling\\nMobiFace, Facial Recognition\\nMobileNetV2, Pretrained CNNs, Audio Classification with\\nCNNs\\nModelCheckpoint class, Keras Callbacks\\nmovie recommendations, Building a Movie Recommendation\\nSystem-Building a Movie Recommendation System\\nMplot3D, Visualizing High-Dimensional Data\\nMSE (mean squared error), Linear Regression, Building\\nNeural Networks with Keras and TensorFlow\\nMTCNNs (multitask cascaded convolutional neural\\nnetworks), Face Detection with Convolutional Neural\\nNetworks-Face Detection with Convolutional Neural\\nNetworks, Putting It All Together: Detecting and\\nRecognizing Faces in Photos-Putting It All Together:\\nDetecting and Recognizing Faces in Photos'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 628, 'file_type': 'pdf'}, page_content='627\\nmulti-head attention (MHA), Transformer Encoder-Decoders\\nmulticlass classification models, Supervised Learning,\\nUsing k-Nearest Neighbors to Classify Flowers-Using k-\\nNearest Neighbors to Classify Flowers, Classification\\nModels, Multiclass Classification-Multiclass\\nClassification, Multiclass Classification with Neural\\nNetworks-Multiclass Classification with Neural Networks\\nmulticollinearity, Linear Regression\\nMultiHeadAttention class, Building a Transformer-Based\\nNMT Model\\nmultilabel classification models, Classification Models\\nmultilayer perceptrons (MLPs), Understanding Neural\\nNetworks, Understanding CNNs\\nmultinomial logistic regression, Multiclass\\nClassification\\nMultinomialNB class, Naive Bayes, Spam Filtering\\nmultiple linear regression, Linear Regression\\nmultitask cascaded convolutional neural networks\\n(MTCNNs), Face Detection with Convolutional Neural\\nNetworks-Face Detection with Convolutional Neural\\nNetworks, Putting It All Together: Detecting and\\nRecognizing Faces in Photos-Putting It All Together:\\nDetecting and Recognizing Faces in Photos\\nmultivariate anomaly detection, Multivariate Anomaly\\nDetection\\nN\\n\\xa0\\nn-grams, Preparing Text for Classification, Factoring\\nWord Order into Predictions\\nNaive Bayes learning algorithm, Naive Bayes-Naive Bayes'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 629, 'file_type': 'pdf'}, page_content='628\\nnamed-entity recognition, The Language Service\\nnatural language processing (NLP), Natural Language\\nProcessing-Summary\\nand AI as a service, Introducing Azure Cognitive\\nServices\\nAzure Language service, Introducing Azure Cognitive\\nServices, The Language Service-The Language Service\\nAzure Speech service, Introducing Azure Cognitive\\nServices, The Speech Service-The Speech Service\\nAzure Translator service, Introducing Azure Cognitive\\nServices, The Translator Service-The Translator\\nService\\nBERT, Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform\\nSentiment Analysis\\ncombining with computer vision, Image Classification\\nwith Convolutional Neural Networks\\nencoder-decoder network for machine translation, LSTM\\nEncoder-Decoders-Transformer Encoder-Decoders\\nextracting text from photos, The Computer Vision\\nService\\nneural machine translation, Neural Machine\\nTranslation-Using Pretrained Models to Translate Text\\nsentiment analysis (see sentiment analysis)\\ntext classification (see text classification and\\nprocessing)\\ntext preparation, Text Preparation-Text Preparation\\nword embeddings, Natural Language Processing, Word\\nEmbeddings-Word Embeddings, Recurrent Neural Networks\\n(RNNs)-Recurrent Neural Networks (RNNs)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 630, 'file_type': 'pdf'}, page_content='629\\nNatural Language Toolkit (NLTK), Preparing Text for\\nClassification, Text Preparation\\nneural machine translation (NMT), Neural Machine\\nTranslation-Using Pretrained Models to Translate Text\\nbuilding a transformer-based model, Building a\\nTransformer-Based NMT Model-Building a Transformer-\\nBased NMT Model\\nLSTM encoder-decoders, LSTM Encoder-Decoders-LSTM\\nEncoder-Decoders\\ntransformer encoder-decoders, Transformer Encoder-\\nDecoders-Transformer Encoder-Decoders\\nneural networks, Deep Learning-Understanding Neural\\nNetworks, Neural Networks-Summary\\nbackpropagation, Training Neural Networks-Training\\nNeural Networks, Building Neural Networks with Keras\\nand TensorFlow, Dropout\\nbinary classification with, Binary Classification\\nwith Neural Networks-Training a Neural Network to\\nDetect Credit Card Fraud\\ndropout, Sizing a Neural Network, Dropout-Dropout,\\nGlobal Pooling\\nfacial recognition, Training a Neural Network to\\nRecognize Faces-Training a Neural Network to\\nRecognize Faces\\nKeras callbacks, Keras Callbacks-Keras Callbacks\\nmulticlass classification with, Multiclass\\nClassification with Neural Networks-Multiclass\\nClassification with Neural Networks\\nmultilayer perceptrons, Understanding Neural\\nNetworks, Understanding CNNs'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 631, 'file_type': 'pdf'}, page_content='630\\nsaving and loading models, Saving and Loading Models-\\nSaving and Loading Models\\nsizing, Sizing a Neural Network\\ntaxi fare prediction, Using a Neural Network to\\nPredict Taxi Fares-Using a Neural Network to Predict\\nTaxi Fares\\ntraining, Training Neural Networks-Training Neural\\nNetworks\\nNimbusML, Building ML Models in C# with ML.NET\\nNLTK (Natural Language Toolkit), Preparing Text for\\nClassification, Text Preparation\\nNMS (non-maximum suppression), R-CNNs\\nNMT (see neural machine translation)\\nnoise, filtering, Filtering Noise-Filtering Noise\\nnon-maximum suppression (NMS), R-CNNs\\nnonparametric models, Linear Regression, Random Forests-\\nGradient-Boosting Machines, Detecting Credit Card Fraud,\\nUsing PCA to Detect Credit Card Fraud\\n(see also decision trees)\\nnormalization, Linear Regression, Data Normalization-Data\\nNormalization, Training a Neural Network to Recognize\\nFaces\\nNumPy, Using Keras and TensorFlow to Build CNNs\\nNumPy arrays, Using ONNX to Bridge the Language Gap\\nNuSVC class, Using SVMs for Facial Recognition\\nNvidia GPU card, Deep Learning, Training a CNN to\\nRecognize Arctic Wildlife\\nO'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 632, 'file_type': 'pdf'}, page_content='631\\nobject detection, Object Detection-Summary\\nbounding boxes, Using the OpenCV Implementation of\\nViola-Jones, R-CNNs, R-CNNs, Mask R-CNN, YOLO,\\nTraining a Custom Object Detection Model with the\\nCustom Vision Service\\nComputer Vision service, The Computer Vision Service-\\nThe Computer Vision Service\\ncustom, Custom Object Detection-Using the Exported\\nModel\\nfaces (see face detection)\\nR-CNNs, R-CNNs-Mask R-CNN\\nYOLO, YOLO-YOLOv3 and Keras\\nobjectness score, R-CNNs\\nocclusions, Training a Custom Object Detection Model with\\nthe Custom Vision Service\\nOLS (ordinary least squares) regression, Linear\\nRegression\\none-class SVM, Anomaly Detection\\none-hot encoding, Categorical Data\\none-versus-all strategy, Multiclass Classification\\none-versus-one strategy, Multiclass Classification\\none-versus-rest strategy, Multiclass Classification\\nOneHotEncoder class, Categorical Data\\nONNX (Open Neural Network Exchange), Operationalizing\\nMachine Learning Models, Using ONNX to Bridge the\\nLanguage Gap-Using ONNX to Bridge the Language Gap, Mask\\nR-CNN-Mask R-CNN\\nopen-set versus closed-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set Classification-'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 633, 'file_type': 'pdf'}, page_content='632\\nHandling Unknown Faces: Closed-Set Versus Open-Set\\nClassification\\nOpenCV library, Using the OpenCV Implementation of Viola-\\nJones-Using the OpenCV Implementation of Viola-Jones\\nopenmax output layer, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\noperationalizing ML models, Operationalizing Machine\\nLearning Models-Summary\\nadding ML capabilities to Excel, Adding Machine\\nLearning Capabilities to Excel-Adding Machine\\nLearning Capabilities to Excel\\nbuilding models in C# with ML.NET, Building ML Models\\nin C# with ML.NET-Saving and Loading ML.NET Models\\nconsuming Python model from C# client, Consuming a\\nPython Model from a C# Client-Consuming a Python\\nModel from a C# Client\\nconsuming Python model from Python client, Consuming\\na Python Model from a Python Client-Consuming a\\nPython Model from a Python Client\\ncontainerizing an ML model, Containerizing a Machine\\nLearning Model-Containerizing a Machine Learning\\nModel\\nONNX to bridge language gap, Operationalizing Machine\\nLearning Models, Using ONNX to Bridge the Language\\nGap-Using ONNX to Bridge the Language Gap\\nversioning pickle files, Versioning Pickle Files\\nOptical Recognition of Handwritten Digits dataset,\\nBuilding a Digit Recognition Model, Visualizing High-\\nDimensional Data\\noptimizers, Training Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 634, 'file_type': 'pdf'}, page_content='633\\nordinary least squares (OLS) regression, Linear\\nRegression\\noutlier detection (see anomaly detection)\\noutliers, Segmenting Customers Using More Than Two\\nDimensions, Linear Regression, Using Regression to\\nPredict Taxi Fares, Anomaly Detection\\nOutput class, ML.NET, Sentiment Analysis with ML.NET\\noutput layer, neural network, Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow, Multiclass\\nClassification with Neural Networks\\nOutput Network (O-Net), Face Detection with Convolutional\\nNeural Networks\\noverfitting of data, Decision Trees, Hyperparameter\\nTuning, Building Neural Networks with Keras and\\nTensorFlow\\nP\\n\\xa0\\npad_sequences function, Text Preparation, Automating Text\\nVectorization\\npair plots, Linear Regression\\npairplot function, Linear Regression\\nparallelism\\nand random forests, Random Forests\\nregion proposal network, R-CNNs\\nparametric learning algorithm, Linear Regression\\n(see also support vector machines)\\nPCA (see principal component analysis)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 635, 'file_type': 'pdf'}, page_content='634\\nPCA transform, Understanding Principal Component Analysis\\nPerceptron class, Understanding Neural Networks\\nPersonalizer service, Introducing Azure Cognitive\\nServices\\npersonally identifiable information (PII), The Language\\nService\\npickle files, versioning, Versioning Pickle Files\\npickle module, Consuming a Python Model from a Python\\nClient\\npipelines\\nSVMs, Pipelining-Pipelining\\ntraining and saving, Consuming a Python Model from a\\nPython Client-Consuming a Python Model from a Python\\nClient\\nplot_confusion_matrix function, Accuracy Measures for\\nClassification Models\\npolynomial kernel, Kernels, Hyperparameter Tuning, Using\\nSVMs for Facial Recognition\\nPolynomialFeatures class, Linear Regression\\npooling layers, Understanding CNNs\\npopularity-based recommender system, Recommender Systems\\npositional encoding (positional embedding), Transformer\\nEncoder-Decoders\\nPower BI, Accuracy Measures for Classification Models\\nprecision and recall, classifier metrics, Accuracy\\nMeasures for Classification Models-Accuracy Measures for\\nClassification Models, Classifying Passengers Who Sailed\\non the Titanic'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 636, 'file_type': 'pdf'}, page_content='635\\npredict method, Using k-Nearest Neighbors to Classify\\nFlowers, Logistic Regression, Spam Filtering, Building\\nNeural Networks with Keras and TensorFlow\\npredictions, What Is Machine Learning?\\naccuracy measures, Training a Neural Network to\\nDetect Credit Card Fraud-Training a Neural Network to\\nDetect Credit Card Fraud, Training a CNN to Recognize\\nArctic Wildlife\\nanomaly detection, Using PCA to Predict Bearing\\nFailure-Using PCA to Predict Bearing Failure\\nconfusion matrix, Accuracy Measures for\\nClassification Models, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\ndecision trees, Decision Trees-Decision Trees\\nk-nearest neighbors for image classification, Using\\nk-Nearest Neighbors to Classify Flowers\\nwith neural network, Making Predictions-Making\\nPredictions\\npipelining, Pipelining\\nregression models, Using Regression to Predict Taxi\\nFares-Using Regression to Predict Taxi Fares\\ntaxi fares, Using Regression to Predict Taxi Fares-\\nUsing Regression to Predict Taxi Fares, Using a\\nNeural Network to Predict Taxi Fares-Using a Neural\\nNetwork to Predict Taxi Fares\\nword order in, Factoring Word Order into Predictions-\\nFactoring Word Order into Predictions\\npredictors, Gradient-Boosting Machines\\npredict_proba method, Logistic Regression, Sentiment\\nAnalysis, Spam Filtering\\npreprocess function, Mask R-CNN, Mask R-CNN'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 637, 'file_type': 'pdf'}, page_content='636\\npreprocessing data\\nimage preprocessing layers, Image Augmentation with\\nImageDataGenerator-Image Augmentation with\\nImageDataGenerator\\nLabelEncoder, Segmenting Customers Using More Than\\nTwo Dimensions, Categorical Data\\nMask R-CNN, Mask R-CNN\\nMinMaxScaler, Data Normalization\\nStandardScaler, Data Normalization, Data\\nNormalization, Pipelining, Training a Neural Network\\nto Recognize Faces\\ntext sequences, Text Preparation\\npreprocessor parameter, Preparing Text for Classification\\npretraining and pretrained models, Pretrained CNNs-Using\\nResNet50V2 to Classify Images\\nArcFace, ArcFace\\nBERT, Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform\\nSentiment Analysis\\ntext classification with, Using Pretrained Models to\\nClassify Text\\nVGGFace, Boosting Transfer Learning with Task-\\nSpecific Weights-Boosting Transfer Learning with\\nTask-Specific Weights\\nword embeddings, Word Embeddings\\nprimary principal component, Understanding Principal\\nComponent Analysis\\nprincipal component analysis (PCA), Linear Regression,\\nDetecting Credit Card Fraud, Principal Component\\nAnalysis-Summary'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 638, 'file_type': 'pdf'}, page_content='637\\nanomaly detection, Anomaly Detection-Multivariate\\nAnomaly Detection\\nanonymizing data, Anonymizing Data-Anonymizing Data\\ncredit card fraud detection, Anonymizing Data, Using\\nPCA to Detect Credit Card Fraud-Using PCA to Detect\\nCredit Card Fraud\\nfiltering noise, Filtering Noise-Filtering Noise\\nhigh-dimensional data visualization, Visualizing\\nHigh-Dimensional Data-Visualizing High-Dimensional\\nData\\nprivacy issue, and facial recognition, Face Detection and\\nRecognition\\nprobabilities, estimating, What Is Machine Learning?\\nProposal Network (P-Net), Face Detection with\\nConvolutional Neural Networks\\nPython, Running the Book’s Code Samples\\n(see also Scikit-Learn)\\nAzure Cognitive Services packages, Calling Azure\\nCognitive Services APIs, The Computer Vision Service\\nconsuming Python model from C# client, Consuming a\\nPython Model from a C# Client-Consuming a Python\\nModel from a C# Client\\nconsuming Python model from Python client, Consuming\\na Python Model from a Python Client-Consuming a\\nPython Model from a Python Client\\nenvironment setup, Running the Book’s Code Samples\\nExcel UDFs converted to, Adding Machine Learning\\nCapabilities to Excel\\noperationalizing machine learning models,\\nOperationalizing Machine Learning Models'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 639, 'file_type': 'pdf'}, page_content='638\\nQ\\n\\xa0\\nquestion-answering modules, Building a BERT-Based\\nQuestion Answering System-Building a BERT-Based Question\\nAnswering System\\nR\\n\\xa0\\nR-CNNs (region-based CNNs), R-CNNs-Mask R-CNN\\nR² (coefficient of determination), Accuracy Measures for\\nRegression Models, Using a Neural Network to Predict Taxi\\nFares\\nradius neighbors, Using k-Nearest Neighbors to Classify\\nFlowers\\nRadiusNeighborsClassifier class, Using k-Nearest\\nNeighbors to Classify Flowers\\nRadiusNeighborsRegressor class, Using k-Nearest Neighbors\\nto Classify Flowers\\nRainforest Connection, Audio Classification with CNNs\\nrainforest sounds dataset, Audio Classification with CNNs\\nrandom forests, Random Forests-Random Forests, Detecting\\nCredit Card Fraud, Using PCA to Detect Credit Card Fraud\\nRandomForestClassifier class, Random Forests, Detecting\\nCredit Card Fraud, Detecting Credit Card Fraud\\nRandomForestRegressor class, Random Forests, Using\\nRegression to Predict Taxi Fares\\nrandom_state parameter, Unsupervised Learning with k-\\nMeans Clustering, Accuracy Measures for Regression Models\\nRBF kernel, Kernels, Hyperparameter Tuning-Hyperparameter\\nTuning, Using SVMs for Facial Recognition'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 640, 'file_type': 'pdf'}, page_content='639\\nRBMT (rules-based machine translation), Neural Machine\\nTranslation\\nread_in_stream method, The Computer Vision Service\\nrecall and precision classifier metrics, Accuracy\\nMeasures for Classification Models-Accuracy Measures for\\nClassification Models, Classifying Passengers Who Sailed\\non the Titanic\\nrecall_score, Accuracy Measures for Classification Models\\nreceiver operating characteristic (ROC) curve, Accuracy\\nMeasures for Classification Models\\nrecognize_once_async method, The Speech Service\\nrecognize_pii_entities method, The Language Service\\nrecognize_printed_text_in_stream method, The Computer\\nVision Service\\nrecommender systems, Recommender Systems-Building a Movie\\nRecommendation System\\nreconstruction error, Anomaly Detection\\nrectified linear units (ReLU), Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow, Sizing a Neural\\nNetwork\\nrecurrent neural networks (RNNs), Understanding Neural\\nNetworks, Recurrent Neural Networks (RNNs)-Recurrent\\nNeural Networks (RNNs)\\nRefine Network (R-Net), Face Detection with Convolutional\\nNeural Networks\\nregion of interest (ROI) alignment, Mask R-CNN\\nregion of interest (ROI) pooling, R-CNNs\\nregion proposal network (RPN), R-CNNs, R-CNNs\\nregion-based CNNs (R-CNNs), R-CNNs-Mask R-CNN'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 641, 'file_type': 'pdf'}, page_content='640\\nregression models, Supervised Learning-Supervised\\nLearning, Regression Models-Summary\\naccuracy measures for, Accuracy Measures for\\nRegression Models-Accuracy Measures for Regression\\nModels\\nand coefficient of determination, Accuracy Measures\\nfor Regression Models\\ndecision trees, Decision Trees-Decision Trees\\nGBMs, Gradient-Boosting Machines-Gradient-Boosting\\nMachines\\nand k-nearest neighbors, k-Nearest Neighbors-k-\\nNearest Neighbors\\nlinear regression, Linear Regression-Linear\\nRegression, Using Regression to Predict Taxi Fares\\nrandom forests, Random Forests-Random Forests,\\nDetecting Credit Card Fraud, Using PCA to Detect\\nCredit Card Fraud\\nsoftmax regression, Handling Unknown Faces: Closed-\\nSet Versus Open-Set Classification\\nSVMs, Support Vector Machines\\ntaxi fare prediction, Using Regression to Predict\\nTaxi Fares-Using Regression to Predict Taxi Fares\\nregularization, Linear Regression, How Support Vector\\nMachines Work, Using SVMs for Facial Recognition\\nreinforcement learning, Machine Learning Versus\\nArtificial Intelligence\\nRekognition service, Azure Cognitive Services\\nReLU (rectified linear units), Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow, Sizing a Neural\\nNetwork'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 642, 'file_type': 'pdf'}, page_content='641\\nRepeatVector layer, LSTM Encoder-Decoders\\nRequests package, Calling Azure Cognitive Services APIs\\nRescaling layer, Image Augmentation with Augmentation\\nLayers\\nresidual layers, Pretrained CNNs\\nresiduals, decision trees, Gradient-Boosting Machines\\nResNet-152, Image Classification with Convolutional\\nNeural Networks\\nResNet-50V2, Pretrained CNNs-Using ResNet50V2 to Classify\\nImages, Using Transfer Learning to Identify Arctic\\nWildlife-Using Transfer Learning to Identify Arctic\\nWildlife\\nResponsible AI initiative, Microsoft, The Computer Vision\\nService\\nREST APIs, and AI as a service, Azure Cognitive Services,\\nKeys and Endpoints\\nretriever-reader architecture, Building a BERT-Based\\nQuestion Answering System\\nRidge class, Linear Regression\\nRNNs (recurrent neural networks), Understanding Neural\\nNetworks, Recurrent Neural Networks (RNNs)-Recurrent\\nNeural Networks (RNNs)\\nROC (receiver operating characteristic) curve, Accuracy\\nMeasures for Classification Models\\nRocCurveDisplay class, Accuracy Measures for\\nClassification Models, Spam Filtering\\nroc_auc_score, Accuracy Measures for Classification\\nModels\\nROI (region of interest) alignment, Mask R-CNN\\nROI (region of interest) pooling, R-CNNs'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 643, 'file_type': 'pdf'}, page_content='642\\nRPN (region proposal network), R-CNNs, R-CNNs\\nrules-based machine translation (RBMT), Neural Machine\\nTranslation\\nS\\n\\xa0\\nSavedModel format, Saving and Loading Models, Using\\nTextVectorization in a Sentiment Analysis Model\\nScaper, soundscape synthesis, Audio Classification with\\nCNNs\\nscatter function, Unsupervised Learning with k-Means\\nClustering\\nScikit-Learn, Unsupervised Learning with k-Means\\nClustering\\nhyperparameter optimizers, Hyperparameter Tuning\\nversus ML.Net, Building ML Models in C# with ML.NET,\\nSentiment Analysis with ML.NET\\nmulticlass classification feature, Multiclass\\nClassification-Multiclass Classification\\nSkl2onnx conversion, Using ONNX to Bridge the\\nLanguage Gap\\nsklearn.cluster.KMeans, Unsupervised Learning with k-\\nMeans Clustering\\nsklearn.datasets.fetch_california_housing, Accuracy\\nMeasures for Regression Models\\nsklearn.datasets.fetch_lfw_people, Using SVMs for\\nFacial Recognition, Training a Neural Network to\\nRecognize Faces\\nsklearn.datasets.load_breast_cancer, Anonymizing Data\\nsklearn.datasets.load_digits, Visualizing High-\\nDimensional Data'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 644, 'file_type': 'pdf'}, page_content='643\\nsklearn.datasets.load_iris, Using k-Nearest Neighbors\\nto Classify Flowers\\nsklearn.decomposition.PCA, Understanding Principal\\nComponent Analysis, Visualizing High-Dimensional\\nData, Using PCA to Detect Credit Card Fraud, Using\\nPCA to Predict Bearing Failure\\nsklearn.ensemble.GradientBoostingClassifier,\\nGradient-Boosting Machines-Gradient-Boosting\\nMachines, Detecting Credit Card Fraud\\nsklearn.ensemble.GradientBoostingRegressor, Gradient-\\nBoosting Machines-Gradient-Boosting Machines, Using\\nRegression to Predict Taxi Fares\\nsklearn.ensemble.RandomForestClassifier, Random\\nForests, Detecting Credit Card Fraud, Detecting\\nCredit Card Fraud\\nsklearn.ensemble.RandomForestRegressor, Random\\nForests, Using Regression to Predict Taxi Fares\\nsklearn.feature_extraction.text.CountVectorizer,\\nPreparing Text for Classification-Preparing Text for\\nClassification, Spam Filtering, Recommender Systems,\\nConsuming a Python Model from a Python Client, Text\\nPreparation\\nsklearn.feature_extraction.text.HashingVectorizer,\\nPreparing Text for Classification, Preparing Text for\\nClassification, Consuming a Python Model from a\\nPython Client\\nsklearn.feature_extraction.text.TfidfVectorizer,\\nPreparing Text for Classification, Preparing Text for\\nClassification\\nsklearn.linear_model.Lasso, Linear Regression\\nsklearn.linear_model.LinearRegression, Linear\\nRegression, Accuracy Measures for Regression Models,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 645, 'file_type': 'pdf'}, page_content='644\\nUsing Regression to Predict Taxi Fares\\nsklearn.linear_model.LogisticRegression, Consuming a\\nPython Model from a Python Client\\nsklearn.linear_model.Perceptron, Understanding Neural\\nNetworks\\nsklearn.linear_model.Ridge, Linear Regression\\nsklearn.manifold.TSNE, Visualizing High-Dimensional\\nData\\nsklearn.metrics.ConfusionMatrixDisplay, Accuracy\\nMeasures for Classification Models, Spam Filtering\\nsklearn.metrics.RocCurveDisplay, Spam Filtering\\nsklearn.metrics.roc_auc_score, Accuracy Measures for\\nClassification Models\\nsklearn.model_selection.GridSearchCV, Hyperparameter\\nTuning, Pipelining, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nsklearn.model_selection.train_test_split, Using k-\\nNearest Neighbors to Classify Flowers, Accuracy\\nMeasures for Regression Models-Accuracy Measures for\\nRegression Models\\nsklearn.naive_bayes.MultinomialNB, Spam Filtering\\nsklearn.neighbors.KNeighborsClassifier, Using k-\\nNearest Neighbors to Classify Flowers\\nsklearn.neural_network.MLPClassifier, Understanding\\nNeural Networks\\nsklearn.neural_network.MLPRegressor, Understanding\\nNeural Networks\\nsklearn.pipeline.make_pipeline, Pipelining, Consuming\\na Python Model from a Python Client'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 646, 'file_type': 'pdf'}, page_content='645\\nsklearn.preprocessing.LabelEncoder, Segmenting\\nCustomers Using More Than Two Dimensions, Categorical\\nData\\nsklearn.preprocessing.MinMaxScaler, Data\\nNormalization\\nsklearn.preprocessing.OneHotEncoder, Categorical Data\\nsklearn.preprocessing.PolynomialFeatures, Linear\\nRegression\\nsklearn.preprocessing.StandardScaler, Data\\nNormalization, Data Normalization, Pipelining,\\nTraining a Neural Network to Recognize Faces\\nsklearn.svm.SVC, Support Vector Machines, Support\\nVector Machines, Hyperparameter Tuning\\nsklearn.svm.SVR, Support Vector Machines, Support\\nVector Machines\\nsklearn.tree.DecisionTreeClassifier, Decision Trees\\nsklearn.tree.DecisionTreeRegressor, Decision Trees\\nsklearn.utils.shuffle, Accuracy Measures for\\nRegression Models\\nscore method, Using k-Nearest Neighbors to Classify\\nFlowers, Accuracy Measures for Regression Models\\nscree plot, Understanding Principal Component Analysis\\nsegmentation masks, Mask R-CNN-Mask R-CNN\\nselective search, R-CNNs\\nself-attention, Transformer Encoder-Decoders\\nsensitivity metric, Accuracy Measures for Classification\\nModels, Classifying Passengers Who Sailed on the Titanic\\nsentiment analysis, What Is Machine Learning?, Text\\nClassification'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 647, 'file_type': 'pdf'}, page_content='646\\nAzure Language service, The Language Service\\nwith BERT, Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\nHugging Face resources, Using Pretrained Models to\\nClassify Text\\nwith ML.NET, Sentiment Analysis with ML.NET\\ntext classification, Sentiment Analysis-Sentiment\\nAnalysis, Using TextVectorization in a Sentiment\\nAnalysis Model-Using TextVectorization in a Sentiment\\nAnalysis Model\\ntraining and saving pipeline, Consuming a Python\\nModel from a Python Client-Consuming a Python Model\\nfrom a Python Client\\nseparable convolutions, Pretrained CNNs\\nsequence-to-sequence model, LSTM Encoder-Decoders\\nsequences, word embeddings, Natural Language Processing,\\nText Preparation\\nsequence_to_texts method, Text Preparation\\nsequential API, Keras, Neural Networks-Building Neural\\nNetworks with Keras and TensorFlow\\nset_weights method, Saving and Loading Models\\nSGD (stochastic gradient descent), Training Neural\\nNetworks\\nshuffling data, Accuracy Measures for Regression Models,\\nBuilding Neural Networks with Keras and TensorFlow, Text\\nClassification\\nsigmoid activation function, Building Neural Networks\\nwith Keras and TensorFlow, Binary Classification with\\nNeural Networks-Making Predictions, Understanding CNNs\\nsigmoid kernel, Kernels'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 648, 'file_type': 'pdf'}, page_content='647\\nsimilarity matrix, recommender system, Cosine Similarity\\nsimple linear regression, Linear Regression\\nsizing a neural network, Sizing a Neural Network\\nSMT (statistical machine translation), Neural Machine\\nTranslation\\nsoft mask, Mask R-CNN\\nsoftmax activation function, Multiclass Classification,\\nBuilding Neural Networks with Keras and TensorFlow\\nbuilding CNNs, Understanding CNNs\\nclosed- versus open-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set\\nClassification-Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nin facial recognition, Training a Neural Network to\\nRecognize Faces\\nLSTM encoder-decoders, LSTM Encoder-Decoders\\nversus sigmoid activation, Multiclass Classification\\nwith Neural Networks\\ntransformer-based LMT model, Building a Transformer-\\nBased NMT Model\\nsoftmax regression, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nspam filtering, What Is Machine Learning?, Supervised\\nVersus Unsupervised Learning, Naive Bayes-Spam Filtering,\\nText Classification-Text Classification\\nsparse_categorical_crossentropy function, Multiclass\\nClassification with Neural Networks, Training a Neural\\nNetwork to Recognize Faces\\nspatial analysis, The Computer Vision Service'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 649, 'file_type': 'pdf'}, page_content='648\\nspecificity metric, Accuracy Measures for Classification\\nModels, Classifying Passengers Who Sailed on the Titanic\\nspectrogram images, Audio Classification with CNNs-Audio\\nClassification with CNNs\\nSpeech service, Introducing Azure Cognitive Services, The\\nSpeech Service-The Speech Service\\nSpeechSynthesizer class, The Speech Service\\nSQuAD 2.0, Building a BERT-Based Question Answering\\nSystem\\nstandardization, Data Normalization\\nStandardScaler class, Data Normalization, Data\\nNormalization, Pipelining, Training a Neural Network to\\nRecognize Faces\\nstatistical machine translation (SMT), Neural Machine\\nTranslation\\nstemming, Preparing Text for Classification\\nstochastic gradient descent (SGD), Training Neural\\nNetworks\\nstop words, removing, Preparing Text for Classification,\\nSentiment Analysis, Text Preparation, Text Preparation,\\nAutomating Text Vectorization\\nsubsampling, Gradient-Boosting Machines\\nsupervised learning models, Supervised Versus\\nUnsupervised Learning, Supervised Learning-Using k-\\nNearest Neighbors to Classify Flowers\\nbinary classification, Binary Classification-\\nDetecting Credit Card Fraud, Binary Classification\\nwith Neural Networks-Training a Neural Network to\\nDetect Credit Card Fraud\\nk-nearest neighbors, k-Nearest Neighbors-Using k-\\nNearest Neighbors to Classify Flowers'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 650, 'file_type': 'pdf'}, page_content='649\\nsupport vector machines (SVMs), Support Vector Machines,\\nSupport Vector Machines-Summary\\nfacial recognition with, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nhyperparameter tuning, Hyperparameter Tuning-\\nHyperparameter Tuning\\nkernels and kernel tricks, Kernels-Kernel Tricks\\nnormalization of data, Data Normalization-Data\\nNormalization\\npipelining, Pipelining-Pipelining\\nSVC class, Support Vector Machines, Support Vector\\nMachines, Hyperparameter Tuning\\nSVR class, Support Vector Machines, Support Vector\\nMachines\\nT\\n\\xa0\\nt-distributed stochastic neighbor embedding (t-SNE),\\nLinear Regression, Visualizing High-Dimensional Data\\ntag_image_in_stream method, The Computer Vision Service\\ntanh activation function, Building Neural Networks with\\nKeras and TensorFlow\\ntask-specific weights to boost transfer learning,\\nBoosting Transfer Learning with Task-Specific Weights-\\nBoosting Transfer Learning with Task-Specific Weights\\nTatoeba project, Building a Transformer-Based NMT Model\\ntaxi fare prediction, Using Regression to Predict Taxi\\nFares-Using Regression to Predict Taxi Fares, Using a\\nNeural Network to Predict Taxi Fares-Using a Neural\\nNetwork to Predict Taxi Fares'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 651, 'file_type': 'pdf'}, page_content='650\\ntensor arrays, Neural Networks\\ntensor processing units (TPUs), Deep Learning\\nTensorBoard, Keras Callbacks\\ntensordot function, Using Keras and TensorFlow to Build\\nCNNs\\nTensorFlow, Training Neural Networks\\n(see also Keras API)\\nbuilding neural networks with, Building Neural\\nNetworks with Keras and TensorFlow-Using a Neural\\nNetwork to Predict Taxi Fares\\nCNNs, Using Keras and TensorFlow to Build CNNs-Using\\nKeras and TensorFlow to Build CNNs\\nconverting BERT tokenized inputs into, Fine-Tuning\\nBERT to Perform Sentiment Analysis\\nsaving models, Saving and Loading Models, Using\\nTextVectorization in a Sentiment Analysis Model\\nTensorFlow Lite, Audio Classification with CNNs\\nterm frequency-inverse document frequency (TF-IDF),\\nPreparing Text for Classification\\ntest set, Using k-Nearest Neighbors to Classify Flowers,\\nAccuracy Measures for Regression Models, Accuracy\\nMeasures for Regression Models\\ntext classification and processing, What Is Machine\\nLearning?, Text Classification-Summary, Text\\nClassification-Using Pretrained Models to Classify Text\\ncleaning text, Preparing Text for Classification\\ndatasets for working with text, What Is Machine\\nLearning?\\nextracting text from photos, The Computer Vision\\nService'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 652, 'file_type': 'pdf'}, page_content='651\\nfactoring word order into predictions, Factoring Word\\nOrder into Predictions-Factoring Word Order into\\nPredictions\\nhandwritten text, Building a Digit Recognition Model-\\nBuilding a Digit Recognition Model, The Computer\\nVision Service-The Computer Vision Service\\nNaive Bayes learning algorithm, Naive Bayes-Naive\\nBayes\\nNMT as extension of (see neural machine translation)\\npretrained models for, Using Pretrained Models to\\nClassify Text\\nrecommender systems, Recommender Systems-Building a\\nMovie Recommendation System\\nand RNNs, Recurrent Neural Networks (RNNs)-Recurrent\\nNeural Networks (RNNs)\\nsentiment analysis, Sentiment Analysis-Sentiment\\nAnalysis, Using TextVectorization in a Sentiment\\nAnalysis Model-Using TextVectorization in a Sentiment\\nAnalysis Model\\nspam filtering, Naive Bayes-Spam Filtering\\nvectorization, Preparing Text for Classification-\\nPreparing Text for Classification, Automating Text\\nVectorization-Using TextVectorization in a Sentiment\\nAnalysis Model\\nTextAnalyticsClient class, Calling Azure Cognitive\\nServices APIs, The Language Service\\ntexts_to_sequences method, Text Preparation\\nTextVectorization layer, Text Preparation, Automating\\nText Vectorization-Using TextVectorization in a Sentiment\\nAnalysis Model'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 653, 'file_type': 'pdf'}, page_content='652\\nTF-IDF (term frequency-inverse document frequency),\\nPreparing Text for Classification\\nTfidfVectorizer class, Preparing Text for Classification,\\nPreparing Text for Classification\\ntime series data, forecasting, Recurrent Neural Networks\\n(RNNs)\\nTimeDistributed wrapper, LSTM Encoder-Decoders\\nTitanic passenger classification dataset, Classifying\\nPassengers Who Sailed on the Titanic-Classifying\\nPassengers Who Sailed on the Titanic\\nTokenAndPositionEmbedding class, Building a Transformer-\\nBased NMT Model, Building a Transformer-Based NMT Model\\ntokenization, BERT, Building a BERT-Based Question\\nAnswering System-Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\ntokenized words, Preparing Text for Classification\\nTokenizer class, Text Preparation-Text Preparation,\\nAutomating Text Vectorization, Building a Transformer-\\nBased NMT Model\\ntokens, word, Text Preparation\\nTPR (true positive rate), Accuracy Measures for\\nClassification Models\\nTPUs (tensor processing units), Deep Learning\\ntrainable parameters, Using a Neural Network to Predict\\nTaxi Fares\\ntraining set, Using k-Nearest Neighbors to Classify\\nFlowers\\ntraining versus validation accuracy, Dropout\\ntrain_test_split function, Using k-Nearest Neighbors to\\nClassify Flowers, Accuracy Measures for Regression'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 654, 'file_type': 'pdf'}, page_content='653\\nModels-Accuracy Measures for Regression Models, Building\\nNeural Networks with Keras and TensorFlow\\ntransfer learning, Building ML Models in C# with ML.NET,\\nTransfer Learning-Using Transfer Learning to Identify\\nArctic Wildlife, Applying Transfer Learning to Facial\\nRecognition-Boosting Transfer Learning with Task-Specific\\nWeights\\ntransformer models, Natural Language Processing,\\nTransformer Encoder-Decoders\\nattention mechanisms, Transformer Encoder-Decoders-\\nTransformer Encoder-Decoders\\nBERT-based question answering, Building a BERT-Based\\nQuestion Answering System-Building a BERT-Based\\nQuestion Answering System\\nbuilding an NMT, Building a Transformer-Based NMT\\nModel-Building a Transformer-Based NMT Model\\nDistilBERT, Bidirectional Encoder Representations\\nfrom Transformers (BERT)\\nencoder-decoders for NMT, Transformer Encoder-\\nDecoders-Transformer Encoder-Decoders\\nHugging Face, Using Pretrained Models to Classify\\nText, Using Pretrained Models to Translate Text,\\nBuilding a BERT-Based Question Answering System-Fine-\\nTuning BERT to Perform Sentiment Analysis\\nTransformerDecoder class, Building a Transformer-Based\\nNMT Model, Building a Transformer-Based NMT Model\\nTransformerEncoder class, Building a Transformer-Based\\nNMT Model, Building a Transformer-Based NMT Model\\ntranslate_text function, Building a Transformer-Based NMT\\nModel\\ntranslation (see neural machine translation)'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 655, 'file_type': 'pdf'}, page_content='654\\nTranslationRecognizer object, The Speech Service\\nTranslator service, Introducing Azure Cognitive Services,\\nThe Translator Service-The Translator Service\\ntrue positive rate (TPR), Accuracy Measures for\\nClassification Models\\nTSNE class, Visualizing High-Dimensional Data\\n2D to 3D space, kernel trick, Kernel Tricks-Kernel Tricks\\nU\\n\\xa0\\nUDFs (user-defined functions), Excel, Adding Machine\\nLearning Capabilities to Excel\\nunderfitting of data, Hyperparameter Tuning, Building\\nNeural Networks with Keras and TensorFlow\\nunit variance, normalizing data to, Data Normalization\\nuniversal approximation theorem, Understanding Neural\\nNetworks\\nunsupervised learning, Supervised Versus Unsupervised\\nLearning\\nanomaly detection, Anomaly Detection\\nclustering (see clustering algorithms)\\nGANs, Understanding Neural Networks\\nUrbanSound8K dataset, Audio Classification with CNNs\\nuser-defined functions (UDFs), Excel, Adding Machine\\nLearning Capabilities to Excel\\nV\\n\\xa0\\nvalidation accuracy, Building Neural Networks with Keras\\nand TensorFlow, Using a Neural Network to Predict Taxi'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 656, 'file_type': 'pdf'}, page_content='655\\nFares-Using a Neural Network to Predict Taxi Fares\\nin building CNNs, Using Keras and TensorFlow to Build\\nCNNs, Training a CNN to Recognize Arctic Wildlife\\ncallbacks to capture peak, Keras Callbacks-Keras\\nCallbacks\\nconfusion matrix to visualize untrained data,\\nTraining a Neural Network to Detect Credit Card Fraud\\nand dropout technique, Dropout\\nand LSTMs, Recurrent Neural Networks (RNNs)\\nin NMT, Building a Transformer-Based NMT Model\\nvalidation_split function, Building Neural Networks with\\nKeras and TensorFlow, Using a Neural Network to Predict\\nTaxi Fares, Text Classification\\nvanishing gradient problem, Sizing a Neural Network\\nvectorization, text, Preparing Text for Classification-\\nPreparing Text for Classification, Automating Text\\nVectorization-Using TextVectorization in a Sentiment\\nAnalysis Model\\nversioning pickle files, Versioning Pickle Files\\nVGGFace\\ndetecting and recognizing faces in photos, Putting It\\nAll Together: Detecting and Recognizing Faces in\\nPhotos-Putting It All Together: Detecting and\\nRecognizing Faces in Photos\\ntask-specific weights to boost transfer learning,\\nBoosting Transfer Learning with Task-Specific\\nWeights-Boosting Transfer Learning with Task-Specific\\nWeights\\nViola-Jones, face detection with, Face Detection with\\nViola-Jones-Using the OpenCV Implementation of Viola-'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 657, 'file_type': 'pdf'}, page_content='656\\nJones\\nVision services\\nComputer Vision service, Image Classification with\\nConvolutional Neural Networks, The Computer Vision\\nService-The Computer Vision Service\\nCustom Vision service, Training a Custom Object\\nDetection Model with the Custom Vision Service-\\nTraining a Custom Object Detection Model with the\\nCustom Vision Service, Introducing Azure Cognitive\\nServices\\nFace service, Introducing Azure Cognitive Services,\\nThe Computer Vision Service\\nvisualization of data\\nconfusion matrix for untrained data, Training a\\nNeural Network to Detect Credit Card Fraud\\nhigh-dimensional for PCA, Visualizing High-\\nDimensional Data-Visualizing High-Dimensional Data\\nvocabulary, word dataset, Text Preparation\\nW\\n\\xa0\\nweak learners, Gradient-Boosting Machines\\nweb service, accessing Python through, Operationalizing\\nMachine Learning Models, Consuming a Python Model from a\\nC# Client-Consuming a Python Model from a C# Client,\\nUsing ONNX to Bridge the Language Gap\\nWeibull distribution, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nweights, Understanding Neural Networks, Understanding\\nNeural Networks, Building Neural Networks with Keras and\\nTensorFlow, Using a Neural Network to Predict Taxi Fares,'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 658, 'file_type': 'pdf'}, page_content='657\\nBoosting Transfer Learning with Task-Specific Weights-\\nBoosting Transfer Learning with Task-Specific Weights\\nword embeddings, Natural Language Processing, Word\\nEmbeddings-Word Embeddings, Recurrent Neural Networks\\n(RNNs)-Recurrent Neural Networks (RNNs)\\nword order, factoring into predictions, Factoring Word\\nOrder into Predictions-Factoring Word Order into\\nPredictions\\nword vectors, Natural Language Processing, Recurrent\\nNeural Networks (RNNs)-Recurrent Neural Networks (RNNs)\\nWordPiece format, Building a BERT-Based Question\\nAnswering System\\nwrapping a model, Operationalizing Machine Learning\\nModels\\nX\\n\\xa0\\nXception (Extreme Inception), Pretrained CNNs\\nXlwings library, Adding Machine Learning Capabilities to\\nExcel\\nY\\n\\xa0\\nYou Only Look Once (YOLO), YOLO-YOLOv3 and Keras\\nZ\\n\\xa0\\nZ-score normalization, Data Normalization\\nZeroes initializer, Building Neural Networks with Keras\\nand TensorFlow'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 659, 'file_type': 'pdf'}, page_content='658\\nAbout the Author\\nJeff Prosise is an engineer whose passion is to introduce\\nother engineers and software developers to the wonders of AI\\nand machine learning. Cofounder of Wintellect, he has written\\nnine books and hundreds of magazine articles, trained\\nthousands of developers at Microsoft, and spoken at some of\\nthe world’s largest software conferences. In another life,\\nJeff worked on high-powered laser systems and fusion-energy\\nresearch at Oak Ridge National Laboratory and Lawrence\\nLivermore National Laboratory. In his spare time, he builds\\nand flies large radio-control jets and goes out of his way to\\nget wet in the world’s best dive spots. Following the\\nacquisition of his company in 2021, Jeff serves as chief\\nlearning officer at Atmosera, where he helps customers infuse\\nAI into their products.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 660, 'file_type': 'pdf'}, page_content='659\\nColophon\\nThe animal on the cover of Applied Machine Learning and AI\\nfor Engineers is a festive parrot (Amazona festiva), also\\nknown as a festive amazon. Festive parrots live in the\\ntropical forests, woodlands, and coastal mangroves of several\\nSouth American countries, including Brazil, Colombia,\\nEcuador, Peru, and Bolivia. They are rarely found far from\\nwater.\\nFestive parrots are brightly—you might even say festively—\\ncolored, medium-sized birds. Their plumage is predominantly a\\nstriking green, turning slightly yellow toward the edges of\\ntheir wings. A motley assortment of colors—including red,\\nblue, and sometimes yellow or orange—adorns their faces.\\nFestive parrots are a highly social species, usually spotted\\nin pairs or small flocks. Large groups of the birds often\\ngather at night for communal roosts or around a localized\\nfood source and are known for being incredibly noisy. They\\nenjoy eating fruits such as mangoes and peach palm, with\\nberries, nuts, seeds, flowers, and leaf buds supplementing\\ntheir diet.\\nWhile still relatively common where their forest habitat\\nremains largely intact, festive parrots have been categorized\\nby IUCN as near threatened due to continued deforestation and\\npredicted declines in habitat. Many of the animals on\\nO’Reilly covers are endangered; all of them are important to\\nthe world.\\nThe cover illustration is by Karen Montgomery, based on an\\nantique line engraving from Wood’s Illustrated Natural\\nHistory. The cover fonts are Gilroy Semibold and Guardian\\nSans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s\\nUbuntu Mono.'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 661, 'file_type': 'pdf'}, page_content='660\\nTable of Contents\\nForeword\\nPreface\\nWho Should Read This Book\\nWhy I Wrote This Book\\nRunning the Book’s Code Samples\\nNavigating This Book\\nConventions Used in This Book\\nUsing Code Examples\\nO’Reilly Online Learning\\nHow to Contact Us\\nAcknowledgments\\nI. Machine Learning with Scikit-Learn\\n1. Machine Learning\\nWhat Is Machine Learning?\\nMachine Learning Versus Artificial\\nIntelligence\\nSupervised \\nVersus \\nUnsupervised\\nLearning\\nUnsupervised Learning with k-Means Clustering\\nApplying \\nk-Means \\nClustering \\nto\\nCustomer Data\\nSegmenting Customers Using More Than\\nTwo Dimensions\\nSupervised Learning\\nk-Nearest Neighbors\\nUsing k-Nearest Neighbors to Classify\\nFlowers\\nSummary\\n2. Regression Models\\nLinear Regression\\nDecision Trees\\nRandom Forests\\nGradient-Boosting Machines\\nSupport Vector Machines'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 662, 'file_type': 'pdf'}, page_content='661\\nAccuracy Measures for Regression Models\\nUsing Regression to Predict Taxi Fares\\nSummary\\n3. Classification Models\\nLogistic Regression\\nAccuracy Measures for Classification Models\\nCategorical Data\\nBinary Classification\\nClassifying Passengers Who Sailed on\\nthe Titanic\\nDetecting Credit Card Fraud\\nMulticlass Classification\\nBuilding a Digit Recognition Model\\nSummary\\n4. Text Classification\\nPreparing Text for Classification\\nSentiment Analysis\\nNaive Bayes\\nSpam Filtering\\nRecommender Systems\\nCosine Similarity\\nBuilding \\na \\nMovie \\nRecommendation\\nSystem\\nSummary\\n5. Support Vector Machines\\nHow Support Vector Machines Work\\nKernels\\nKernel Tricks\\nHyperparameter Tuning\\nData Normalization\\nPipelining\\nUsing SVMs for Facial Recognition\\nSummary\\n6. Principal Component Analysis\\nUnderstanding Principal Component Analysis\\nFiltering Noise\\nAnonymizing Data'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 663, 'file_type': 'pdf'}, page_content='662\\nVisualizing High-Dimensional Data\\nAnomaly Detection\\nUsing PCA to Detect Credit Card Fraud\\nUsing PCA to Predict Bearing Failure\\nMultivariate Anomaly Detection\\nSummary\\n7. Operationalizing Machine Learning Models\\nConsuming a Python Model from a Python Client\\nVersioning Pickle Files\\nConsuming a Python Model from a C# Client\\nContainerizing a Machine Learning Model\\nUsing ONNX to Bridge the Language Gap\\nBuilding ML Models in C# with ML.NET\\nSentiment Analysis with ML.NET\\nSaving and Loading ML.NET Models\\nAdding Machine Learning Capabilities to Excel\\nSummary\\nII. Deep Learning with Keras and TensorFlow\\n8. Deep Learning\\nUnderstanding Neural Networks\\nTraining Neural Networks\\nSummary\\n9. Neural Networks\\nBuilding Neural Networks with Keras and TensorFlow\\nSizing a Neural Network\\nUsing a Neural Network to Predict\\nTaxi Fares\\nBinary Classification with Neural Networks\\nMaking Predictions\\nTraining a Neural Network to Detect\\nCredit Card Fraud\\nMulticlass Classification with Neural Networks\\nTraining a Neural Network to Recognize Faces\\nDropout\\nSaving and Loading Models\\nKeras Callbacks\\nSummary'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 664, 'file_type': 'pdf'}, page_content='663\\n10. Image Classification with Convolutional Neural Networks\\nUnderstanding CNNs\\nUsing Keras and TensorFlow to Build\\nCNNs\\nTraining a CNN to Recognize Arctic\\nWildlife\\nPretrained CNNs\\nUsing ResNet50V2 to Classify Images\\nTransfer Learning\\nUsing Transfer Learning to Identify Arctic Wildlife\\nData Augmentation\\nImage \\nAugmentation \\nwith\\nImageDataGenerator\\nImage Augmentation with Augmentation\\nLayers\\nApplying Image Augmentation to Arctic\\nWildlife\\nGlobal Pooling\\nAudio Classification with CNNs\\nSummary\\n11. Face Detection and Recognition\\nFace Detection\\nFace Detection with Viola-Jones\\nUsing the OpenCV Implementation of\\nViola-Jones\\nFace Detection with Convolutional\\nNeural Networks\\nExtracting Faces from Photos\\nFacial Recognition\\nApplying Transfer Learning to Facial\\nRecognition\\nBoosting Transfer Learning with Task-\\nSpecific Weights\\nArcFace\\nPutting It All Together: Detecting and Recognizing\\nFaces in Photos'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 665, 'file_type': 'pdf'}, page_content='664\\nHandling Unknown Faces: Closed-Set Versus Open-Set\\nClassification\\nSummary\\n12. Object Detection\\nR-CNNs\\nMask R-CNN\\nYOLO\\nYOLOv3 and Keras\\nCustom Object Detection\\nTraining a Custom Object Detection\\nModel with the Custom Vision Service\\nUsing the Exported Model\\nSummary\\n13. Natural Language Processing\\nText Preparation\\nWord Embeddings\\nText Classification\\nAutomating Text Vectorization\\nUsing \\nTextVectorization \\nin \\na\\nSentiment Analysis Model\\nFactoring Word Order into Predictions\\nRecurrent Neural Networks (RNNs)\\nUsing Pretrained Models to Classify\\nText\\nNeural Machine Translation\\nLSTM Encoder-Decoders\\nTransformer Encoder-Decoders\\nBuilding \\na \\nTransformer-Based \\nNMT\\nModel\\nUsing Pretrained Models to Translate\\nText\\nBidirectional \\nEncoder \\nRepresentations \\nfrom\\nTransformers (BERT)\\nBuilding \\na \\nBERT-Based \\nQuestion\\nAnswering System\\nFine-Tuning BERT to Perform Sentiment\\nAnalysis'), Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 666, 'file_type': 'pdf'}, page_content='665\\nSummary\\n14. Azure Cognitive Services\\nIntroducing Azure Cognitive Services\\nKeys and Endpoints\\nCalling Azure Cognitive Services APIs\\nAzure Cognitive Services Containers\\nThe Computer Vision Service\\nThe Language Service\\nThe Translator Service\\nThe Speech Service\\nPutting It All Together: Contoso Travel\\nSummary\\nIndex'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 1, 'file_type': 'pdf'}, page_content='DATA STRUCTURES\\n\\n—— AND —\\nALGORITHMS\\nMADE EASY\\n\\n2S and Algorithmic Puzzles\\n\\nNarasimha Karumanchi, mech, wt Bombay\\nFounder, CareerMonk.com'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 2, 'file_type': 'pdf'}, page_content='Data Structures\\nAnd\\nAlgorithms\\nMade Easy\\n-To All My Readers\\nBy\\nNarasimha Karumanchi'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 3, 'file_type': 'pdf'}, page_content='Copyright© 2017 by CareerMonk.com\\nAll rights reserved.\\nDesigned by Narasimha Karumanchi\\nCopyright© 2017 CareerMonk Publications. All rights reserved.\\nAll rights reserved. No part of this book may be reproduced in any form or by any electronic or mechanical means, including\\ninformation storage and retrieval systems, without written permission from the publisher or author.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 4, 'file_type': 'pdf'}, page_content='Acknowledgements\\nMother and Father, it is impossible to thank you adequately for everything you have done, from\\nloving me unconditionally to raising me in a stable household, where your persistent efforts and\\ntraditional values taught your children to celebrate and embrace life. I could not have asked for\\nbetter parents or role-models. You showed me that anything is possible with faith, hard work and\\ndetermination.\\nThis book would not have been possible without the help of many people. I would like to express\\nmy gratitude to all of the people who provided support, talked things over, read, wrote, offered\\ncomments, allowed me to quote their remarks and assisted in the editing, proofreading and design.\\nIn particular, I would like to thank the following individuals:\\n▪\\nMohan Mullapudi, IIT Bombay, Architect, dataRPM Pvt. Ltd.\\n▪\\nNavin Kumar Jaiswal, Senior Consultant, Juniper Networks Inc.\\n▪\\nA. Vamshi Krishna, IIT Kanpur, Mentor Graphics Inc.\\n▪\\nCathy Reed, BA, MA, Copy Editor\\n–Narasimha Karumanchi\\nM-Tech, IIT Bombay\\nFounder, CareerMonk.com'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 5, 'file_type': 'pdf'}, page_content='Preface\\nDear Reader,\\nPlease hold on! I know many people typically do not read the Preface of a book. But I strongly\\nrecommend that you read this particular Preface.\\nIt is not the main objective of this book to present you with the theorems and proofs on data\\nstructures and algorithms. I have followed a pattern of improving the problem solutions with\\ndifferent complexities (for each problem, you will find multiple solutions with different, and\\nreduced, complexities). Basically, it’s an enumeration of possible solutions. With this approach,\\neven if you get a new question, it will show you a way to think about the possible solutions. You\\nwill find this book useful for interview preparation, competitive exams preparation, and campus\\ninterview preparations.\\nAs a job seeker, if you read the complete book, I am sure you will be able to challenge the\\ninterviewers. If you read it as an instructor, it will help you to deliver lectures with an approach\\nthat is easy to follow, and as a result your students will appreciate the fact that they have opted for\\nComputer Science / Information Technology as their degree.\\nThis book is also useful for Engineering degree students and Masters degree students during\\ntheir academic preparations. In all the chapters you will see that there is more emphasis on\\nproblems and their analysis rather than on theory. In each chapter, you will first read about the\\nbasic required theory, which is then followed by a section on problem sets. In total, there are\\napproximately 700 algorithmic problems, all with solutions.\\nIf you read the book as a student preparing for competitive exams for Computer Science /\\nInformation Technology, the content covers all the required topics in full detail. While writing\\nthis book, my main focus was to help students who are preparing for these exams.\\nIn all the chapters you will see more emphasis on problems and analysis rather than on theory. In\\neach chapter, you will first see the basic required theory followed by various problems.\\nFor many problems, multiple solutions are provided with different levels of complexity. We start\\nwith the brute force solution and slowly move toward the best solution possible for that problem.\\nFor each problem, we endeavor to understand how much time the algorithm takes and how much\\nmemory the algorithm uses.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 6, 'file_type': 'pdf'}, page_content='It is recommended that the reader does at least one complete reading of this book to gain a full\\nunderstanding of all the topics that are covered. Then, in subsequent readings you can skip\\ndirectly to any chapter to refer to a specific topic. Even though many readings have been done for\\nthe purpose of correcting errors, there could still be some minor typos in the book. If any are\\nfound, they will be updated at www.CareerMonk.com. You can monitor this site for any\\ncorrections and also for new problems and solutions. Also, please provide your valuable\\nsuggestions at: Info@CareerMonk.com.\\nI wish you all the best and I am confident that you will find this book useful.\\n–Narasimha Karumanchi\\nM-Tech, I IT Bombay\\nFounder, CareerMonk.com'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 7, 'file_type': 'pdf'}, page_content='Other Books by Narasimha Karumanchi\\nIT Interview Questions\\nData Structures and Algorithms for GATE\\nData Structures and Aigorithms Made Easy in Java\\nCoding Interview Questions\\nPeeling Design Patterns\\nElements of Computer Networking\\nData Structures and Algorithmic Thinking with Python'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 8, 'file_type': 'pdf'}, page_content='Table of Contents\\n1.   Introduction\\n1.1\\u2003Variables\\n1.2\\u2003Data Types\\n1.3\\u2003Data Structures\\n1.4\\u2003Abstract Data Types (ADTs)\\n1.5\\u2003What is an Algorithm?\\n1.6\\u2003Why the Analysis of Algorithms?\\n1.7\\u2003Goal of the Analysis of Algorithms\\n1.8\\u2003What is Running Time Analysis?\\n1.9\\u2003How to Compare Algorithms\\n1.10\\u2002What is Rate of Growth?\\n1.11\\u2002Commonly Used Rates of Growth\\n1.12\\u2002Types of Analysis\\n1.13\\u2002Asymptotic Notation\\n1.14\\u2002Big-O Notation [Upper Bounding Function]\\n1.15\\u2002Omega-Q Notation [Lower Bounding Function]\\n1.16\\u2002Theta-Θ Notation [Order Function]\\n1.17\\u2002Important Notes\\n1.18\\u2002Why is it called Asymptotic Analysis?\\n1.19\\u2002Guidelines for Asymptotic Analysis\\n1.20\\u2002Simplyfying properties of asymptotic notations\\n1.21\\u2002Commonly used Logarithms and Summations\\n1.22\\u2002Master Theorem for Divide and Conquer Recurrences\\n1.23\\u2002Divide and Conquer Master Theorem: Problems & Solutions\\n1.24\\u2002Master Theorem for Subtract and Conquer Recurrences\\n1.25\\u2002Variant of Subtraction and Conquer Master Theorem\\n1.26\\u2002Method of Guessing and Confirming'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 9, 'file_type': 'pdf'}, page_content='1.27\\u2002Amortized Analysis\\n1.28\\u2002Algorithms Analysis: Problems & Solutions\\n2.   Recursion and Backtracking\\n2.1\\u2003Introduction\\n2.2\\u2003What is Recursion?\\n2.3\\u2003Why Recursion?\\n2.4\\u2003Format of a Recursive Function\\n2.5\\u2003Recursion and Memory (Visualization)\\n2.6\\u2003Recursion versus Iteration\\n2.7\\u2003Notes on Recursion\\n2.8\\u2003Example Algorithms of Recursion\\n2.9\\u2003Recursion: Problems & Solutions\\n2.10\\u2002What is Backtracking?\\n2.11\\u2002Example Algorithms of Backtracking\\n2.12\\u2002Backtracking: Problems & Solutions\\n3.   Linked Lists\\n3.1\\u2003What is a Linked List?\\n3.2\\u2003Linked Lists ADT\\n3.3\\u2003Why Linked Lists?\\n3.4\\u2003Arrays Overview\\n3.5\\u2003Comparison of Linked Lists with Arrays & Dynamic Arrays\\n3.6\\u2003Singly Linked Lists\\n3.7\\u2003Doubly Linked Lists\\n3.8\\u2003Circular Linked Lists\\n3.9\\u2003A Memory-efficient Doubly Linked List\\n3.10\\u2002Unrolled Linked Lists\\n3.11\\u2002Skip Lists\\n3.12\\u2002Linked Lists: Problems & Solutions\\n4.   Stacks\\n4.1\\u2003What is a Stack?\\n4.2\\u2003How Stacks are used\\n4.3\\u2003Stack ADT'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 10, 'file_type': 'pdf'}, page_content='4.4\\u2003Applications\\n4.5\\u2003Implementation\\n4.6\\u2003Comparison of Implementations\\n4.7\\u2003Stacks: Problems & Solutions\\n5.   Queues\\n5.1\\u2003What is a Queue?\\n5.2\\u2003How are Queues Used?\\n5.3\\u2003Queue ADT\\n5.4\\u2003Exceptions\\n5.5\\u2003Applications\\n5.6\\u2003Implementation\\n5.7\\u2003Queues: Problems & Solutions\\n6.   Trees\\n6.1\\u2003What is a Tree?\\n6.2\\u2003Glossary\\n6.3\\u2003Binary Trees\\n6.4\\u2003Types of Binary Trees\\n6.5\\u2003Properties of Binary Trees\\n6.6\\u2003Binary Tree Traversals\\n6.7\\u2003Generic Trees (N-ary Trees)\\n6.8\\u2003Threaded Binary Tree Traversals (Stack or Queue-less Traversals)\\n6.9\\u2003Expression Trees\\n6.10\\u2002XOR Trees\\n6.11\\u2002Binary Search Trees (BSTs)\\n6.12\\u2003Balanced Binary Search Trees\\n6.13\\u2002AVL (Adelson-Velskii and Landis) Trees\\n6.14\\u2002Other Variations on Trees\\n7.   Priority Queues and Heaps\\n7.1\\u2003What is a Priority Queue?\\n7.2\\u2003Priority Queue ADT\\n7.3\\u2003Priority Queue Applications\\n7.4\\u2003Priority Queue Implementations'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 11, 'file_type': 'pdf'}, page_content='7.5\\u2003Heaps and Binary Heaps\\n7.6\\u2003Binary Heaps\\n7.7\\u2003Heapsort\\n7.8\\u2003Priority Queues [Heaps]: Problems & Solutions\\n8.   Disjoint Sets ADT\\n8.1\\u2003Introduction\\n8.2\\u2003Equivalence Relations and Equivalence Classes\\n8.3\\u2003Disjoint Sets ADT\\n8.4\\u2003Applications\\n8.5\\u2003Tradeoffs in Implementing Disjoint Sets ADT\\n8.8\\u2003Fast UNION Implementation (Slow FIND)\\n8.9\\u2003Fast UNION Implementations (Quick FIND)\\n8.10\\u2002Summary\\n8.11\\u2002Disjoint Sets: Problems & Solutions\\n9.   Graph Algorithms\\n9.1\\u2003Introduction\\n9.2\\u2003Glossary\\n9.3\\u2003Applications of Graphs\\n9.4\\u2003Graph Representation\\n9.5\\u2003Graph Traversals\\n9.6\\u2003Topological Sort\\n9.7\\u2003Shortest Path Algorithms\\n9.8\\u2003Minimal Spanning Tree\\n9.9\\u2003Graph Algorithms: Problems & Solutions\\n10.  Sorting\\n10.1\\u2003What is Sorting?\\n10.2\\u2003Why is Sorting Necessary?\\n10.3\\u2003Classification of Sorting Algorithms\\n10.4\\u2003Other Classifications\\n10.5\\u2003Bubble Sort\\n10.6\\u2003Selection Sort\\n10.7\\u2003Insertion Sort'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 12, 'file_type': 'pdf'}, page_content='10.8\\u2003Shell Sort\\n10.9\\u2003Merge Sort\\n10.10\\u2002Heap Sort\\n10.11\\u2002Quick Sort\\n10.12\\u2002Tree Sort\\n10.13\\u2002Comparison of Sorting Algorithms\\n10.14\\u2002Linear Sorting Algorithms\\n10.15\\u2002Counting Sort\\n10.16\\u2002Bucket Sort (or Bin Sort)\\n10.17\\u2002Radix Sort\\n10.18\\u2002Topological Sort\\n10.19\\u2002External Sorting\\n10.20\\u2002Sorting: Problems & Solutions\\n11.  Searching\\n11.1\\u2003What is Searching?\\n11.2\\u2003Why do we need Searching?\\n11.3\\u2003Types of Searching\\n11.4\\u2003Unordered Linear Search\\n11.5\\u2003Sorted/Ordered Linear Search\\n11.6\\u2003Binary Search\\n11.7\\u2003Interpolation Search\\n11.8\\u2003Comparing Basic Searching Algorithms\\n11.9\\u2003Symbol Tables and Hashing\\n11.10\\u2002String Searching Algorithms\\n11.11\\u2002Searching: Problems & Solutions\\n12.  Selection Algorithms [Medians]\\n12.1\\u2003What are Selection Algorithms?\\n12.2\\u2003Selection by Sorting\\n12.3\\u2003Partition-based Selection Algorithm\\n12.4\\u2003Linear Selection Algorithm - Median of Medians Algorithm\\n12.5\\u2003Finding the K Smallest Elements in Sorted Order\\n12.6\\u2003Selection Algorithms: Problems & Solutions'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 13, 'file_type': 'pdf'}, page_content='13.  Symbol Tables\\n13.1\\u2003Introduction\\n13.2\\u2003What are Symbol Tables?\\n13.3\\u2003Symbol Table Implementations\\n13.4\\u2003Comparison Table of Symbols for Implementations\\n14.  Hashing\\n14.1\\u2003What is Hashing?\\n14.2\\u2003Why Hashing?\\n14.3\\u2003HashTable ADT\\n14.4\\u2003Understanding Hashing\\n14.5\\u2003Components of Hashing\\n14.6\\u2003Hash Table\\n14.7\\u2003Hash Function\\n14.8\\u2003Load Factor\\n14.9\\u2003Collisions\\n14.10\\u2002Collision Resolution Techniques\\n14.11\\u2002Separate Chaining\\n14.12\\u2002Open Addressing\\n14.13\\u2002Comparison of Collision Resolution Techniques\\n14.14\\u2002How Hashing Gets O(1) Complexity?\\n14.15\\u2002Hashing Techniques\\n14.16\\u2002Problems for which Hash Tables are not suitable\\n14.17\\u2002Bloom Filters\\n14.18\\u2002Hashing: Problems & Solutions\\n15.  String Algorithms\\n15.1\\u2003Introduction\\n15.2\\u2003String Matching Algorithms\\n15.3\\u2003Brute Force Method\\n15.4\\u2003Rabin-Karp String Matching Algorithm\\n15.5\\u2003String Matching with Finite Automata\\n15.6\\u2003KMP Algorithm\\n15.7\\u2003Boyer-Moore Algorithm'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 14, 'file_type': 'pdf'}, page_content='15.8\\u2003Data Structures for Storing Strings\\n15.9\\u2003Hash Tables for Strings\\n15.10\\u2002Binary Search Trees for Strings\\n15.11\\u2002Tries\\n15.12\\u2002Ternary Search Trees\\n15.13\\u2002Comparing BSTs, Tries and TSTs\\n15.14\\u2002Suffix Trees\\n15.15\\u2002String Algorithms: Problems & Solutions\\n16.  Algorithms Design Techniques\\n16.1\\u2003Introduction\\n16.2\\u2003Classification\\n16.3\\u2003Classification by Implementation Method\\n16.4\\u2003Classification by Design Method\\n16.5\\u2003Other Classifications\\n17.  Greedy Algorithms\\n17.1\\u2003Introduction\\n17.2\\u2003Greedy Strategy\\n17.3\\u2003Elements of Greedy Algorithms\\n17.4\\u2003Does Greedy Always Work?\\n17.5\\u2003Advantages and Disadvantages of Greedy Method\\n17.6\\u2003Greedy Applications\\n17.7\\u2003Understanding Greedy Technique\\n17.8\\u2003Greedy Algorithms: Problems & Solutions\\n18.  Divide and Conquer Algorithms\\n18.1\\u2003Introduction\\n18.2\\u2003What is the Divide and Conquer Strategy?\\n18.3\\u2003Does Divide and Conquer Always Work?\\n18.4\\u2003Divide and Conquer Visualization\\n18.5\\u2003Understanding Divide and Conquer\\n18.6\\u2003Advantages of Divide and Conquer\\n18.7\\u2003Disadvantages of Divide and Conquer\\n18.8\\u2003Master Theorem'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 15, 'file_type': 'pdf'}, page_content='18.9\\u2003Divide and Conquer Applications\\n18.10\\u2002Divide and Conquer: Problems & Solutions\\n19.  Dynamic Programming\\n19.1\\u2003Introduction\\n19.2\\u2003What is Dynamic Programming Strategy?\\n19.3\\u2003Properties of Dynamic Programming Strategy\\n19.4\\u2003Can Dynamic Programming Solve All Problems?\\n19.5\\u2003Dynamic Programming Approaches\\n19.6\\u2003Examples of Dynamic Programming Algorithms\\n19.7\\u2003Understanding Dynamic Programming\\n19.8\\u2003Longest Common Subsequence\\n19.9\\u2003Dynamic Programming: Problems & Solutions\\n20.  Complexity Classes\\n20.1\\u2003Introduction\\n20.2\\u2003Polynomial/Exponential Time\\n20.3\\u2003What is a Decision Problem?\\n20.4\\u2003Decision Procedure\\n20.5\\u2003What is a Complexity Class?\\n20.6\\u2003Types of Complexity Classes\\n20.7\\u2003Reductions\\n20.8\\u2003Complexity Classes: Problems & Solutions\\n21.  Miscellaneous Concepts\\n21.1\\u2003Introduction\\n21.2\\u2003Hacks on Bit-wise Programming\\n21.3\\u2003Other Programming Questions\\nReferences'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 16, 'file_type': 'pdf'}, page_content='The objective of this chapter is to explain the importance of the analysis of algorithms, their\\nnotations, relationships and solving as many problems as possible. Let us first focus on\\nunderstanding the basic elements of algorithms, the importance of algorithm analysis, and then\\nslowly move toward the other topics as mentioned above. After completing this chapter, you\\nshould be able to find the complexity of any given algorithm (especially recursive functions).\\n1.1 Variables\\nBefore going to the definition of variables, let us relate them to old mathematical equations. All of\\nus have solved many mathematical equations since childhood. As an example, consider the below\\nequation:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 17, 'file_type': 'pdf'}, page_content='We don’t have to worry about the use of this equation. The important thing that we need to\\nunderstand is that the equation has names (x and y), which hold values (data). That means the\\nnames (x and y) are placeholders for representing data. Similarly, in computer science\\nprogramming we need something for holding data, and variables is the way to do that.\\n1.2 Data Types\\nIn the above-mentioned equation, the variables x and y can take any values such as integral\\nnumbers (10, 20), real numbers (0.23, 5.5), or just 0 and 1. To solve the equation, we need to\\nrelate them to the kind of values they can take, and data type is the name used in computer science\\nprogramming for this purpose. A data type in a programming language is a set of data with\\npredefined values. Examples of data types are: integer, floating point, unit number, character,\\nstring, etc.\\nComputer memory is all filled with zeros and ones. If we have a problem and we want to code it,\\nit’s very difficult to provide the solution in terms of zeros and ones. To help users, programming\\nlanguages and compilers provide us with data types. For example, integer takes 2 bytes (actual\\nvalue depends on compiler), float takes 4 bytes, etc. This says that in memory we are combining\\n2 bytes (16 bits) and calling it an integer. Similarly, combining 4 bytes (32 bits) and calling it a\\nfloat. A data type reduces the coding effort. At the top level, there are two types of data types:\\n•\\nSystem-defined data types (also called Primitive data types)\\n•\\nUser-defined data types\\nSystem-defined data types (Primitive data types)\\nData types that are defined by system are called primitive data types. The primitive data types\\nprovided by many programming languages are: int, float, char, double, bool, etc. The number of\\nbits allocated for each primitive data type depends on the programming languages, the compiler\\nand the operating system. For the same primitive data type, different languages may use different\\nsizes. Depending on the size of the data types, the total available values (domain) will also\\nchange.\\nFor example, “int” may take 2 bytes or 4 bytes. If it takes 2 bytes (16 bits), then the total possible\\nvalues are minus 32,768 to plus 32,767 (-215 to 215-1). If it takes 4 bytes (32 bits), then the\\npossible values are between -2,147,483,648 and +2,147,483,647 (-231 to 231-1). The same is the\\ncase with other data types.\\nUser defined data types\\nIf the system-defined data types are not enough, then most programming languages allow the users'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 18, 'file_type': 'pdf'}, page_content='to define their own data types, called user – defined data types. Good examples of user defined\\ndata types are: structures in C/C + + and classes in Java. For example, in the snippet below, we\\nare combining many system-defined data types and calling the user defined data type by the name\\n“newType”. This gives more flexibility and comfort in dealing with computer memory.\\n1.3 Data Structures\\nBased on the discussion above, once we have data in variables, we need some mechanism for\\nmanipulating that data to solve problems. Data structure is a particular way of storing and\\norganizing data in a computer so that it can be used efficiently. A data structure is a special\\nformat for organizing and storing data. General data structure types include arrays, files, linked\\nlists, stacks, queues, trees, graphs and so on.\\nDepending on the organization of the elements, data structures are classified into two types:\\n1)\\nLinear data structures: Elements are accessed in a sequential order but it is not\\ncompulsory to store all elements sequentially. Examples: Linked Lists, Stacks and\\nQueues.\\n2)\\nNon – linear data structures: Elements of this data structure are stored/accessed in a\\nnon-linear order. Examples: Trees and graphs.\\n1.4 Abstract Data Types (ADTs)\\nBefore defining abstract data types, let us consider the different view of system-defined data\\ntypes. We all know that, by default, all primitive data types (int, float, etc.) support basic\\noperations such as addition and subtraction. The system provides the implementations for the\\nprimitive data types. For user-defined data types we also need to define operations. The\\nimplementation for these operations can be done when we want to actually use them. That means,\\nin general, user defined data types are defined along with their operations.\\nTo simplify the process of solving problems, we combine the data structures with their operations\\nand we call this Abstract Data Types (ADTs). An ADT consists of two parts:\\n1.\\nDeclaration of data'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 19, 'file_type': 'pdf'}, page_content='2.\\nDeclaration of operations\\nCommonly used ADTs include: Linked Lists, Stacks, Queues, Priority Queues, Binary Trees,\\nDictionaries, Disjoint Sets (Union and Find), Hash Tables, Graphs, and many others. For\\nexample, stack uses LIFO (Last-In-First-Out) mechanism while storing the data in data structures.\\nThe last element inserted into the stack is the first element that gets deleted. Common operations\\nof it are: creating the stack, pushing an element onto the stack, popping an element from stack,\\nfinding the current top of the stack, finding number of elements in the stack, etc.\\nWhile defining the ADTs do not worry about the implementation details. They come into the\\npicture only when we want to use them. Different kinds of ADTs are suited to different kinds of\\napplications, and some are highly specialized to specific tasks. By the end of this book, we will\\ngo through many of them and you will be in a position to relate the data structures to the kind of\\nproblems they solve.\\n1.5 What is an Algorithm?\\nLet us consider the problem of preparing an omelette. To prepare an omelette, we follow the\\nsteps given below:\\n1)\\nGet the frying pan.\\n2)\\nGet the oil.\\na.\\nDo we have oil?\\n  i. If yes, put it in the pan.\\nii. If no, do we want to buy oil?\\n1. If yes, then go out and buy.\\n2. If no, we can terminate.\\n3)\\nTurn on the stove, etc...\\nWhat we are doing is, for a given problem (preparing an omelette), we are providing a step-by-\\nstep procedure for solving it. The formal definition of an algorithm can be stated as:\\nAn algorithm is the step-by-step unambiguous instructions to solve a given problem.\\nIn the traditional study of algorithms, there are two main criteria for judging the merits of\\nalgorithms: correctness (does the algorithm give solution to the problem in a finite number of\\nsteps?) and efficiency (how much resources (in terms of memory and time) does it take to execute\\nthe).\\nNote: We do not have to prove each step of the algorithm.\\n1.6 Why the Analysis of Algorithms?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 20, 'file_type': 'pdf'}, page_content='To go from city “A” to city “B”, there can be many ways of accomplishing this: by flight, by bus,\\nby train and also by bicycle. Depending on the availability and convenience, we choose the one\\nthat suits us. Similarly, in computer science, multiple algorithms are available for solving the\\nsame problem (for example, a sorting problem has many algorithms, like insertion sort, selection\\nsort, quick sort and many more). Algorithm analysis helps us to determine which algorithm is\\nmost efficient in terms of time and space consumed.\\n1.7 Goal of the Analysis of Algorithms\\nThe goal of the analysis of algorithms is to compare algorithms (or solutions) mainly in terms of\\nrunning time but also in terms of other factors (e.g., memory, developer effort, etc.)\\n1.8 What is Running Time Analysis?\\nIt is the process of determining how processing time increases as the size of the problem (input\\nsize) increases. Input size is the number of elements in the input, and depending on the problem\\ntype, the input may be of different types. The following are the common types of inputs.\\n•\\nSize of an array\\n•\\nPolynomial degree\\n•\\nNumber of elements in a matrix\\n•\\nNumber of bits in the binary representation of the input\\n•\\nVertices and edges in a graph.\\n1.9 How to Compare Algorithms\\nTo compare algorithms, let us define a few objective measures:\\nExecution times? Not a good measure as execution times are specific to a particular computer.\\nNumber of statements executed? Not a good measure, since the number of statements varies\\nwith the programming language as well as the style of the individual programmer.\\nIdeal solution? Let us assume that we express the running time of a given algorithm as a function\\nof the input size n (i.e., f(n)) and compare these different functions corresponding to running\\ntimes. This kind of comparison is independent of machine time, programming style, etc.\\n1.10 What is Rate of Growth?\\nThe rate at which the running time increases as a function of input is called rate of growth. Let us'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 21, 'file_type': 'pdf'}, page_content='assume that you go to a shop to buy a car and a bicycle. If your friend sees you there and asks\\nwhat you are buying, then in general you say buying a car. This is because the cost of the car is\\nhigh compared to the cost of the bicycle (approximating the cost of the bicycle to the cost of the\\ncar).\\nFor the above-mentioned example, we can represent the cost of the car and the cost of the bicycle\\nin terms of function, and for a given function ignore the low order terms that are relatively\\ninsignificant (for large value of input size, n). As an example, in the case below, n4, 2n2, 100n\\nand 500 are the individual costs of some function and approximate to n4 since n4 is the highest\\nrate of growth.\\n1.11 Commonly Used Rates of Growth\\nThe diagram below shows the relationship between different rates of growth.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 22, 'file_type': 'pdf'}, page_content='Below is the list of growth rates you will come across in the following chapters.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 23, 'file_type': 'pdf'}, page_content='1.12 Types of Analysis\\nTo analyze the given algorithm, we need to know with which inputs the algorithm takes less time\\n(performing wel1) and with which inputs the algorithm takes a long time. We have already seen\\nthat an algorithm can be represented in the form of an expression. That means we represent the\\nalgorithm with multiple expressions: one for the case where it takes less time and another for the\\ncase where it takes more time.\\nIn general, the first case is called the best case and the second case is called the worst case for\\nthe algorithm. To analyze an algorithm we need some kind of syntax, and that forms the base for\\nasymptotic analysis/notation. There are three types of analysis:\\n•\\nWorst case\\n○\\nDefines the input for which the algorithm takes a long time (slowest\\ntime to complete).\\n○\\nInput is the one for which the algorithm runs the slowest.\\n•\\nBest case\\n○\\nDefines the input for which the algorithm takes the least time (fastest\\ntime to complete).\\n○\\nInput is the one for which the algorithm runs the fastest.\\n•\\nAverage case\\n○\\nProvides a prediction about the running time of the algorithm.\\n○\\nRun the algorithm many times, using many different inputs that come\\nfrom some distribution that generates these inputs, compute the total\\nrunning time (by adding the individual times), and divide by the\\nnumber of trials.\\n○\\nAssumes that the input is random.\\nLower Bound <= Average Time <= Upper Bound'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 24, 'file_type': 'pdf'}, page_content='For a given algorithm, we can represent the best, worst and average cases in the form of\\nexpressions. As an example, let f(n) be the function which represents the given algorithm.\\nSimilarly for the average case. The expression defines the inputs with which the algorithm takes\\nthe average running time (or memory).\\n1.13 Asymptotic Notation\\nHaving the expressions for the best, average and worst cases, for all three cases we need to\\nidentify the upper and lower bounds. To represent these upper and lower bounds, we need some\\nkind of syntax, and that is the subject of the following discussion. Let us assume that the given\\nalgorithm is represented in the form of function f(n).\\n1.14 Big-O Notation [Upper Bounding Function]\\nThis notation gives the tight upper bound of the given function. Generally, it is represented as f(n)\\n= O(g(n)). That means, at larger values of n, the upper bound of f(n) is g(n). For example, if f(n)\\n= n4 + 100n2 + 10n + 50 is the given algorithm, then n4 is g(n). That means g(n) gives the\\nmaximum rate of growth for f(n) at larger values of n.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 25, 'file_type': 'pdf'}, page_content='Let us see the O–notation with a little more detail. O–notation defined as O(g(n)) = {f(n): there\\nexist positive constants c and n0 such that 0 ≤ f(n) ≤ cg(n) for all n > n0}. g(n) is an asymptotic\\ntight upper bound for f(n). Our objective is to give the smallest rate of growth g(n) which is\\ngreater than or equal to the given algorithms’ rate of growth /(n).\\nGenerally we discard lower values of n. That means the rate of growth at lower values of n is not\\nimportant. In the figure, n0 is the point from which we need to consider the rate of growth for a\\ngiven algorithm. Below n0, the rate of growth could be different. n0 is called threshold for the\\ngiven function.\\nBig-O Visualization\\nO(g(n)) is the set of functions with smaller or the same order of growth as g(n). For example;\\nO(n2) includes O(1), O(n), O(nlogn), etc.\\nNote: Analyze the algorithms at larger values of n only. What this means is, below n0 we do not\\ncare about the rate of growth.\\nBig-O Examples\\nExample-1 Find upper bound for f(n) = 3n + 8\\nSolution: 3n + 8 ≤ 4n, for all n ≥ 8\\n∴ 3n + 8 = O(n) with c = 4 and n0 = 8'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 26, 'file_type': 'pdf'}, page_content='Example-2 Find upper bound for f(n) = n2 + 1\\nSolution: n2 + 1 ≤ 2n2, for all n ≥ 1\\n∴ n2 + 1 = O(n2) with c = 2 and n0 = 1\\nExample-3 Find upper bound for f(n) = n4 + 100n2 + 50\\nSolution: n4 + 100n2 + 50 ≤ 2n4, for all n ≥ 11\\n∴ n4 + 100n2 + 50 = O(n4 ) with c = 2 and n0 = 11\\nExample-4 Find upper bound for f(n) = 2n3 – 2n2\\nSolution: 2n3 – 2n2 ≤ 2n3, for all n > 1\\n∴ 2n3 – 2n2 = O(n3 ) with c = 2 and n0 = 1\\nExample-5 Find upper bound for f(n) = n\\nSolution: n ≤ n, for all n ≥ 1\\n∴ n = O(n) with c = 1 and n0 = 1\\nExample-6 Find upper bound for f(n) = 410\\nSolution: 410 ≤ 410, for all n > 1\\n∴ 410 = O(1) with c = 1 and n0 = 1\\nNo Uniqueness?\\nThere is no unique set of values for n0 and c in proving the asymptotic bounds. Let us consider,\\n100n + 5 = O(n). For this function there are multiple n0 and c values possible.\\nSolution1: 100n + 5 ≤ 100n + n = 101n ≤ 101n, for all n ≥ 5, n0 = 5 and c = 101 is a solution.\\nSolution2: 100n + 5 ≤ 100n + 5n = 105n ≤ 105n, for all n > 1, n0 = 1 and c = 105 is also a\\nsolution.\\n1.15 Omega-Q Notation [Lower Bounding Function]\\nSimilar to the O discussion, this notation gives the tighter lower bound of the given algorithm and\\nwe represent it as f(n) = Ω(g(n)). That means, at larger values of n, the tighter lower bound of\\nf(n) is g(n). For example, if f(n) = 100n2 + 10n + 50, g(n) is Ω(n2).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 27, 'file_type': 'pdf'}, page_content='The Ω notation can be defined as Ω(g(n)) = {f(n): there exist positive constants c and n0 such that\\n0 ≤ cg(n) ≤ f(n) for all n ≥ n0}. g(n) is an asymptotic tight lower bound for f(n). Our objective is\\nto give the largest rate of growth g(n) which is less than or equal to the given algorithm’s rate of\\ngrowth f(n).\\nΩ Examples\\nExample-1 Find lower bound for f(n) = 5n2.\\nSolution: ∃ c, n0 Such that: 0 ≤ cn2≤ 5n2 ⇒ cn2 ≤ 5n2 ⇒ c = 5 and n0 = 1\\n∴ 5n2 = Ω(n2) with c = 5 and n0 = 1\\nExample-2 Prove f(n) = 100n + 5 ≠ Ω(n2).\\nSolution: ∃ c, n0 Such that: 0 ≤ cn2 ≤ 100n + 5\\n100n + 5 ≤ 100n + 5n(∀n ≥ 1) = 105n\\ncn2 ≤ 105n ⇒ n(cn - 105) ≤ 0\\nSince n is positive ⇒cn - 105 ≤0 ⇒ n ≤105/c\\n⇒ Contradiction: n cannot be smaller than a constant\\nExample-3 2n = Q(n), n3 = Q(n3), = O(logn).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 28, 'file_type': 'pdf'}, page_content='1.16 Theta-Θ Notation [Order Function]\\nThis notation decides whether the upper and lower bounds of a given function (algorithm) are the\\nsame. The average running time of an algorithm is always between the lower bound and the upper\\nbound. If the upper bound (O) and lower bound (Ω) give the same result, then the Θ notation will\\nalso have the same rate of growth.\\nAs an example, let us assume that f(n) = 10n + n is the expression. Then, its tight upper bound\\ng(n) is O(n). The rate of growth in the best case is g(n) = O(n).\\nIn this case, the rates of growth in the best case and worst case are the same. As a result, the\\naverage case will also be the same. For a given function (algorithm), if the rates of growth\\n(bounds) for O and Ω are not the same, then the rate of growth for the Θ case may not be the same.\\nIn this case, we need to consider all possible time complexities and take the average of those (for\\nexample, for a quick sort average case, refer to the Sorting chapter).\\nNow consider the definition of Θ notation. It is defined as Θ(g(n)) = {f(n): there exist positive\\nconstants c1,c2 and n0 such that 0 ≤ c1g(n) ≤ f(n) ≤ c2g(n) for all n ≥ n0}. g(n) is an asymptotic\\ntight bound for f(n). Θ(g(n)) is the set of functions with the same order of growth as g(n).\\nΘ Examples'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 29, 'file_type': 'pdf'}, page_content='Example 1 Find Θ bound for \\nSolution: \\n for all, n ≥ 2\\n∴ \\n with c1 = 1/5,c2 = 1 and n0 = 2\\nExample 2 Prove n ≠ Θ(n2)\\nSolution: c1 n2 ≤ n ≤ c2n2 ⇒ only holds for: n ≤ 1/c1\\n∴ n ≠ Θ(n2)\\nExample 3 Prove 6n3 ≠ Θ(n2)\\nSolution: c1 n2≤ 6n3 ≤ c2 n2 ⇒ only holds for: n ≤ c2 /6\\n∴ 6n3 ≠ Θ(n2)\\nExample 4 Prove n ≠ Θ(logn)\\nSolution: c1logn ≤ n ≤ c2logn ⇒ c2 ≥ \\n, ∀ n ≥ n0 – Impossible\\n1.17 Important Notes\\nFor analysis (best case, worst case and average), we try to give the upper bound (O) and lower\\nbound (Ω) and average running time (Θ). From the above examples, it should also be clear that,\\nfor a given function (algorithm), getting the upper bound (O) and lower bound (Ω) and average\\nrunning time (Θ) may not always be possible. For example, if we are discussing the best case of\\nan algorithm, we try to give the upper bound (O) and lower bound (Ω) and average running time\\n(Θ).\\nIn the remaining chapters, we generally focus on the upper bound (O) because knowing the lower\\nbound (Ω) of an algorithm is of no practical importance, and we use the Θ notation if the upper\\nbound (O) and lower bound (Ω) are the same.\\n1.18 Why is it called Asymptotic Analysis?\\nFrom the discussion above (for all three notations: worst case, best case, and average case), we\\ncan easily understand that, in every case for a given function f(n) we are trying to find another\\nfunction g(n) which approximates f(n) at higher values of n. That means g(n) is also a curve\\nwhich approximates f(n) at higher values of n.\\nIn mathematics we call such a curve an asymptotic curve. In other terms, g(n) is the asymptotic'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 30, 'file_type': 'pdf'}, page_content='curve for f(n). For this reason, we call algorithm analysis asymptotic analysis.\\n1.19 Guidelines for Asymptotic Analysis\\nThere are some general rules to help us determine the running time of an algorithm.\\n1)\\nLoops: The running time of a loop is, at most, the running time of the statements\\ninside the loop (including tests) multiplied by the number of iterations.\\nTotal time = a constant c × n = c n = O(n).\\n2)\\nNested loops: Analyze from the inside out. Total running time is the product of the\\nsizes of all the loops.\\nTotal time = c × n × n = cn2 = O(n2).\\n3)\\nConsecutive statements: Add the time complexities of each statement.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 31, 'file_type': 'pdf'}, page_content='Total time = c0 + c1n + c2n2 = O(n2).\\n4)\\nIf-then-else statements: Worst-case running time: the test, plus either the then part\\nor the else part (whichever is the larger).\\nTotal time = c0 + c1 + (c2 + c3) * n = O(n).\\n5)\\nLogarithmic complexity: An algorithm is O(logn) if it takes a constant time to cut\\nthe problem size by a fraction (usually by ½). As an example let us consider the\\nfollowing program:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 32, 'file_type': 'pdf'}, page_content='If we observe carefully, the value of i is doubling every time. Initially i = 1, in next step i\\n= 2, and in subsequent steps i = 4,8 and so on. Let us assume that the loop is executing\\nsome k times. At kth step 2k = n, and at (k + 1)th step we come out of the loop. Taking\\nlogarithm on both sides, gives\\nTotal time = O(logn).\\nNote: Similarly, for the case below, the worst case rate of growth is O(logn). The same\\ndiscussion holds good for the decreasing sequence as well.\\nAnother example: binary search (finding a word in a dictionary of n pages)\\n•\\nLook at the center point in the dictionary\\n•\\nIs the word towards the left or right of center?\\n•\\nRepeat the process with the left or right part of the dictionary until the word is found.\\n1.20 Simplyfying properties of asymptotic notations\\n•\\nTransitivity: f(n) = Θ(g(n)) and g(n) = Θ(h(n)) ⇒ f(n) = Θ(h(n)). Valid for O and Ω\\nas well.\\n•\\nReflexivity: f(n) = Θ(f(n)). Valid for O and Ω.\\n•\\nSymmetry: f(n) = Θ(g(n)) if and only if g(n) = Θ(f(n)).\\n•\\nTranspose symmetry: f(n) = O(g(n)) if and only if g(n) = Ω(f(n)).\\n•\\nIf f(n) is in O(kg(n)) for any constant k > 0, then f(n) is in O(g(n)).\\n•\\nIf f1(n) is in O(g1(n)) and f2(n) is in O(g2(n)), then (f1 + f2)(n) is in O(max(g1(n)),\\n(g1(n))).\\n•\\nIf f1(n) is in O(g1(n)) and f2(n) is in O(g2(n)) then f1(n) f2(n) is in O(g1(n) g1(n)).\\n1.21 Commonly used Logarithms and Summations\\nLogarithms'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 33, 'file_type': 'pdf'}, page_content='Arithmetic series\\nGeometric series\\nHarmonic series\\nOther important formulae\\n1.22 Master Theorem for Divide and Conquer Recurrences\\nAll divide and conquer algorithms (also discussed in detail in the Divide and Conquer chapter)\\ndivide the problem into sub-problems, each of which is part of the original problem, and then\\nperform some additional work to compute the final answer. As an example, a merge sort\\nalgorithm [for details, refer to Sorting chapter] operates on two sub-problems, each of which is\\nhalf the size of the original, and then performs O(n) additional work for merging. This gives the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 34, 'file_type': 'pdf'}, page_content='running time equation:\\nThe following theorem can be used to determine the running time of divide and conquer\\nalgorithms. For a given program (algorithm), first we try to find the recurrence relation for the\\nproblem. If the recurrence is of the below form then we can directly give the answer without fully\\nsolving it. If the recurrence is of the form \\n, where a ≥ 1,b >\\n1,k ≥ 0 and p is a real number, then:\\n1)\\nIf a > bk, then \\n2)\\nIf a= bk\\na.\\nIf p > –1, then \\nb.\\nIf p = –1, then \\nc.\\nIf p < –1, then \\n3)\\nIf a < bk\\na.\\nIf p ≥ 0, then T(n) = Θ(nklogpn)\\nb.\\nIf p < 0, then T(n) = O(nk)\\n1.23 Divide and Conquer Master Theorem: Problems & Solutions\\nFor each of the following recurrences, give an expression for the runtime T(n) if the recurrence\\ncan be solved with the Master Theorem. Otherwise, indicate that the Master Theorem does not\\napply.\\nProblem-1\\u2003\\u2003T(n) = 3T (n/2) + n2\\nSolution: T(n) = 3T (n/2) + n2 => T (n) =Θ(n2) (Master Theorem Case 3.a)\\nProblem-2\\u2003\\u2003T(n) = 4T (n/2) + n2\\nSolution: T(n) = 4T (n/2) + n2 => T (n) = Θ(n2logn) (Master Theorem Case 2.a)\\nProblem-3\\u2003\\u2003T(n) = T(n/2) + n2\\nSolution: T(n) = T(n/2) + n2 => Θ(n2) (Master Theorem Case 3.a)\\nProblem-4\\u2003\\u2003T(n) = 2nT(n/2) + nn\\nSolution: T(n) = 2nT(n/2) + nn => Does not apply (a is not constant)\\nProblem-5\\u2003\\u2003T(n) = 16T(n/4) + n\\nSolution: T(n) = 16T (n/4) + n => T(n) = Θ(n2) (Master Theorem Case 1)\\nProblem-6\\u2003\\u2003T(n) = 2T(n/2) + nlogn'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 35, 'file_type': 'pdf'}, page_content='Solution: T(n) = 2T(n/2) + nlogn => T(n) = Θ(nlog2n) (Master Theorem Case 2.a)\\nProblem-7\\u2003\\u2003T(n) = 2T(n/2) + n/logn\\nSolution: T(n) = 2T(n/2)+ n/logn =>T(n) = Θ(nloglogn) (Master Theorem Case 2. b)\\nProblem-8\\u2003\\u2003T(n) = 2T (n/4) + n051\\nSolution: T(n) = 2T(n/4) + n051 => T (n) = Θ(n0.51) (Master Theorem Case 3.b)\\nProblem-9\\u2003\\u2003T(n) = 0.5T(n/2) + 1/n\\nSolution: T(n) = 0.5T(n/2) + 1/n => Does not apply (a < 1)\\nProblem-10\\u2003\\u2003T (n) = 6T(n/3)+ n2 logn\\nSolution: T(n) = 6T(n/3) + n2logn => T(n) = Θ(n2logn) (Master Theorem Case 3.a)\\nProblem-11\\u2003\\u2003T(n) = 64T(n/8) – n2logn\\nSolution: T(n) = 64T(n/8) – n2logn => Does not apply (function is not positive)\\nProblem-12\\u2003\\u2003T(n) = 7T(n/3) + n2\\nSolution: T(n) = 7T(n/3) + n2 => T(n) = Θ(n2) (Master Theorem Case 3.as)\\nProblem-13\\u2003\\u2003T(n) = 4T(n/2) + logn\\nSolution: T(n) = 4T(n/2) + logn => T(n) = Θ(n2) (Master Theorem Case 1)\\nProblem-14\\u2003\\u2003T(n) = 16T (n/4) + n!\\nSolution: T(n) = 16T (n/4) + n! => T(n) = Θ(n!) (Master Theorem Case 3.a)\\nProblem-15\\u2003\\u2003T(n) = \\nT(n/2) + logn\\nSolution: T(n) = \\nT(n/2) + logn => T(n) = Θ(\\n) (Master Theorem Case 1)\\nProblem-16\\u2003\\u2003T(n) = 3T(n/2) + n\\nSolution: T(n) = 3T(n/2) + n =>T(n) = Θ(nlog3) (Master Theorem Case 1)\\nProblem-17\\u2003\\u2003T(n) = 3T(n/3) + \\nSolution: T(n) = 3T(n/3) + \\n => T(n) = Θ(n) (Master Theorem Case 1)\\nProblem-18\\u2003\\u2003T(n) = 4T(n/2) + cn\\nSolution: T(n) = 4T(n/2) + cn => T(n) = Θ(n2) (Master Theorem Case 1)\\nProblem-19\\u2003\\u2003T(n) = 3T(n/4) + nlogn\\nSolution: T(n) = 3T(n/4) + nlogn => T(n) = Θ(nlogn) (Master Theorem Case 3.a)\\nProblem-20\\u2003\\u2003T (n) = 3T(n/3) + n/2\\nSolution: T(n) = 3T(n/3)+ n/2 => T (n) = Θ(nlogn) (Master Theorem Case 2.a)\\n1.24 Master Theorem for Subtract and Conquer Recurrences'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 36, 'file_type': 'pdf'}, page_content='Let T(n) be a function defined on positive n, and having the property\\nfor some constants c,a > 0,b ≥ 0,k ≥ 0, and function f(n). If f(n) is in O(nk), then\\n1.25 Variant of Subtraction and Conquer Master Theorem\\nThe solution to the equation T(n) = T(α n) + T((1 – α)n) + βn, where 0 < α < 1 and β > 0 are\\nconstants, is O(nlogn).\\n1.26 Method of Guessing and Confirming\\nNow, let us discuss a method which can be used to solve any recurrence. The basic idea behind\\nthis method is:\\nguess the answer; and then prove it correct by induction.\\nIn other words, it addresses the question: What if the given recurrence doesn’t seem to match with\\nany of these (master theorem) methods? If we guess a solution and then try to verify our guess\\ninductively, usually either the proof will succeed (in which case we are done), or the proof will\\nfail (in which case the failure will help us refine our guess).\\nAs an example, consider the recurrence \\n. This doesn’t fit into the form\\nrequired by the Master Theorems. Carefully observing the recurrence gives us the impression that\\nit is similar to the divide and conquer method (dividing the problem into \\n subproblems each\\nwith size \\n). As we can see, the size of the subproblems at the first level of recursion is n. So,\\nlet us guess that T(n) = O(nlogn), and then try to prove that our guess is correct.\\nLet’s start by trying to prove an upper bound T(n) < cnlogn:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 37, 'file_type': 'pdf'}, page_content='The last inequality assumes only that 1 ≤ c. .logn. This is correct if n is sufficiently large and for\\nany constant c, no matter how small. From the above proof, we can see that our guess is correct\\nfor the upper bound. Now, let us prove the lower bound for this recurrence.\\nThe last inequality assumes only that 1 ≥ k. .logn. This is incorrect if n is sufficiently large and\\nfor any constant k. From the above proof, we can see that our guess is incorrect for the lower\\nbound.\\nFrom the above discussion, we understood that Θ(nlogn) is too big. How about Θ(n)? The lower\\nbound is easy to prove directly:\\nNow, let us prove the upper bound for this Θ(n).\\nFrom the above induction, we understood that Θ(n) is too small and Θ(nlogn) is too big. So, we\\nneed something bigger than n and smaller than nlogn. How about \\n?\\nProving the upper bound for \\n:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 38, 'file_type': 'pdf'}, page_content='Proving the lower bound for \\n:\\nThe last step doesn’t work. So, Θ(\\n) doesn’t work. What else is between n and nlogn?\\nHow about nloglogn? Proving upper bound for nloglogn:\\nProving lower bound for nloglogn:\\nFrom the above proofs, we can see that T(n) ≤ cnloglogn, if c ≥ 1 and T(n) ≥ knloglogn, if k ≤ 1.\\nTechnically, we’re still missing the base cases in both proofs, but we can be fairly confident at\\nthis point that T(n) = Θ(nloglogn).\\n1.27 Amortized Analysis'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 39, 'file_type': 'pdf'}, page_content='Amortized analysis refers to determining the time-averaged running time for a sequence of\\noperations. It is different from average case analysis, because amortized analysis does not make\\nany assumption about the distribution of the data values, whereas average case analysis assumes\\nthe data are not “bad” (e.g., some sorting algorithms do well on average over all input orderings\\nbut very badly on certain input orderings). That is, amortized analysis is a worst-case analysis,\\nbut for a sequence of operations rather than for individual operations.\\nThe motivation for amortized analysis is to better understand the running time of certain\\ntechniques, where standard worst case analysis provides an overly pessimistic bound. Amortized\\nanalysis generally applies to a method that consists of a sequence of operations, where the vast\\nmajority of the operations are cheap, but some of the operations are expensive. If we can show\\nthat the expensive operations are particularly rare we can change them to the cheap operations,\\nand only bound the cheap operations.\\nThe general approach is to assign an artificial cost to each operation in the sequence, such that the\\ntotal of the artificial costs for the sequence of operations bounds the total of the real costs for the\\nsequence. This artificial cost is called the amortized cost of an operation. To analyze the running\\ntime, the amortized cost thus is a correct way of understanding the overall running time – but note\\nthat particular operations can still take longer so it is not a way of bounding the running time of\\nany individual operation in the sequence.\\nWhen one event in a sequence affects the cost of later events:\\n•\\nOne particular task may be expensive.\\n•\\nBut it may leave data structure in a state that the next few operations become easier.\\nExample: Let us consider an array of elements from which we want to find the kth smallest\\nelement. We can solve this problem using sorting. After sorting the given array, we just need to\\nreturn the kth element from it. The cost of performing the sort (assuming comparison based sorting\\nalgorithm) is O(nlogn). If we perform n such selections then the average cost of each selection is\\nO(nlogn/n) = O(logn). This clearly indicates that sorting once is reducing the complexity of\\nsubsequent operations.\\n1.28 Algorithms Analysis: Problems & Solutions\\nNote: From the following problems, try to understand the cases which have different\\ncomplexities (O(n), O(logn), O(loglogn) etc.).\\nProblem-21\\u2003\\u2003Find the complexity of the below recurrence:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 40, 'file_type': 'pdf'}, page_content='Solution: Let us try solving this function with substitution.\\nT(n) = 3T(n – 1)\\nT(n) = 3(3T(n – 2)) = 32T(n – 2)\\nT(n) = 32(3T(n – 3))\\n.\\n.\\nT(n) = 3nT(n – n) = 3nT(0) = 3n\\nThis clearly shows that the complexity of this function is O(3n).\\nNote: We can use the Subtraction and Conquer master theorem for this problem.\\nProblem-22\\u2003\\u2003Find the complexity of the below recurrence:\\nSolution: Let us try solving this function with substitution.\\nT(n) = 2T(n – 1) – 1\\nT(n) = 2(2T(n – 2) – 1) – 1 = 22T(n – 2) – 2 – 1\\nT(n) = 22(2T(n – 3) – 2 – 1) – 1 = 23T(n – 4) – 22 – 21 – 20\\nT(n) = 2nT(n – n) – 2n–1 – 2n–2 – 2n–3 .... 22 – 21 – 20\\nT(n) =2n – 2n–1 – 2n–2 – 2n – 3 .... 22 – 21 – 20\\nT(n) =2n – (2n – 1) [note: 2n–1 + 2n–2 + ··· + 20 = 2n]\\nT(n) = 1\\n∴ Time Complexity is O(1). Note that while the recurrence relation looks exponential, the\\nsolution to the recurrence relation here gives a different result.\\nProblem-23\\u2003\\u2003What is the running time of the following function?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 41, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the below function:\\nWe can define the ‘s’ terms according to the relation si = si–1 + i. The value oft’ increases by 1\\nfor each iteration. The value contained in ‘s’ at the ith iteration is the sum of the first ‘(‘positive\\nintegers. If k is the total number of iterations taken by the program, then the while loop terminates\\nif:\\nProblem-24\\u2003\\u2003Find the complexity of the function given below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 42, 'file_type': 'pdf'}, page_content='Solution:\\nIn the above-mentioned function the loop will end, if i2 > n ⇒ T(n) = O(\\n). This is similar to\\nProblem-23.\\nProblem-25\\u2003\\u2003What is the complexity of the program given below:\\nSolution: Consider the comments in the following function.\\nThe complexity of the above function is O(n2logn).\\nProblem-26\\u2003\\u2003What is the complexity of the program given below:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 43, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the following function.\\nThe complexity of the above function is O(nlog2n).\\nProblem-27\\u2003\\u2003Find the complexity of the program below.\\nSolution: Consider the comments in the function below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 44, 'file_type': 'pdf'}, page_content='The complexity of the above function is O(n). Even though the inner loop is bounded by n, due to\\nthe break statement it is executing only once.\\nProblem-28\\u2003\\u2003Write a recursive function for the running time T(n) of the function given below.\\nProve using the iterative method that T(n) = Θ(n3).\\nSolution: Consider the comments in the function below:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 45, 'file_type': 'pdf'}, page_content='The recurrence for this code is clearly T(n) = T(n – 3) + cn2 for some constant c > 0 since each\\ncall prints out n2 asterisks and calls itself recursively on n – 3. Using the iterative method we get:\\nT(n) = T(n – 3) + cn2. Using the Subtraction and Conquer master theorem, we get T(n) = Θ(n3).\\nProblem-29\\u2003\\u2003Determine Θ bounds for the recurrence relation: \\nSolution: Using Divide and Conquer master theorem, we get O(nlog2n).\\nProblem-30\\u2003\\u2003Determine \\nΘ \\nbounds \\nfor \\nthe \\nrecurrence: \\nSolution: \\nSubstituting \\nin \\nthe \\nrecurrence \\nequation, \\nwe \\nget: \\n, where k is a constant. This clearly\\nsays Θ(n).\\nProblem-31\\u2003\\u2003Determine Θ bounds for the recurrence relation: T(n) = T(⌈n/2⌉) + 7.\\nSolution: Using Master Theorem we get: Θ(logn).\\nProblem-32\\u2003\\u2003Prove that the running time of the code below is Ω(logn).\\nSolution: The while loop will terminate once the value of ‘k’ is greater than or equal to the value\\nof ‘n’. In each iteration the value of ‘k’ is multiplied by 3. If i is the number of iterations, then ‘k’\\nhas the value of 3i after i iterations. The loop is terminated upon reaching i iterations when 3i ≥ n'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 46, 'file_type': 'pdf'}, page_content='↔ i ≥ log3 n, which shows that i = Ω(logn).\\nProblem-33\\u2003\\u2003Solve the following recurrence.\\nSolution: By iteration:\\nNote: We can use the Subtraction and Conquer master theorem for this problem.\\nProblem-34\\u2003\\u2003Consider the following program:\\nSolution: The recurrence relation for the running time of this program is: T(n) = T(n – 1) + T(n –\\n2) + c. Note T(n) has two recurrence calls indicating a binary tree. Each step recursively calls the\\nprogram for n reduced by 1 and 2, so the depth of the recurrence tree is O(n). The number of\\nleaves at depth n is 2n since this is a full binary tree, and each leaf takes at least O(1)\\ncomputations for the constant factor. Running time is clearly exponential in n and it is O(2n).\\nProblem-35\\u2003\\u2003Running time of following program?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 47, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the function below:\\nIn the above code, inner loop executes n/i times for each value of i. Its running time is \\n.\\nProblem-36\\u2003\\u2003What is the complexity of \\nSolution: Using the logarithmic property, logxy = logx + logy, we can see that this problem is\\nequivalent to\\nThis shows that the time complexity = O(nlogn).\\nProblem-37\\u2003\\u2003What is the running time of the following recursive function (specified as a\\nfunction of the input value n)? First write the recurrence formula and then find its\\ncomplexity.\\nSolution: Consider the comments in the below function:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 48, 'file_type': 'pdf'}, page_content='We can assume that for asymptotical analysis k = ⌈k⌉ for every integer k ≥ 1. The recurrence for\\nthis code is \\n. Using master theorem, we get T(n) = Θ(n).\\nProblem-38\\u2003\\u2003What is the running time of the following recursive function (specified as a\\nfunction of the input value n)? First write a recurrence formula, and show its solution using\\ninduction.\\nSolution: Consider the comments in the function below:\\nThe if statement requires constant time [O(1)]. With the for loop, we neglect the loop overhead\\nand only count three times that the function is called recursively. This implies a time complexity\\nrecurrence:\\nUsing the Subtraction and Conquer master theorem, we get T(n) = Θ(3n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 49, 'file_type': 'pdf'}, page_content='Problem-39\\u2003\\u2003Write a recursion formula for the running time T(n) of the function whose code\\nis below.\\nSolution: Consider the comments in the function below:\\nThe recurrence for this piece of code is T(n) = T(.8n) + O(n) = T(4/5n) + O(n) =4/5 T(n) + O(n).\\nApplying master theorem, we get T(n) = O(n).\\nProblem-40\\u2003\\u2003Find the complexity of the recurrence: T(n) = 2T(\\n) + logn\\nSolution: The given recurrence is not in the master theorem format. Let us try to convert this to the\\nmaster theorem format by assuming n = 2m. Applying the logarithm on both sides gives, logn =\\nmlogl ⇒ m = logn. Now, the given function becomes:\\nTo \\nmake \\nit \\nsimple \\nwe \\nassume \\n.\\nApplying the master theorem format would result in S(m) = O(mlogm).\\nIf we substitute m = logn back, T(n) = S(logn) = O((logn) loglogn).\\nProblem-41\\u2003\\u2003Find the complexity of the recurrence: T(n) = T(\\n) + 1\\nSolution: Applying the logic of Problem-40 gives \\n. Applying the master'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 50, 'file_type': 'pdf'}, page_content='theorem would result in S(m) = O(logm). Substituting m = logn, gives T(n) = S(logn) =\\nO(loglogn).\\nProblem-42\\u2003\\u2003Find the complexity of the recurrence: T(n) = 2T(\\n) + 1\\nSolution: Applying the logic of Problem-40 gives: \\n. Using the master\\ntheorem results S(m) = \\n. Substituting m = logn gives T(n) =O(logn).\\nProblem-43\\u2003\\u2003Find the complexity of the below function.\\nSolution: Consider the comments in the function below:\\nFor the above code, the recurrence function can be given as: T(n) = T(\\n) + 1. This is same as\\nthat of Problem-41.\\nProblem-44\\u2003\\u2003Analyze the running time of the following recursive pseudo-code as a function of\\nn.\\nSolution: Consider the comments in below pseudo-code and call running time of function(n) as\\nT(n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 51, 'file_type': 'pdf'}, page_content='T(n) can be defined as follows:\\nUsing the master theorem gives: \\n.\\nProblem-45\\u2003\\u2003Find the complexity of the below pseudocode:\\nSolution: Consider the comments in the pseudocode below:\\nThe recurrence for this function is T(n) = T(n/2) + n. Using master theorem, we get T(n) = O(n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 52, 'file_type': 'pdf'}, page_content='Problem-46\\u2003\\u2003Running time of the following program?\\nSolution: Consider the comments in the below function:\\nComplexity of above program is: O(nlogn).\\nProblem-47\\u2003\\u2003Running time of the following program?\\nSolution: Consider the comments in the below function:\\nThe time complexity of this program is: O(n2).\\nProblem-48\\u2003\\u2003Find the complexity of the below function:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 53, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the below function:\\nThe recurrence for this function is: \\n. Using master theorem, we get T(n) =\\nO(n).\\nProblem-49\\u2003\\u2003Find the complexity of the below function:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 54, 'file_type': 'pdf'}, page_content='Solution:\\nTime Complexity: O(logn * logn) = O(log2n).\\nProblem-50\\u2003\\u2003∑i≤k≤n O(n), where O(n) stands for order n is:\\n(A)\\nO(n)\\n(B)\\nO(n2)\\n(C)\\nO(n3)\\n(D)\\nO(3n2)\\n(E)\\nO(1.5n2)\\nSolution: (B). ∑i≤k≤n O(n) = O(n) ∑i≤k≤n 1 = O(n2).\\nProblem-51\\u2003\\u2003Which of the following three claims are correct?\\nI\\u2003(n + k)m = Θ(nm), where k and m are constants\\nII\\u20032n+1 = O(2n)\\nIII\\u200322n+1 = O(2n)\\n(A)\\nI and II\\n(B)\\nI and III\\n(C)\\nII and III\\n(D)\\nI, II and III\\nSolution: (A). (I) (n + k)m =nh + c1*nk–1 + ... km = Θ(nh) and (II) 2n+1 = 2*2n = O(2n)\\nProblem-52\\u2003\\u2003Consider the following functions:\\nf(n) = 2n\\ng(n) = n!\\nh(n) = nlogn\\nWhich of the following statements about the asymptotic behavior of f(n), g(n), and h(n) is\\ntrue?\\n(A)\\nf(n) = O(g(n)); g(n) = O(h(n))\\n(B)\\nf(n) = Ω (g(n)); g(n) = O(h(n))'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 55, 'file_type': 'pdf'}, page_content='(C)\\ng(n) = O(f(n)); h(n) = O(f(n))\\n(D)\\nh(n) = O(f(n)); g(n) = Ω (f(n))\\nSolution: (D). According to the rate of growth: h(n) < f(n) < g(n) (g(n) is asymptotically greater\\nthan f(n), and f(n) is asymptotically greater than h(n)). We can easily see the above order by\\ntaking logarithms of the given 3 functions: lognlogn < n < log(n!). Note that, log(n!) = O(nlogn).\\nProblem-53\\u2003\\u2003Consider the following segment of C-code:\\nThe number of comparisons made in the execution of the loop for any n > 0 is:\\n(A)\\n(B)\\nn\\n(C)\\n(D)\\nSolution: (a). Let us assume that the loop executes k times. After kth step the value of j is 2k.\\nTaking logarithms on both sides gives \\n. Since we are doing one more comparison for\\nexiting from the loop, the answer is \\n.\\nProblem-54\\u2003\\u2003Consider the following C code segment. Let T(n) denote the number of times the\\nfor loop is executed by the program on input n. Which of the following is true?\\n(A)\\nT(n) = O(\\n) and T(n) = Ω(\\n)\\n(B)\\nT(n) = O(\\n) and T(n) = Ω(1)\\n(C)\\nT(n) = O(n) and T(n) = Ω(\\n)\\n(D)\\nNone of the above\\nSolution: (B). Big O notation describes the tight upper bound and Big Omega notation describes\\nthe tight lower bound for an algorithm. The for loop in the question is run maximum \\n times and'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 56, 'file_type': 'pdf'}, page_content='minimum 1 time. Therefore, T(n) = O(\\n) and T(n) = Ω(1).\\nProblem-55\\u2003\\u2003In the following C function, let n ≥ m. How many recursive calls are made by\\nthis function?\\n(A)\\n(B)\\nΩ(n)\\n(C)\\n(D)\\nΘ(n)\\nSolution: No option is correct. Big O notation describes the tight upper bound and Big Omega\\nnotation describes the tight lower bound for an algorithm. For m = 2 and for all n = 2i, the running\\ntime is O(1) which contradicts every option.\\nProblem-56\\u2003\\u2003Suppose T(n) = 2T(n/2) + n, T(O)=T(1)=1. Which one of the following is false?\\n(A)\\nT(n) = O(n2)\\n(B)\\nT(n) = Θ(nlogn)\\n(C)\\nT(n) = Q(n2)\\n(D)\\nT(n) = O(nlogn)\\nSolution: (C). Big O notation describes the tight upper bound and Big Omega notation describes\\nthe tight lower bound for an algorithm. Based on master theorem, we get T(n) = Θ(nlogn). This\\nindicates that tight lower bound and tight upper bound are the same. That means, O(nlogn) and\\nΩ(nlogn) are correct for given recurrence. So option (C) is wrong.\\nProblem-57\\u2003\\u2003Find the complexity of the below function:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 57, 'file_type': 'pdf'}, page_content='Solution:\\nTime Complexity: O(n5).\\nProblem-58\\u2003\\u2003To calculate 9n, give an algorithm and discuss its complexity.\\nSolution: Start with 1 and multiply by 9 until reaching 9n.\\nTime Complexity: There are n – 1 multiplications and each takes constant time giving a Θ(n)\\nalgorithm.\\nProblem-59\\u2003\\u2003For Problem-58, can we improve the time complexity?\\nSolution: Refer to the Divide and Conquer chapter.\\nProblem-60\\u2003\\u2003Find the time complexity of recurrence \\n.\\nSolution: Let us solve this problem by method of guessing. The total size on each level of the\\nrecurrance tree is less than n, so we guess that f(n) = n will dominate. Assume for all i < n that\\nc1n ≤ T(i) < c2n. Then,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 58, 'file_type': 'pdf'}, page_content='If c1 ≥ 8k and c2 ≤ 8k, then c1n = T(n) = c2n. So, T(n) = Θ(n). In general, if you have multiple\\nrecursive calls, the sum of the arguments to those calls is less than n (in this case \\n),\\nand f(n) is reasonably large, a good guess is T(n) = Θ(f(n)).\\nProblem-61\\u2003\\u2003Solve the following recurrence relation using the recursion tree method: \\n.\\nSolution: How much work do we do in each level of the recursion tree?\\nIn level 0, we take n2 time. At level 1, the two subproblems take time:\\nAt level 2 the four subproblems are of size \\n and \\n respectively. These two\\nsubproblems take time:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 59, 'file_type': 'pdf'}, page_content='Similarly the amount of work at level k is at most \\n.\\nLet \\n, the total runtime is then:\\nThat is, the first level provides a constant fraction of the total runtime.\\nProblem-62\\u2003\\u2003Rank the following functions by order of growth: (n + 1)!, n!, 4n, n × 3n, 3n + n2\\n+ 20n, \\n, n2 + 200, 20n + 500, 2lgn, n2/3, 1.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 60, 'file_type': 'pdf'}, page_content='Problem-63\\u2003\\u2003Find the complexity of the below function:\\nSolution: Consider the worst-case.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 61, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2).\\nProblem-64\\u2003\\u2003Can we say \\n?\\nSolution: Yes: because \\nProblem-65\\u2003\\u2003Can we say 23n = O(2n)?\\nSolution: No: because 23n = (23)n = 8n not less than 2n.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 62, 'file_type': 'pdf'}, page_content='2.1 Introduction\\nIn this chapter, we will look at one of the important topics, “recursion”, which will be used in\\nalmost every chapter, and also its relative “backtracking”.\\n2.2 What is Recursion?\\nAny function which calls itself is called recursive. A recursive method solves a problem by\\ncalling a copy of itself to work on a smaller problem. This is called the recursion step. The\\nrecursion step can result in many more such recursive calls.\\nIt is important to ensure that the recursion terminates. Each time the function calls itself with a\\nslightly simpler version of the original problem. The sequence of smaller problems must\\neventually converge on the base case.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 63, 'file_type': 'pdf'}, page_content='2.3 Why Recursion?\\nRecursion is a useful technique borrowed from mathematics. Recursive code is generally shorter\\nand easier to write than iterative code. Generally, loops are turned into recursive functions when\\nthey are compiled or interpreted.\\nRecursion is most useful for tasks that can be defined in terms of similar subtasks. For example,\\nsort, search, and traversal problems often have simple recursive solutions.\\n2.4 Format of a Recursive Function\\nA recursive function performs a task in part by calling itself to perform the subtasks. At some\\npoint, the function encounters a subtask that it can perform without calling itself. This case, where\\nthe function does not recur, is called the base case. The former, where the function calls itself to\\nperform a subtask, is referred to as the ecursive case. We can write all recursive functions using\\nthe format:\\nAs an example consider the factorial function: n! is the product of all integers between n and 1.\\nThe definition of recursive factorial looks like:\\nThis definition can easily be converted to recursive implementation. Here the problem is\\ndetermining the value of n!, and the subproblem is determining the value of (n – l)!. In the\\nrecursive case, when n is greater than 1, the function calls itself to determine the value of (n – l)!\\nand multiplies that with n.\\nIn the base case, when n is 0 or 1, the function simply returns 1. This looks like the following:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 64, 'file_type': 'pdf'}, page_content='2.5 Recursion and Memory (Visualization)\\nEach recursive call makes a new copy of that method (actually only the variables) in memory.\\nOnce a method ends (that is, returns some data), the copy of that returning method is removed\\nfrom memory. The recursive solutions look simple but visualization and tracing takes time. For\\nbetter understanding, let us consider the following example.\\nFor this example, if we call the print function with n=4, visually our memory assignments may\\nlook like:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 65, 'file_type': 'pdf'}, page_content='Now, let us consider our factorial function. The visualization of factorial function with n=4 will\\nlook like:\\n2.6 Recursion versus Iteration\\nWhile discussing recursion, the basic question that comes to mind is: which way is better? –\\niteration or recursion? The answer to this question depends on what we are trying to do. A\\nrecursive approach mirrors the problem that we are trying to solve. A recursive approach makes\\nit simpler to solve a problem that may not have the most obvious of answers. But, recursion adds'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 66, 'file_type': 'pdf'}, page_content='overhead for each recursive call (needs space on the stack frame).\\nRecursion\\n•\\nTerminates when a base case is reached.\\n•\\nEach recursive call requires extra space on the stack frame (memory).\\n•\\nIf we get infinite recursion, the program may run out of memory and result in stack\\noverflow.\\n•\\nSolutions to some problems are easier to formulate recursively.\\nIteration\\n•\\nTerminates when a condition is proven to be false.\\n•\\nEach iteration does not require extra space.\\n•\\nAn infinite loop could loop forever since there is no extra memory being created.\\n•\\nIterative solutions to a problem may not always be as obvious as a recursive\\nsolution.\\n2.7 Notes on Recursion\\n•\\nRecursive algorithms have two types of cases, recursive cases and base cases.\\n•\\nEvery recursive function case must terminate at a base case.\\n•\\nGenerally, iterative solutions are more efficient than recursive solutions [due to the\\noverhead of function calls].\\n•\\nA recursive algorithm can be implemented without recursive function calls using a\\nstack, but it’s usually more trouble than its worth. That means any problem that can\\nbe solved recursively can also be solved iteratively.\\n•\\nFor some problems, there are no obvious iterative algorithms.\\n•\\nSome problems are best suited for recursive solutions while others are not.\\n2.8 Example Algorithms of Recursion\\n•\\nFibonacci Series, Factorial Finding\\n•\\nMerge Sort, Quick Sort\\n•\\nBinary Search\\n•\\nTree Traversals and many Tree Problems: InOrder, PreOrder PostOrder\\n•\\nGraph Traversals: DFS [Depth First Search] and BFS [Breadth First Search]\\n•\\nDynamic Programming Examples\\n•\\nDivide and Conquer Algorithms\\n•\\nTowers of Hanoi'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 67, 'file_type': 'pdf'}, page_content='•\\nBacktracking Algorithms [we will discuss in next section]\\n2.9 Recursion: Problems & Solutions\\nIn this chapter we cover a few problems with recursion and we will discuss the rest in other\\nchapters. By the time you complete reading the entire book, you will encounter many recursion\\nproblems.\\nProblem-1\\u2003\\u2003Discuss Towers of Hanoi puzzle.\\nSolution: The Towers of Hanoi is a mathematical puzzle. It consists of three rods (or pegs or\\ntowers), and a number of disks of different sizes which can slide onto any rod. The puzzle starts\\nwith the disks on one rod in ascending order of size, the smallest at the top, thus making a conical\\nshape. The objective of the puzzle is to move the entire stack to another rod, satisfying the\\nfollowing rules:\\n•\\nOnly one disk may be moved at a time.\\n•\\nEach move consists of taking the upper disk from one of the rods and sliding it onto\\nanother rod, on top of the other disks that may already be present on that rod.\\n•\\nNo disk may be placed on top of a smaller disk.\\nAlgorithm:\\n•\\nMove the top n – 1 disks from Source to Auxiliary tower,\\n•\\nMove the nth disk from Source to Destination tower,\\n•\\nMove the n – 1 disks from Auxiliary tower to Destination tower.\\n•\\nTransferring the top n – 1 disks from Source to Auxiliary tower can again be thought\\nof as a fresh problem and can be solved in the same manner. Once we solve Towers\\nof Hanoi with three disks, we can solve it with any number of disks with the above\\nalgorithm.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 68, 'file_type': 'pdf'}, page_content='Problem-2\\u2003\\u2003Given an array, check whether the array is in sorted order with recursion.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n) for recursive stack space.\\n2.10 What is Backtracking?\\nBacktracking is an improvement of the brute force approach. It systematically searches for a\\nsolution to a problem among all available options. In backtracking, we start with one possible\\noption out of many available options and try to solve the problem if we are able to solve the\\nproblem with the selected move then we will print the solution else we will backtrack and select\\nsome other option and try to solve it. If none if the options work out we will claim that there is no\\nsolution for the problem.\\nBacktracking is a form of recursion. The usual scenario is that you are faced with a number of\\noptions, and you must choose one of these. After you make your choice you will get a new set of'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 69, 'file_type': 'pdf'}, page_content='options; just what set of options you get depends on what choice you made. This procedure is\\nrepeated over and over until you reach a final state. If you made a good sequence of choices, your\\nfinal state is a goal state; if you didn’t, it isn’t.\\nBacktracking can be thought of as a selective tree/graph traversal method. The tree is a way of\\nrepresenting some initial starting position (the root node) and a final goal state (one of the\\nleaves). Backtracking allows us to deal with situations in which a raw brute-force approach\\nwould explode into an impossible number of options to consider. Backtracking is a sort of refined\\nbrute force. At each node, we eliminate choices that are obviously not possible and proceed to\\nrecursively check only those that have potential.\\nWhat’s interesting about backtracking is that we back up only as far as needed to reach a previous\\ndecision point with an as-yet-unexplored alternative. In general, that will be at the most recent\\ndecision point. Eventually, more and more of these decision points will have been fully explored,\\nand we will have to backtrack further and further. If we backtrack all the way to our initial state\\nand have explored all alternatives from there, we can conclude the particular problem is\\nunsolvable. In such a case, we will have done all the work of the exhaustive recursion and known\\nthat there is no viable solution possible.\\n•\\nSometimes the best algorithm for a problem is to try all possibilities.\\n•\\nThis is always slow, but there are standard tools that can be used to help.\\n•\\nTools: algorithms for generating basic objects, such as binary strings [2n\\npossibilities for n-bit string], permutations [n!], combinations [n!/r!(n – r)!],\\ngeneral strings [k –ary strings of length n has kn possibilities], etc...\\n•\\nBacktracking speeds the exhaustive search by pruning.\\n2.11 Example Algorithms of Backtracking\\n•\\nBinary Strings: generating all binary strings\\n•\\nGenerating k – ary Strings\\n•\\nN-Queens Problem\\n•\\nThe Knapsack Problem\\n•\\nGeneralized Strings\\n•\\nHamiltonian Cycles [refer to Graphs chapter]\\n•\\nGraph Coloring Problem\\n2.12 Backtracking: Problems & Solutions\\nProblem-3\\u2003\\u2003Generate all the strings of n bits. Assume A[0..n – 1] is an array of size n.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 70, 'file_type': 'pdf'}, page_content='Let T(n) be the running time of binary(n). Assume function printf takes time O(1).\\nUsing Subtraction and Conquer Master theorem we get: T(n) = O(2n). This means the algorithm\\nfor generating bit-strings is optimal.\\nProblem-4\\u2003\\u2003Generate all the strings of length n drawn from 0... k – 1.\\nSolution: Let us assume we keep current k-ary string in an array A[0.. n – 1]. Call function k-\\nstring(n, k):\\nLet T(n) be the running time of k – string(n). Then,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 71, 'file_type': 'pdf'}, page_content='Using Subtraction and Conquer Master theorem we get: T(n) = O(kn).\\nNote: For more problems, refer to String Algorithms chapter.\\nProblem-5\\u2003\\u2003Finding the length of connected cells of 1s (regions) in an matrix of Os and\\n1s: Given a matrix, each of which may be 1 or 0. The filled cells that are connected form a\\nregion. Two cells are said to be connected if they are adjacent to each other horizontally,\\nvertically or diagonally. There may be several regions in the matrix. How do you find the\\nlargest region (in terms of number of cells) in the matrix?\\nSolution: The simplest idea is: for each location traverse in all 8 directions and in each of those\\ndirections keep track of maximum region found.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 72, 'file_type': 'pdf'}, page_content='in getvalint (45 int int jn L, int H)\\niffieO || i>*L || js 0 || j>*H)\\nreturn 0;\\nelse\\n\\nreturn Al\\n\\n)\\n‘oid finaxBlockfint (4, nt, int int Lint Hint sie, bool entarr in masz)\\nif{r>=L || c>=H)\\nreturn;\\ncntar (true;\\nsize;\\nif [size > mansze\\nmaxsize = sie;\\n[search in eight directions\\nint direction [21,0 1-1}0- (1, 1}{0,0,0s1 0h 5\\nforint 0; i i+4)\\nint newi=rirectioni0};\\nint newjsctdretioni[;\\nint valegetval (A,newi,new},L,H);\\nif vabO Qs entarr{newilinewi|>*fals})\\nfindMaxBlock(A.newi ne LH, size ntarr,maxsie);\\n)\\n\\n}\\ncntar¢false;\\n\\n)\\n\\nint getMaxOnes(int (4[5}, int rmex, int cola,\\n‘int masize=0;\\nint size=0;\\nbool “entarereatddafmacoa'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 73, 'file_type': 'pdf'}, page_content='Sample Call:\\nProblem-6\\u2003\\u2003Solve the recurrence T(n) = 2T(n – 1) + 2n.\\nSolution: At each level of the recurrence tree, the number of problems is double from the\\nprevious level, while the amount of work being done in each problem is half from the previous\\nlevel. Formally, the ith level has 2i problems, each requiring 2n–i work. Thus the ith level requires\\nexactly 2n work. The depth of this tree is n, because at the ith level, the originating call will be\\nT(n – i). Thus the total complexity for T(n) is T(n2n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 74, 'file_type': 'pdf'}, page_content='3.1 What is a Linked List?\\nA linked list is a data structure used for storing collections of data. A linked list has the following\\nproperties.\\n•\\nSuccessive elements are connected by pointers\\n•\\nThe last element points to NULL\\n•\\nCan grow or shrink in size during execution of a program\\n•\\nCan be made just as long as required (until systems memory exhausts)\\n•\\nDoes not waste memory space (but takes some extra memory for pointers). It\\nallocates memory as list grows.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 75, 'file_type': 'pdf'}, page_content='3.2 Linked Lists ADT\\nThe following operations make linked lists an ADT:\\nMain Linked Lists Operations\\n•\\nInsert: inserts an element into the list\\n•\\nDelete: removes and returns the specified position element from the list\\nAuxiliary Linked Lists Operations\\n•\\nDelete List: removes all elements of the list (disposes the list)\\n•\\nCount: returns the number of elements in the list\\n•\\nFind nth node from the end of the list\\n3.3 Why Linked Lists?\\nThere are many other data structures that do the same thing as linked lists. Before discussing\\nlinked lists it is important to understand the difference between linked lists and arrays. Both\\nlinked lists and arrays are used to store collections of data, and since both are used for the same\\npurpose, we need to differentiate their usage. That means in which cases arrays are suitable and\\nin which cases linked lists are suitable.\\n3.4 Arrays Overview\\nOne memory block is allocated for the entire array to hold the elements of the array. The array\\nelements can be accessed in constant time by using the index of the particular element as the\\nsubscript.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 76, 'file_type': 'pdf'}, page_content='Why Constant Time for Accessing Array Elements?\\nTo access an array element, the address of an element is computed as an offset from the base\\naddress of the array and one multiplication is needed to compute what is supposed to be added to\\nthe base address to get the memory address of the element. First the size of an element of that data\\ntype is calculated and then it is multiplied with the index of the element to get the value to be\\nadded to the base address.\\nThis process takes one multiplication and one addition. Since these two operations take constant\\ntime, we can say the array access can be performed in constant time.\\nAdvantages of Arrays\\n•\\nSimple and easy to use\\n•\\nFaster access to the elements (constant access)\\nDisadvantages of Arrays\\n•\\nPreallocates all needed memory up front and wastes memory space for indices in the\\narray that are empty.\\n•\\nFixed size: The size of the array is static (specify the array size before using it).\\n•\\nOne block allocation: To allocate the array itself at the beginning, sometimes it may\\nnot be possible to get the memory for the complete array (if the array size is big).\\n•\\nComplex position-based insertion: To insert an element at a given position, we may\\nneed to shift the existing elements. This will create a position for us to insert the\\nnew element at the desired position. If the position at which we want to add an\\nelement is at the beginning, then the shifting operation is more expensive.\\nDynamic Arrays\\nDynamic array (also called as growable array, resizable array, dynamic table, or array list) is a\\nrandom access, variable-size list data structure that allows elements to be added or removed.\\nOne simple way of implementing dynamic arrays is to initially start with some fixed size array.\\nAs soon as that array becomes full, create the new array double the size of the original array.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 77, 'file_type': 'pdf'}, page_content='Similarly, reduce the array size to half if the elements in the array are less than half.\\nNote: We will see the implementation for dynamic arrays in the Stacks, Queues and Hashing\\nchapters.\\nAdvantages of Linked Lists\\nLinked lists have both advantages and disadvantages. The advantage of linked lists is that they can\\nbe expanded in constant time. To create an array, we must allocate memory for a certain number\\nof elements. To add more elements to the array when full, we must create a new array and copy\\nthe old array into the new array. This can take a lot of time.\\nWe can prevent this by allocating lots of space initially but then we might allocate more than we\\nneed and waste memory. With a linked list, we can start with space for just one allocated element\\nand add on new elements easily without the need to do any copying and reallocating.\\nIssues with Linked Lists (Disadvantages)\\nThere are a number of issues with linked lists. The main disadvantage of linked lists is access\\ntime to individual elements. Array is random-access, which means it takes O(1) to access any\\nelement in the array. Linked lists take O(n) for access to an element in the list in the worst case.\\nAnother advantage of arrays in access time is spacial locality in memory. Arrays are defined as\\ncontiguous blocks of memory, and so any array element will be physically near its neighbors. This\\ngreatly benefits from modern CPU caching methods.\\nAlthough the dynamic allocation of storage is a great advantage, the overhead with storing and\\nretrieving data can make a big difference. Sometimes linked lists are hard to manipulate. If the\\nlast item is deleted, the last but one must then have its pointer changed to hold a NULL reference.\\nThis requires that the list is traversed to find the last but one link, and its pointer set to a NULL\\nreference.\\nFinally, linked lists waste memory in terms of extra reference points.\\n3.5 Comparison of Linked Lists with Arrays & Dynamic Arrays'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 78, 'file_type': 'pdf'}, page_content='3.6 Singly Linked Lists\\nGenerally “linked list” means a singly linked list. This list consists of a number of nodes in which\\neach node has a next pointer to the following element. The link of the last node in the list is\\nNULL, which indicates the end of the list.\\nFollowing is a type declaration for a linked list of integers:\\nBasic Operations on a List'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 79, 'file_type': 'pdf'}, page_content='•\\nTraversing the list\\n•\\nInserting an item in the list\\n•\\nDeleting an item from the list\\nTraversing the Linked List\\nLet us assume that the head points to the first node of the list. To traverse the list we do the\\nfollowing\\n•\\nFollow the pointers.\\n•\\nDisplay the contents of the nodes (or count) as they are traversed.\\n•\\nStop when the next pointer points to NULL.\\nThe ListLength() function takes a linked list as input and counts the number of nodes in the list.\\nThe function given below can be used for printing the list data with extra print function.\\nTime Complexity: O(n), for scanning the list of size n.\\nSpace Complexity: O(1), for creating a temporary variable.\\nSingly Linked List Insertion\\nInsertion into a singly-linked list has three cases:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 80, 'file_type': 'pdf'}, page_content='•\\nInserting a new node before the head (at the beginning)\\n•\\nInserting a new node after the tail (at the end of the list)\\n•\\nInserting a new node at the middle of the list (random location)\\nNote: To insert an element in the linked list at some position p, assume that after inserting the\\nelement the position of this new node is p.\\nInserting a Node in Singly Linked List at the Beginning\\nIn this case, a new node is inserted before the current head node. Only one next pointer needs to\\nbe modified (new node’s next pointer) and it can be done in two steps:\\n•\\nUpdate the next pointer of new node, to point to the current head.\\n•\\nUpdate head pointer to point to the new node.\\nInserting a Node in Singly Linked List at the Ending\\nIn this case, we need to modify two next pointers (last nodes next pointer and new nodes next\\npointer).\\n•\\nNew nodes next pointer points to NULL.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 81, 'file_type': 'pdf'}, page_content='•\\nLast nodes next pointer points to the new node.\\nInserting a Node in Singly Linked List at the Middle\\nLet us assume that we are given a position where we want to insert the new node. In this case\\nalso, we need to modify two next pointers.\\n•\\nIf we want to add an element at position 3 then we stop at position 2. That means we\\ntraverse 2 nodes and insert the new node. For simplicity let us assume that the\\nsecond node is called position node. The new node points to the next node of the\\nposition where we want to add this node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 82, 'file_type': 'pdf'}, page_content='•\\nPosition node’s next pointer now points to the new node.\\nLet us write the code for all three cases. We must update the first element pointer in the calling\\nfunction, not just in the called function. For this reason we need to send a double pointer. The\\nfollowing code inserts a node in the singly linked list.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 83, 'file_type': 'pdf'}, page_content='Note: We can implement the three variations of the insert operation separately.\\nTime Complexity: O(n), since, in the worst case, we may need to insert the node at the end of the\\nlist.\\nSpace Complexity: O(1), for creating one temporary variable.\\nSingly Linked List Deletion'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 84, 'file_type': 'pdf'}, page_content='Similar to insertion, here we also have three cases.\\n•\\nDeleting the first node\\n•\\nDeleting the last node\\n•\\nDeleting an intermediate node.\\nDeleting the First Node in Singly Linked List\\nFirst node (current head node) is removed from the list. It can be done in two steps:\\n•\\nCreate a temporary node which will point to the same node as that of head.\\n•\\nNow, move the head nodes pointer to the next node and dispose of the temporary\\nnode.\\nDeleting the Last Node in Singly Linked List\\nIn this case, the last node is removed from the list. This operation is a bit trickier than removing\\nthe first node, because the algorithm should find a node, which is previous to the tail. It can be\\ndone in three steps:\\n•\\nTraverse the list and while traversing maintain the previous node address also. By\\nthe time we reach the end of the list, we will have two pointers, one pointing to the\\ntail node and the other pointing to the node before the tail node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 85, 'file_type': 'pdf'}, page_content='•\\nUpdate previous node’s next pointer with NULL.\\n•\\nDispose of the tail node.\\nDeleting an Intermediate Node in Singly Linked List\\nIn this case, the node to be removed is always located between two nodes. Head and tail links\\nare not updated in this case. Such a removal can be done in two steps:\\n•\\nSimilar to the previous case, maintain the previous node while traversing the list.\\nOnce we find the node to be deleted, change the previous node’s next pointer to the\\nnext pointer of the node to be deleted.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 86, 'file_type': 'pdf'}, page_content='4 ena “\\n\\n4 40 > NULL\\n\\nHead Previous node Node to be deleted\\n\\n+ Dispose of the current node to be deleted.\\n\\n> NULL\\n\\nHead Previous node Node to be deleted'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 87, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). In the worst case, we may need to delete the node at the end of the list.\\nSpace Complexity: O(1), for one temporary variable.\\nDeleting Singly Linked List'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 88, 'file_type': 'pdf'}, page_content='This works by storing the current node in some temporary variable and freeing the current node.\\nAfter freeing the current node, go to the next node with a temporary variable and repeat this\\nprocess for all nodes.\\nTime Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for creating one temporary variable.\\n3.7 Doubly Linked Lists\\nThe advantage of a doubly linked list (also called two – way linked list) is that given a node in\\nthe list, we can navigate in both directions. A node in a singly linked list cannot be removed\\nunless we have the pointer to its predecessor. But in a doubly linked list, we can delete a node\\neven if we don’t have the previous node’s address (since each node has a left pointer pointing to\\nthe previous node and can move backward).\\nThe primary disadvantages of doubly linked lists are:\\n•\\nEach node requires an extra pointer, requiring more space.\\n•\\nThe insertion or deletion of a node takes a bit longer (more pointer operations).\\nSimilar to a singly linked list, let us implement the operations of a doubly linked list. If you\\nunderstand the singly linked list operations, then doubly linked list operations are obvious.\\nFollowing is a type declaration for a doubly linked list of integers:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 89, 'file_type': 'pdf'}, page_content='Doubly Linked List Insertion\\nInsertion into a doubly-linked list has three cases (same as singly linked list):\\n•\\nInserting a new node before the head.\\n•\\nInserting a new node after the tail (at the end of the list).\\n•\\nInserting a new node at the middle of the list.\\nInserting a Node in Doubly Linked List at the Beginning\\nIn this case, new node is inserted before the head node. Previous and next pointers need to be\\nmodified and it can be done in two steps:\\n•\\nUpdate the right pointer of the new node to point to the current head node (dotted\\nlink in below figure) and also make left pointer of new node as NULL.\\n•\\nUpdate head node’s left pointer to point to the new node and make new node as\\nhead. Head'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 90, 'file_type': 'pdf'}, page_content='Inserting a Node in Doubly Linked List at the Ending\\nIn this case, traverse the list till the end and insert the new node.\\n•\\nNew node right pointer points to NULL and left pointer points to the end of the list.\\n•\\nUpdate right pointer of last node to point to new node.\\nInserting a Node in Doubly Linked List at the Middle\\nAs discussed in singly linked lists, traverse the list to the position node and insert the new node.\\n•\\nNew node right pointer points to the next node of the position node where we want\\nto insert the new node. Also, new node left pointer points to the position node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 91, 'file_type': 'pdf'}, page_content='•\\nPosition node right pointer points to the new node and the next node of position node\\nleft pointer points to new node.\\nNow, let us write the code for all of these three cases. We must update the first element pointer in\\nthe calling function, not just in the called function. For this reason we need to send a double\\npointer. The following code inserts a node in the doubly linked list'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 92, 'file_type': 'pdf'}, page_content=\"void DLLinsert(struct DLLNode “head, int data, int position) {\\nintk= 1;\\nstruct DLINode *temp, ‘newNode;\\n‘newNode = (struct DLLNode *) malloc(szeof ( struct DLLNode };\\n\\niffinewNode)( [Always check for memory errors\\nprintf Memory Ero)\\nreturn,\\n\\n}\\n\\nnewNode-data * data;\\n\\niflposition == 1} [ | /lnsertng a node atthe beginning\\n\\nnewNode-next = *head;\\nnewNode-prev = NULL;\\nifhead)\\n(head) prev = newNode;\\n“head = newNode;\\nreturn;\\n\\\\\\ntemp = *head;\\nwhile ( < position - 1) 8 temp-net!*NULL {\\ntemp = temp—next;\\nkt\\n|\\niffkl=position},\\nprint{l’Desired position does not exist\\\\n');\\n}\\nnewNode-next=temp—next;\\nnewNode-prev=temp;\\nifftemp-next)\\ntemp-mnextsprevenewNode;\\n\\n‘temp—next=newNode;\\nreturn;\"), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 93, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). In the worst case, we may need to insert the node at the end of the list.\\nSpace Complexity: O(1), for creating one temporary variable.\\nDoubly Linked List Deletion\\nSimilar to singly linked list deletion, here we have three cases:\\n•\\nDeleting the first node\\n•\\nDeleting the last node\\n•\\nDeleting an intermediate node\\nDeleting the First Node in Doubly Linked List\\nIn this case, the first node (current head node) is removed from the list. It can be done in two\\nsteps:\\n•\\nCreate a temporary node which will point to the same node as that of head.\\n•\\nNow, move the head nodes pointer to the next node and change the heads left pointer\\nto NULL. Then, dispose of the temporary node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 94, 'file_type': 'pdf'}, page_content='Deleting the Last Node in Doubly Linked List\\nThis operation is a bit trickier than removing the first node, because the algorithm should find a\\nnode, which is previous to the tail first. This can be done in three steps:\\n•\\nTraverse the list and while traversing maintain the previous node address also. By\\nthe time we reach the end of the list, we will have two pointers, one pointing to the\\ntail and the other pointing to the node before the tail.\\n•\\nUpdate the next pointer of previous node to the tail node with NULL.\\n•\\nDispose the tail node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 95, 'file_type': 'pdf'}, page_content='Deleting an Intermediate Node in Doubly Linked List\\nIn this case, the node to be removed is always located between two nodes, and the head and tail\\nlinks are not updated. The removal can be done in two steps:\\n•\\nSimilar to the previous case, maintain the previous node while also traversing the\\nlist. Upon locating the node to be deleted, change the previous node’s next pointer\\nto the next node of the node to be deleted.\\n•\\nDispose of the current node to be deleted.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 96, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for creating one temporary variable.\\n3.8 Circular Linked Lists'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 97, 'file_type': 'pdf'}, page_content='In singly linked lists and doubly linked lists, the end of lists are indicated with NULL value. But\\ncircular linked lists do not have ends. While traversing the circular linked lists we should be\\ncareful; otherwise we will be traversing the list infinitely. In circular linked lists, each node has a\\nsuccessor. Note that unlike singly linked lists, there is no node with NULL pointer in a circularly\\nlinked list. In some situations, circular linked lists are useful.\\nFor example, when several processes are using the same computer resource (CPU) for the same\\namount of time, we have to assure that no process accesses the resource before all other\\nprocesses do (round robin algorithm). The following is a type declaration for a circular linked\\nlist of integers:\\nIn a circular linked list, we access the elements using the head node (similar to head node in\\nsingly linked list and doubly linked lists).\\nCounting Nodes in a Circular Linked List\\nThe circular list is accessible through the node marked head. To count the nodes, the list has to be\\ntraversed from the node marked head, with the help of a dummy node current, and stop the\\ncounting when current reaches the starting node head.\\nIf the list is empty, head will be NULL, and in that case set count = 0. Otherwise, set the current\\npointer to the first node, and keep on counting till the current pointer reaches the starting node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 98, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for creating one temporary variable.\\nPrinting the Contents of a Circular Linked List\\nWe assume here that the list is being accessed by its head node. Since all the nodes are arranged\\nin a circular fashion, the tail node of the list will be the node previous to the head node. Let us\\nassume we want to print the contents of the nodes starting with the head node. Print its contents,\\nmove to the next node and continue printing till we reach the head node again.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 99, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for temporary variable.\\nInserting a Node at the End of a Circular Linked List\\nLet us add a node containing data, at the end of a list (circular list) headed by head. The new\\nnode will be placed just after the tail node (which is the last node of the list), which means it will\\nhave to be inserted in between the tail node and the first node.\\n•\\nCreate a new node and initially keep its next pointer pointing to itself.\\n•\\nUpdate the next pointer of the new node with the head node and also traverse the list\\nto the tail. That means in a circular list we should stop at the node whose next node\\nis head.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 100, 'file_type': 'pdf'}, page_content='•\\nUpdate the next pointer of the previous node to point to the new node and we get the\\nlist as shown below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 101, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for temporary variable.\\nInserting a Node at the Front of a Circular Linked List\\nThe only difference between inserting a node at the beginning and at the end is that, after inserting\\nthe new node, we just need to update the pointer. The steps for doing this are given below:\\n•\\nCreate a new node and initially keep its next pointer pointing to itself.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 102, 'file_type': 'pdf'}, page_content='•\\nUpdate the next pointer of the new node with the head node and also traverse the list\\nuntil the tail. That means in a circular list we should stop at the node which is its\\nprevious node in the list.\\n•\\nUpdate the previous head node in the list to point to the new node.\\n•\\nMake the new node as the head.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 103, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for temporary variable.\\nDeleting the Last Node in a Circular Linked List\\nThe list has to be traversed to reach the last but one node. This has to be named as the tail node,\\nand its next field has to point to the first node. Consider the following list.\\nTo delete the last node 40, the list has to be traversed till you reach 7. The next field of 7 has to'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 104, 'file_type': 'pdf'}, page_content='be changed to point to 60, and this node must be renamed pTail.\\n•\\nTraverse the list and find the tail node and its previous node.\\n•\\nUpdate the next pointer of tail node’s previous node to point to head.\\n•\\nDispose of the tail node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 105, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n. Space Complexity: O(1), for a\\ntemporary variable.\\nDeleting the First Node in a Circular List\\nThe first node can be deleted by simply replacing the next field of the tail node with the next field\\nof the first node.\\n•\\nFind the tail node of the linked list by traversing the list. Tail node is the previous\\nnode to the head node which we want to delete.\\n•\\nCreate a temporary node which will point to the head. Also, update the tail nodes\\nnext pointer to point to next node of head (as shown below).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 106, 'file_type': 'pdf'}, page_content='•\\nNow, move the head pointer to next node. Create a temporary node which will point\\nto head. Also, update the tail nodes next pointer to point to next node of head (as\\nshown below).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 107, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for a temporary variable.\\nApplications of Circular List\\nCircular linked lists are used in managing the computing resources of a computer. We can use\\ncircular lists for implementing stacks and queues.\\n3.9 A Memory-efficient Doubly Linked List\\nIn conventional implementation, we need to keep a forward pointer to the next item on the list and\\na backward pointer to the previous item. That means elements in doubly linked list\\nimplementations consist of data, a pointer to the next node and a pointer to the previous node in\\nthe list as shown below.\\nConventional Node Definition'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 108, 'file_type': 'pdf'}, page_content='Recently a journal (Sinha) presented an alternative implementation of the doubly linked list ADT,\\nwith insertion, traversal and deletion operations. This implementation is based on pointer\\ndifference. Each node uses only one pointer field to traverse the list back and forth.\\nNew Node Definition\\nThe ptrdiff pointer field contains the difference between the pointer to the next node and the\\npointer to the previous node. The pointer difference is calculated by using exclusive-or (⊕)\\noperation.\\nptrdiff = pointer to previous node ⊕ pointer to next node.\\nThe ptrdiff of the start node (head node) is the ⊕ of NULL and next node (next node to head).\\nSimilarly, the ptrdiff of end node is the ⊕ of previous node (previous to end node) and NULL. As\\nan example, consider the following linked list.\\nIn the example above,\\n•\\nThe next pointer of A is: NULL ⊕ B\\n•\\nThe next pointer of B is: A ⊕ C\\n•\\nThe next pointer of C is: B ⊕ D\\n•\\nThe next pointer of D is: C ⊕ NULL'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 109, 'file_type': 'pdf'}, page_content='Why does it work?\\nTo find the answer to this question let us consider the properties of ⊕:\\nX ⊕ X=0\\nX ⊕ 0 = X\\nX ⊕ Y = Y ⊕ X (symmetric)\\n(X ⊕ Y) ⊕ Z = X ⊕ (Y ⊕ Z) (transitive)\\nFor the example above, let us assume that we are at C node and want to move to B. We know that\\nC’s ptrdiff is defined as B ⊕ D. If we want to move to B, performing ⊕ on C’s ptrdiff with D\\nwould give B. This is due to the fact that\\n(B ⊕ D) ⊕ D = B(since, D ⊕ D= 0)\\nSimilarly, if we want to move to D, then we have to apply ⊕ to C’s ptrdiff with B to give D.\\n(B ⊕ D) ⊕ B = D (since, B © B=0)\\nFrom the above discussion we can see that just by using a single pointer, we can move back and\\nforth. A memory-efficient implementation of a doubly linked list is possible with minimal\\ncompromising of timing efficiency.\\n3.10 Unrolled Linked Lists\\nOne of the biggest advantages of linked lists over arrays is that inserting an element at any\\nlocation takes only O(1) time. However, it takes O(n) to search for an element in a linked list.\\nThere is a simple variation of the singly linked list called unrolled linked lists.\\nAn unrolled linked list stores multiple elements in each node (let us call it a block for our\\nconvenience). In each block, a circular linked list is used to connect all nodes.\\nAssume that there will be no more than n elements in the unrolled linked list at any time. To\\nsimplify this problem, all blocks, except the last one, should contain exactly \\n elements. Thus,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 110, 'file_type': 'pdf'}, page_content='there will be no more than \\n blocks at any time.\\nSearching for an element in Unrolled Linked Lists\\nIn unrolled linked lists, we can find the kth element in O(\\n):\\n1.\\nTraverse the list of blocks to the one that contains the kth node, i.e., the \\nblock. It takes O(\\n) since we may find it by going through no more than \\nblocks.\\n2.\\nFind the (k mod \\n)th node in the circular linked list of this block. It also takes O(\\n) since there are no more than \\n nodes in a single block.\\nInserting an element in Unrolled Linked Lists'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 111, 'file_type': 'pdf'}, page_content='When inserting a node, we have to re-arrange the nodes in the unrolled linked list to maintain the\\nproperties previously mentioned, that each block contains \\n nodes. Suppose that we insert a\\nnode x after the ith node, and x should be placed in the jth block. Nodes in the jth block and in the\\nblocks after the jth block have to be shifted toward the tail of the list so that each of them still\\nhave \\n nodes. In addition, a new block needs to be added to the tail if the last block of the list\\nis out of space, i.e., it has more than \\n nodes.\\nPerforming Shift Operation\\nNote that each shift operation, which includes removing a node from the tail of the circular linked\\nlist in a block and inserting a node to the head of the circular linked list in the block after, takes\\nonly O(1). The total time complexity of an insertion operation for unrolled linked lists is therefore\\nO(\\n); there are at most O(\\n) blocks and therefore at most O(\\n) shift operations.\\n1.\\nA temporary pointer is needed to store the tail of A.\\n2.\\nIn block A, move the next pointer of the head node to point to the second-to-last\\nnode, so that the tail node of A can be removed.\\n3.\\nLet the next pointer of the node, which will be shifted (the tail node of A), point\\nto the tail node of B.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 112, 'file_type': 'pdf'}, page_content='4.\\nLet the next pointer of the head node of B point to the node temp points to.\\n5.\\nFinally, set the head pointer of B to point to the node temp points to. Now the\\nnode temp points to becomes the new head node of B.\\n6.\\ntemp pointer can be thrown away. We have completed the shift operation to\\nmove the original tail node of A to become the new head node of B.\\nPerformance'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 113, 'file_type': 'pdf'}, page_content='With unrolled linked lists, there are a couple of advantages, one in speed and one in space. First,\\nif the number of elements in each block is appropriately sized (e.g., at most the size of one cache\\nline), we get noticeably better cache performance from the improved memory locality. Second,\\nsince we have O(n/m) links, where n is the number of elements in the unrolled linked list and m is\\nthe number of elements we can store in any block, we can also save an appreciable amount of\\nspace, which is particularly noticeable if each element is small.\\nComparing Linked Lists and Unrolled Linked Lists\\nTo compare the overhead for an unrolled list, elements in doubly linked list implementations\\nconsist of data, a pointer to the next node, and a pointer to the previous node in the list, as shown\\nbelow.\\nAssuming we have 4 byte pointers, each node is going to take 8 bytes. But the allocation overhead\\nfor the node could be anywhere between 8 and 16 bytes. Let’s go with the best case and assume it\\nwill be 8 bytes. So, if we want to store IK items in this list, we are going to have 16KB of\\noverhead.\\nNow, let’s think about an unrolled linked list node (let us call it LinkedBlock). It will look\\nsomething like this:\\nTherefore, allocating a single node (12 bytes + 8 bytes of overhead) with an array of 100\\nelements (400 bytes + 8 bytes of overhead) will now cost 428 bytes, or 4.28 bytes per element.\\nThinking about our IK items from above, it would take about 4.2KB of overhead, which is close\\nto 4x better than our original list. Even if the list becomes severely fragmented and the item arrays\\nare only 1/2 full on average, this is still an improvement. Also, note that we can tune the array\\nsize to whatever gets us the best overhead for our application.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 114, 'file_type': 'pdf'}, page_content='Implementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 115, 'file_type': 'pdf'}, page_content='struct LinkedBlockt\\n‘struct LinkedBlock *next;\\n{nt nodeCoust\\n\\nis\\n\\nstruct LinkedBlock* blockHead;\\n\\nJ /eveate an empty block\\n\\nruct LinkedBlock* nevLinleedBlock)}|\\nstruct LinkedBlock\" blocks(struct LinkedBlock\")mallocsizeofistruct LinkedBlock));\\n‘lock--next=NULL;\\nblockhead =NUL\\n\\n}\\n[/ereate a node\\nstruct ListNode\" newListNodelint value),\\n‘struct ListNede\" temp>(struct ListNode*)malloc{sizeofstruct ListNode)};\\ntemp-next>NULL:\\ntemp—value-value;\\nreturn temps\\n)\\nvoid searchElementiint k,struct LinkedBlock *LinkedBlock,struct ListNode **ListNode\\n[find the block\\nint jofkblockSize-1)/blockSize; //k-th node is inthe jth block\\nstruct Linkedilock\" prblockHead:\\nwhile\\naa\\n*LinkedBloce\\n[fod the nose\\n‘struct ListNede* qrp-—head:\\neeloblock Size;\\niffk==0} k-blockSize:\\n\\n}\\n{/staxt shift operation from block\\n‘oid shiftstruct LinkedBlock *A),\\n‘struct LinkedBlock B;\\nStruct ListNede* temp:\\n‘while(A-snodeCount > blockSize), //if this block still have to shift\\nMA-next~-NULL //reach the end. A little different\\nAnsnext=newLinkedBleck):\\nBeA-next,\\ntemp-A--head—nest;\\nAvshead-next-A-shead-next-né'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 116, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 117, 'file_type': 'pdf'}, page_content='B-enodeCount++;\\n,\\nAB;\\n’\\n’\\nvoid addElement(int k.int 3),\\n‘struct ListNode *p,\\nstruct LinkedBlock\\nif fblockHead) / intial, first node and block\\nblockHead-newLinkedBlock’);\\nblockHead-shead=newListNode(x);\\nDlockHead-shead-next-blockHead-shead;\\nblockdfead—-nodeCount* +5,\\nJetset\\niffke=O}, //special case fork\\nprblockHead--head:\\n\\n-pnext;\\npoonext=newListNode x}\\nPoonext-snexteg:\\nDlockHtead—head-p—next;\\nDlockHead-nodeCount*:\\n\\nap:\\nwhile(q-next!=p) q-q-~next\\nqunext-newLiatNode(x)\\nquonext-onext-p:\\nFonodeCount\\n\\nint searchElementfint kj,\\nstruct ListNode *p:\\nstruct LinkedBlock *q\\nsearchElement(k,84,8p):\\nreturn p-value:\\n\\nint testUnRolledLinkedList{\\nint ttrelockt)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 118, 'file_type': 'pdf'}, page_content='3.11 Skip Lists\\nBinary trees can be used for representing abstract data types such as dictionaries and ordered\\nlists. They work well when the elements are inserted in a random order. Some sequences of\\noperations, such as inserting the elements in order, produce degenerate data structures that give\\nvery poor performance. If it were possible to randomly permute the list of items to be inserted,\\ntrees would work well with high probability for any input sequence. In most cases queries must\\nbe answered on-line, so randomly permuting the input is impractical. Balanced tree algorithms re-\\narrange the tree as operations are performed to maintain certain balance conditions and assure\\ngood performance.\\nSkip lists are a probabilistic alternative to balanced trees. Skip list is a data structure that can be\\nused as an alternative to balanced binary trees (refer to Trees chapter). As compared to a binary\\ntree, skip lists allow quick search, insertion and deletion of elements. This is achieved by using\\nprobabilistic balancing rather than strictly enforce balancing. It is basically a linked list with\\nadditional pointers such that intermediate nodes can be skipped. It uses a random number\\ngenerator to make some decisions.\\nIn an ordinary sorted linked list, search, insert, and delete are in O(n) because the list must be\\nscanned node-by-node from the head to find the relevant node. If somehow we could scan down\\nthe list in bigger steps (skip down, as it were), we would reduce the cost of scanning. This is the\\nfundamental idea behind Skip Lists.\\nSkip Lists with One Level\\nSkip Lists with Two Levels\\nSkip Lists with Three Levels'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 119, 'file_type': 'pdf'}, page_content='Performance\\nIn a simple linked list that consists of n elements, to perform a search n comparisons are required\\nin the worst case. If a second pointer pointing two nodes ahead is added to every node, the\\nnumber of comparisons goes down to n/2 + 1 in the worst case.\\nAdding one more pointer to every fourth node and making them point to the fourth node ahead\\nreduces the number of comparisons to ⌈n/2⌉ + 2. If this strategy is continued so that every node\\nwith i pointers points to 2 * i – 1 nodes ahead, O(logn) performance is obtained and the number\\nof pointers has only doubled (n + n/2 + n/4 + n/8 + n/16 + .... = 2n).\\nThe find, insert, and remove operations on ordinary binary search trees are efficient, O(logn),\\nwhen the input data is random; but less efficient, O(n), when the input data is ordered. Skip List\\nperformance for these same operations and for any data set is about as good as that of randomly-\\nbuilt binary search trees - namely O(logn).\\nComparing Skip Lists and Unrolled Linked Lists\\nIn simple terms, Skip Lists are sorted linked lists with two differences:\\n•\\nThe nodes in an ordinary list have one next reference. The nodes in a Skip List have\\nmany next references (also called forward references).\\n•\\nThe number of forward references for a given node is determined probabilistically.\\nWe speak of a Skip List node having levels, one level per forward reference. The number of\\nlevels in a node is called the size of the node. In an ordinary sorted list, insert, remove, and find\\noperations require sequential traversal of the list. This results in O(n) performance per operation.\\nSkip Lists allow intermediate nodes in the list to be skipped during a traversal - resulting in an\\nexpected performance of O(logn) per operation.\\nImplementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 120, 'file_type': 'pdf'}, page_content='‘struct ListNode *insertElementint data) (\\ninti, newLevel;\\nstruct ListNode *update/MAXSKIPLEVEL* 1];\\nstruct ListNode “temp:\\ntemp = list header:\\nfor (= list istLevel i >= 0; i\\n‘while temp-nexti] Ilia. header && temp-nextf}-sdata < data)\\ntemp = temp--nextil;\\nupdate( = temp;\\n\\n{or (newLevel = 0; rand) < RAND_MAX/2 && newLevel « MAXSKIPLEVEL; newLevelt +);\\nif newLevel > list listLeve) {\\nfor (i= ist lstLevel + 1:4 <= newLevel\\n‘updatel = ist header,\\nlit ntLevel = nena\\n)\\n// make new node\\nif (temp = malloc(sizeof{Node) +\\nrnewLevelsizeof{Node *)) == 0) |\\nprintf insufficient memory (insertElement)\\\\n\")\\nexit);\\n)\\ntemp—data = data;\\nfor i= 0: i<= newLevel: i+*){ // update next links\\n‘temp--nexti] = updateli--nextis\\njoe toe be!\\n, ttsrntempk\\n// delete node containing data\\nvoid deleteElementint data) (\\nint fs\\nstruct ListNode *update[MAXSKIPLEVEL* 1}, *temp;\\ntemp = list header:\\nfor (= list listLevel i >= 0:5)\\n‘while (temp-onext[ = lit header && temp-onextf|-edata < data)\\ntemp = temp—nexti;\\n\\nee\\ntemp = temp—next{0};\\n\\nif (temp == lis-header | | temp-edata == data) return;\\n{ /adjust next pointers\\n\\nfor i= 0: 1 <= list listLevel: i++) {\\n\\nif (upaateli|~nexti = temp) break:\\n1 Neel « toe'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 121, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 122, 'file_type': 'pdf'}, page_content='free (temp);\\n\\nadjust header level\\n\\nhile (list.listLevel > 0) && list header—>next[istlistLevel == list.header})\\nlist listLevel--;\\n\\n}\\n// find node containing data\\n‘struct ListNode “indElementint data) {\\ninti;\\nstruct ListNode *temp = list header;\\nfor (i= list.listLevel i >= 0; i-)\\nwhile (temp--nexti I= ls header\\n‘&& temp-nexti|data < data)\\ntemp = temp-next{l;\\n\\ntemp = temp-next};\\nif temp I ist header && temp\\nreturn(0};\\n}\\n1 [initialize skip ist\\n‘void inthis) (\\ninti\\nif istheader = mallo(sizeofstruct ListNode) + MAXSKIPLEVEL*sizeofistruc ListNode*) == 0)\\nprintf (Memory Error\\\\n\\'}\\nexit)\\n}\\n\\n== data) return (temp);\\n\\n‘MAXSKIPLEVEL; i++)\\nlistheader-snexth]» list header;\\nlist.istLevel = 0;\\n}\\n/* command-line: skipList maxnum skipList 2000: process 2000 sequential records */\\nint mainjint arge, char *argy){\\n\\nsmalloc(maxnum * sizeof(a}) =~ 0) {\\n{print (stderr, “insuficient memory (a)\\\\n\"}\\nexit);\\n\\n)\\n\\nfor = 0; §< maxnum; i+) ai = rand;\\nprint! (Random, Yi items\\\\n’, maxnum);\\n\\nfor (i= 0;i< maxnum; i+) {\\ninsertElement(lji;\\n\\n)\\n\\nfor (i= maxnum-t; i>= 0; +) {\\nfindElement(ai);'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 123, 'file_type': 'pdf'}, page_content='3.12 Linked Lists: Problems & Solutions\\nProblem-1\\u2003\\u2003Implement Stack using Linked List.\\nSolution: Refer to Stacks chapter.\\nProblem-2\\u2003\\u2003Find nth node from the end of a Linked List.\\nSolution: Brute-Force Method: Start with the first node and count the number of nodes present\\nafter that node. If the number of nodes is < n – 1 then return saying “fewer number of nodes in the\\nlist”. If the number of nodes is > n – 1 then go to next node. Continue this until the numbers of\\nnodes after current node are n – 1.\\nTime Complexity: O(n2), for scanning the remaining list (from current node) for each node.\\nSpace Complexity: O(1).\\nProblem-3\\u2003\\u2003Can we improve the complexity of Problem-2?\\nSolution: Yes, using hash table. As an example consider the following list.\\nIn this approach, create a hash table whose entries are < position of node, node address >. That\\nmeans, key is the position of the node in the list and value is the address of that node.\\nPosition in List\\nAddress of Node\\n1\\nAddress of 5 node\\n2\\nAddress of 1 node\\n3\\nAddress of 17 node\\n4\\nAddress of 4 node\\nBy the time we traverse the complete list (for creating the hash table), we can find the list length.\\nLet us say the list length is M. To find nth from the end of linked list, we can convert this to M- n\\n+ 1th from the beginning. Since we already know the length of the list, it is just a matter of'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 124, 'file_type': 'pdf'}, page_content='returning M- n + 1th key value from the hash table.\\nTime Complexity: Time for creating the hash table, T(m) = O(m).\\nSpace Complexity: Since we need to create a hash table of size m, O(m).\\nProblem-4\\u2003\\u2003Can we use the Problem-3 approach for solving Problem-2 without creating the\\nhash table?\\nSolution: Yes. If we observe the Problem-3 solution, what we are actually doing is finding the\\nsize of the linked list. That means we are using the hash table to find the size of the linked list. We\\ncan find the length of the linked list just by starting at the head node and traversing the list.\\nSo, we can find the length of the list without creating the hash table. After finding the length,\\ncompute M – n + 1 and with one more scan we can get the M – n+ 1th node from the beginning.\\nThis solution needs two scans: one for finding the length of the list and the other for finding M –\\nn+ 1th node from the beginning.\\nTime Complexity: Time for finding the length + Time for finding the M – n + 1th node from the\\nbeginning. Therefore, T(n) = O(n) + O(n) ≈ O(n). Space Complexity: O(1). Hence, no need to\\ncreate the hash table.\\nProblem-5\\u2003\\u2003Can we solve Problem-2 in one scan?\\nSolution: Yes. Efficient Approach: Use two pointers pNthNode and pTemp. Initially, both point\\nto head node of the list. pNthNode starts moving only after pTemp has made n moves.\\nFrom there both move forward until pTemp reaches the end of the list. As a result pNthNode\\npoints to nth node from the end of the linked list.\\nNote: At any point of time both move one node at a time.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 125, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-6\\u2003\\u2003Check whether the given linked list is either NULL-terminated or ends in a cycle\\n(cyclic).\\nSolution: Brute-Force Approach. As an example, consider the following linked list which has a\\nloop in it. The difference between this list and the regular list is that, in this list, there are two\\nnodes whose next pointers are the same. In regular singly linked lists (without a loop) each node’s\\nnext pointer is unique.\\nThat means the repetition of next pointers indicates the existence of a loop.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 126, 'file_type': 'pdf'}, page_content='One simple and brute force way of solving this is, start with the first node and see whether there\\nis any node whose next pointer is the current node’s address. If there is a node with the same\\naddress then that indicates that some other node is pointing to the current node and we can say a\\nloop exists. Continue this process for all the nodes of the linked list.\\nDoes this method work? As per the algorithm, we are checking for the next pointer addresses,\\nbut how do we find the end of the linked list (otherwise we will end up in an infinite loop)?\\nNote: If we start with a node in a loop, this method may work depending on the size of the loop.\\nProblem-7\\u2003\\u2003Can we use the hashing technique for solving Problem-6?\\nSolution: Yes. Using Hash Tables we can solve this problem.\\nAlgorithm:\\n•\\nTraverse the linked list nodes one by one.\\n•\\nCheck if the address of the node is available in the hash table or not.\\n•\\nIf it is already available in the hash table, that indicates that we are visiting the node\\nthat was already visited. This is possible only if the given linked list has a loop in\\nit.\\n•\\nIf the address of the node is not available in the hash table, insert that node’s address\\ninto the hash table.\\n•\\nContinue this process until we reach the end of the linked list or we find the loop.\\nTime Complexity; O(n) for scanning the linked list. Note that we are doing a scan of only the\\ninput.\\nSpace Complexity; O(n) for hash table.\\nProblem-8\\u2003\\u2003Can we solve Problem-6 using the sorting technique?\\nSolution: No. Consider the following algorithm which is based on sorting. Then we see why this'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 127, 'file_type': 'pdf'}, page_content='algorithm fails.\\nAlgorithm:\\n•\\nTraverse the linked list nodes one by one and take all the next pointer values into an\\narray.\\n•\\nSort the array that has the next node pointers.\\n•\\nIf there is a loop in the linked list, definitely two next node pointers will be pointing\\nto the same node.\\n•\\nAfter sorting if there is a loop in the list, the nodes whose next pointers are the same\\nwill end up adjacent in the sorted list.\\n•\\nIf any such pair exists in the sorted list then we say the linked list has a loop in it.\\nTime Complexity; O(nlogn) for sorting the next pointers array.\\nSpace Complexity; O(n) for the next pointers array.\\nProblem with the above algorithm: The above algorithm works only if we can find the length of\\nthe list. But if the list has a loop then we may end up in an infinite loop. Due to this reason the\\nalgorithm fails.\\nProblem-9\\u2003\\u2003Can we solve the Problem-6 in O(n)?\\nSolution: Yes. Efficient Approach (Memoryless Approach): This problem was solved by\\nFloyd. The solution is named the Floyd cycle finding algorithm. It uses two pointers moving at\\ndifferent speeds to walk the linked list. Once they enter the loop they are expected to meet, which\\ndenotes that there is a loop.\\nThis works because the only way a faster moving pointer would point to the same location as a\\nslower moving pointer is if somehow the entire list or a part of it is circular. Think of a tortoise\\nand a hare running on a track. The faster running hare will catch up with the tortoise if they are\\nrunning in a loop. As an example, consider the following example and trace out the Floyd\\nalgorithm. From the diagrams below we can see that after the final step they are meeting at some\\npoint in the loop which may not be the starting point of the loop.\\nNote: slowPtr (tortoise) moves one pointer at a time and fastPtr (hare) moves two pointers at a\\ntime.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 128, 'file_type': 'pdf'}, page_content='slowPtr\\n\\nfastPtr\\n\\nslowPtr  fastPtr\\n\\nfastPtr\\nslowPtr\\nslowPtr\\nfastPtr\\nslowPtr\\nfastPt\\nfastPtr slowPt\\nslowPtr\\n\\nfastPtr'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 129, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-10\\u2003\\u2003are given a pointer to the first element of a linked list L. There are two\\npossibilities for L: it either ends (snake) or its last element points back to one of the\\nearlier elements in the list (snail). Give an algorithm that tests whether a given list L is a\\nsnake or a snail.\\nSolution: It is the same as Problem-6.\\nProblem-11\\u2003\\u2003Check whether the given linked list is NULL-terminated or not. If there is a\\ncycle find the start node of the loop.\\nSolution: The solution is an extension to the solution in Problem-9. After finding the loop in the\\nlinked list, we initialize the slowPtr to the head of the linked list. From that point onwards both\\nslowPtr and fastPtr move only one node at a time. The point at which they meet is the start of the\\nloop. Generally we use this method for removing the loops.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 130, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-12\\u2003\\u2003From the previous discussion and problems we understand that the meeting of\\ntortoise and hare concludes the existence of the loop, but how does moving the tortoise to\\nthe beginning of the linked list while keeping the hare at the meeting place, followed by\\nmoving both one step at a time, make them meet at the starting point of the cycle?\\nSolution: This problem is at the heart of number theory. In the Floyd cycle finding algorithm,\\nnotice that the tortoise and the hare will meet when they are n × L, where L is the loop length.\\nFurthermore, the tortoise is at the midpoint between the hare and the beginning of the sequence\\nbecause of the way they move. Therefore the tortoise is n × L away from the beginning of the\\nsequence as well. If we move both one step at a time, from the position of the tortoise and from\\nthe start of the sequence, we know that they will meet as soon as both are in the loop, since they\\nare n × L, a multiple of the loop length, apart. One of them is already in the loop, so we just move\\nthe other one in single step until it enters the loop, keeping the other n × L away from it at all\\ntimes.\\nProblem-13\\u2003\\u2003In the Floyd cycle finding algorithm, does it work if we use steps 2 and 3\\ninstead of 1 and 2?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 131, 'file_type': 'pdf'}, page_content='Solution: Yes, but the complexity might be high. Trace out an example.\\nProblem-14\\u2003\\u2003Check whether the given linked list is NULL-terminated. If there is a cycle, find\\nthe length of the loop.\\nSolution: This solution is also an extension of the basic cycle detection problem. After finding the\\nloop in the linked list, keep the slowPtr as it is. The fastPtr keeps on moving until it again comes\\nback to slowPtr. While moving fastPtr, use a counter variable which increments at the rate of 1.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-15\\u2003\\u2003Insert a node in a sorted linked list.\\nSolution: Traverse the list and find a position for the element and insert it.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 132, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-16\\u2003\\u2003Reverse a singly linked list.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(1).\\nRecursive version: We will find it easier to start from the bottom up, by asking and answering\\ntiny questions (this is the approach in The Little Lisper):\\n•\\nWhat is the reverse of NULL (the empty list)? NULL.\\n•\\nWhat is the reverse of a one element list? The element itself.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 133, 'file_type': 'pdf'}, page_content='•\\nWhat is the reverse of an n element list? The reverse of the second element followed\\nby the first element.\\nTime Complexity: O(n). Space Complexity: O(n),for recursive stack.\\nProblem-17\\u2003\\u2003Suppose there are two singly linked lists both of which intersect at some point\\nand become a single linked list. The head or start pointers of both the lists are known, but\\nthe intersecting node is not known. Also, the number of nodes in each of the lists before\\nthey intersect is unknown and may be different in each list. List1 may have n nodes before\\nit reaches the intersection point, and List2 might have m nodes before it reaches the\\nintersection point where m and n may be m = n,m < n or m > n. Give an algorithm for\\nfinding the merging point.\\nSolution: Brute-Force Approach: One easy solution is to compare every node pointer in the first\\nlist with every other node pointer in the second list by which the matching node pointers will lead\\nus to the intersecting node. But, the time complexity in this case will be O(mn) which will be\\nhigh.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 134, 'file_type': 'pdf'}, page_content='Time Complexity: O(mn). Space Complexity: O(1).\\nProblem-18\\u2003\\u2003Can we solve Problem-17 using the sorting technique?\\nSolution: No. Consider the following algorithm which is based on sorting and see why this\\nalgorithm fails.\\nAlgorithm:\\n•\\nTake first list node pointers and keep them in some array and sort them.\\n•\\nTake second list node pointers and keep them in some array and sort them.\\n•\\nAfter sorting, use two indexes: one for the first sorted array and the other for the\\nsecond sorted array.\\n•\\nStart comparing values at the indexes and increment the index according to\\nwhichever has the lower value (increment only if the values are not equal).\\n•\\nAt any point, if we are able to find two indexes whose values are the same, then that\\nindicates that those two nodes are pointing to the same node and we return that\\nnode.\\nTime Complexity: Time for sorting lists + Time for scanning (for comparing)\\n= O(mlogm) +O(nlogn) +O(m + n) We need to consider the one that gives the\\nmaximum value.\\nSpace Complexity: O(1).\\nAny problem with the above algorithm? Yes. In the algorithm, we are storing all the node\\npointers of both the lists and sorting. But we are forgetting the fact that there can be many repeated\\nelements. This is because after the merging point, all node pointers are the same for both the lists.\\nThe algorithm works fine only in one case and it is when both lists have the ending node at their\\nmerge point.\\nProblem-19\\u2003\\u2003Can we solve Problem-17 using hash tables?\\nSolution: Yes.\\nAlgorithm:\\n•\\nSelect a list which has less number of nodes (If we do not know the lengths\\nbeforehand then select one list randomly).\\n•\\nNow, traverse the other list and for each node pointer of this list check whether the\\nsame node pointer exists in the hash table.\\n•\\nIf there is a merge point for the given lists then we will definitely encounter the node\\npointer in the hash table.\\nTime Complexity: Time for creating the hash table + Time for scanning the second list = O(m) +\\nO(n) (or O(n) + O(m), depending on which list we select for creating the hash table. But in both'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 135, 'file_type': 'pdf'}, page_content='cases the time complexity is the same. Space Complexity: O(n) or O(m).\\nProblem-20\\u2003\\u2003Can we use stacks for solving the Problem-17?\\nSolution: Yes.\\nAlgorithm:\\n•\\nCreate two stacks: one for the first list and one for the second list.\\n•\\nTraverse the first list and push all the node addresses onto the first stack.\\n•\\nTraverse the second list and push all the node addresses onto the second stack.\\n•\\nNow both stacks contain the node address of the corresponding lists.\\n•\\nNow compare the top node address of both stacks.\\n•\\nIf they are the same, take the top elements from both the stacks and keep them in\\nsome temporary variable (since both node addresses are node, it is enough if we\\nuse one temporary variable).\\n•\\nContinue this process until the top node addresses of the stacks are not the same.\\n•\\nThis point is the one where the lists merge into a single list.\\n•\\nReturn the value of the temporary variable.\\nTime Complexity: O(m + n), for scanning both the lists.\\nSpace Complexity: O(m + n), for creating two stacks for both the lists.\\nProblem-21\\u2003\\u2003Is there any other way of solving Problem-17?\\nSolution: Yes. Using “finding the first repeating number” approach in an array (for algorithm\\nrefer to Searching chapter).\\nAlgorithm:\\n•\\nCreate an array A and keep all the next pointers of both the lists in the array.\\n•\\nIn the array find the first repeating element [Refer to Searching chapter for\\nalgorithm].\\n•\\nThe first repeating number indicates the merging point of both the lists.\\nTime Complexity: O(m + n). Space Complexity: O(m + n).\\nProblem-22\\u2003\\u2003Can we still think of finding an alternative solution for Problem-17?\\nSolution: Yes. By combining sorting and search techniques we can reduce the complexity.\\nAlgorithm:\\n•\\nCreate an array A and keep all the next pointers of the first list in the array.\\n•\\nSort these array elements.\\n•\\nThen, for each of the second list elements, search in the sorted array (let us assume'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 136, 'file_type': 'pdf'}, page_content='that we are using binary search which gives O(logn)).\\n•\\nSince we are scanning the second list one by one, the first repeating element that\\nappears in the array is nothing but the merging point.\\nTime Complexity: Time for sorting + Time for searching = O(Max(mlogm, nlogn)).\\nSpace Complexity: O(Max(m, n)).\\nProblem-23\\u2003\\u2003Can we improve the complexity for Problem-17?\\nSolution: Yes.\\nEfficient Approach:\\n•\\nFind lengths (L1 and L2) of both lists - O(n) + O(m) = O(max(m, n)).\\n•\\nTake the difference d of the lengths -- O(1).\\n•\\nMake d steps in longer list -- O(d).\\n•\\nStep in both lists in parallel until links to next node match -- O(min(m, n)).\\n•\\nTotal time complexity = O(max(m, n)).\\n•\\nSpace Complexity = O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 137, 'file_type': 'pdf'}, page_content='Problem-24\\u2003\\u2003How will you find the middle of the linked list?\\nSolution: Brute-Force Approach: For each of the node, count how many nodes are there in the\\nlist, and see whether it is the middle node of the list.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-25\\u2003\\u2003Can we improve the complexity of Problem-24?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 138, 'file_type': 'pdf'}, page_content='Solution: Yes.\\nAlgorithm:\\n•\\nTraverse the list and find the length of the list.\\n•\\nAfter finding the length, again scan the list and locate n/2 node from the beginning.\\nTime Complexity: Time for finding the length of the list + Time for locating middle node = O(n) +\\nO(n) ≈ O(n).\\nSpace Complexity: O(1).\\nProblem-26\\u2003\\u2003Can we use the hash table for solving Problem-24?\\nSolution: Yes. The reasoning is the same as that of Problem-3.\\nTime Complexity: Time for creating the hash table. Therefore, T(n) = O(n).\\nSpace Complexity: O(n). Since we need to create a hash table of size n.\\nProblem-27\\u2003\\u2003Can we solve Problem-24 just in one scan?\\nSolution: Efficient Approach: Use two pointers. Move one pointer at twice the speed of the\\nsecond. When the first pointer reaches the end of the list, the second pointer will be pointing to\\nthe middle node.\\nNote: If the list has an even number of nodes, the middle node will be of ⌊n/2⌋.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 139, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-28\\u2003\\u2003How will you display a Linked List from the end?\\nSolution: Traverse recursively till the end of the linked list. While coming back, start printing the\\nelements.\\nTime Complexity: O(n). Space Complexity: O(n)→ for Stack.\\nProblem-29\\u2003\\u2003Check whether the given Linked List length is even or odd?\\nSolution: Use a 2x pointer. Take a pointer that moves at 2x [two nodes at a time]. At the end, if\\nthe length is even, then the pointer will be NULL; otherwise it will point to the last node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 140, 'file_type': 'pdf'}, page_content='Time Complexity: O(⌊n/2⌋) ≈ O(n). Space Complexity: O(1).\\nProblem-30\\u2003\\u2003If the head of a Linked List is pointing to kth element, then how will you get the\\nelements before kth element?\\nSolution: Use Memory Efficient Linked Lists [XOR Linked Lists].\\nProblem-31\\u2003\\u2003Given two sorted Linked Lists, how to merge them into the third list in sorted\\norder?\\nSolution: Assume the sizes of lists are m and n.\\nRecursive:\\nTime Complexity: O(n + m), where n and m are lengths of two lists.\\nIterative:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 141, 'file_type': 'pdf'}, page_content='Time Complexity: O(n + m), where n and m are lengths of two lists.\\nProblem-32\\u2003\\u2003Reverse the linked list in pairs. If you have a linked list that holds 1 → 2 → 3\\n→ 4 → X, then after the function has been called the linked list would hold 2 → 1 → 4 →\\n3 → X.\\nSolution:\\nRecursive:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 142, 'file_type': 'pdf'}, page_content='Iterative:\\nTime Complexity: O(n). Space Complexity: O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 143, 'file_type': 'pdf'}, page_content='Problem-33\\u2003\\u2003Given a binary tree convert it to doubly linked list.\\nSolution: Refer Trees chapter.\\nProblem-34\\u2003\\u2003How do we sort the Linked Lists?\\nSolution: Refer Sorting chapter.\\nProblem-35\\u2003\\u2003Split a Circular Linked List into two equal parts. If the number of nodes in the\\nlist are odd then make first list one node extra than second list.\\nSolution:\\nAlgorithm:\\n•\\nStore the mid and last pointers of the circular linked list using Floyd cycle finding\\nalgorithm.\\n•\\nMake the second half circular.\\n•\\nMake the first half circular.\\n•\\nSet head pointers of the two linked lists.\\nAs an example, consider the following circular list.\\nAfter the split, the above list will look like:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 144, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-36\\u2003\\u2003If we want to concatenate two linked lists which of the following gives O(1)\\ncomplexity?\\n1)\\nSingly linked lists\\n2)\\nDoubly linked lists\\n3)\\nCircular doubly linked lists\\nSolution: Circular Doubly Linked Lists. This is because for singly and doubly linked lists, we'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 145, 'file_type': 'pdf'}, page_content='need to traverse the first list till the end and append the second list. But in the case of circular\\ndoubly linked lists we don’t have to traverse the lists.\\nProblem-37\\u2003\\u2003How will you check if the linked list is palindrome or not?\\nSolution:\\nAlgorithm:\\n1.\\nGet the middle of the linked list.\\n2.\\nReverse the second half of the linked list.\\n3.\\nCompare the first half and second half.\\n4.\\nConstruct the original linked list by reversing the second half again and\\nattaching it back to the first half.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-38\\u2003\\u2003For a given K value (K > 0) reverse blocks of K nodes in a list.\\nExample: Input: 1 2 3 4 5 6 7 8 9 10. Output for different K values:\\nFor K = 2: 2 1 4 3 6 5 8 7 10 9\\nFor K = 3: 3 2 1 6 5 4 9 8 7 10\\nFor K = 4: 4 3 2 1 8 7 6 5 9 10\\nSolution:\\nAlgorithm: This is an extension of swapping nodes in a linked list.\\n1)\\nCheck if remaining list has K nodes.\\na.\\nIf yes get the pointer of K + 1th node.\\nb.\\nElse return.\\n2)\\nReverse first K nodes.\\n3)\\nSet next of last node (after reversal) to K + 1th node.\\n4)\\nMove to K + 1th node.\\n5)\\nGo to step 1.\\n6)\\nK – 1th node of first K nodes becomes the new head if available. Otherwise, we can\\nreturn the head.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 146, 'file_type': 'pdf'}, page_content='struct ListNode * GetkPhusOneThNodefintK, struct ListNode “head {\\nstruct ListNode*Kth;\\ninti=0;\\nithead)\\nreturn head;\\nfor (i= 0, Kth = head; Kth & (i<j; t+, Kth» Kth-sneat)\\nili== K 88 Kth I= NULL)\\nreturn Kt;\\nretum head-neat;\\n\\n}\\n\\n{nt HasKnodes(struct ListNode *head, int K){\\ninti=0;\\n\\nead & (i < K); i++, head = head—next);\\n\\nstruct ListNode *ReverseBlockOIK-nodestnlinkedList{structListNode *head, int K){\\nstruet ListNode ‘cur = head, temp, ‘next, newHead;\\ninti\\n‘ffK==0 || Ke=1)\\nreturn head;\\niflHasKnodes(cur, K-))\\nnewHead = GetKPlusOneThNode(K-1, cu)\\nelse\\nnewHead » head;\\n\\nwhile(cur && HasKnodes(cur, K) {\\ntemp = GetkPlusOneThNode(K, cur);\\n0;\\nwhile <K)\\nnext » cur-snext;\\nccur-snext=temp;\\ntemp = cur;\\nccur = next;\\nitt;\\n\\ni\\nreturn newHead;'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 147, 'file_type': 'pdf'}, page_content='Problem-39\\u2003\\u2003Is it possible to get O(1) access time for Linked Lists?\\nSolution: Yes. Create a linked list and at the same time keep it in a hash table. For n elements we\\nhave to keep all the elements in a hash table which gives a preprocessing time of O(n).To read\\nany element we require only constant time O(1) and to read n elements we require n * 1 unit of\\ntime = n units. Hence by using amortized analysis we can say that element access can be\\nperformed within O(1) time.\\nTime Complexity – O(1) [Amortized]. Space Complexity - O(n) for Hash Table.\\nProblem-40\\u2003\\u2003Josephus Circle: N people have decided to elect a leader by arranging\\nthemselves in a circle and eliminating every Mth person around the circle, closing ranks as\\neach person drops out. Find which person will be the last one remaining (with rank 1).\\nSolution: Assume the input is a circular linked list with N nodes and each node has a number\\n(range 1 to N) associated with it. The head node has number 1 as data.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 148, 'file_type': 'pdf'}, page_content='Problem-41\\u2003\\u2003Given a linked list consists of data, a next pointer and also a random pointer\\nwhich points to a random node of the list. Give an algorithm for cloning the list.\\nSolution: We can use a hash table to associate newly created nodes with the instances of node in\\nthe given list.\\nAlgorithm:\\n•\\nScan the original list and for each node X, create a new node Y with data of X, then\\nstore the pair (X, Y) in hash table using X as a key. Note that during this scan set Y\\n→ next and Y → random to NULL and we will fix them in the next scan.\\n•\\nNow for each node X in the original list we have a copy Y stored in our hash table.\\nWe scan the original list again and set the pointers building the new list.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 149, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-42\\u2003\\u2003Can we solve Problem-41 without any extra space?\\nSolution: Yes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 150, 'file_type': 'pdf'}, page_content='Time Complexity: O(3n) ≈ O(n). Space Complexity: O(1).\\nProblem-43\\u2003\\u2003We are given a pointer to a node (not the tail node) in a singly linked list. Delete\\nthat node from the linked list.\\nSolution: To delete a node, we have to adjust the next pointer of the previous node to point to the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 151, 'file_type': 'pdf'}, page_content='next node instead of the current one. Since we don’t have a pointer to the previous node, we can’t\\nredirect its next pointer. So what do we do? We can easily get away by moving the data from the\\nnext node into the current node and then deleting the next node.\\nTime Complexity: O(1). Space Complexity: O(1).\\nProblem-44\\u2003\\u2003Given a linked list with even and odd numbers, create an algorithm for making\\nchanges to the list in such a way that all even numbers appear at the beginning.\\nSolution: To solve this problem, we can use the splitting logic. While traversing the list, split the\\nlinked list into two: one contains all even nodes and the other contains all odd nodes. Now, to get\\nthe final list, we can simply append the odd node linked list after the even node linked list.\\nTo split the linked list, traverse the original linked list and move all odd nodes to a separate\\nlinked list of all odd nodes. At the end of the loop, the original list will have all the even nodes\\nand the odd node list will have all the odd nodes. To keep the ordering of all nodes the same, we\\nmust insert all the odd nodes at the end of the odd node list.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-45\\u2003\\u2003In a linked list with n nodes, the time taken to insert an element after an element\\npointed by some pointer is\\n(A)\\nO(1)\\n(B)\\nO(logn)\\n(C)\\nO(n)\\n(D)\\nO(nlogn)\\nSolution: A.\\nProblem-46\\u2003\\u2003Find modular node: Given a singly linked list, write a function to find the last\\nelement from the beginning whose n%k == 0, where n is the number of elements in the list\\nand k is an integer constant. For example, if n = 19 and k = 3 then we should return 18th\\nnode.\\nSolution: For this problem the value of n is not known in advance.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 152, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-47\\u2003\\u2003Find modular node from the end: Given a singly linked list, write a function to\\nfind the first from the end whose n%k == 0, where n is the number of elements in the list\\nand k is an integer constant. If n = 19 and k = 3 then we should return 16th node.\\nSolution: For this problem the value of n is not known in advance and it is the same as finding the\\nkth element from the end of the the linked list.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 153, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-48\\u2003\\u2003Find fractional node: Given a singly linked list, write a function to find the \\n element, where n is the number of elements in the list.\\nSolution: For this problem the value of n is not known in advance.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 154, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-49\\u2003\\u2003Find \\n node: Given a singly linked list, write a function to find the \\nelement, where n is the number of elements in the list. Assume the value of n is not known\\nin advance.\\nSolution: For this problem the value of n is not known in advance.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 155, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-50\\u2003\\u2003Given two lists List 1 = {A1, A2, . . . , An) and List2 = {B1, B2, . . . , Bm} with\\ndata (both lists) in ascending order. Merge them into the third list in ascending order so\\nthat the merged list will be:\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 156, 'file_type': 'pdf'}, page_content='Time Complexity: The while loop takes O(min(n,m)) time as it will run for min(n,m) times. The\\nother steps run in O(1). Therefore the total time complexity is O(min(n,m)). Space Complexity:\\nO(1).\\nProblem-51\\u2003\\u2003Median in an infinite series of integers\\nSolution: Median is the middle number in a sorted list of numbers (if we have an odd number of\\nelements). If we have an even number of elements, the median is the average of two middle\\nnumbers in a sorted list of numbers. We can solve this problem with linked lists (with both sorted\\nand unsorted linked lists).\\nFirst, let us try with an unsorted linked list. In an unsorted linked list, we can insert the element\\neither at the head or at the tail. The disadvantage with this approach is that finding the median\\ntakes O(n). Also, the insertion operation takes O(1).\\nNow, let us try with a sorted linked list. We can find the median in O(1) time if we keep track of'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 157, 'file_type': 'pdf'}, page_content='the middle elements. Insertion to a particular location is also O(1) in any linked list. But, finding\\nthe right location to insert is not O(logn) as in a sorted array, it is instead O(n) because we can’t\\nperform binary search in a linked list even if it is sorted. So, using a sorted linked list isn’t worth\\nthe effort as insertion is O(n) and finding median is O(1), the same as the sorted array. In the\\nsorted array the insertion is linear due to shifting, but here it’s linear because we can’t do a binary\\nsearch in a linked list.\\nNote: For an efficient algorithm refer to the Priority Queues and Heaps chapter.\\nProblem-52\\u2003\\u2003Given a linked list, how do you modify it such that all the even numbers appear\\nbefore all the odd numbers in the modified linked list?\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 158, 'file_type': 'pdf'}, page_content='struct ListNode “exchangeEvenOdLis{structListNode *head),\\n| initializing the odd and even lst headers\\nstruct ListNode *odList = NULL, ‘evenList =NULL;\\n\\n| creating tail variables for both the list\\nstruct ListNode *oddListEnd = NULL, ‘evenListEnd = NULL;\\nstruct ListNode ‘itr=head;\\n\\nillhead == NULL),\\nreturn;\\n}\\nelse\\nwhile itr = NULL\\nilitr-sdata % 2 == 0)\\nil evnlist*= NULL}\\n|/ first even node\\nevenlist = evenlistEnd = itr;\\n}\\nelse,\\n// inserting the noe atthe end of inked list\\nevenbist2nd-—mnext = itr;\\nevenlistBnd = itr;\\n}\\n)\\nelse\\nill oddList == NULL}\\n|/ first odd node\\noniList = oddListnd = itr;\\n}\\nelse\\n// inserting the node atthe end of inked list\\nodlListEnd-snext = it;\\nodListEnd = itr,\\n}\\n)\\nitr = itrnext;\\nevenlistEnd-mext = odds;\\nreturn head;\\n}\\n}'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 159, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-53\\u2003\\u2003Given two linked lists, each list node with one integer digit, add these two\\nlinked lists. The result should be stored in the third linked list. Also note that the head node\\ncontains the most significant digit of the number.\\nSolution: Since the integer addition starts from the least significant digit, we first need to visit the\\nlast node of both lists and add them up, create a new node to store the result, take care of the carry\\nif any, and link the resulting node to the node which will be added to the second least significant\\nnode and continue.\\nFirst of all, we need to take into account the difference in the number of digits in the two numbers.\\nSo before starting recursion, we need to do some calculation and move the longer list pointer to\\nthe appropriate place so that we need the last node of both lists at the same time. The other thing\\nwe need to take care of is carry. If two digits add up to more than 10, we need to forward the\\ncarry to the next node and add it. If the most significant digit addition results in a carry, we need\\nto create an extra node to store the carry.\\nThe function below is actually a wrapper function which does all the housekeeping like\\ncalculating lengths of lists, calling recursive implementation, creating an extra node for the carry\\nin the most significant digit, and adding any remaining nodes left in the longer list.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 160, 'file_type': 'pdf'}, page_content='void addListNumbersWrapper(stract ListNode ‘ist, struct ListNode “st2 int “cary, struct ListNode “result\\n‘int istLength = 0, list2Length = 0, dif “0;\\nstruct ListNode ‘current = list;\\nwhile(current)\\n‘current = current-snext:\\nlist Length\\n\\n‘current = lis;\\n‘while(current)\\n\\nfhellengthess\\n,\\n‘fist Length < list2Length)t\\n‘current = list];\\nfiat =\\n\\nD\\n‘iff = abslist!Length-tist2Length);\\ncurrent « list;\\nwhile\\n\\n‘current = current-snext:\\n‘addListNumbers{current,list2, carry, result)\\n‘iff = abelist Length list2Length),\\n\\ncary, result, dif)\\n\\n,\\n‘oid addLitNumbers(struct ListNode “list, struct ListNode “list2, int ‘carry, struct ListNode “result\\n\\n‘ituisty\\naddLiatNumberafiat—snext,list2—mnext, carry, result)\\n\\n1 /End of both ists, add then\\nStruct ListNode * temp = struct ListNode *tmallocsizeoistruct ListNode )\\n\\nSm = list —data + hnt2-sdata + earyls\\n1/ Store carry\\ncarry = sum/10;\\nsum = suenitO;\\ntemap-—data = wm;\\ntemp_onext = (rent\\n\\n“result = temp:\\nseturn:\\n\\n,\\n‘void addRemainingNumbers(struct ListNode * list, int ‘carry, struct ListNode “result int dif\\nint sum =O;\\n\\n‘fist || att == 0)\\n‘addRemainingNumbers(list—next, carry result, dif\\n‘xruct ListNode * temp = (struct ListNode *}mallociszeofistruct ListNode ):\\n\\nrum = list >data * Peary)\\n\\nary = sum/ 10;\\nsum ~ sum10;'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 161, 'file_type': 'pdf'}, page_content='Time Complexity: O(max(List1 length,List2 length)).\\nSpace Complexity: O(min(List1 length, List1 length)) for recursive stack.\\nNote: It can also be solved using stacks.\\nProblem-54\\u2003\\u2003Which sorting algorithm is easily adaptable to singly linked lists?\\nSolution: Simple Insertion sort is easily adabtable to singly linked lists. To insert an element, the\\nlinked list is traversed until the proper position is found, or until the end of the list is reached. It\\nis inserted into the list by merely adjusting the pointers without shifting any elements, unlike in the\\narray. This reduces the time required for insertion but not the time required for searching for the\\nproper position.\\nProblem-55\\u2003\\u2003Given a list, List1 = {A1, A2, . . . An–1; An) with data, reorder it to {A1,\\nAn,A2,An–1} without using any extra space.\\nSolution: Find the middle of the linked list. We can do it by slow and fast pointer approach. After\\nfinding the middle node, we reverse the right halfl then we do a in place merge of the two halves\\nof the linked list.\\nProblem-56\\u2003\\u2003Given two sorted linked lists, given an algorithm for the printing common\\nelements of them.\\nSolution: The solution is based on merge sort logic. Assume the given two linked lists are: list1\\nand list2. Since the elements are in sorted order, we run a loop till we reach the end of either of\\nthe list. We compare the values of list1 and list2. If the values are equal, we add it to the common\\nlist. We move list1/list2/both nodes ahead to the next pointer if the values pointed by list1 was\\nless / more / equal to the value pointed by list2.\\nTime complexity O(m + n), where m is the lengh of list1 and n is the length of list2. Space\\nComplexity: O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 162, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 163, 'file_type': 'pdf'}, page_content='4.1 What is a Stack?\\nA stack is a simple data structure used for storing data (similar to Linked Lists). In a stack, the\\norder in which the data arrives is important. A pile of plates in a cafeteria is a good example of a\\nstack. The plates are added to the stack as they are cleaned and they are placed on the top. When a\\nplate, is required it is taken from the top of the stack. The first plate placed on the stack is the last\\none to be used.\\nDefinition: A stack is an ordered list in which insertion and deletion are done at one end, called\\ntop. The last element inserted is the first one to be deleted. Hence, it is called the Last in First out\\n(LIFO) or First in Last out (FILO) list.\\nSpecial names are given to the two changes that can be made to a stack. When an element is\\ninserted in a stack, the concept is called push, and when an element is removed from the stack, the\\nconcept is called pop. Trying to pop out an empty stack is called underflow and trying to push an\\nelement in a full stack is called overflow. Generally, we treat them as exceptions. As an example,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 164, 'file_type': 'pdf'}, page_content='consider the snapshots of the stack.\\n4.2 How Stacks are used\\nConsider a working day in the office. Let us assume a developer is working on a long-term\\nproject. The manager then gives the developer a new task which is more important. The\\ndeveloper puts the long-term project aside and begins work on the new task. The phone rings, and\\nthis is the highest priority as it must be answered immediately. The developer pushes the present\\ntask into the pending tray and answers the phone.\\nWhen the call is complete the task that was abandoned to answer the phone is retrieved from the\\npending tray and work progresses. To take another call, it may have to be handled in the same\\nmanner, but eventually the new task will be finished, and the developer can draw the long-term\\nproject from the pending tray and continue with that.\\n4.3 Stack ADT\\nThe following operations make a stack an ADT. For simplicity, assume the data is an integer type.\\nMain stack operations\\n•\\nPush (int data): Inserts data onto stack.\\n•\\nint Pop(): Removes and returns the last inserted element from the stack.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 165, 'file_type': 'pdf'}, page_content='Auxiliary stack operations\\n•\\nint Top(): Returns the last inserted element without removing it.\\n•\\nint Size(): Returns the number of elements stored in the stack.\\n•\\nint IsEmptyStack(): Indicates whether any elements are stored in the stack or not.\\n•\\nint IsFullStack(): Indicates whether the stack is full or not.\\nExceptions\\nAttempting the execution of an operation may sometimes cause an error condition, called an\\nexception. Exceptions are said to be “thrown” by an operation that cannot be executed. In the\\nStack ADT, operations pop and top cannot be performed if the stack is empty. Attempting the\\nexecution of pop (top) on an empty stack throws an exception. Trying to push an element in a full\\nstack throws an exception.\\n4.4 Applications\\nFollowing are some of the applications in which stacks play an important role.\\nDirect applications\\n•\\nBalancing of symbols\\n•\\nInfix-to-postfix conversion\\n•\\nEvaluation of postfix expression\\n•\\nImplementing function calls (including recursion)\\n•\\nFinding of spans (finding spans in stock markets, refer to Problems section)\\n•\\nPage-visited history in a Web browser [Back Buttons]\\n•\\nUndo sequence in a text editor\\n•\\nMatching Tags in HTML and XML\\nIndirect applications\\n•\\nAuxiliary data structure for other algorithms (Example: Tree traversal algorithms)\\n•\\nComponent of other data structures (Example: Simulating queues, refer Queues\\nchapter)\\n4.5 Implementation\\nThere are many ways of implementing stack ADT; below are the commonly used methods.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 166, 'file_type': 'pdf'}, page_content='•\\nSimple array based implementation\\n•\\nDynamic array based implementation\\n•\\nLinked lists implementation\\nSimple Array Implementation\\nThis implementation of stack ADT uses an array. In the array, we add elements from left to right\\nand use a variable to keep track of the index of the top element.\\nThe array storing the stack elements may become full. A push operation will then throw a full\\nstack exception. Similarly, if we try deleting an element from an empty stack it will throw stack\\nempty exception.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 167, 'file_type': 'pdf'}, page_content='define MAXSIZE 10\\nstruct ArrayStack {\\nint top:\\nint capacity;\\nint ‘array;\\nE\\nstruct ArrayStack *CreateStaci |\\nstruct ArrayStack *S = mallocsizeoistruct ArrayStack}\\nsifts)\\nreturn NULL;\\n‘S-apacity = MAXSIZE;\\nSoop *\\n‘S-array* mallo(S-apacity * sizofint);\\nif) Saray)\\nreturn NULL;\\nretum S;\\n\\n}\\n\\nint IsEmptyStack(struct ArrayStack *S){\\nreturn (Stop ==-1}; — // ithe condition is true then 1 is returned else 0 is returned\\n\\n}\\nint IsFullStack(struct ArrayStack *S){\\n[ifthe condition is true then 1 is retumed else 0 is returned\\nreturn (Stop == S-rcapacity - 1);\\n}\\nvoid Push(struct ArrayStack *S, int data\\n[* Stop == capacity-1 indicates thatthe stack is fll/\\niflsFullStack(S)\\nprint “Stack Overiow”};\\nse /*increasing the top’ by 1 and storing the value at top’ position*/\\n$+ arrayi++S-top}= dat;\\n}\\nint Pop(structArrayStack *S)\\n/* Stop =~ | indicates empty stack\\niflsEmptyStack(S)\\nprint Stack is Empty’);\\nseturn INT_MIN;;\\n)\\nelse /* Removing element from ‘op of the array and reducing ‘op’ by 1*/\\nretum (S- array[S-top-;\\n)\\nvoid DeleteStack{struct DynArrayStack *S}\\nsf) {\\nif(Sarray)\\nfree(S—array);\\nfree(S);\\n\\n}'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 168, 'file_type': 'pdf'}, page_content='Performance & Limitations\\nPerformance\\nLet n be the number of elements in the stack. The complexities of stack operations with this\\nrepresentation can be given as:\\nSpace Complexity (for n push operations)\\nO(n)\\nTime Complexity of Push()\\nO(1)\\nTime Complexity of Pop()\\nO(1)\\nTime Complexity of Size()\\nO(1)\\nTime Complexity of IsEmptyStack()\\nO(1)\\nTime Complexity of IsFullStackf)\\nO(1)\\nTime Complexity of DeleteStackQ\\nO(1)\\nLimitations\\nThe maximum size of the stack must first be defined and it cannot be changed. Trying to push a\\nnew element into a full stack causes an implementation-specific exception.\\nDynamic Array Implementation\\nFirst, let’s consider how we implemented a simple array based stack. We took one index variable\\ntop which points to the index of the most recently inserted element in the stack. To insert (or push)\\nan element, we increment top index and then place the new element at that index.\\nSimilarly, to delete (or pop) an element we take the element at top index and then decrement the\\ntop index. We represent an empty queue with top value equal to –1. The issue that still needs to\\nbe resolved is what we do when all the slots in the fixed size array stack are occupied?\\nFirst try: What if we increment the size of the array by 1 every time the stack is full?\\n•\\nPush(); increase size of S[] by 1\\n•\\nPop(): decrease size of S[] by 1\\nProblems with this approach?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 169, 'file_type': 'pdf'}, page_content='This way of incrementing the array size is too expensive. Let us see the reason for this. For\\nexample, at n = 1, to push an element create a new array of size 2 and copy all the old array\\nelements to the new array, and at the end add the new element. At n = 2, to push an element create\\na new array of size 3 and copy all the old array elements to the new array, and at the end add the\\nnew element.\\nSimilarly, at n = n – 1, if we want to push an element create a new array of size n and copy all the\\nold array elements to the new array and at the end add the new element. After n push operations\\nthe total time T(n) (number of copy operations) is proportional to 1 + 2 + ... + n ≈ O(n2).\\nAlternative Approach: Repeated Doubling\\nLet us improve the complexity by using the array doubling technique. If the array is full, create a\\nnew array of twice the size, and copy the items. With this approach, pushing n items takes time\\nproportional to n (not n2).\\nFor simplicity, let us assume that initially we started with n = 1 and moved up to n = 32. That\\nmeans, we do the doubling at 1,2,4,8,16. The other way of analyzing the same approach is: at n =\\n1, if we want to add (push) an element, double the current size of the array and copy all the\\nelements of the old array to the new array.\\nAt n = 1, we do 1 copy operation, at n = 2, we do 2 copy operations, and at n = 4, we do 4 copy\\noperations and so on. By the time we reach n = 32, the total number of copy operations is 1+2 + 4\\n+ 8+16 = 31 which is approximately equal to 2n value (32). If we observe carefully, we are\\ndoing the doubling operation logn times. Now, let us generalize the discussion. For n push\\noperations we double the array size logn times. That means, we will have logn terms in the\\nexpression below. The total time T(n) of a series of n push operations is proportional to\\nT(n) is O(n) and the amortized time of a push operation is O(1) .'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 170, 'file_type': 'pdf'}, page_content=\"struct DymArrayStack {\\nint top;\\nint capacity;\\nint array;\\nih\\nstruct DynArrayStack ‘CreateStack)t\\nstruct DynArrayStack*S = mallo(sizeofstruct DynArrayStack);\\nifts)\\nreturn NULL;\\nSweapacity\\nStop\\nScary\\nAft array)\\nreturn NULL;\\nreturn S;\\n,\\nint IFullStack(struct DynArrayStack ‘),\\nreturn (Stop == S-capacity- 1;\\n‘oid DoubleStack(struct DynArrayStack\\nScapacty = 2;\\nScarray + realoeSaray, Scapacty * sizeof);\\n)\\n‘oid Push(struct DynArrayStack ', nt x\\n1 [No oer in this implementation\\niflsFullStack'S)\\nDoubleStak(S);\\nS~array|++8-—top] =x;\\n)\\nint IsEmptyStack(struct DynArayStack 8)\\nreturn Stop ==;\\n)\\nint Top(struct DymArrayStack ‘5\\niffsEmptyStack(S))\\nreturn INT MIN;\\nreturn Sarray{S-top;\\n)\\n\\n{nt Pop(struct DynArrayStack *S),\\n\\nlo(Scapacity*sizeofint); // allocate an array of size 1 initaly\"), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 171, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the stack. The complexities for operations with this\\nrepresentation can be given as:\\nSpace Complexity (for n push operations)\\nO(n)\\nTime Complexity of CreateStack()\\nO(1)\\nTime Complexity of PushQ\\nO(1) (Average)\\nTime Complexity of PopQ\\nO(1)\\nTime Complexity of Top()\\nO(1)\\nTime Complexity of IsEmpryStackf)\\nO(1))\\nTime Complexity of IsFullStackf)\\nO(1)\\nTime Complexity of DeleteStackQ\\nO(1)\\nNote: Too many doublings may cause memory overflow exception.\\nLinked List Implementation\\nThe other way of implementing stacks is by using Linked lists. Push operation is implemented by\\ninserting element at the beginning of the list. Pop operation is implemented by deleting the node\\nfrom the beginning (the header/top node).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 172, 'file_type': 'pdf'}, page_content='struct LstNode|\\nint data;\\nstruct ListNode ‘next;\\n\\n%\\n\\nstruct Stack *CreateStakf},\\nretum NULL;\\n\\n|\\n\\n‘voi Push(struct Stack top, int data\\nstruct Stack “temp;\\ntemp = mallcsizefitruct Stack)\\nitem\\n\\nreturn NULL\\n\\ntemp-data =\\ntemp-nest*\\n“top = temp;\\n\\n|\\n\\nint IsEmpty Stack(struct Stack “op\\nretum top == NULL;\\n\\n}\\n\\nint Pop(struct Stack “top,\\nint data\\nstruct Stack “emp;\\niflsEmptyStack(op)\\n\\n}\\nint Top(struct Stack * top),\\nilsEmpryStac(op))\\n\\nretumn INT MIN;\\nreturn top-next-data;\\n}\\nvoid DeleteStack(struct Stack “*top){\\nstruct Stack ‘temp, \"p;\\np= Mop\\nhile p-next\\ntemp = pert;\\npenext = temp~net;\\nfreetemp);\\n)\\nfret\\n}'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 173, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the stack. The complexities for operations with this\\nrepresentation can be given as:\\nSpace Complexity (for n push operations)\\nO(n)\\nTime Complexity of CreateStack()\\nO(1)\\nTime Complexity of Push()\\nO(1) (Average)\\nTime Complexity of Pop()\\nO(1)\\nTime Complexity of Top()\\nO(1)\\nTime Complexity of IsEmptyStack()\\nO(1)\\nTime Complexity of DeleteStack()\\nO(n)\\n4.6 Comparison of Implementations\\nComparing Incremental Strategy and Doubling Strategy\\nWe compare the incremental strategy and doubling strategy by analyzing the total time T(n)\\nneeded to perform a series of n push operations. We start with an empty stack represented by an\\narray of size 1.\\nWe call amortized time of a push operation is the average time taken by a push over the series of\\noperations, that is, T(n)/n.\\nIncremental Strategy\\nThe amortized time (average time per operation) of a push operation is O(n) [O(n2)/n].\\nDoubling Strategy\\nIn this method, the amortized time of a push operation is O(1) [O(n)/n].\\nNote: For analysis, refer to the Implementation section.\\nComparing Array Implementation and Linked List Implementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 174, 'file_type': 'pdf'}, page_content='Array Implementation\\n•\\nOperations take constant time.\\n•\\nExpensive doubling operation every once in a while.\\n•\\nAny sequence of n operations (starting from empty stack) – “amortized” bound takes\\ntime proportional to n.\\nLinked List Implementation\\n•\\nGrows and shrinks gracefully.\\n•\\nEvery operation takes constant time O(1).\\n•\\nEvery operation uses extra space and time to deal with references.\\n4.7 Stacks: Problems & Solutions\\nProblem-1\\u2003\\u2003Discuss how stacks can be used for checking balancing of symbols.\\nSolution: Stacks can be used to check whether the given expression has balanced symbols. This\\nalgorithm is very useful in compilers. Each time the parser reads one character at a time. If the\\ncharacter is an opening delimiter such as (, {, or [- then it is written to the stack. When a closing\\ndelimiter is encountered like ), }, or ]-the stack is popped.\\nThe opening and closing delimiters are then compared. If they match, the parsing of the string\\ncontinues. If they do not match, the parser indicates that there is an error on the line. A linear-time\\nO(n) algorithm based on stack can be given as:\\nAlgorithm:\\na)\\nCreate a stack.\\nb)\\nwhile (end of input is not reached) {\\n1)\\nIf the character read is not a symbol to be balanced, ignore it.\\n2)\\nIf the character is an opening symbol like (, [, {, push it onto the stack\\n3)\\nIf it is a closing symbol like ),],}, then if the stack is empty report an\\nerror. Otherwise pop the stack.\\n4)\\nIf the symbol popped is not the corresponding opening symbol, report an\\nerror.\\n}\\nc)\\nAt end of input, if the stack is not empty report an error\\nExamples:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 175, 'file_type': 'pdf'}, page_content='For tracing the algorithm let us assume that the input is: () (() [()])'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 176, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Since we are scanning the input only once. Space Complexity: O(n) [for\\nstack].\\nProblem-2\\u2003\\u2003Discuss infix to postfix conversion algorithm using stack.\\nSolution: Before discussing the algorithm, first let us see the definitions of infix, prefix and\\npostfix expressions.\\nInfix: An infix expression is a single letter, or an operator, proceeded by one infix string and\\nfollowed by another Infix string.\\nPrefix: A prefix expression is a single letter, or an operator, followed by two prefix strings.\\nEvery prefix string longer than a single variable contains an operator, first operand and second\\noperand.\\nPostfix: A postfix expression (also called Reverse Polish Notation) is a single letter or an\\noperator, preceded by two postfix strings. Every postfix string longer than a single variable\\ncontains first and second operands followed by an operator.\\nPrefix and postfix notions are methods of writing mathematical expressions without parenthesis.\\nTime to evaluate a postfix and prefix expression is O(n), where n is the number of elements in the\\narray.\\nNow, let us focus on the algorithm. In infix expressions, the operator precedence is implicit'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 177, 'file_type': 'pdf'}, page_content='unless we use parentheses. Therefore, for the infix to postfix conversion algorithm we have to\\ndefine the operator precedence (or priority) inside the algorithm.\\nThe table shows the precedence and their associativity (order of evaluation) among operators.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 178, 'file_type': 'pdf'}, page_content='Important Properties\\n•\\nLet us consider the infix expression 2 + 3*4 and its postfix equivalent 234*+. Notice\\nthat between infix and postfix the order of the numbers (or operands) is unchanged.\\nIt is 2 3 4 in both cases. But the order of the operators * and + is affected in the two\\nexpressions.\\n•\\nOnly one stack is enough to convert an infix expression to postfix expression. The\\nstack that we use in the algorithm will be used to change the order of operators from\\ninfix to postfix. The stack we use will only contain operators and the open\\nparentheses symbol ‘(‘.\\nPostfix expressions do not contain parentheses. We shall not output the parentheses in the postfix\\noutput.\\nAlgorithm:\\na)\\nCreate a stack\\nb)\\nfor each character t in the input stream}\\nc)\\npop and output tokens until the stack is empty\\nFor better understanding let us trace out an example: A * B- (C + D) + E'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 179, 'file_type': 'pdf'}, page_content='Problem-3\\u2003\\u2003Discuss postfix evaluation using stacks?\\nSolution:\\nAlgorithm:\\n1\\nScan the Postfix string from left to right.\\n2\\nInitialize an empty stack.\\n3\\nRepeat steps 4 and 5 till all the characters are scanned.\\n4\\nIf the scanned character is an operand, push it onto the stack.\\n5\\nIf the scanned character is an operator, and if the operator is a unary operator, then\\npop an element from the stack. If the operator is a binary operator, then pop two\\nelements from the stack. After popping the elements, apply the operator to those\\npopped elements. Let the result of this operation be retVal onto the stack.\\n6\\nAfter all characters are scanned, we will have only one element in the stack.\\n7\\nReturn top of the stack as result.\\nExample: Let us see how the above-mentioned algorithm works using an example. Assume that\\nthe postfix string is 123*+5-.\\nInitially the stack is empty. Now, the first three characters scanned are 1, 2 and 3, which are\\noperands. They will be pushed into the stack in that order.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 180, 'file_type': 'pdf'}, page_content='The next character scanned is “*”, which is an operator. Thus, we pop the top two elements from\\nthe stack and perform the “*” operation with the two operands. The second operand will be the\\nfirst element that is popped.\\nThe value of the expression (2*3) that has been evaluated (6) is pushed into the stack.\\nThe next character scanned is “+”, which is an operator. Thus, we pop the top two elements from'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 181, 'file_type': 'pdf'}, page_content='the stack and perform the “+” operation with the two operands. The second operand will be the\\nfirst element that is popped.\\nThe value of the expression (1+6) that has been evaluated (7) is pushed into the stack.\\nThe next character scanned is “5”, which is added to the stack.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 182, 'file_type': 'pdf'}, page_content='The next character scanned is “-”, which is an operator. Thus, we pop the top two elements from\\nthe stack and perform the “-” operation with the two operands. The second operand will be the\\nfirst element that is popped.\\nThe value of the expression(7-5) that has been evaluated(23) is pushed into the stack.\\nNow, since all the characters are scanned, the remaining element in the stack (there will be only\\none element in the stack) will be returned. End result:\\n•\\nPostfix String : 123*+5-\\n•\\nResult : 2\\nProblem-4\\u2003\\u2003Can we evaluate the infix expression with stacks in one pass?\\nSolution: Using 2 stacks we can evaluate an infix expression in 1 pass without converting to\\npostfix.\\nAlgorithm:\\n1)\\nCreate an empty operator stack\\n2)\\nCreate an empty operand stack'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 183, 'file_type': 'pdf'}, page_content='3)\\nFor each token in the input string\\na.\\nGet the next token in the infix string\\nb.\\nIf next token is an operand, place it on the operand stack\\nc.\\nIf next token is an operator\\n  i. Evaluate the operator (next op)\\n4)\\nWhile operator stack is not empty, pop operator and operands (left and right),\\nevaluate left operator right and push result onto operand stack\\n5)\\nPop result from operator stack\\nProblem-5\\u2003\\u2003How to design a stack such that GetMinimum( ) should be O(1)?\\nSolution: Take an auxiliary stack that maintains the minimum of all values in the stack. Also,\\nassume that each element of the stack is less than its below elements. For simplicity let us call the\\nauxiliary stack min stack.\\nWhen we pop the main stack, pop the min stack too. When we push the main stack, push either the\\nnew element or the current minimum, whichever is lower. At any point, if we want to get the\\nminimum, then we just need to return the top element from the min stack. Let us take an example\\nand trace it out. Initially let us assume that we have pushed 2, 6, 4, 1 and 5. Based on the above-\\nmentioned algorithm the min stack will look like:\\nMain stack\\nMin stack\\n5 → top\\n1 → top\\n1\\n1\\n4\\n2\\n6\\n2\\n2\\n2\\nAfter popping twice we get:\\nMain stack\\nMin stack\\n4 -→ top\\n2 → top\\n6\\n2\\n2\\n2\\nBased on the discussion above, now let us code the push, pop and GetMinimum() operations.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 184, 'file_type': 'pdf'}, page_content='Time complexity: O(1). Space complexity: O(n) [for Min stack]. This algorithm has much better\\nspace usage if we rarely get a “new minimum or equal”.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 185, 'file_type': 'pdf'}, page_content='Problem-6\\u2003\\u2003For Problem-5 is it possible to improve the space complexity?\\nSolution: Yes. The main problem of the previous approach is, for each push operation we are\\npushing the element on to min stack also (either the new element or existing minimum element).\\nThat means, we are pushing the duplicate minimum elements on to the stack.\\nNow, let us change the algorithm to improve the space complexity. We still have the min stack, but\\nwe only pop from it when the value we pop from the main stack is equal to the one on the min\\nstack. We only push to the min stack when the value being pushed onto the main stack is less than\\nor equal to the current min value. In this modified algorithm also, if we want to get the minimum\\nthen we just need to return the top element from the min stack. For example, taking the original\\nversion and pushing 1 again, we’d get:\\nMain stack\\nMin stack\\n1 → top\\n \\n5\\n1\\n4\\n1 → top\\n6\\n1\\n2\\n2\\nPopping from the above pops from both stacks because 1 == 1, leaving:\\nMain stack\\nMin stack\\n5 → top\\n \\n1\\n4\\n6\\n1 → top\\n2\\n2\\nPopping again only pops from the main stack, because 5 > 1:\\nMain stack\\nMin stack\\n1 → top\\n \\n4'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 186, 'file_type': 'pdf'}, page_content='6\\n1 → top\\n2\\n2\\nPopping again pops both stacks because 1 == 1:\\nMain stack\\nMin stack\\n4 → top\\n \\n6\\n \\n2\\n2 → top\\nNote: The difference is only in push & pop operations.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 187, 'file_type': 'pdf'}, page_content='Time complexity: O(1). Space complexity: O(n) [for Min stack]. But this algorithm has much\\nbetter space usage if we rarely get a “new minimum or equal”.\\nProblem-7\\u2003\\u2003For a given array with n symbols how many stack permutations are possible?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 188, 'file_type': 'pdf'}, page_content='Solution: The number of stack permutations with n symbols is represented by Catalan number and\\nwe will discuss this in the Dynamic Programming chapter.\\nProblem-8\\u2003\\u2003Given an array of characters formed with a’s and b’s. The string is marked with\\nspecial character X which represents the middle of the list (for example:\\nababa...ababXbabab baaa). Check whether the string is palindrome.\\nSolution: This is one of the simplest algorithms. What we do is, start two indexes, one at the\\nbeginning of the string and the other at the end of the string. Each time compare whether the values\\nat both the indexes are the same or not. If the values are not the same then we say that the given\\nstring is not a palindrome.\\nIf the values are the same then increment the left index and decrement the right index. Continue\\nthis process until both the indexes meet at the middle (at X) or if the string is not palindrome.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-9\\u2003\\u2003For Problem-8, if the input is in singly linked list then how do we check whether\\nthe list elements form a palindrome (That means, moving backward is not possible).\\nSolution: Refer Linked Lists chapter.\\nProblem-10\\u2003\\u2003Can we solve Problem-8 using stacks?\\nSolution: Yes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 189, 'file_type': 'pdf'}, page_content='Algorithm:\\n•\\nTraverse the list till we encounter X as input element.\\n•\\nDuring the traversal push all the elements (until X) on to the stack.\\n•\\nFor the second half of the list, compare each element’s content with top of the stack.\\nIf they are the same then pop the stack and go to the next element in the input list.\\n•\\nIf they are not the same then the given string is not a palindrome.\\n•\\nContinue this process until the stack is empty or the string is not a palindrome.\\nTime Complexity: O(n). Space Complexity: O(n/2) ≈ O(n).\\nProblem-11\\u2003\\u2003Given a stack, how to reverse the elements of the stack using only stack\\noperations (push & pop)?\\nSolution:\\nAlgorithm:\\n•\\nFirst pop all the elements of the stack till it becomes empty.\\n•\\nFor each upward step in recursion, insert the element at the bottom of the stack.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 190, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). Space Complexity: O(n), for recursive stack.\\nProblem-12\\u2003\\u2003Show how to implement one queue efficiently using two stacks. Analyze the\\nrunning time of the queue operations.\\nSolution: Refer Queues chapter.\\nProblem-13\\u2003\\u2003Show how to implement one stack efficiently using two queues. Analyze the\\nrunning time of the stack operations.\\nSolution: Refer Queues chapter.\\nProblem-14\\u2003\\u2003How do we implement two stacks using only one array? Our stack routines\\nshould not indicate an exception unless every slot in the array is used?\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 191, 'file_type': 'pdf'}, page_content='Algorithm:\\n•\\nStart two indexes one at the left end and the other at the right end.\\n•\\nThe left index simulates the first stack and the right index simulates the second stack.\\n•\\nIf we want to push an element into the first stack then put the element at the left\\nindex.\\n•\\nSimilarly, if we want to push an element into the second stack then put the element at\\nthe right index.\\n•\\nThe first stack grows towards the right, and the second stack grows towards the left.\\nTime Complexity of push and pop for both stacks is O(1). Space Complexity is O(1).\\nProblem-15\\u2003\\u20033 stacks in one array: How to implement 3 stacks in one array?\\nSolution: For this problem, there could be other ways of solving it. Given below is one\\npossibility and it works as long as there is an empty space in the array.\\nTo implement 3 stacks we keep the following information.\\n•\\nThe index of the first stack (Topi): this indicates the size of the first stack.\\n•\\nThe index of the second stack (Top2): this indicates the size of the second stack.\\n•\\nStarting index of the third stack (base address of third stack).\\n•\\nTop index of the third stack.\\nNow, let us define the push and pop operations for this implementation.\\nPushing:\\n•\\nFor pushing on to the first stack, we need to see if adding a new element causes it to\\nbump into the third stack. If so, try to shift the third stack upwards. Insert the new'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 192, 'file_type': 'pdf'}, page_content='element at (start1 + Top1).\\n•\\nFor pushing to the second stack, we need to see if adding a new element causes it to\\nbump into the third stack. If so, try to shift the third stack downward. Insert the new\\nelement at (start2 - Top2).\\n•\\nWhen pushing to the third stack, see if it bumps into the second stack. If so, try to\\nshift the third stack downward and try pushing again. Insert the new element at\\n(start3 + Top3).\\nTime Complexity: O(n). Since we may need to adjust the third stack. Space Complexity: O(1).\\nPopping: For popping, we don’t need to shift, just decrement the size of the appropriate stack.\\nTime Complexity: O(1). Space Complexity: O(1).\\nProblem-16\\u2003\\u2003For Problem-15, is there any other way implementing the middle stack?\\nSolution: Yes. When either the left stack (which grows to the right) or the right stack (which\\ngrows to the left) bumps into the middle stack, we need to shift the entire middle stack to make\\nroom. The same happens if a push on the middle stack causes it to bump into the right stack.\\nTo solve the above-mentioned problem (number of shifts) what we can do is: alternating pushes\\ncan be added at alternating sides of the middle list (For example, even elements are pushed to the\\nleft, odd elements are pushed to the right). This would keep the middle stack balanced in the\\ncenter of the array but it would still need to be shifted when it bumps into the left or right stack,\\nwhether by growing on its own or by the growth of a neighboring stack. We can optimize the\\ninitial locations of the three stacks if they grow/shrink at different rates and if they have different\\naverage sizes. For example, suppose one stack doesn’t change much. If we put it at the left, then\\nthe middle stack will eventually get pushed against it and leave a gap between the middle and\\nright stacks, which grow toward each other. If they collide, then it’s likely we’ve run out of space\\nin the array. There is no change in the time complexity but the average number of shifts will get\\nreduced.\\nProblem-17\\u2003\\u2003Multiple (m) stacks in one array: Similar to Problem-15, what if we want to\\nimplement m stacks in one array?\\nSolution: Let us assume that array indexes are from 1 to n. Similar to the discussion in Problem-\\n15, to implement m stacks in one array, we divide the array into m parts (as shown below). The\\nsize of each part is \\n.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 193, 'file_type': 'pdf'}, page_content='From the above representation we can see that, first stack is starting at index 1 (starting index is\\nstored in Base[l]), second stack is starting at index \\n (starting index is stored in Base[2]), third\\nstack is starting at index \\n (starting index is stored in Base[3]), and so on. Similar to Base array,\\nlet us assume that Top array stores the top indexes for each of the stack. Consider the following\\nterminology for the discussion.\\n•\\nTop[i], for 1 ≤ i ≤ m will point to the topmost element of the stack i.\\n•\\nIf Base[i] == Top[i], then we can say the stack i is empty.\\n•\\nIf Top[i] == Base[i+1], then we can say the stack i is full.\\nInitially Base[i] = Top[i] = \\n (i – 1), for 1 ≤ i ≤ m.\\n•\\nThe ith stack grows from Base[i]+1 to Base[i+1].\\nPushing on to ith stack:\\n1)\\nFor pushing on to the ith stack, we check whether the top of ith stack is pointing to\\nBase[i+1] (this case defines that ith stack is full). That means, we need to see if\\nadding a new element causes it to bump into the i + 1th stack. If so, try to shift the\\nstacks from i + 1th stack to mth stack toward the right. Insert the new element at\\n(Base[i] + Top[i]).\\n2)\\nIf right shifting is not possible then try shifting the stacks from 1 to i –1th stack toward\\nthe left.\\n3)\\nIf both of them are not possible then we can say that all stacks are full.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 194, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Since we may need to adjust the stacks. Space Complexity: O(1).\\nPopping from ith stack: For popping, we don’t need to shift, just decrement the size of the\\nappropriate stack. The only case to check is stack empty case.\\nTime Complexity: O(1). Space Complexity: O(1).\\nProblem-18\\u2003\\u2003Consider an empty stack of integers. Let the numbers 1,2,3,4,5,6 be pushed on to\\nthis stack in the order they appear from left to right. Let 5 indicate a push and X indicate a\\npop operation. Can they be permuted in to the order 325641(output) and order 154623?\\nSolution: SSSXXSSXSXXX outputs 325641. 154623 cannot be output as 2 is pushed much\\nbefore 3 so can appear only after 3 is output.\\nProblem-19\\u2003\\u2003Earlier in this chapter, we discussed that for dynamic array implementation of\\nstacks, the ‘repeated doubling’ approach is used. For the same problem, what is the\\ncomplexity if we create a new array whose size is n + if instead of doubling?\\nSolution: Let us assume that the initial stack size is 0. For simplicity let us assume that K = 10.\\nFor inserting the element we create a new array whose size is 0 + 10 = 10. Similarly, after 10\\nelements we again create a new array whose size is 10 + 10 = 20 and this process continues at\\nvalues: 30,40 ... That means, for a given n value, we are creating the new arrays at: \\n The total number of copy operations is:\\nIf we are performing n push operations, the cost per operation is O(logn).\\nProblem-20\\u2003\\u2003Given a string containing n S’s and n X’s where 5 indicates a push operation and'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 195, 'file_type': 'pdf'}, page_content='X indicates a pop operation, and with the stack initially empty, formulate a rule to check\\nwhether a given string 5 of operations is admissible or not?\\nSolution: Given a string of length 2n, we wish to check whether the given string of operations is\\npermissible or not with respect to its functioning on a stack. The only restricted operation is pop\\nwhose prior requirement is that the stack should not be empty. So while traversing the string from\\nleft to right, prior to any pop the stack shouldn’t be empty, which means the number of S’s is\\nalways greater than or equal to that of X’s. Hence the condition is at any stage of processing of the\\nstring, the number of push operations (S) should be greater than the number of pop operations (X).\\nProblem-21\\u2003\\u2003Suppose there are two singly linked lists which intersect at some point and\\nbecome a single linked list. The head or start pointers of both the lists are known, but the\\nintersecting node is not known. Also, the number of nodes in each of the lists before they\\nintersect are unknown and both lists may have a different number. List1 may have n nodes\\nbefore it reaches the intersection point and List2 may have m nodes before it reaches the\\nintersection point where m and n may be m = n,m < n or m > n. Can we find the merging\\npoint using stacks?\\nSolution: Yes. For algorithm refer to Linked Lists chapter.\\nProblem-22\\u2003\\u2003Finding Spans: Given an array A, the span S[i] of A[i] is the maximum number\\nof consecutive elements A[j] immediately preceding A[i] and such that A[j] < A[i]?\\nOther way of asking: Given an array A of integers, find the maximum of j – i subjected to\\nthe constraint of A[i] < A[j].\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 196, 'file_type': 'pdf'}, page_content='This is a very common problem in stock markets to find the peaks. Spans are used in financial\\nanalysis (E.g., stock at 52-week high). The span of a stock price on a certain day, i, is the\\nmaximum number of consecutive days (up to the current day) the price of the stock has been less\\nthan or equal to its price on i.\\nAs an example, let us consider the table and the corresponding spans diagram. In the figure the\\narrows indicate the length of the spans. Now, let us concentrate on the algorithm for finding the\\nspans. One simple way is, each day, check how many contiguous days have a stock price that is'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 197, 'file_type': 'pdf'}, page_content='less than the current price.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-23\\u2003\\u2003Can we improve the complexity of Problem-22?\\nSolution: From the example above, we can see that span S[i] on day i can be easily calculated if\\nwe know the closest day preceding i, such that the price is greater on that day than the price on\\nday i. Let us call such a day as P. If such a day exists then the span is now defined as S[i] = i – P.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 198, 'file_type': 'pdf'}, page_content='Time Complexity: Each index of the array is pushed into the stack exactly once and also popped\\nfrom the stack at most once. The statements in the while loop are executed at most n times. Even\\nthough the algorithm has nested loops, the complexity is O(n) as the inner loop is executing only n\\ntimes during the course of the algorithm (trace out an example and see how many times the inner\\nloop becomes successful). Space Complexity: O(n) [for stack].\\nProblem-24\\u2003\\u2003Largest rectangle under histogram: A histogram is a polygon composed of a\\nsequence of rectangles aligned at a common base line. For simplicity, assume that the\\nrectangles have equal widths but may have different heights. For example, the figure on the\\nleft shows a histogram that consists of rectangles with the heights 3,2,5,6,1,4,4, measured\\nin units where 1 is the width of the rectangles. Here our problem is: given an array with\\nheights of rectangles (assuming width is 1), we need to find the largest rectangle possible.\\nFor the given example, the largest rectangle is the shared part.\\nSolution: A straightforward answer is to go to each bar in the histogram and find the maximum\\npossible area in the histogram for it. Finally, find the maximum of these values. This will require\\nO(n2).\\nProblem-25\\u2003\\u2003For Problem-24, can we improve the time complexity?\\nSolution: Linear search using a stack of incomplete sub problems: There are many ways of\\nsolving this problem. Judge has given a nice algorithm for this problem which is based on stack.\\nProcess the elements in left-to-right order and maintain a stack of information about started but yet\\nunfinished sub histograms.\\nIf the stack is empty, open a new sub problem by pushing the element onto the stack. Otherwise\\ncompare it to the element on top of the stack. If the new one is greater we again push it. If the new\\none is equal we skip it. In all these cases, we continue with the next new element. If the new one\\nis less, we finish the topmost sub problem by updating the maximum area with respect to the\\nelement at the top of the stack. Then, we discard the element at the top, and repeat the procedure\\nkeeping the current new element.\\nThis way, all sub problems are finished when the stack becomes empty, or its top element is less\\nthan or equal to the new element, leading to the actions described above. If all elements have\\nbeen processed, and the stack is not yet empty, we finish the remaining sub problems by updating\\nthe maximum area with respect to the elements at the top.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 199, 'file_type': 'pdf'}, page_content='At the first impression, this solution seems to be having O(n2) complexity. But if we look\\ncarefully, every element is pushed and popped at most once, and in every step of the function at\\nleast one element is pushed or popped. Since the amount of work for the decisions and the update\\nis constant, the complexity of the algorithm is O(n) by amortized analysis. Space Complexity:\\nO(n) [for stack].\\nProblem-26\\u2003\\u2003On a given machine, how do you check whether the stack grows up or down?\\nSolution: Try noting down the address of a local variable. Call another function with a local\\nvariable declared in it and check the address of that local variable and compare.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 200, 'file_type': 'pdf'}, page_content='Time Complexity: O(1). Space Complexity: O(1).\\nProblem-27\\u2003\\u2003Given a stack of integers, how do you check whether each successive pair of\\nnumbers in the stack is consecutive or not. The pairs can be increasing or decreasing, and\\nif the stack has an odd number of elements, the element at the top is left out of a pair. For\\nexample, if the stack of elements are [4, 5, -2, -3, 11, 10, 5, 6, 20], then the output should\\nbe true because each of the pairs (4, 5), (-2, -3), (11, 10), and (5, 6) consists of\\nconsecutive numbers.\\nSolution: Refer to Queues chapter.\\nProblem-28\\u2003\\u2003Recursively remove all adjacent duplicates: Given a string of characters,\\nrecursively remove adjacent duplicate characters from string. The output string should not\\nhave any adjacent duplicates.\\nInput: careermonk\\nOutput: camonk\\nInput: mississippi\\nOutput: m\\nSolution: This solution runs with the concept of in-place stack. When element on stack doesn’t\\nmatch the current character, we add it to stack. When it matches to stack top, we skip characters\\nuntil the element matches the top of stack and remove the element from stack.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 201, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1) as the stack simulation is done inplace.\\nProblem-29\\u2003\\u2003Given an array of elements, replace every element with nearest greater element\\non the right of that element.\\nSolution: One simple approach would involve scanning the array elements and for each of the\\nelements, scan the remaining elements and find the nearest greater element.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 202, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). Space Complexity: O(1).\\nProblem-30\\u2003\\u2003For Problem-29, can we improve the complexity?\\nSolution: The approach is pretty much similar to Problem-22. Create a stack and push the first\\nelement. For the rest of the elements, mark the current element as nextNearestGreater. If stack is\\nnot empty, then pop an element from stack and compare it with nextNearestGreater. If\\nnextNearestGreater is greater than the popped element, then nextNearestGreater is the next\\ngreater element for the popped element. Keep popping from the stack while the popped element is\\nsmaller than nextNearestGreater. nextNearestGreater becomes the next greater element for all\\nsuch popped elements. If nextNearestGreater is smaller than the popped element, then push the\\npopped element back.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 203, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-31\\u2003\\u2003How to implement a stack which will support following operations in O(1) time\\ncomplexity?\\n•\\nPush which adds an element to the top of stack.\\n•\\nPop which removes an element from top of stack.\\n•\\nFind Middle which will return middle element of the stack.\\n•\\nDelete Middle which will delete the middle element.\\nSolution: We can use a LinkedList data structure with an extra pointer to the middle element.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 204, 'file_type': 'pdf'}, page_content='Also, we need another variable to store whether the LinkedList has an even or odd number of\\nelements.\\n•\\nPush: Add the element to the head of the LinkedList. Update the pointer to the\\nmiddle element according to variable.\\n•\\nPop: Remove the head of the LinkedList. Update the pointer to the middle element\\naccording to variable.\\n•\\nFind Middle: Find Middle which will return middle element of the stack.\\n•\\nDelete Middle: Delete Middle which will delete the middle element use the logic of\\nProblem-43 from Linked Lists chapter.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 205, 'file_type': 'pdf'}, page_content='5.1 What is a Queue?\\nA queue is a data structure used for storing data (similar to Linked Lists and Stacks). In queue, the\\norder in which data arrives is important. In general, a queue is a line of people or things waiting\\nto be served in sequential order starting at the beginning of the line or sequence.\\nDefinition: A queue is an ordered list in which insertions are done at one end (rear) and\\ndeletions are done at other end (front). The first element to be inserted is the first one to be\\ndeleted. Hence, it is called First in First out (FIFO) or Last in Last out (LILO) list.\\nSimilar to Stacks, special names are given to the two changes that can be made to a queue. When\\nan element is inserted in a queue, the concept is called EnQueue, and when an element is\\nremoved from the queue, the concept is called DeQueue.\\nDeQueueing an empty queue is called underflow and EnQueuing an element in a full queue is\\ncalled overflow. Generally, we treat them as exceptions. As an example, consider the snapshot of'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 206, 'file_type': 'pdf'}, page_content='the queue.\\n5.2 How are Queues Used?\\nThe concept of a queue can be explained by observing a line at a reservation counter. When we\\nenter the line we stand at the end of the line and the person who is at the front of the line is the one\\nwho will be served next. He will exit the queue and be served.\\nAs this happens, the next person will come at the head of the line, will exit the queue and will be\\nserved. As each person at the head of the line keeps exiting the queue, we move towards the head\\nof the line. Finally we will reach the head of the line and we will exit the queue and be served.\\nThis behavior is very useful in cases where there is a need to maintain the order of arrival.\\n5.3 Queue ADT\\nThe following operations make a queue an ADT. Insertions and deletions in the queue must\\nfollow the FIFO scheme. For simplicity we assume the elements are integers.\\nMain Queue Operations\\n•\\nEnQueue(int data): Inserts an element at the end of the queue\\n•\\nint DeQueue(): Removes and returns the element at the front of the queue\\nAuxiliary Queue Operations\\n•\\nint Front(): Returns the element at the front without removing it\\n•\\nint QueueSize(): Returns the number of elements stored in the queue\\n•\\nint IsEmptyQueueQ: Indicates whether no elements are stored in the queue or not\\n5.4 Exceptions'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 207, 'file_type': 'pdf'}, page_content='Similar to other ADTs, executing DeQueue on an empty queue throws an “Empty Queue\\nException” and executing EnQueue on a full queue throws “Full Queue Exception”.\\n5.5 Applications\\nFollowing are some of the applications that use queues.\\nDirect Applications\\n•\\nOperating systems schedule jobs (with equal priority) in the order of arrival (e.g., a\\nprint queue).\\n•\\nSimulation of real-world queues such as lines at a ticket counter or any other first-\\ncome first-served scenario requires a queue.\\n•\\nMultiprogramming.\\n•\\nAsynchronous data transfer (file IO, pipes, sockets).\\n•\\nWaiting times of customers at call center.\\n•\\nDetermining number of cashiers to have at a supermarket.\\nIndirect Applications\\n•\\nAuxiliary data structure for algorithms\\n•\\nComponent of other data structures\\n5.6 Implementation\\nThere are many ways (similar to Stacks) of implementing queue operations and some of the\\ncommonly used methods are listed below.\\n•\\nSimple circular array based implementation\\n•\\nDynamic circular array based implementation\\n•\\nLinked list implementation\\nWhy Circular Arrays?\\nFirst, let us see whether we can use simple arrays for implementing queues as we have done for\\nstacks. We know that, in queues, the insertions are performed at one end and deletions are\\nperformed at the other end. After performing some insertions and deletions the process becomes\\neasy to understand.\\nIn the example shown below, it can be seen clearly that the initial slots of the array are getting\\nwasted. So, simple array implementation for queue is not efficient. To solve this problem we\\nassume the arrays as circular arrays. That means, we treat the last element and the first array'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 208, 'file_type': 'pdf'}, page_content='elements as contiguous. With this representation, if there are any free slots at the beginning, the\\nrear pointer can easily go to its next free slot.\\nNote: The simple circular array and dynamic circular array implementations are very similar to\\nstack array implementations. Refer to Stacks chapter for analysis of these implementations.\\nSimple Circular Array Implementation\\nThis simple implementation of Queue ADT uses an array. In the array, we add elements circularly\\nand use two variables to keep track of the start element and end element. Generally, front is used\\nto indicate the start element and rear is used to indicate the end element in the queue. The array\\nstoring the queue elements may become full. An EnQueue operation will then throw a full queue\\nexception. Similarly, if we try deleting an element from an empty queue it will throw empty\\nqueue exception.\\nNote: Initially, both front and rear points to -1 which indicates that the queue is empty.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 209, 'file_type': 'pdf'}, page_content='struct ArrayQueuie *Queveint size) {\\nstruct Ayu = maleic Arey Queteh\\n‘return NULL;\\nQueapacity = size\\nQufront = Qu—rear = -1;\\nQ-array~ malloc(Q—capacity * sizeoftint)}:\\niftQ—array)\\nreturn NULL;\\nreturn Q:\\n’\\nint IsEmpty Queue(struct ArrayQueue *Q) (\\ntrue then 1 is returned elae 0 is returned\\n\\nint IsFullQueue(struct ArrayQueue *Q) {\\n[ifthe condition is true then 1 is returned else 0 is retumed\\n1 aa OO-ame 51% O-npncty = O-tend\\nint QueveSize() {\\n1 tum @eapciy = @-tont + Q-rear © 1% Q-eapaciy\\nvoid EnQueuefstruct ArrayQueue *Q, int data) {\\nif _sFullQueweQ))\\npine\" Queue Overflow”)\\ntse |\\nQ-rear = (Q-rear+1) % Queapacity:\\nQ- arraylQ—rearl- data;\\nf{Q-tront == -1)\\n‘Q-tront = Q-rear:\\n’\\n’\\n{nt DeQueue(struct ArrayQueue *Q){\\n{nt data 0;//or element which does not exist in Queue\\nifflsEmptvOuewe(O |\\nrin Queue is Empty’)\\nreturn 0;\\n’\\nelse {\\n‘data = Q-arraylQ-tront);\\n‘f\\\\Q—front == Q-rear)\\n‘Q-front = Q-rear = -1;\\nelse: Q-front = (Q-front+i) % Q-reapacity;\\n’\\nreturn data;\\n’\\n‘eld DeleteQueoetuct ArayQuei \"0 (\\nHt\\n‘i(Q-array)\\nfree(Q—array;\\n, har\\n’'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 210, 'file_type': 'pdf'}, page_content='Performance and Limitations\\nPerformance: Let n be the number of elements in the queue:\\nSpace Complexity (for n EnQueue operations)\\nO(n)\\nTime Complexity of EnQueue()\\nO(1)\\nTime Complexity of DeQueue()\\nO(1)\\nTime Complexity of IsEmptyQueue()\\nO(1)\\nTime Complexity of IsFullQueue()\\nO(1)\\nTime Complexity of QueueSize()\\nO(1)\\nTime Complexity of DeleteQueue()\\nO(1)\\nLimitations: The maximum size of the queue must be defined as prior and cannot be changed.\\nTrying to EnQueue a new element into a full queue causes an implementation-specific exception.\\nDynamic Circular Array Implementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 211, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 212, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the queue.\\nSpace Complexity (for n EnQueue operations)\\nO(n)\\nTime Complexity of EnQueue()\\nO(1) (Average)\\nTime Complexity of DeQueue()\\nO(1)\\nTime Complexity of QueueSize()\\nO(1)\\nTime Complexity of IsEmptyQueue()\\nO(1)\\nTime Complexity of IsFullQueue()\\nO(1)\\nTime Complexity of QueueSize()\\nO(1)\\nTime Complexity of DeleteQueue()\\nO(1)\\nLinked List Implementation\\nAnother way of implementing queues is by using Linked lists. EnQueue operation is implemented\\nby inserting an element at the end of the list. DeQueue operation is implemented by deleting an\\nelement from the beginning of the list.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 213, 'file_type': 'pdf'}, page_content='struct Queue *CreateQueuel) {\\nstruct Quete *Q;\\nstruct ListNode \"temp;\\nQ= malloisizeofistruct Queue);\\nintQ)\\nreturn NULL;\\ntemp ~ malloc(szeofstruct ListNode)};\\nQ-front = Q-rear = NULL;\\nreturn Q;\\n}\\nint IsEmpty Queue(struct Queue *Q) {\\n// ifthe condition is true then 1 is returned else 0 is returned\\nreturn (Q~front == NULL};\\n)\\nvoid EnQueue(struct Queue *Q, int data) {\\nstruct ListNode *newNode;\\nnewNode = malloc(sizeof(struct ListNode!);\\niffinewNode)\\nreturn NULL:\\nnewNode—data = data;\\nnewNode-rnext = NULL;\\nif(Q—rear) Q—rear—next = newNode;\\n(Qrrear = newNode;\\n\\n‘if(Q—front == NULL)\\nQofront = Qorear;\\nint DeQueue(struct Queue *Q) [\\nint data=0; — //or element which does not exist in Queue\\nstruct ListNode “temp;\\nisEmpty Quewe(Q) |\\nprin\" Queue is empty’);\\nreturn 0;\\n)\\nelse (\\ntemp = Q—front;\\ndata - Q-ront-rdata;\\nQufront== Q-front—-next;\\nfree(temp);\\n)\\nreturn data;\\nt\\nvoid DeleteQueue(struct Queue *Q) {\\nstruct ListNode \"temp;\\nwhile(Q) {\\ntemp = Q:\\nQ=Q-next\\nfree(temp];\\ni\\nfree(Q);'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 214, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the queue, then\\nSpace Complexity (for n EnQueue operations)\\nO(n)\\nTime Complexity of EnQueue()\\nO(1) (Average)\\nTime Complexity of DeQueue()\\nO(1)\\nTime Complexity of IsEmptyQueue()\\nO(1)\\nTime Complexity of DeleteQueue()\\nO(1)\\nComparison of Implementations\\nNote: Comparison is very similar to stack implementations and Stacks chapter.\\n5.7 Queues: Problems & Solutions\\nProblem-1\\u2003\\u2003Give an algorithm for reversing a queue Q. To access the queue, we are only\\nallowed to use the methods of queue ADT.\\nSolution:\\nTime Complexity: O(n).\\nProblem-2\\u2003\\u2003How can you implement a queue using two stacks?\\nSolution: Let SI and S2 be the two stacks to be used in the implementation of queue. All we have\\nto do is to define the EnQueue and DeQueue operations for the queue.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 215, 'file_type': 'pdf'}, page_content='EnQueue Algorithm\\n•\\nJust push on to stack S1\\nTime Complexity: O(1).\\nDeQueue Algorithm\\n•\\nIf stack S2 is not empty then pop from S2 and return that element.\\n•\\nIf stack is empty, then transfer all elements from SI to S2 and pop the top element\\nfrom S2 and return that popped element [we can optimize the code a little by\\ntransferring only n – 1 elements from SI to S2 and pop the nth element from SI and\\nreturn that popped element].\\n•\\nIf stack S1 is also empty then throw error.\\nTime Complexity: From the algorithm, if the stack S2 is not empty then the complexity is O(1). If\\nthe stack S2 is empty, then we need to transfer the elements from SI to S2. But if we carefully\\nobserve, the number of transferred elements and the number of popped elements from S2 are\\nequal. Due to this the average complexity of pop operation in this case is O(1).The amortized\\ncomplexity of pop operation is O(1).\\nProblem-3\\u2003\\u2003Show how you can efficiently implement one stack using two queues. Analyze the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 216, 'file_type': 'pdf'}, page_content='running time of the stack operations.\\nSolution: Yes, it is possible to implement the Stack ADT using 2 implementations of the Queue\\nADT. One of the queues will be used to store the elements and the other to hold them temporarily\\nduring the pop and top methods. The push method would enqueue the given element onto the\\nstorage queue. The top method would transfer all but the last element from the storage queue onto\\nthe temporary queue, save the front element of the storage queue to be returned, transfer the last\\nelement to the temporary queue, then transfer all elements back to the storage queue. The pop\\nmethod would do the same as top, except instead of transferring the last element onto the\\ntemporary queue after saving it for return, that last element would be discarded. Let Q1 and Q2 be\\nthe two queues to be used in the implementation of stack. All we have to do is to define the push\\nand pop operations for the stack.\\nIn the algorithms below, we make sure that one queue is always empty.\\nPush Operation Algorithm: Insert the element in whichever queue is not empty.\\n•\\nCheck whether queue Q1 is empty or not. If Q1 is empty then Enqueue the element\\ninto Q2.\\n•\\nOtherwise EnQueue the element into Q1.\\nTime Complexity: O(1).\\nPop Operation Algorithm: Transfer n – 1 elements to the other queue and delete last from queue\\nfor performing pop operation.\\n•\\nIf queue Q1 is not empty then transfer n – 1 elements from Q1 to Q2 and then,\\nDeQueue the last element of Q1 and return it.\\n•\\nIf queue Q2 is not empty then transfer n – 1 elements from Q2 to Q1 and then,\\nDeQueue the last element of Q2 and return it.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 217, 'file_type': 'pdf'}, page_content='Time Complexity: Running time of pop operation is O(n) as each time pop is called, we are\\ntransferring all the elements from one queue to the other.\\nProblem-4\\u2003\\u2003Maximum sum in sliding window: Given array A[] with sliding window of size\\nw which is moving from the very left of the array to the very right. Assume that we can\\nonly see the w numbers in the window. Each time the sliding window moves rightwards by\\none position. For example: The array is [1 3 -1 -3 5 3 6 7], and w is 3.\\nWindow position\\nMax\\n[1 3 -1] -3 5 3 6 7\\n3\\n1 [3 -1 -3] 5 3 6 7\\n3\\n1 3 [-1 -3 5] 3 6 7\\n5\\n1 3 -1 [-3 5 3] 6 7\\n5\\n1 3 -1 -3 [5 3 6] 7\\n6\\n1 3 -1 -3 5 [3 6 7]\\n7'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 218, 'file_type': 'pdf'}, page_content='Input: A long array A[], and a window width w. Output: An array B[], B[i] is the\\nmaximum value from A[i] to A[i+w-1]. Requirement: Find a good optimal way to get\\nB[i]\\nSolution: This problem can be solved with doubly ended queue (which supports insertion and\\ndeletion at both ends). Refer Priority Queues chapter for algorithms.\\nProblem-5\\u2003\\u2003Given a queue Q containing n elements, transfer these items on to a stack S\\n(initially empty) so that front element of Q appears at the top of the stack and the order of\\nall other items is preserved. Using enqueue and dequeue operations for the queue, and push\\nand pop operations for the stack, outline an efficient O(n) algorithm to accomplish the\\nabove task, using only a constant amount of additional storage.\\nSolution: Assume the elements of queue Q are a1:a2 ...an. Dequeuing all elements and pushing\\nthem onto the stack will result in a stack with an at the top and a1 at the bottom. This is done in\\nO(n) time as dequeue and each push require constant time per operation. The queue is now empty.\\nBy popping all elements and pushing them on the queue we will get a1 at the top of the stack. This\\nis done again in O(n) time.\\nAs in big-oh arithmetic we can ignore constant factors. The process is carried out in O(n) time.\\nThe amount of additional storage needed here has to be big enough to temporarily hold one item.\\nProblem-6\\u2003\\u2003A queue is set up in a circular array A[O..n - 1] with front and rear defined as\\nusual. Assume that n – 1 locations in the array are available for storing the elements (with\\nthe other element being used to detect full/empty condition). Give a formula for the number\\nof elements in the queue in terms of rear, front, and n.\\nSolution: Consider the following figure to get a clear idea of the queue.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 219, 'file_type': 'pdf'}, page_content='•\\nRear of the queue is somewhere clockwise from the front.\\n•\\nTo enqueue an element, we move rear one position clockwise and write the element\\nin that position.\\n•\\nTo dequeue, we simply move front one position clockwise.\\n•\\nQueue migrates in a clockwise direction as we enqueue and dequeue.\\n•\\nEmptiness and fullness to be checked carefully.\\n•\\nAnalyze the possible situations (make some drawings to see where front and rear\\nare when the queue is empty, and partially and totally filled). We will get this:\\nProblem-7\\u2003\\u2003What is the most appropriate data structure to print elements of queue in reverse\\norder?\\nSolution: Stack.\\nProblem-8\\u2003\\u2003Implement doubly ended queues. A double-ended queue is an abstract data\\nstructure that implements a queue for which elements can only be added to or removed\\nfrom the front (head) or back (tail). It is also often called a head-tail linked list.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 220, 'file_type': 'pdf'}, page_content='‘wid pushBackDEQistruct LstNode \"head, int data\\nstruct ListNde *newNode = (struct ListNode!) malloc(sizeofistrutListNode));\\nnewNode-data = data;\\nifaead «= NULL),\\n‘head = newNode;\\n(thead)next = #he\\n(thead)~prev = *head\\n}\\nelse{\\nnewNode-prev = /head-sprev;\\nnewNode-next » *head;\\n(thead)prev-mext = newNode;\\n(thead)prev = newNode;\\n}\\n)\\n‘wid pushFrontDEQ(struct ListNode “head, int data\\npushBackDEQihead data;\\n“head = ‘head)-prey;\\n1\\nint popBackDEQistrut LstNode *head)t\\nint data;\\nif *head)-prev == ‘head j\\ndata = \"head)-sdata;\\nfree(*head);\\n*head = NULL;\\n}\\nelse{\\nstruct LstNode *newTal = head)-spret-prev;\\ndata = ‘head)-prev-sdata;\\nnewTail-next = *head;\\nfre((head)~prev);\\n(head) ~prev » newTal;\\n}\\nreturn data;\\n4\\nint popFrontstructListNode head\\nint data;\\n*head = ‘head}~next;\\ndata = popBackDEQ(head);\\nreturn data;\\n)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 221, 'file_type': 'pdf'}, page_content='Problem-9\\u2003\\u2003Given a stack of integers, how do you check whether each successive pair of\\nnumbers in the stack is consecutive or not. The pairs can be increasing or decreasing, and\\nif the stack has an odd number of elements, the element at the top is left out of a pair. For\\nexample, if the stack of elements are [4, 5, -2, -3, 11, 10, 5, 6, 20], then the output should\\nbe true because each of the pairs (4, 5), (-2, -3), (11, 10), and (5, 6) consists of\\nconsecutive numbers.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-10\\u2003\\u2003Given a queue of integers, rearrange the elements by interleaving the first half of\\nthe list with the second half of the list. For example, suppose a queue stores the following\\nsequence of values: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. Consider the two halves of\\nthis list: first half: [11, 12, 13, 14, 15] second half: [16, 17, 18, 19, 20]. These are'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 222, 'file_type': 'pdf'}, page_content='combined in an alternating fashion to form a sequence of interleave pairs: the first values\\nfrom each half (11 and 16), then the second values from each half (12 and 17), then the\\nthird values from each half (13 and 18), and so on. In each pair, the value from the first\\nhalf appears before the value from the second half. Thus, after the call, the queue stores the\\nfollowing values: [11, 16, 12, 17, 13, 18, 14, 19, 15, 20].\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-11\\u2003\\u2003Given an integer k and a queue of integers, how do you reverse the order of the\\nfirst k elements of the queue, leaving the other elements in the same relative order? For\\nexample, if k=4 and queue has the elements [10, 20, 30, 40, 50, 60, 70, 80, 90]; the output\\nshould be [40, 30, 20, 10, 50, 60, 70, 80, 90].\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 223, 'file_type': 'pdf'}, page_content='‘Time Complexity: O(n). Space Complexity: O(n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 224, 'file_type': 'pdf'}, page_content='6.1 What is a Tree?\\nA tree is a data structure similar to a linked list but instead of each node pointing simply to the\\nnext node in a linear fashion, each node points to a number of nodes. Tree is an example of a non-\\nlinear data structure. A tree structure is a way of representing the hierarchical nature of a structure\\nin a graphical form.\\nIn trees ADT (Abstract Data Type), the order of the elements is not important. If we need ordering\\ninformation, linear data structures like linked lists, stacks, queues, etc. can be used.\\n6.2 Glossary'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 225, 'file_type': 'pdf'}, page_content='•\\nThe root of a tree is the node with no parents. There can be at most one root node in\\na tree (node A in the above example).\\n•\\nAn edge refers to the link from parent to child (all links in the figure).\\n•\\nA node with no children is called leaf node (E,J,K,H and I).\\n•\\nChildren of same parent are called siblings (B,C,D are siblings of A, and E,F are the\\nsiblings of B).\\n•\\nA node p is an ancestor of node q if there exists a path from root to q and p appears\\non the path. The node q is called a descendant of p. For example, A,C and G are the\\nancestors of if.\\n•\\nThe set of all nodes at a given depth is called the level of the tree (B, C and D are\\nthe same level). The root node is at level zero.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 226, 'file_type': 'pdf'}, page_content='•\\nThe depth of a node is the length of the path from the root to the node (depth of G is\\n2, A – C – G).\\n•\\nThe height of a node is the length of the path from that node to the deepest node. The\\nheight of a tree is the length of the path from the root to the deepest node in the tree.\\nA (rooted) tree with only one node (the root) has a height of zero. In the previous\\nexample, the height of B is 2 (B – F – J).\\n•\\nHeight of the tree is the maximum height among all the nodes in the tree and depth of\\nthe tree is the maximum depth among all the nodes in the tree. For a given tree,\\ndepth and height returns the same value. But for individual nodes we may get\\ndifferent results.\\n•\\nThe size of a node is the number of descendants it has including itself (the size of the\\nsubtree C is 3).\\n•\\nIf every node in a tree has only one child (except leaf nodes) then we call such trees\\nskew trees. If every node has only left child then we call them left skew trees.\\nSimilarly, if every node has only right child then we call them right skew trees.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 227, 'file_type': 'pdf'}, page_content='6.3 Binary Trees\\nA tree is called binary tree if each node has zero child, one child or two children. Empty tree is\\nalso a valid binary tree. We can visualize a binary tree as consisting of a root and two disjoint\\nbinary trees, called the left and right subtrees of the root.\\nGeneric Binary Tree'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 228, 'file_type': 'pdf'}, page_content='6.4 Types of Binary Trees\\nStrict Binary Tree: A binary tree is called strict binary tree if each node has exactly two\\nchildren or no children.\\nFull Binary Tree: A binary tree is called full binary tree if each node has exactly two children\\nand all leaf nodes are at the same level.\\nComplete Binary Tree: Before defining the complete binary tree, let us assume that the height of\\nthe binary tree is h. In complete binary trees, if we give numbering for the nodes by starting at the\\nroot (let us say the root node has 1) then we get a complete sequence from 1 to the number of\\nnodes in the tree. While traversing we should give numbering for NULL pointers also. A binary\\ntree is called complete binary tree if all leaf nodes are at height h or h – 1 and also without any\\nmissing number in the sequence.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 229, 'file_type': 'pdf'}, page_content='6.5 Properties of Binary Trees\\nFor the following properties, let us assume that the height of the tree is h. Also, assume that root\\nnode is at height zero.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 230, 'file_type': 'pdf'}, page_content='From the diagram we can infer the following properties:\\n•\\nThe number of nodes n in a full binary tree is 2h+1 – 1. Since, there are h levels we\\nneed to add all nodes at each level [20 + 21+ 22 + ··· + 2h = 2h+1 – 1].\\n•\\nThe number of nodes n in a complete binary tree is between 2h (minimum) and 2h+1\\n– 1 (maximum). For more information on this, refer to Priority Queues chapter.\\n•\\nThe number of leaf nodes in a full binary tree is 2h.\\n•\\nThe number of NULL links (wasted pointers) in a complete binary tree of n nodes is\\nn + 1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 231, 'file_type': 'pdf'}, page_content='Structure of Binary Trees\\nNow let us define structure of the binary tree. For simplicity, assume that the data of the nodes are\\nintegers. One way to represent a node (which contains data) is to have two links which point to\\nleft and right children along with data fields as shown below:\\nNote: In trees, the default flow is from parent to children and it is not mandatory to show directed\\nbranches. For our discussion, we assume both the representations shown below are the same.\\nOperations on Binary Trees\\nBasic Operations\\n•\\nInserting an element into a tree\\n•\\nDeleting an element from a tree\\n•\\nSearching for an element\\n•\\nTraversing the tree\\nAuxiliary Operations\\n•\\nFinding the size of the tree\\n•\\nFinding the height of the tree\\n•\\nFinding the level which has maximum sum\\n•\\nFinding the least common ancestor (LCA) for a given pair of nodes, and many more.\\nApplications of Binary Trees'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 232, 'file_type': 'pdf'}, page_content='Following are the some of the applications where binary trees play an important role:\\n•\\nExpression trees are used in compilers.\\n•\\nHuffman coding trees that are used in data compression algorithms.\\n•\\nBinary Search Tree (BST), which supports search, insertion and deletion on a\\ncollection of items in O(logn) (average).\\n•\\nPriority Queue (PQ), which supports search and deletion of minimum (or maximum)\\non a collection of items in logarithmic time (in worst case).\\n6.6 Binary Tree Traversals\\nIn order to process trees, we need a mechanism for traversing them, and that forms the subject of\\nthis section. The process of visiting all nodes of a tree is called tree traversal. Each node is\\nprocessed only once but it may be visited more than once. As we have already seen in linear data\\nstructures (like linked lists, stacks, queues, etc.), the elements are visited in sequential order. But,\\nin tree structures there are many different ways.\\nTree traversal is like searching the tree, except that in traversal the goal is to move through the\\ntree in a particular order. In addition, all nodes are processed in the traversal but searching\\nstops when the required node is found.\\nTraversal Possibilities\\nStarting at the root of a binary tree, there are three main steps that can be performed and the order\\nin which they are performed defines the traversal type. These steps are: performing an action on\\nthe current node (referred to as “visiting” the node and denoted with “D”), traversing to the left\\nchild node (denoted with “L”), and traversing to the right child node (denoted with “R”). This\\nprocess can be easily described through recursion. Based on the above definition there are 6\\npossibilities:\\n1.\\nLDR: Process left subtree, process the current node data and then process right\\nsubtree\\n2.\\nLRD: Process left subtree, process right subtree and then process the current node\\ndata\\n3.\\nDLR: Process the current node data, process left subtree and then process right\\nsubtree\\n4.\\nDRL: Process the current node data, process right subtree and then process left\\nsubtree\\n5.\\nRDL: Process right subtree, process the current node data and then process left\\nsubtree\\n6.\\nRLD: Process right subtree, process left subtree and then process the current node\\ndata'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 233, 'file_type': 'pdf'}, page_content='Classifying the Traversals\\nThe sequence in which these entities (nodes) are processed defines a particular traversal method.\\nThe classification is based on the order in which current node is processed. That means, if we are\\nclassifying based on current node (D) and if D comes in the middle then it does not matter\\nwhether L is on left side of D or R is on left side of D.\\nSimilarly, it does not matter whether L is on right side of D or R is on right side of D. Due to this,\\nthe total 6 possibilities are reduced to 3 and these are:\\n•\\nPreorder (DLR) Traversal\\n•\\nInorder (LDR) Traversal\\n•\\nPostorder (LRD) Traversal\\nThere is another traversal method which does not depend on the above orders and it is:\\n•\\nLevel Order Traversal: This method is inspired from Breadth First Traversal (BFS\\nof Graph algorithms).\\nLet us use the diagram below for the remaining discussion.\\nPreOrder Traversal\\nIn preorder traversal, each node is processed before (pre) either of its subtrees. This is the\\nsimplest traversal to understand. However, even though each node is processed before the\\nsubtrees, it still requires that some information must be maintained while moving down the tree.\\nIn the example above, 1 is processed first, then the left subtree, and this is followed by the right\\nsubtree.\\nTherefore, processing must return to the right subtree after finishing the processing of the left\\nsubtree. To move to the right subtree after processing the left subtree, we must maintain the root'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 234, 'file_type': 'pdf'}, page_content='information. The obvious ADT for such information is a stack. Because of its LIFO structure, it is\\npossible to get the information about the right subtrees back in the reverse order.\\nPreorder traversal is defined as follows:\\n•\\nVisit the root.\\n•\\nTraverse the left subtree in Preorder.\\n•\\nTraverse the right subtree in Preorder.\\nThe nodes of tree would be visited in the order: 1 2 4 5 3 6 7\\nTime Complexity: O(n). Space Complexity: O(n).\\nNon-Recursive Preorder Traversal\\nIn the recursive version, a stack is required as we need to remember the current node so that after\\ncompleting the left subtree we can go to the right subtree. To simulate the same, first we process\\nthe current node and before going to the left subtree, we store the current node on stack. After\\ncompleting the left subtree processing, pop the element and go to its right subtree. Continue this\\nprocess until stack is nonempty.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 235, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nInOrder Traversal\\nIn Inorder Traversal the root is visited between the subtrees. Inorder traversal is defined as\\nfollows:\\n•\\nTraverse the left subtree in Inorder.\\n•\\nVisit the root.\\n•\\nTraverse the right subtree in Inorder.\\nThe nodes of tree would be visited in the order: 4 2 5 1 6 3 7'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 236, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nNon-Recursive Inorder Traversal\\nThe Non-recursive version of Inorder traversal is similar to Preorder. The only change is, instead\\nof processing the node before going to left subtree, process it after popping (which is indicated\\nafter completion of left subtree processing).\\nTime Complexity: O(n). Space Complexity: O(n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 237, 'file_type': 'pdf'}, page_content='PostOrder Traversal\\nIn postorder traversal, the root is visited after both subtrees. Postorder traversal is defined as\\nfollows:\\n•\\nTraverse the left subtree in Postorder.\\n•\\nTraverse the right subtree in Postorder.\\n•\\nVisit the root.\\nThe nodes of the tree would be visited in the order: 4 5 2 6 7 3 1\\nTime Complexity: O(n). Space Complexity: O(n).\\nNon-Recursive Postorder Traversal\\nIn preorder and inorder traversals, after popping the stack element we do not need to visit the\\nsame vertex again. But in postorder traversal, each node is visited twice. That means, after\\nprocessing the left subtree we will visit the current node and after processing the right subtree we\\nwill visit the same current node. But we should be processing the node during the second visit.\\nHere the problem is how to differentiate whether we are returning from the left subtree or the\\nright subtree.\\nWe use a previous variable to keep track of the earlier traversed node. Let’s assume current is the\\ncurrent node that is on top of the stack. When previous is current’s parent, we are traversing\\ndown the tree. In this case, we try to traverse to current’s left child if available (i.e., push left\\nchild to the stack). If it is not available, we look at current’s right child. If both left and right child\\ndo not exist (ie, current is a leaf node), we print current’s value and pop it off the stack.\\nIf prev is current’s left child, we are traversing up the tree from the left. We look at current’s right\\nchild. If it is available, then traverse down the right child (i.e., push right child to the stack);\\notherwise print current’s value and pop it off the stack. If previous is current’s right child, we are\\ntraversing up the tree from the right. In this case, we print current’s value and pop it off the stack.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 238, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nLevel Order Traversal\\nLevel order traversal is defined as follows:\\n•\\nVisit the root.\\n•\\nWhile traversing level (, keep all the elements at level ( + 1 in queue.\\n•\\nGo to the next level and visit all the nodes at that level.\\n•\\nRepeat this until all levels are completed.\\nThe nodes of the tree are visited in the order: 1 2 3 4 5 6 7'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 239, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n). Since, in the worst case, all the nodes on the\\nentire last level could be in the queue simultaneously.\\nBinary Trees: Problems & Solutions\\nProblem-1\\u2003\\u2003Give an algorithm for finding maximum element in binary tree.\\nSolution: One simple way of solving this problem is: find the maximum element in left subtree,\\nfind the maximum element in right sub tree, compare them with root data and select the one which\\nis giving the maximum value. This approach can be easily implemented with recursion.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 240, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-2\\u2003\\u2003Give an algorithm for finding the maximum element in binary tree without\\nrecursion.\\nSolution: Using level order traversal: just observe the element’s data while deleting.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 241, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-3\\u2003\\u2003Give an algorithm for searching an element in binary tree.\\nSolution: Given a binary tree, return true if a node with data is found in the tree. Recurse down\\nthe tree, choose the left or right branch by comparing data with each node’s data.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 242, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-4\\u2003\\u2003Give an algorithm for searching an element in binary tree without recursion.\\nSolution: We can use level order traversal for solving this problem. The only change required in\\nlevel order traversal is, instead of printing the data, we just need to check whether the root data is\\nequal to the element we want to search.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 243, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-5\\u2003\\u2003Give an algorithm for inserting an element into binary tree.\\nSolution: Since the given tree is a binary tree, we can insert the element wherever we want. To\\ninsert an element, we can use the level order traversal and insert the element wherever we find\\nthe node whose left or right child is NULL.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 244, 'file_type': 'pdf'}, page_content='‘oid InsertInBinaryTree(struct BinaryTreeNode *root, int data\\nstruct Queue *Q;\\nstruct BinaryTreeNode *temp;\\nstruct BinaryTreeNode ‘newNode;\\nnewode = (struct BinaryTreeNode #] mallocsizeofstruc BinaryTreeNode);\\nnewNode—left = newNode-right = NULL;\\niffnewNode (\\nprint{(\"Memory Error’); return;\\n\\n!\\n\\nifftrot){\\nroot = newNode;\\nreturn;\\n\\n)\\nQ = CreateQueuel);\\nEnQueue(Q,ro0t);\\n\\nwhile(tsEmpty Queue(Q)) {\\ntemp » DeQueue{Q);\\niftemp-left\\nEnQueue(Q, temple\\nese (\\ntemp-left=newNode;\\nDeleteQueue(Q);\\nreturn;\\n}\\niftemp-right)\\nEnQueuel0, temp-right);\\n\\ntemp-right=newNode;\\nDeleteQueue(Q);\\nreturn;\\n\\n}\\nDeleteQueue(Q);\\n|'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 245, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-6\\u2003\\u2003Give an algorithm for finding the size of binary tree.\\nSolution: Calculate the size of left and right subtrees recursively, add 1 (current node) and return\\nto its parent.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-7\\u2003\\u2003Can we solve Problem-6 without recursion?\\nSolution: Yes, using level order traversal.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 246, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-8\\u2003\\u2003Give an algorithm for printing the level order data in reverse order. For example,\\nthe output for the below tree should be: 4 5 6 7 2 3 1\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 247, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-9\\u2003\\u2003Give an algorithm for deleting the tree.\\nSolution:\\nTo delete a tree, we must traverse all the nodes of the tree and delete them one by one. So which\\ntraversal should we use: Inorder, Preorder, Postorder or Level order Traversal?\\nBefore deleting the parent node we should delete its children nodes first. We can use postorder\\ntraversal as it does the work without storing anything. We can delete tree with other traversals\\nalso with extra space complexity. For the following, tree nodes are deleted in order – 4,5,2,3,1.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-10\\u2003\\u2003Give an algorithm for finding the height (or depth) of the binary tree.\\nSolution: Recursively calculate height of left and right subtrees of a node and assign height to the\\nnode as max of the heights of two children plus 1. This is similar to PreOrder tree traversal (and\\nDFS of Graph algorithms).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 248, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-11\\u2003\\u2003Can we solve Problem-10 without recursion?\\nSolution: Yes, using level order traversal. This is similar to BFS of Graph algorithms. End of\\nlevel is identified with NULL.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 249, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-12\\u2003\\u2003Give an algorithm for finding the deepest node of the binary tree.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 250, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-13\\u2003\\u2003Give an algorithm for deleting an element (assuming data is given) from binary\\ntree.\\nSolution: The deletion of a node in binary tree can be implemented as\\n•\\nStarting at root, find the node which we want to delete.\\n•\\nFind the deepest node in the tree.\\n•\\nReplace the deepest node’s data with node to be deleted.\\n•\\nThen delete the deepest node.\\nProblem-14\\u2003\\u2003Give an algorithm for finding the number of leaves in the binary tree without\\nusing recursion.\\nSolution: The set of nodes whose both left and right children are NULL are called leaf nodes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 251, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-15\\u2003\\u2003Give an algorithm for finding the number of full nodes in the binary tree without\\nusing recursion.\\nSolution: The set of all nodes with both left and right children are called full nodes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 252, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-16\\u2003\\u2003Give an algorithm for finding the number of half nodes (nodes with only one\\nchild) in the binary tree without using recursion.\\nSolution: The set of all nodes with either left or right child (but not both) are called half nodes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 253, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-17\\u2003\\u2003Given two binary trees, return true if they are structurally identical.\\nSolution:\\nAlgorithm:\\n•\\nIf both trees are NULL then return true.\\n•\\nIf both trees are not NULL, then compare data and recursively check left and right\\nsubtree structures.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 254, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for recursive stack.\\nProblem-18\\u2003\\u2003Give an algorithm for finding the diameter of the binary tree. The diameter of a\\ntree (sometimes called the width) is the number of nodes on the longest path between two\\nleaves in the tree.\\nSolution: To find the diameter of a tree, first calculate the diameter of left subtree and right\\nsubtrees recursively. Among these two values, we need to send maximum value along with\\ncurrent level (+1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 255, 'file_type': 'pdf'}, page_content='There is another solution and the complexity is O(n). The main idea of this approach is that the\\nnode stores its left child’s and right child’s maximum diameter if the node’s child is the “root”,\\ntherefore, there is no need to recursively call the height method. The drawback is we need to add\\ntwo extra variables in the node structure.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 256, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-19\\u2003\\u2003Give an algorithm for finding the level that has the maximum sum in the binary\\ntree.\\nSolution: The logic is very much similar to finding the number of levels. The only change is, we'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 257, 'file_type': 'pdf'}, page_content='need to keep track of the sums as well.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 258, 'file_type': 'pdf'}, page_content=\"int FindLevelwithMaxSumstruct BinaryTreeNode ‘root\\nstruct BinaryTreeNode “temp;\\nint level=0, maxLevel=0;\\nstruct Queue *Q;\\n{nt currentSum = 0, maxSum = 0;\\niflroot}\\nreturn 0;\\nQ-CreateQueue')\\nEnQueue(Q,root};\\nEnQueue(Q,NULL); | Bnd of irs level.\\nwhile({IsEmptyQueue(Q)) |\\ntemp =DeQueue(Q);\\n/| Ifthe curtent levels completed then compare sums\\n{ftemp == NULL {\\niffeurrentSum> maxSum){\\n‘maxSum = currentSum;\\nrmaxLevel = level;\\n}\\ncurrentSum = 0;\\n[place the indicator for end of next level at the end of queue\\nifsEmptyQueue(Q))\\nEnQueue(Q,NULL\\nlevel;\\n}\\nse {\\ncurrentSum += temp-data;\\ntemplet\\nEnQueue(temp, temp—left);\\niflroot-right)\\nEnQueue(temp, temp—right);\\n\\nreturn maxLevel;\"), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 259, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-20\\u2003\\u2003Given a binary tree, print out all its root-to-leaf paths.\\nSolution: Refer to comments in functions.\\nTime Complexity: O(n). Space Complexity: O(n), for recursive stack.\\nProblem-21\\u2003\\u2003Give an algorithm for checking the existence of path with given sum. That\\nmeans, given a sum, check whether there exists a path from root to any of the nodes.\\nSolution: For this problem, the strategy is: subtract the node value from the sum before calling its\\nchildren recursively, and check to see if the sum is 0 when we run out of tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 260, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-22\\u2003\\u2003Give an algorithm for finding the sum of all elements in binary tree.\\nSolution: Recursively, call left subtree sum, right subtree sum and add their values to current\\nnodes data.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 261, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-23\\u2003\\u2003Can we solve Problem-22 without recursion?\\nSolution: We can use level order traversal with simple change. Every time after deleting an\\nelement from queue, add the nodes data value to sum variable.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-24\\u2003\\u2003Give an algorithm for converting a tree to its mirror. Mirror of a tree is another\\ntree with left and right children of all non-leaf nodes interchanged. The trees below are\\nmirrors to each other.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 262, 'file_type': 'pdf'}, page_content='Solution:\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-25\\u2003\\u2003Given two trees, give an algorithm for checking whether they are mirrors of\\neach other.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 263, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-26\\u2003\\u2003Give an algorithm for finding LCA (Least Common Ancestor) of two nodes in a\\nBinary Tree.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n) for recursion.\\nProblem-27\\u2003\\u2003Give an algorithm for constructing binary tree from given Inorder and Preorder\\ntraversals.\\nSolution: Let us consider the traversals below:\\nInorder sequence: D B E A F C\\nPreorder sequence: A B D E C F'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 264, 'file_type': 'pdf'}, page_content='In a Preorder sequence, leftmost element denotes the root of the tree. So we know ‘A’ is the root\\nfor given sequences. By searching ‘A’ in Inorder sequence we can find out all elements on the left\\nside of ‘A’, which come under the left subtree, and elements on the right side of ‘A’, which come\\nunder the right subtree. So we get the structure as seen below.\\nWe recursively follow the above steps and get the following tree.\\nAlgorithm: BuildTree()\\n1\\nSelect an element from Preorder. Increment a Preorder index variable\\n(preOrderIndex in code below) to pick next element in next recursive call.\\n2\\nCreate a new tree node (newNode) from heap with the data as selected element.\\n3\\nFind the selected element’s index in Inorder. Let the index be inOrderIndex.\\n4\\nCall BuildBinaryTree for elements before inOrderIndex and make the built tree as left\\nsubtree of newNode.\\n5\\nCall BuildBinaryTree for elements after inOrderIndex and make the built tree as right\\nsubtree of newNode.\\n6\\nreturn newNode.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 265, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-28\\u2003\\u2003If we are given two traversal sequences, can we construct the binary tree\\nuniquely?\\nSolution: It depends on what traversals are given. If one of the traversal methods is Inorder then\\nthe tree can be constructed uniquely, otherwise not.\\nTherefore, the following combinations can uniquely identify a tree:\\n•\\nInorder and Preorder\\n•\\nInorder and Postorder\\n•\\nInorder and Level-order'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 266, 'file_type': 'pdf'}, page_content='The following combinations do not uniquely identify a tree.\\n•\\nPostorder and Preorder\\n•\\nPreorder and Level-order\\n•\\nPostorder and Level-order\\nFor example, Preorder, Level-order and Postorder traversals are the same for the above trees:\\nSo, even if three of them (PreOrder, Level-Order and PostOrder) are given, the tree cannot be\\nconstructed uniquely.\\nProblem-29\\u2003\\u2003Give an algorithm for printing all the ancestors of a node in a Binary tree. For\\nthe tree below, for 7 the ancestors are 1 3 7.\\nSolution: Apart from the Depth First Search of this tree, we can use the following recursive way\\nto print the ancestors.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 267, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n) for recursion.\\nProblem-30\\u2003\\u2003Zigzag Tree Traversal: Give an algorithm to traverse a binary tree in Zigzag\\norder. For example, the output for the tree below should be: 1 3 2 4 5 6 7\\nSolution: This problem can be solved easily using two stacks. Assume the two stacks are:\\ncurrentLevel and nextLevel. We would also need a variable to keep track of the current level\\norder (whether it is left to right or right to left).\\nWe pop from currentLevel stack and print the node’s value. Whenever the current level order is\\nfrom left to right, push the node’s left child, then its right child, to stack nextLevel. Since a stack\\nis a Last In First Out (LIFO) structure, the next time that nodes are popped off nextLevel, it will\\nbe in the reverse order.\\nOn the other hand, when the current level order is from right to left, we would push the node’s\\nright child first, then its left child. Finally, don’t forget to swap those two stacks at the end of each\\nlevel (i. e., when currentLevel is empty).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 268, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: Space for two stacks = O(n) + O(n) = O(n).\\nProblem-31\\u2003\\u2003Give an algorithm for finding the vertical sum of a binary tree. For example, The\\ntree has 5 vertical lines\\nVertical-1: nodes-4 => vertical sum is 4\\nVertical-2: nodes-2 => vertical sum is 2\\nVertical-3: nodes-1,5,6 => vertical sum is 1 + 5 + 6 = 12\\nVertical-4: nodes-3 => vertical sum is 3\\nVertical-5: nodes-7 => vertical sum is 7\\nWe need to output: 4 2 12 3 7'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 269, 'file_type': 'pdf'}, page_content='Solution: \\nWe \\ncan \\ndo \\nan \\ninorder \\ntraversal \\nand \\nhash \\nthe \\ncolumn. \\nWe \\ncall\\nVerticalSumlnBinaryTreefroot, 0) which means the root is at column 0. While doing the traversal,\\nhash the column and increase its value by root → data.\\nProblem-32\\u2003\\u2003How many different binary trees are possible with n nodes?\\nSolution: For example, consider a tree with 3 nodes (n = 3). It will have the maximum\\ncombination of 5 different (i.e., 23 -3 = 5) trees.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 270, 'file_type': 'pdf'}, page_content='In general, if there are n nodes, there exist 2n –n different trees.\\nProblem-33\\u2003\\u2003Given a tree with a special property where leaves are represented with ‘L’ and\\ninternal node with ‘I’. Also, assume that each node has either 0 or 2 children. Given\\npreorder traversal of this tree, construct the tree.\\nExample: Given preorder string => ILILL\\nSolution: First, we should see how preorder traversal is arranged. Pre-order traversal means\\nfirst put root node, then pre-order traversal of left subtree and then pre-order traversal of right\\nsubtree. In a normal scenario, it’s not possible to detect where left subtree ends and right subtree\\nstarts using only pre-order traversal. Since every node has either 2 children or no child, we can\\nsurely say that if a node exists then its sibling also exists. So every time when we are computing a\\nsubtree, we need to compute its sibling subtree as well.\\nSecondly, whenever we get ‘L’ in the input string, that is a leaf and we can stop for a particular\\nsubtree at that point. After this ‘L’ node (left child of its parent ‘L’), its sibling starts. If ‘L’ node is\\nright child of its parent, then we need to go up in the hierarchy to find the next subtree to compute.\\nKeeping the above invariant in mind, we can easily determine when a subtree ends and the next\\none starts. It means that we can give any start node to our method and it can easily complete the\\nsubtree it generates going outside of its nodes. We just need to take care of passing the correct\\nstart nodes to different sub-trees.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 271, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nProblem-34\\u2003\\u2003Given a binary tree with three pointers (left, right and nextSibling), give an\\nalgorithm for filling the nextSibling pointers assuming they are NULL initially.\\nSolution: We can use simple queue (similar to the solution of Problem-11). Let us assume that the\\nstructure of binary tree is:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 272, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-35\\u2003\\u2003Is there any other way of solving Problem-34?\\nSolution: The trick is to re-use the populated nextSibling pointers. As mentioned earlier, we just'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 273, 'file_type': 'pdf'}, page_content='need one more step for it to work. Before we pass the left and right to the recursion function\\nitself, we connect the right child’s nextSibling to the current node’s nextSibling left child. In order\\nfor this to work, the current node nextSibling pointer must be populated, which is true in this\\ncase.\\nTime Complexity: O(n).\\n6.7 Generic Trees (N-ary Trees)\\nIn the previous section we discussed binary trees where each node can have a maximum of two\\nchildren and these are represented easily with two pointers. But suppose if we have a tree with\\nmany children at every node and also if we do not know how many children a node can have, how\\ndo we represent them?\\nFor example, consider the tree shown below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 274, 'file_type': 'pdf'}, page_content='How do we represent the tree?\\nIn the above tree, there are nodes with 6 children, with 3 children, with 2 children, with 1 child,\\nand with zero children (leaves). To present this tree we have to consider the worst case (6\\nchildren) and allocate that many child pointers for each node. Based on this, the node\\nrepresentation can be given as:\\nSince we are not using all the pointers in all the cases, there is a lot of memory wastage. Another\\nproblem is that we do not know the number of children for each node in advance. In order to'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 275, 'file_type': 'pdf'}, page_content='solve this problem we need a representation that minimizes the wastage and also accepts nodes\\nwith any number of children.\\nRepresentation of Generic Trees\\nSince our objective is to reach all nodes of the tree, a possible solution to this is as follows:\\n•\\nAt each node link children of same parent (siblings) from left to right.\\n•\\nRemove the links from parent to all children except the first child.\\nWhat these above statements say is if we have a link between children then we do not need extra\\nlinks from parent to all children. This is because we can traverse all the elements by starting at\\nthe first child of the parent. So if we have a link between parent and first child and also links\\nbetween all children of same parent then it solves our problem.\\nThis representation is sometimes called first child/next sibling representation. First child/next\\nsibling representation of the generic tree is shown above. The actual representation for this tree\\nis:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 276, 'file_type': 'pdf'}, page_content='Based on this discussion, the tree node declaration for general tree can be given as:\\nNote: Since we are able to convert any generic tree to binary representation; in practice we use\\nbinary trees. We can treat all generic trees with a first child/next sibling representation as binary\\ntrees.\\nGeneric Trees: Problems & Solutions\\nProblem-36\\u2003\\u2003Given a tree, give an algorithm for finding the sum of all the elements of the tree.\\nSolution: The solution is similar to what we have done for simple binary trees. That means,\\ntraverse the complete list and keep on adding the values. We can either use level order traversal'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 277, 'file_type': 'pdf'}, page_content='or simple recursion.\\nTime Complexity: O(n). Space Complexity: O(1) (if we do not consider stack space), otherwise\\nO(n).\\nNote: All problems which we have discussed for binary trees are applicable for generic trees\\nalso. Instead of left and right pointers we just need to use firstChild and nextSibling.\\nProblem-37\\u2003\\u2003For a 4-ary tree (each node can contain maximum of 4 children), what is the\\nmaximum possible height with 100 nodes? Assume height of a single node is 0.\\nSolution: In 4-ary tree each node can contain 0 to 4 children, and to get maximum height, we need\\nto keep only one child for each parent. With 100 nodes, the maximum possible height we can get\\nis 99.\\nIf we have a restriction that at least one node has 4 children, then we keep one node with 4\\nchildren and the remaining nodes with 1 child. In this case, the maximum possible height is 96.\\nSimilarly, with n nodes the maximum possible height is n – 4.\\nProblem-38\\u2003\\u2003For a 4-ary tree (each node can contain maximum of 4 children), what is the\\nminimum possible height with n nodes?\\nSolution: Similar to the above discussion, if we want to get minimum height, then we need to fill\\nall nodes with maximum children (in this case 4). Now let’s see the following table, which\\nindicates the maximum number of nodes for a given height.\\nFor a given height h the maximum possible nodes are: \\n. To get minimum height, take\\nlogarithm on both sides:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 278, 'file_type': 'pdf'}, page_content='Problem-39\\u2003\\u2003Given a parent array P, where P[i] indicates the parent of ith node in the tree\\n(assume parent of root node is indicated with –1). Give an algorithm for finding the height\\nor depth of the tree.\\nSolution:\\nFor example: if the P is\\nIts corresponding tree is:\\nFrom the problem definition, the given array represents the parent array. That means, we need to\\nconsider the tree for that array and find the depth of the tree. The depth of this given tree is 4. If\\nwe carefully observe, we just need to start at every node and keep going to its parent until we\\nreach –1 and also keep track of the maximum depth among all nodes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 279, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). For skew trees we will be re-calculating the same values. Space\\nComplexity: O(1).\\nNote: We can optimize the code by storing the previous calculated nodes’ depth in some hash\\ntable or other array. This reduces the time complexity but uses extra space.\\nProblem-40\\u2003\\u2003Given a node in the generic tree, give an algorithm for counting the number of\\nsiblings for that node.\\nSolution: Since tree is represented with the first child/next sibling method, the tree structure can\\nbe given as:\\nFor a given node in the tree, we just need to traverse all its next siblings.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 280, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-41\\u2003\\u2003Given a node in the generic tree, give an algorithm for counting the number of\\nchildren for that node.\\nSolution: Since the tree is represented as first child/next sibling method, the tree structure can be\\ngiven as:\\nFor a given node in the tree, we just need to point to its first child and keep traversing all its next\\nsiblings.\\nTime Complexity: O(n). Space Complexity: O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 281, 'file_type': 'pdf'}, page_content='Problem-42\\u2003\\u2003Given two trees how do we check whether the trees are isomorphic to each\\nother or not?\\nSolution:\\nTwo binary trees root1 and root2 are isomorphic if they have the same structure. The values of\\nthe nodes does not affect whether two trees are isomorphic or not. In the diagram below, the tree\\nin the middle is not isomorphic to the other trees, but the tree on the right is isomorphic to the tree\\non the left.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-43\\u2003\\u2003Given two trees how do we check whether they are quasi-isomorphic to each\\nother or not?\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 282, 'file_type': 'pdf'}, page_content='Two trees root1 and root2 are quasi-isomorphic if root1 can be transformed into root2 by\\nswapping the left and right children of some of the nodes of root1. Data in the nodes are not\\nimportant in determining quasi-isomorphism; only the shape is important. The trees below are\\nquasi-isomorphic because if the children of the nodes on the left are swapped, the tree on the right\\nis obtained.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-44\\u2003\\u2003A full k –ary tree is a tree where each node has either 0 or k children. Given an\\narray which contains the preorder traversal of full k –ary tree, give an algorithm for\\nconstructing the full k –ary tree.\\nSolution: In k –ary tree, for a node at ith position its children will be at k * i + 1 to k * i + k. For\\nexample, the example below is for full 3-ary tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 283, 'file_type': 'pdf'}, page_content='As we have seen, in preorder traversal first left subtree is processed then followed by root node\\nand right subtree. Because of this, to construct a full k-ary, we just need to keep on creating the\\nnodes without bothering about the previous constructed nodes. We can use this trick to build the\\ntree recursively by using one global index. The declaration for k-ary tree can be given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 284, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), where n is the size of the pre-order array. This is because we are moving\\nsequentially and not visiting the already constructed nodes.\\n6.8 Threaded Binary Tree Traversals (Stack or Queue-less Traversals)\\nIn earlier sections we have seen that, preorder, inorder and postorder binary tree traversals used\\nstacks and level order traversals used queues as an auxiliary data structure. In this section we\\nwill discuss new traversal algorithms which do not need both stacks and queues. Such traversal'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 285, 'file_type': 'pdf'}, page_content='algorithms are called threaded binary tree traversals or stack/queue – less traversals.\\nIssues with Regular Binary Tree Traversals\\n•\\nThe storage space required for the stack and queue is large.\\n•\\nThe majority of pointers in any binary tree are NULL. For example, a binary tree\\nwith n nodes has n + 1 NULL pointers and these were wasted.\\n•\\nIt is difficult to find successor node (preorder, inorder and postorder successors) for\\na given node.\\nMotivation for Threaded Binary Trees\\nTo solve these problems, one idea is to store some useful information in NULL pointers. If we\\nobserve the previous traversals carefully, stack/ queue is required because we have to record the\\ncurrent position in order to move to the right subtree after processing the left subtree. If we store\\nthe useful information in NULL pointers, then we don’t have to store such information in stack/\\nqueue.\\nThe binary trees which store such information in NULL pointers are called threaded binary trees.\\nFrom the above discussion, let us assume that we want to store some useful information in NULL'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 286, 'file_type': 'pdf'}, page_content='pointers. The next question is what to store?\\nThe common convention is to put predecessor/successor information. That means, if we are\\ndealing with preorder traversals, then for a given node, NULL left pointer will contain preorder\\npredecessor information and NULL right pointer will contain preorder successor information.\\nThese special pointers are called threads.\\nClassifying Threaded Binary Trees\\nThe classification is based on whether we are storing useful information in both NULL pointers or\\nonly in one of them.\\n•\\nIf we store predecessor information in NULL left pointers only, then we can call\\nsuch binary trees left threaded binary trees.\\n•\\nIf we store successor information in NULL right pointers only, then we can call such\\nbinary trees right threaded binary trees.\\n•\\nIf we store predecessor information in NULL left pointers and successor information\\nin NULL right pointers, then we can call such binary trees fully threaded binary\\ntrees or simply threaded binary trees.\\nNote: For the remaining discussion we consider only (fully) threaded binary trees.\\nTypes of Threaded Binary Trees\\nBased on above discussion we get three representations for threaded binary trees.\\n•\\nPreorder Threaded Binary Trees: NULL left pointer will contain PreOrder\\npredecessor information and NULL right pointer will contain PreOrder successor\\ninformation.\\n•\\nInorder Threaded Binary Trees: NULL left pointer will contain InOrder\\npredecessor information and NULL right pointer will contain InOrder successor\\ninformation.\\n•\\nPostorder Threaded Binary Trees: NULL left pointer will contain PostOrder\\npredecessor information and NULL right pointer will contain PostOrder successor\\ninformation.\\nNote: As the representations are similar, for the remaining discussion we will use InOrder\\nthreaded binary trees.\\nThreaded Binary Tree structure\\nAny program examining the tree must be able to differentiate between a regular left/right pointer'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 287, 'file_type': 'pdf'}, page_content='and a thread. To do this, we use two additional fields in each node, giving us, for threaded trees,\\nnodes of the following form:\\nDifference between Binary Tree and Threaded Binary Tree Structures\\nNote: Similarly, we can define preorder/postorder differences as well.\\nAs an example, let us try representing a tree in inorder threaded binary tree form. The tree below\\nshows what an inorder threaded binary tree will look like. The dotted arrows indicate the\\nthreads. If we observe, the left pointer of left most node (2) and right pointer of right most node\\n(31) are hanging.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 288, 'file_type': 'pdf'}, page_content='What should leftmost and rightmost pointers point to?\\nIn the representation of a threaded binary tree, it is convenient to use a special node Dummy\\nwhich is always present even for an empty tree. Note that right tag of Dummy node is 1 and its\\nright child points to itself.\\nWith this convention the above tree can be represented as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 289, 'file_type': 'pdf'}, page_content='Finding Inorder Successor in Inorder Threaded Binary Tree\\nTo find inorder successor of a given node without using a stack, assume that the node for which\\nwe want to find the inorder successor is P.\\nStrategy: If P has a no right subtree, then return the right child of P. If P has right subtree, then\\nreturn the left of the nearest node whose left subtree contains P.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 290, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nInorder Traversal in Inorder Threaded Binary Tree\\nWe can start with dummy node and call InorderSuccessor() to visit each node until we reach\\ndummy node.\\nAlternative coding:\\nTime Complexity: O(n). Space Complexity: O(1).\\nFinding PreOrder Successor in InOrder Threaded Binary Tree\\nStrategy: If P has a left subtree, then return the left child of P. If P has no left subtree, then return\\nthe right child of the nearest node whose right subtree contains P.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 291, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nPreOrder Traversal of InOrder Threaded Binary Tree\\nAs in inorder traversal, start with dummy node and call PreorderSuccessorf) to visit each node\\nuntil we get dummy node again.\\nAlternative coding:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 292, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nNote: From the above discussion, it should be clear that inorder and preorder successor finding\\nis easy with threaded binary trees. But finding postorder successor is very difficult if we do not\\nuse stack.\\nInsertion of Nodes in InOrder Threaded Binary Trees\\nFor simplicity, let us assume that there are two nodes P and Q and we want to attach Q to right of\\nP. For this we will have two cases.\\n•\\nNode P does not have right child: In this case we just need to attach Q to P and\\nchange its left and right pointers.\\n•\\nNode P has right child (say, R): In this case we need to traverse R’s left subtree and\\nfind the left most node and then update the left and right pointer of that node (as\\nshown below).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 293, 'file_type': 'pdf'}, page_content='Tsao Ng\\nXo wo\\nby? bY?\\n\\n‘void InsertRightndnorderTBT|structThreadedBinaryTreeNode *, struct ThreadedBinaryTreeNode \"Q\\\\\\nstruct ThreadedBinaryTreeNode Temp;\\nQuright» P-right;\\n(QaRTag= P-RTag,\\nQuiet= P;\\nQui Tag = 0;\\nPeoright = Q;\\nPaRTag =I;\\ni(Q--RTag == 1) | | [Case-2\\nTemp = Q-right;\\nWwhile(Temp-LTag)\\nTemp = Temp—left\\nTemp-left = Q;\\n}\\n|\\n\\n‘Time Complexity: O(n). Space Complexity: O(1)..'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 294, 'file_type': 'pdf'}, page_content='Threaded Binary Trees: Problems & Solutions\\nProblem-45\\u2003\\u2003For a given binary tree (not threaded) how do we find the preorder successor?\\nSolution: For solving this problem, we need to use an auxiliary stack S. On the first call, the\\nparameter node is a pointer to the head of the tree, and thereafter its value is NULL. Since we are\\nsimply asking for the successor of the node we got the last time we called the function.\\nIt is necessary that the contents of the stack S and the pointer P to the last node “visited” are\\npreserved from one call of the function to the next; they are defined as static variables.\\nProblem-46\\u2003\\u2003For a given binary tree (not threaded) how do we find the inorder successor?\\nSolution: Similar to the above discussion, we can find the inorder successor of a node as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 295, 'file_type': 'pdf'}, page_content='6.9 Expression Trees\\nA tree representing an expression is called an expression tree. In expression trees, leaf nodes are\\noperands and non-leaf nodes are operators. That means, an expression tree is a binary tree where\\ninternal nodes are operators and leaves are operands. An expression tree consists of binary\\nexpression. But for a u-nary operator, one subtree will be empty. The figure below shows a\\nsimple expression tree for (A + B * C) / D.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 296, 'file_type': 'pdf'}, page_content='Algorithm for Building Expression Tree from Postfix Expression'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 297, 'file_type': 'pdf'}, page_content='Example: Assume that one symbol is read at a time. If the symbol is an operand, we create a tree\\nnode and push a pointer to it onto a stack. If the symbol is an operator, pop pointers to two trees\\nT1 and T2 from the stack (T1 is popped first) and form a new tree whose root is the operator and\\nwhose left and right children point to T2 and T1 respectively. A pointer to this new tree is then\\npushed onto the stack.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 298, 'file_type': 'pdf'}, page_content='As an example, assume the input is A B C * + D /. The first three symbols are operands, so create\\ntree nodes and push pointers to them onto a stack as shown below.\\nNext, an operator ‘*’ is read, so two pointers to trees are popped, a new tree is formed and a\\npointer to it is pushed onto the stack.\\nNext, an operator ‘+’ is read, so two pointers to trees are popped, a new tree is formed and a\\npointer to it is pushed onto the stack.\\nNext, an operand ‘D’ is read, a one-node tree is created and a pointer to the corresponding tree is\\npushed onto the stack.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 299, 'file_type': 'pdf'}, page_content='Finally, the last symbol (‘/’) is read, two trees are merged and a pointer to the final tree is left on\\nthe stack.\\n6.10 XOR Trees\\nThis concept is similar to memory efficient doubly linked lists of Linked Lists chapter. Also, like\\nthreaded binary trees this representation does not need stacks or queues for traversing the trees.\\nThis representation is used for traversing back (to parent) and forth (to children) using ⊕\\noperation. To represent the same in XOR trees, for each node below are the rules used for\\nrepresentation:\\n•\\nEach nodes left will have the ⊕ of its parent and its left children.\\n•\\nEach nodes right will have the ⊕ of its parent and its right children.\\n•\\nThe root nodes parent is NULL and also leaf nodes children are NULL nodes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 300, 'file_type': 'pdf'}, page_content='Based on the above rules and discussion, the tree can be represented as:\\nThe major objective of this presentation is the ability to move to parent as well to children. Now,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 301, 'file_type': 'pdf'}, page_content='let us see how to use this representation for traversing the tree. For example, if we are at node B\\nand want to move to its parent node A, then we just need to perform ⊕ on its left content with its\\nleft child address (we can use right child also for going to parent node).\\nSimilarly, if we want to move to its child (say, left child D) then we have to perform ⊕ on its left\\ncontent with its parent node address. One important point that we need to understand about this\\nrepresentation is: When we are at node B, how do we know the address of its children D? Since\\nthe traversal starts at node root node, we can apply ⊕ on root’s left content with NULL. As a\\nresult we get its left child, B. When we are at B, we can apply ⊕ on its left content with A\\naddress.\\n6.11 Binary Search Trees (BSTs)\\nWhy Binary Search Trees?\\nIn previous sections we have discussed different tree representations and in all of them we did\\nnot impose any restriction on the nodes data. As a result, to search for an element we need to\\ncheck both in left subtree and in right subtree. Due to this, the worst case complexity of search\\noperation is O(n).\\nIn this section, we will discuss another variant of binary trees: Binary Search Trees (BSTs). As\\nthe name suggests, the main use of this representation is for searching. In this representation we\\nimpose restriction on the kind of data a node can contain. As a result, it reduces the worst case\\naverage search operation to O(logn).\\nBinary Search Tree Property\\nIn binary search trees, all the left subtree elements should be less than root data and all the right\\nsubtree elements should be greater than root data. This is called binary search tree property. Note\\nthat, this property should be satisfied at every node in the tree.\\n•\\nThe left subtree of a node contains only nodes with keys less than the nodes key.\\n•\\nThe right subtree of a node contains only nodes with keys greater than the nodes key.\\n•\\nBoth the left and right subtrees must also be binary search trees.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 302, 'file_type': 'pdf'}, page_content='Example: The left tree is a binary search tree and the right tree is not a binary search tree (at\\nnode 6 it’s not satisfying the binary search tree property).\\nBinary Search Tree Declaration\\nThere is no difference between regular binary tree declaration and binary search tree declaration.\\nThe difference is only in data but not in structure. But for our convenience we change the structure\\nname as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 303, 'file_type': 'pdf'}, page_content='Operations on Binary Search Trees\\nMain operations: Following are the main operations that are supported by binary search trees:\\n•\\nFind/ Find Minimum / Find Maximum element in binary search trees\\n•\\nInserting an element in binary search trees\\n•\\nDeleting an element from binary search trees\\nAuxiliary operations: Checking whether the given tree is a binary search tree or not\\n•\\nFinding kth-smallest element in tree\\n•\\nSorting the elements of binary search tree and many more\\nImportant Notes on Binary Search Trees\\n•\\nSince root data is always between left subtree data and right subtree data,\\nperforming inorder traversal on binary search tree produces a sorted list.\\n•\\nWhile solving problems on binary search trees, first we process left subtree, then\\nroot data, and finally we process right subtree. This means, depending on the\\nproblem, only the intermediate step (processing root data) changes and we do not\\ntouch the first and third steps.\\n•\\nIf we are searching for an element and if the left subtree root data is less than the\\nelement we want to search, then skip it. The same is the case with the right subtree..\\nBecause of this, binary search trees take less time for searching an element than\\nregular binary trees. In other words, the binary search trees consider either left or\\nright subtrees for searching an element but not both.\\n•\\nThe basic operations that can be performed on binary search tree (BST) are\\ninsertion of element, deletion of element, and searching for an element. While\\nperforming these operations on BST the height of the tree gets changed each time.\\nHence there exists variations in time complexities of best case, average case, and\\nworst case.\\n•\\nThe basic operations on a binary search tree take time proportional to the height of\\nthe tree. For a complete binary tree with node n, such operations runs in O(lgn)\\nworst-case time. If the tree is a linear chain of n nodes (skew-tree), however, the\\nsame operations takes O(n) worst-case time.\\nFinding an Element in Binary Search Trees\\nFind operation is straightforward in a BST. Start with the root and keep moving left or right using\\nthe BST property. If the data we are searching is same as nodes data then we return current node.\\nIf the data we are searching is less than nodes data then search left subtree of current node;\\notherwise search right subtree of current node. If the data is not present, we end up in a NULL'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 304, 'file_type': 'pdf'}, page_content='link.\\nTime Complexity: O(n), in worst case (when BST is a skew tree). Space Complexity: O(n), for\\nrecursive stack.\\nNon recursive version of the above algorithm can be given as:\\nTime Complexity: O(n). Space Complexity: O(1).\\nFinding Minimum Element in Binary Search Trees\\nIn BSTs, the minimum element is the left-most node, which does not has left child. In the BST\\nbelow, the minimum element is 4.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 305, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), in worst case (when BST is a left skew tree).\\nSpace Complexity: O(n), for recursive stack.\\nNon recursive version of the above algorithm can be given as:\\nTime Complexity: O(n). Space Complexity: O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 306, 'file_type': 'pdf'}, page_content='Finding Maximum Element in Binary Search Trees\\nIn BSTs, the maximum element is the right-most node, which does not have right child. In the BST\\nbelow, the maximum element is 16.\\nTime Complexity: O(n), in worst case (when BST is a right skew tree).\\nSpace Complexity: O(n), for recursive stack.\\nNon recursive version of the above algorithm can be given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 307, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nWhere is Inorder Predecessor and Successor?\\nWhere is the inorder predecessor and successor of node X in a binary search tree assuming all\\nkeys are distinct?\\nIf X has two children then its inorder predecessor is the maximum value in its left subtree and its\\ninorder successor the minimum value in its right subtree.\\nIf it does not have a left child, then a node’s inorder predecessor is its first left ancestor.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 308, 'file_type': 'pdf'}, page_content='Inserting an Element from Binary Search Tree\\nTo insert data into binary search tree, first we need to find the location for that element. We can\\nfind the location of insertion by following the same mechanism as that of find operation. While\\nfinding the location, if the data is already there then we can simply neglect and come out.\\nOtherwise, insert data at the last location on the path traversed.\\nAs an example let us consider the following tree. The dotted node indicates the element (5) to be\\ninserted. To insert 5, traverse the tree using find function. At node with key 4, we need to go right,\\nbut there is no subtree, so 5 is not in the tree, and this is the correct location for insertion.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 309, 'file_type': 'pdf'}, page_content='Note: In the above code, after inserting an element in subtrees, the tree is returned to its parent.\\nAs a result, the complete tree will get updated.\\nTime Complexity:O(n).\\nSpace Complexity:O(n), for recursive stack. For iterative version, space complexity is O(1).\\nDeleting an Element from Binary Search Tree\\nThe delete operation is more complicated than other operations. This is because the element to be\\ndeleted may not be the leaf node. In this operation also, first we need to find the location of the\\nelement which we want to delete.\\nOnce we have found the node to be deleted, consider the following cases:\\n•\\nIf the element to be deleted is a leaf node: return NULL to its parent. That means\\nmake the corresponding child pointer NULL. In the tree below to delete 5, set NULL'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 310, 'file_type': 'pdf'}, page_content='to its parent node 2.\\n•\\nIf the element to be deleted has one child: In this case we just need to send the\\ncurrent node’s child to its parent. In the tree below, to delete 4, 4 left subtree is set\\nto its parent node 2.\\n•\\nIf the element to be deleted has both children: The general strategy is to replace the\\nkey of this node with the largest element of the left subtree and recursively delete\\nthat node (which is now empty). The largest node in the left subtree cannot have a\\nright child, so the second delete is an easy one. As an example, let us consider the\\nfollowing tree. In the tree below, to delete 8, it is the right child of the root. The key\\nvalue is 8. It is replaced with the largest key in its left subtree (7), and then that\\nnode is deleted as before (second case).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 311, 'file_type': 'pdf'}, page_content='Note: We can replace with minimum element in right subtree also.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 312, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n) for recursive stack. For iterative version, space\\ncomplexity is O(1).\\nBinary Search Trees: Problems & Solutions\\nNote: For ordering related problems with binary search trees and balanced binary search trees,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 313, 'file_type': 'pdf'}, page_content='Inorder traversal has advantages over others as it gives the sorted order.\\nProblem-47\\u2003\\u2003Given pointers to two nodes in a binary search tree, find the lowest common\\nancestor (LCA). Assume that both values already exist in the tree.\\nSolution:\\nThe main idea of the solution is: while traversing BST from root to bottom, the first node we\\nencounter with value between α and β, i.e., α < node → data < β, is the Least Common\\nAncestor(LCA) of α and β (where α < β). So just traverse the BST in pre-order, and if we find a\\nnode with value in between α and β, then that node is the LCA. If its value is greater than both α\\nand β, then the LCA lies on the left side of the node, and if its value is smaller than both α and β,\\nthen the LCA lies on the right side.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 314, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for skew trees.\\nProblem-48\\u2003\\u2003Give an algorithm for finding the shortest path between two nodes in a BST.\\nSolution: It’s nothing but finding the LCA of two nodes in BST.\\nProblem-49\\u2003\\u2003Give an algorithm for counting the number of BSTs possible with n nodes.\\nSolution: This is a DP problem. Refer to chapter on Dynamic Programming for the algorithm.\\nProblem-50\\u2003\\u2003Give an algorithm to check whether the given binary tree is a BST or not.\\nSolution:\\nConsider the following simple program. For each node, check if the node on its left is smaller and\\ncheck if the node on its right is greater. This approach is wrong as this will return true for binary\\ntree below. Checking only at current node is not enough.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 315, 'file_type': 'pdf'}, page_content='Problem-51\\u2003\\u2003Can we think of getting the correct algorithm?\\nSolution: For each node, check if max value in left subtree is smaller than the current node data\\nand min value in right subtree greater than the node data. It is assumed that we have helper\\nfunctions FindMin() and FindMax() that return the min or max integer value from a non-empty\\ntree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 316, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). Space Complexity: O(n).\\nProblem-52\\u2003\\u2003Can we improve the complexity of Problem-51?\\nSolution: Yes. A better solution is to look at each node only once. The trick is to write a utility\\nhelper function IsBSTUtil(struct BinaryTreeNode* root, int min, int max) that traverses down the\\ntree keeping track of the narrowing min and max allowed values as it goes, looking at each node\\nonly once. The initial values for min and max should be INT_MIN and INT_MAX – they narrow\\nfrom there.\\nTime Complexity: O(n). Space Complexity: O(n), for stack space.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 317, 'file_type': 'pdf'}, page_content='Problem-53\\u2003\\u2003Can we further improve the complexity of Problem-51?\\nSolution: Yes, by using inorder traversal. The idea behind this solution is that inorder traversal of\\nBST produces sorted lists. While traversing the BST in inorder, at each node check the condition\\nthat its key value should be greater than the key value of its previous visited node. Also, we need\\nto initialize the prev with possible minimum integer value (say, INT_MIN).\\nTime Complexity: O(n). Space Complexity: O(n), for stack space.\\nProblem-54\\u2003\\u2003Give an algorithm for converting BST to circular DLL with space complexity\\nO(1).\\nSolution: Convert left and right subtrees to DLLs and maintain end of those lists. Then, adjust the\\npointers.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 318, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nProblem-55\\u2003\\u2003For Problem-54, is there any other way of solving it?\\nSolution: Yes. There is an alternative solution based on the divide and conquer method which is\\nquite neat.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 319, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nProblem-56\\u2003\\u2003Given a sorted doubly linked list, give an algorithm for converting it into\\nbalanced binary search tree.\\nSolution: Find the middle node and adjust the pointers.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 320, 'file_type': 'pdf'}, page_content='Time Complexity: 2T(n/2) + O(n) [for finding the middle node] = O(nlogn).\\nNote: For FindMiddleNode function refer Linked Lists chapter.\\nProblem-57\\u2003\\u2003Given a sorted array, give an algorithm for converting the array to BST.\\nSolution: If we have to choose an array element to be the root of a balanced BST, which element\\nshould we pick? The root of a balanced BST should be the middle element from the sorted array.\\nWe would pick the middle element from the sorted array in each iteration. We then create a node\\nin the tree initialized with this element. After the element is chosen, what is left? Could you\\nidentify the sub-problems within the problem?\\nThere are two arrays left – the one on its left and the one on its right. These two arrays are the\\nsub-problems of the original problem, since both of them are sorted. Furthermore, they are\\nsubtrees of the current node’s left and right child.\\nThe code below creates a balanced BST from the sorted array in O(n) time (n is the number of\\nelements in the array). Compare how similar the code is to a binary search algorithm. Both are\\nusing the divide and conquer methodology.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 321, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for stack space.\\nProblem-58\\u2003\\u2003Given a singly linked list where elements are sorted in ascending order, convert\\nit to a height balanced BST.\\nSolution: A naive way is to apply the Problem-56 solution directly. In each recursive call, we\\nwould have to traverse half of the list’s length to find the middle element. The run time complexity\\nis clearly O(nlogn), where n is the total number of elements in the list. This is because each level\\nof recursive call requires a total of n/2 traversal steps in the list, and there are a total of logn\\nnumber of levels (ie, the height of the balanced tree).\\nProblem-59\\u2003\\u2003For Problem-58, can we improve the complexity?\\nSolution: Hint: How about inserting nodes following the list order? If we can achieve this, we no\\nlonger need to find the middle element as we are able to traverse the list while inserting nodes to\\nthe tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 322, 'file_type': 'pdf'}, page_content='Best Solution: As usual, the best solution requires us to think from another perspective. In other\\nwords, we no longer create nodes in the tree using the top-down approach. Create nodes bottom-\\nup, and assign them to their parents. The bottom-up approach enables us to access the list in its\\norder while creating nodes [42].\\nIsn’t the bottom-up approach precise? Any time we are stuck with the top-down approach, we can\\ngive bottom-up a try. Although the bottom-up approach is not the most natural way we think, it is\\nhelpful in some cases. However, we should prefer top-down instead of bottom-up in general,\\nsince the latter is more difficult to verify.\\nBelow is the code for converting a singly linked list to a balanced BST. Please note that the\\nalgorithm requires the list length to be passed in as the function parameters. The list length can be\\nfound in O(n) time by traversing the entire list once. The recursive calls traverse the list and\\ncreate tree nodes by the list order, which also takes O(n) time. Therefore, the overall run time\\ncomplexity is still O(n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 323, 'file_type': 'pdf'}, page_content='Problem-60\\u2003\\u2003Give an algorithm for finding the kth smallest element in BST.\\nSolution: The idea behind this solution is that, inorder traversal of BST produces sorted lists.\\nWhile traversing the BST in inorder, keep track of the number of elements visited.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-61\\u2003\\u2003Floor and ceiling: If a given key is less than the key at the root of a BST then the\\nfloor of the key (the largest key in the BST less than or equal to the key) must be in the left\\nsubtree. If the key is greater than the key at the root, then the floor of the key could be in the\\nright subtree, but only if there is a key smaller than or equal to the key in the right subtree;\\nif not (or if the key is equal to the the key at the root) then the key at the root is the floor of\\nthe key. Finding the ceiling is similar, with interchanging right and left. For example, if the\\nsorted with input array is {1, 2, 8, 10, 10, 12, 19}, then\\nFor x = 0: floor doesn’t exist in array, ceil = 1, For x = 1: floor = 1, ceil = 1\\nFor x = 5: floor =2, ceil = 8, For x = 20: floor = 19, ceil doesn’t exist in array\\nSolution: The idea behind this solution is that, inorder traversal of BST produces sorted lists.\\nWhile traversing the BST in inorder, keep track of the values being visited. If the roots data is\\ngreater than the given value then return the previous value which we have maintained during\\ntraversal. If the roots data is equal to the given data then return root data.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 324, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for stack space.\\nFor ceiling, we just need to call the right subtree first, followed by left subtree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 325, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for stack space.\\nProblem-62\\u2003\\u2003Give an algorithm for finding the union and intersection of BSTs. Assume parent\\npointers are available (say threaded binary trees). Also, assume the lengths of two BSTs\\nare m and n respectively.\\nSolution: If parent pointers are available then the problem is same as merging of two sorted lists.\\nThis is because if we call inorder successor each time we get the next highest element. It’s just a\\nmatter of which InorderSuccessor to call.\\nTime Complexity: O(m + n). Space complexity: O(1).\\nProblem-63\\u2003\\u2003For Problem-62, what if parent pointers are not available?\\nSolution: If parent pointers are not available, the BSTs can be converted to linked lists and then\\nmerged.\\n1\\nConvert both the BSTs into sorted doubly linked lists in O(n + m) time. This produces\\n2 sorted lists.\\n2\\nMerge the two double linked lists into one and also maintain the count of total\\nelements in O(n + m) time.\\n3\\nConvert the sorted doubly linked list into height balanced tree in O(n + m) time.\\nProblem-64\\u2003\\u2003For Problem-62, is there any alternative way of solving the problem?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 326, 'file_type': 'pdf'}, page_content='Solution: Yes, by using inorder traversal.\\n•\\nPerform inorder traversal on one of the BSTs.\\n•\\nWhile performing the traversal store them in table (hash table).\\n•\\nAfter completion of the traversal of first BST, start traversal of second BST and\\ncompare them with hash table contents.\\nTime Complexity: O(m + n). Space Complexity: O(Max(m,n)).\\nProblem-65\\u2003\\u2003Given a BST and two numbers K1 and K2, give an algorithm for printing all the\\nelements of BST in the range K1 and K2.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n), for stack space.\\nProblem-66\\u2003\\u2003For Problem-65, is there any alternative way of solving the problem?\\nSolution: We can use level order traversal: while adding the elements to queue check for the\\nrange.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 327, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for queue.\\nProblem-67\\u2003\\u2003For Problem-65, can we still think of an alternative way to solve the problem?\\nSolution: First locate K1 with normal binary search and after that use InOrder successor until we\\nencounter K2. For algorithm, refer to problems section of threaded binary trees.\\nProblem-68\\u2003\\u2003Given root of a Binary Search tree, trim the tree, so that all elements returned in\\nthe new tree are between the inputs A and B.\\nSolution: It’s just another way of asking Problem-65.\\nProblem-69\\u2003\\u2003Given two BSTs, check whether the elements of them are the same or not. For\\nexample: two BSTs with data 10 5 20 15 30 and 10 20 15 30 5 should return true and the\\ndataset with 10 5 20 15 30 and 10 15 30 20 5 should return false. Note: BSTs data can be\\nin any order.\\nSolution: One simple way is performing an inorder traversal on first tree and storing its data in\\nhash table. As a second step, perform inorder traversal on second tree and check whether that\\ndata is already there in hash table or not (if it exists in hash table then mark it with -1 or some\\nunique value).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 328, 'file_type': 'pdf'}, page_content='During the traversal of second tree if we find any mismatch return false. After traversal of second\\ntree check whether it has all -1s in the hash table or not (this ensures extra data available in\\nsecond tree).\\nTime Complexity: O(max(m, n)), where m and n are the number of elements in first and second\\nBST. Space Complexity: O(max(m,n)). This depends on the size of the first tree.\\nProblem-70\\u2003\\u2003For Problem-69, can we reduce the time complexity?\\nSolution: Instead of performing the traversals one after the other, we can perform in – order\\ntraversal of both the trees in parallel. Since the in – order traversal gives the sorted list, we can\\ncheck whether both the trees are generating the same sequence or not.\\nTime Complexity: O(max(m,n)). Space Complexity: O(1). This depends on the size of the first\\ntree.\\nProblem-71\\u2003\\u2003For the key values 1... n, how many structurally unique BSTs are possible that\\nstore those keys.\\nSolution: Strategy: consider that each value could be the root. Recursively find the size of the left\\nand right subtrees.\\nProblem-72\\u2003\\u2003Given a BST of size n, in which each node r has an additional field r → size,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 329, 'file_type': 'pdf'}, page_content='the number of the keys in the sub-tree rooted at r (including the root node r). Give an O(h)\\nalgorithm GreaterthanConstant(r,k) to find the number of keys that are strictly greater than\\nk (h is the height of the binary search tree).\\nSolution:\\nThe suggested algorithm works well if the key is a unique value for each node. Otherwise when\\nreaching k=r→data, we should start a process of moving to the right until reaching a node y with\\na key that is bigger then k, and then we should return keysCount + y→size. Time Complexity:\\nO(h) where h=O(n) in the worst case and O(logn) in the average case.\\n6.12 Balanced Binary Search Trees\\nIn earlier sections we have seen different trees whose worst case complexity is O(n), where n is\\nthe number of nodes in the tree. This happens when the trees are skew trees. In this section we\\nwill try to reduce this worst case complexity to O(logn) by imposing restrictions on the heights.\\nIn general, the height balanced trees are represented with HB(k), where k is the difference\\nbetween left subtree height and right subtree height. Sometimes k is called balance factor.\\nFull Balanced Binary Search Trees'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 330, 'file_type': 'pdf'}, page_content='In HB(k), if k = 0 (if balance factor is zero), then we call such binary search trees as full\\nbalanced binary search trees. That means, in HB(0) binary search tree, the difference between left\\nsubtree height and right subtree height should be at most zero. This ensures that the tree is a full\\nbinary tree. For example,\\nNote: For constructing HB(0) tree refer to Problems section.\\n6.13 AVL (Adelson-Velskii and Landis) Trees\\nIn HB(k), if k = 1 (if balance factor is one), such a binary search tree is called an AVL tree. That\\nmeans an AVL tree is a binary search tree with a balance condition: the difference between left\\nsubtree height and right subtree height is at most 1.\\nProperties of AVL Trees\\nA binary tree is said to be an AVL tree, if:\\n•\\nIt is a binary search tree, and\\n•\\nFor any node X, the height of left subtree of X and height of right subtree of X differ\\nby at most 1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 331, 'file_type': 'pdf'}, page_content='As an example, among the above binary search trees, the left one is not an AVL tree, whereas the\\nright binary search tree is an AVL tree.\\nMinimum/Maximum Number of Nodes in AVL Tree\\nFor simplicity let us assume that the height of an AVL tree is h and N(K) indicates the number of\\nnodes in AVL tree with height h. To get the minimum number of nodes with height h, we should\\nfill the tree with the minimum number of nodes possible. That means if we fill the left subtree\\nwith height h – 1 then we should fill the right subtree with height h – 2. As a result, the minimum\\nnumber of nodes with height h is:\\nN(h) = N(h – 1) + N(h – 2) + 1\\nIn the above equation:\\n•\\nN(h – 1) indicates the minimum number of nodes with height h – 1.\\n•\\nN(h – 2) indicates the minimum number of nodes with height h – 2.\\n•\\nIn the above expression, “1” indicates the current node.\\nWe can give N(h – 1) either for left subtree or right subtree. Solving the above recurrence gives:\\nN(h) = O(1.618h) ⇒ h = 1.44logn ≈ O(logn)\\nWhere n is the number of nodes in AVL tree. Also, the above derivation says that the maximum\\nheight in AVL trees is O(logn). Similarly, to get maximum number of nodes, we need to fill both\\nleft and right subtrees with height h – 1. As a result, we get:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 332, 'file_type': 'pdf'}, page_content='N(h) = N(h – 1) + N(h – 1) + 1 = 2N(h – 1) + 1\\nThe above expression defines the case of full binary tree. Solving the recurrence we get:\\nN(h) = O(2h) ⇒ h = logn ≈ O(logn)\\n∴ In both the cases, AVL tree property is ensuring that the height of an AVL tree with n nodes is\\nO(logn).\\nAVL Tree Declaration\\nSince AVL tree is a BST, the declaration of AVL is similar to that of BST. But just to simplify the\\noperations, we also include the height as part of the declaration.\\nFinding the Height of an AVL tree'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 333, 'file_type': 'pdf'}, page_content='Time Complexity: O(1).\\nRotations\\nWhen the tree structure changes (e.g., with insertion or deletion), we need to modify the tree to\\nrestore the AVL tree property. This can be done using single rotations or double rotations. Since\\nan insertion/deletion involves adding/deleting a single node, this can only increase/decrease the\\nheight of a subtree by 1.\\nSo, if the AVL tree property is violated at a node X, it means that the heights of left(X) and\\nright(X) differ by exactly 2. This is because, if we balance the AVL tree every time, then at any\\npoint, the difference in heights of left(X) and right(X) differ by exactly 2. Rotations is the\\ntechnique used for restoring the AVL tree property. This means, we need to apply the rotations for\\nthe node X.\\nObservation: One important observation is that, after an insertion, only nodes that are on the path\\nfrom the insertion point to the root might have their balances altered, because only those nodes\\nhave their subtrees altered. To restore the AVL tree property, we start at the insertion point and\\nkeep going to the root of the tree.\\nWhile moving to the root, we need to consider the first node that is not satisfying the AVL\\nproperty. From that node onwards, every node on the path to the root will have the issue.\\nAlso, if we fix the issue for that first node, then all other nodes on the path to the root will\\nautomatically satisfy the AVL tree property. That means we always need to care for the first node\\nthat is not satisfying the AVL property on the path from the insertion point to the root and fix it.\\nTypes of Violations\\nLet us assume the node that must be rebalanced is X. Since any node has at most two children, and\\na height imbalance requires that X’s two subtree heights differ by two, we can observe that a\\nviolation might occur in four cases:\\n1.\\nAn insertion into the left subtree of the left child of X.\\n2.\\nAn insertion into the right subtree of the left child of X.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 334, 'file_type': 'pdf'}, page_content='3.\\nAn insertion into the left subtree of the right child of X.\\n4.\\nAn insertion into the right subtree of the right child of X.\\nCases 1 and 4 are symmetric and easily solved with single rotations. Similarly, cases 2 and 3 are\\nalso symmetric and can be solved with double rotations (needs two single rotations).\\nSingle Rotations\\nLeft Left Rotation (LL Rotation) [Case-1]: In the case below, node X is not satisfying the AVL\\ntree property. As discussed earlier, the rotation does not have to be done at the root of a tree. In\\ngeneral, we start at the node inserted and travel up the tree, updating the balance information at\\nevery node on the path.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 335, 'file_type': 'pdf'}, page_content='For example, in the figure above, after the insertion of 7 in the original AVL tree on the left, node\\n9 becomes unbalanced. So, we do a single left-left rotation at 9. As a result we get the tree on the\\nright.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 336, 'file_type': 'pdf'}, page_content='Time Complexity: O(1). Space Complexity: O(1).\\nRight Right Rotation (RR Rotation) [Case-4]: In this case, node X is not satisfying the AVL\\ntree property.\\nFor example, in the figure, after the insertion of 29 in the original AVL tree on the left, node 15\\nbecomes unbalanced. So, we do a single right-right rotation at 15. As a result we get the tree on\\nthe right.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 337, 'file_type': 'pdf'}, page_content='Time Complexity: O(1). Space Complexity: O(1).\\nDouble Rotations\\nLeft Right Rotation (LR Rotation) [Case-2]: For case-2 and case-3 single rotation does not fix\\nthe problem. We need to perform two rotations.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 338, 'file_type': 'pdf'}, page_content='As an example, let us consider the following tree: The insertion of 7 is creating the case-2\\nscenario and the right side tree is the one after the double rotation.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 339, 'file_type': 'pdf'}, page_content='Code for left-right double rotation can be given as:\\nRight Left Rotation (RL Rotation) [Case-3]: Similar to case-2, we need to perform two\\nrotations to fix this scenario.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 340, 'file_type': 'pdf'}, page_content='As an example, let us consider the following tree: The insertion of 6 is creating the case-3\\nscenario and the right side tree is the one after the double rotation.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 341, 'file_type': 'pdf'}, page_content='Insertion into an AVL tree\\nInsertion into an AVL tree is similar to a BST insertion. After inserting the element, we just need\\nto check whether there is any height imbalance. If there is an imbalance, call the appropriate\\nrotation functions.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 342, 'file_type': 'pdf'}, page_content='Time Complexity: O(logn). Space Complexity: O(logn).\\nAVL Trees: Problems & Solutions'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 343, 'file_type': 'pdf'}, page_content='Problem-73\\u2003\\u2003Given a height h, give an algorithm for generating the HB(0).\\nSolution: As we have discussed, HB(0) is nothing but generating full binary tree. In full binary\\ntree the number of nodes with height h is: 2h+1 – 1 (let us assume that the height of a tree with one\\nnode is 0). As a result the nodes can be numbered as: 1 to 2h+1 – 1.\\nTime Complexity: O(n).\\nSpace Complexity: O(logn), where logn indicates the maximum stack size which is equal to\\nheight of tree.\\nProblem-74\\u2003\\u2003Is there any alternative way of solving Problem-73?\\nSolution: Yes, we can solve it following Mergesort logic. That means, instead of working with\\nheight, we can take the range. With this approach we do not need any global counter to be\\nmaintained.\\nThe initial call to the BuildHBO function could be: BuildHB0(1, 1 ≪ h). 1 ≪ h does the shift\\noperation for calculating the 2h+1 – 1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 344, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(login). Where logn indicates maximum stack size\\nwhich is equal to the height of the tree.\\nProblem-75\\u2003\\u2003Construct minimal AVL trees of height 0,1,2,3,4, and 5. What is the number of\\nnodes in a minimal AVL tree of height 6?\\nSolution Let N(h) be the number of nodes in a minimal AVL tree with height h.\\nProblem-76\\u2003\\u2003For Problem-73, how many different shapes can there be of a minimal AVL tree'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 345, 'file_type': 'pdf'}, page_content='of height h?\\nSolution: Let NS(h) be the number of different shapes of a minimal AVL tree of height h.\\nProblem-77\\u2003\\u2003Given a binary search tree, check whether it is an AVL tree or not?\\nSolution: Let us assume that IsAVL is the function which checks whether the given binary search\\ntree is an AVL tree or not. IsAVL returns –1 if the tree is not an AVL tree. During the checks each\\nnode sends its height to its parent.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 346, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-78\\u2003\\u2003Given a height h, give an algorithm to generate an AVL tree with minimum\\nnumber of nodes.\\nSolution: To get minimum number of nodes, fill one level with h – 1 and the other with h – 2.\\nProblem-79\\u2003\\u2003Given an AVL tree with n integer items and two integers a and b, where a and b\\ncan be any integers with a <= b. Implement an algorithm to count the number of nodes in\\nthe range [a,b].\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 347, 'file_type': 'pdf'}, page_content='The idea is to make use of the recursive property of binary search trees. There are three cases to\\nconsider: whether the current node is in the range [a, b], on the left side of the range [a, b], or on\\nthe right side of the range [a,b]. Only subtrees that possibly contain the nodes will be processed\\nunder each of the three cases.\\nThe complexity is similar to in – order traversal of the tree but skipping left or right sub-trees\\nwhen they do not contain any answers. So in the worst case, if the range covers all the nodes in\\nthe tree, we need to traverse all the n nodes to get the answer. The worst time complexity is\\ntherefore O(n).\\nIf the range is small, which only covers a few elements in a small subtree at the bottom of the tree,\\nthe time complexity will be O(h) = O(logn), where h is the height of the tree. This is because only\\na single path is traversed to reach the small subtree at the bottom and many higher level subtrees'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 348, 'file_type': 'pdf'}, page_content='have been pruned along the way.\\nNote: Refer similar problem in BST.\\nProblem-80\\u2003\\u2003Given a BST (applicable to AVL trees as well) where each node contains two\\ndata elements (its data and also the number of nodes in its subtrees) as shown below.\\nConvert the tree to another BST by replacing the second data element (number of nodes in\\nits subtrees) with previous node data in inorder traversal. Note that each node is merged\\nwith inorder previous node data. Also make sure that conversion happens in-place.\\nSolution: The simplest way is to use level order traversal. If the number of elements in the left\\nsubtree is greater than the number of elements in the right subtree, find the maximum element in\\nthe left subtree and replace the current node second data element with it. Similarly, if the number\\nof elements in the left subtree is less than the number of elements in the right subtree, find the\\nminimum element in the right subtree and replace the current node second data element with it.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 349, 'file_type': 'pdf'}, page_content='Time Complexity: O(nlogn) on average since BST takes O(logn) on average to find the maximum\\nor minimum element. Space Complexity: O(n). Since, in the worst case, all the nodes on the entire\\nlast level could be in the queue simultaneously.\\nProblem-81\\u2003\\u2003Can we reduce time complexity for the previous problem?\\nSolution: Let us try using an approach that is similar to what we followed in Problem-60. The\\nidea behind this solution is that inorder traversal of BST produces sorted lists. While traversing\\nthe BST in inorder, keep track of the elements visited and merge them.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 350, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nSpace Complexity: O(1). Note that, we are still having recursive stack space for inorder\\ntraversal.\\nProblem-82\\u2003\\u2003Given a BST and a key, find the element in the BST which is closest to the given\\nkey.\\nSolution: As a simple solution, we can use level-order traversal and for every element compute\\nthe difference between the given key and the element’s value. If that difference is less than the\\nprevious maintained difference, then update the difference with this new minimum value. With\\nthis approach, at the end of the traversal we will get the element which is closest to the given key.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 351, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-83\\u2003\\u2003For Problem-82, can we solve it using the recursive approach?\\nSolution: The approach is similar to Problem-18. Following is a simple algorithm for finding the\\nclosest Value in BST.\\n1.\\nIf the root is NULL, then the closest value is zero (or NULL).\\n2.\\nIf the root’s data matches the given key, then the closest is the root.\\n3.\\nElse, consider the root as the closest and do the following:\\na.\\nIf the key is smaller than the root data, find the closest on the left side\\ntree of the root recursively and call it temp.\\nb.\\nIf the key is larger than the root data, find the closest on the right side\\ntree of the root recursively and call it temp.\\n4.\\nReturn the root or temp depending on whichever is nearer to the given key.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 352, 'file_type': 'pdf'}, page_content='Time Complexity: O(n) in worst case, and in average case it is O(logn).\\nSpace Complexity: O(n) in worst case, and in average case it is O(logn).\\nProblem-84\\u2003\\u2003Median in an infinite series of integers\\nSolution: Median is the middle number in a sorted list of numbers (if we have odd number of\\nelements). If we have even number of elements, median is the average of two middle numbers in a\\nsorted list of numbers.\\nFor solving this problem we can use a binary search tree with additional information at each\\nnode, and the number of children on the left and right subtrees. We also keep the number of total\\nnodes in the tree. Using this additional information we can find the median in O(logn) time, taking\\nthe appropriate branch in the tree based on the number of children on the left and right of the\\ncurrent node. But, the insertion complexity is O(n) because a standard binary search tree can\\ndegenerate into a linked list if we happen to receive the numbers in sorted order.\\nSo, let’s use a balanced binary search tree to avoid worst case behavior of standard binary search\\ntrees. For this problem, the balance factor is the number of nodes in the left subtree minus the\\nnumber of nodes in the right subtree. And only the nodes with a balance factor of+ 1 or 0 are\\nconsidered to be balanced.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 353, 'file_type': 'pdf'}, page_content='So, the number of nodes on the left subtree is either equal to or 1 more than the number of nodes\\non the right subtree, but not less.\\nIf we ensure this balance factor on every node in the tree, then the root of the tree is the median, if\\nthe number of elements is odd. In the number of elements is even, the median is the average of the\\nroot and its inorder successor, which is the leftmost descendent of its right subtree.\\nSo, the complexity of insertion maintaining a balanced condition is O(logn) and finding a median\\noperation is O(1) assuming we calculate the inorder successor of the root at every insertion if the\\nnumber of nodes is even.\\nInsertion and balancing is very similar to AVL trees. Instead of updating the heights, we update the\\nnumber of nodes information. Balanced binary search trees seem to be the most optimal solution,\\ninsertion is O(logn) and find median is O(1).\\nNote: For an efficient algorithm refer to the Priority Queues and Heaps chapter.\\nProblem-85\\u2003\\u2003Given a binary tree, how do you remove all the half nodes (which have only one\\nchild)? Note that we should not touch leaves.\\nSolution: By using post-order traversal we can solve this problem efficiently. We first process\\nthe left children, then the right children, and finally the node itself. So we form the new tree\\nbottom up, starting from the leaves towards the root. By the time we process the current node,\\nboth its left and right subtrees have already been processed.\\nTime Complexity: O(n).\\nProblem-86\\u2003\\u2003Given a binary tree, how do you remove its leaves?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 354, 'file_type': 'pdf'}, page_content='Solution: By using post-order traversal we can solve this problem (other traversals would also\\nwork).\\nTime Complexity: O(n).\\nProblem-87\\u2003\\u2003Given a BST and two integers (minimum and maximum integers) as parameters,\\nhow do you remove (prune) elements that are not within that range?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 355, 'file_type': 'pdf'}, page_content='Solution: Observation: Since we need to check each and every element in the tree, and the\\nsubtree changes should be reflected in the parent, we can think about using post order traversal.\\nSo we process the nodes starting from the leaves towards the root. As a result, while processing\\nthe node itself, both its left and right subtrees are valid pruned BSTs. At each node we will return'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 356, 'file_type': 'pdf'}, page_content='a pointer based on its value, which will then be assigned to its parent’s left or right child pointer,\\ndepending on whether the current node is the left or right child of the parent. If the current node’s\\nvalue is between A and B (A <= node’s data <= B) then no action needs to be taken, so we return\\nthe reference to the node itself.\\nIf the current node’s value is less than A, then we return the reference to its right subtree and\\ndiscard the left subtree. Because if a node’s value is less than A, then its left children are\\ndefinitely less than A since this is a binary search tree. But its right children may or may not be\\nless than A; we can’t be sure, so we return the reference to it. Since we’re performing bottom-up\\npost-order traversal, its right subtree is already a trimmed valid binary search tree (possibly\\nNULL), and its left subtree is definitely NULL because those nodes were surely less than A and\\nthey were eliminated during the post-order traversal.\\nA similar situation occurs when the node’s value is greater than B, so we now return the reference\\nto its left subtree. Because if a node’s value is greater than B, then its right children are definitely\\ngreater than B. But its left children may or may not be greater than B; So we discard the right\\nsubtree and return the reference to the already valid left subtree.\\nTime Complexity: O(n) in worst case and in average case it is O(logn).\\nNote: If the given BST is an AVL tree then O(n) is the average time complexity.\\nProblem-88\\u2003\\u2003Given a binary tree, how do you connect all the adjacent nodes at the same\\nlevel? Assume that given binary tree has next pointer along with left and right pointers as\\nshown below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 357, 'file_type': 'pdf'}, page_content='Solution: One simple approach is to use level-order traversal and keep updating the next\\npointers. While traversing, we will link the nodes on the next level. If the node has left and right\\nnode, we will link left to right. If node has next node, then link rightmost child of current node to\\nleftmost child of next node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 358, 'file_type': 'pdf'}, page_content='‘oid linkingNodesOtSameLevel(struct BinaryTreeNode ‘oot,\\nstruct Queue *Q = CreateQueuel)\\nstruct BinaryTreeNode *prev; _// Pointer to the previous node of the current level\\nstruct BinaryTreeNode “temp;\\nint currentLevelNodeCount, nextLevelNodeCount;\\niftroot)\\nreturn;\\nEnQueue(Q, root};\\ncurrentLevelNodeCount = 1;\\nnextLevelNodeCount = 0;\\nprev = NULL;\\n‘tie (UIsEmptyQueue(Q) {\\ntemp = DeQueue(Q);\\nif (temp-left),\\nEnQueue(Q, temp-slet};\\nnextLevelNodeCountt+;\\n\\n}\\n\\n}\\n\\nif temp-right)\\nEnQueue(Q, temp-right)\\nnextLevelNodeCount++;\\n\\n}\\n\\n|| Link the previous node of the current level to this node\\n\\nAlpe)\\nprevnext = temp;\\n\\n|| Set the previous node tothe curent\\n\\nprev = temp\\n\\ncourrentLevelNodeCount-~\\n\\nif(curentLevelNodeCount == 0){_// itis is the last node of the curent level\\ncurrentLeveINodeCount = nextLevelNodeCount;\\nnextLevelNodeCount\\nprev = NULL;'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 359, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-89\\u2003\\u2003Can we improve space complexity for Problem-88?\\nSolution: We can process the tree level by level, but without a queue. The logical part is that\\nwhen we process the nodes of the next level, we make sure that the current level has already been\\nlinked.\\nTime Complexity: O(n). Space Complexity: O(depth of tree) for stack space.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 360, 'file_type': 'pdf'}, page_content='Problem-90\\u2003\\u2003Assume that a set S of n numbers are stored in some form of balanced binary\\nsearch tree; i.e. the depth of the tree is O(logn). In addition to the key value and the\\npointers to children, assume that every node contains the number of nodes in its subtree.\\nSpecify a reason(s) why a balanced binary tree can be a better option than a complete\\nbinary tree for storing the set S.\\nSolution: Implementation of a balanced binary tree requires less RAM space as we do not need\\nto keep the complete tree in RAM (since they use pointers).\\nProblem-91\\u2003\\u2003For the Problem-90, specify a reason (s) why a complete binary tree can be a\\nbetter option than a balanced binary tree for storing the set S.\\nSolution: A complete binary tree is more space efficient as we do not need any extra flags. A\\nbalanced binary tree usually takes more space since we need to store some flags. For example, in\\na Red-Black tree we need to store a bit for the color. Also, a complete binary tree can be stored\\nin a RAM as an array without using pointers.\\nProblem-92\\u2003\\u2003Given a binary tree, find the maximum path sum. The path may start and end at\\nany node in the tree.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 361, 'file_type': 'pdf'}, page_content='Problem-93\\u2003\\u2003Let T be a proper binary tree with root r. Consider the following algorithm.\\nWhat does the algorithm do?\\nA. It always returns the value 1.\\nB. It computes the number of nodes in the tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 362, 'file_type': 'pdf'}, page_content='C. It computes the depth of the nodes.\\nD. It computes the height of the tree.\\nE. It computes the number of leaves in the tree.\\nSolution: E.\\n6.14 Other Variations on Trees\\nIn this section, let us enumerate the other possible representations of trees. In the earlier sections,\\nwe have looked at AVL trees, which is a binary search tree (BST) with balancing property. Now,\\nlet us look at a few more balanced binary search trees: Red-black Trees and Splay Trees.\\n6.14.1 Red-Black Trees\\nIn Red-black trees each node is associated with an extra attribute: the color, which is either red\\nor black. To get logarithmic complexity we impose the following restrictions.\\nDefinition: A Red-black tree is a binary search tree that satisfies the following properties:\\n•\\nRoot Property: the root is black\\n•\\nExternal Property: every leaf is black\\n•\\nInternal Property: the children of a red node are black\\n•\\nDepth Property: all the leaves have the same black\\nSimilar to AVL trees, if the Red-black tree becomes imbalanced, then we perform rotations to\\nreinforce the balancing property. With Red-black trees, we can perform the following operations\\nin O(logn) in worst case, where n is the number of nodes in the trees.\\n•\\nInsertion, Deletion\\n•\\nFinding predecessor, successor\\n•\\nFinding minimum, maximum\\n6.14.2 Splay Trees\\nSplay-trees are BSTs with a self-adjusting property. Another interesting property of splay-trees\\nis: starting with an empty tree, any sequence of K operations with maximum of n nodes takes\\nO(Klogn) time complexity in worst case. Splay trees are easier to program and also ensure faster\\naccess to recently accessed items. Similar to AVL and Red-Black trees, at any point that the splay\\ntree becomes imbalanced, we can perform rotations to reinforce the balancing property.\\nSplay-trees cannot guarantee the O(logn) complexity in worst case. But it gives amortized\\nO(logn) complexity. Even though individual operations can be expensive, any sequence of\\noperations gets the complexity of logarithmic behavior. One operation may take more time (a'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 363, 'file_type': 'pdf'}, page_content='single operation may take O(n) time) but the subsequent operations may not take worst case\\ncomplexity and on the average per operation complexity is O{logn).\\n6.14.3 B-Trees\\nB-Tree is like other self-balancing trees such as AVL and Red-black tree such that it maintains its\\nbalance of nodes while opertions are performed against it. B-Tree has the following properties:\\n•\\nMinimum degree “£” where, except root node, all other nodes must have no less than\\nt – 1 keys\\n•\\nEach node with n keys has n + 1 children\\n•\\nKeys in each node are lined up where k1 < k2 < .. kn\\n•\\nEach node cannot have more than 2t-l keys, thus 2t children\\n•\\nRoot node at least must contain one key. There is no root node if the tree is empty.\\n•\\nTree grows in depth only when root node is split.\\nUnlike a binary-tree, each node of a b-tree may have a variable number of keys and children. The\\nkeys are stored in non-decreasing order. Each key has an associated child that is the root of a\\nsubtree containing all nodes with keys less than or equal to the key but greater than the preceeding\\nkey. A node also has an additional rightmost child that is the root for a subtree containing all keys\\ngreater than any keys in the node.\\nA b-tree has a minumum number of allowable children for each node known as the minimization\\nfactor. If t is this minimization factor, every node must have at least t – 1 keys. Under certain\\ncircumstances, the root node is allowed to violate this property by having fewer than t – 1 keys.\\nEvery node may have at most 2t – 1 keys or, equivalently, 2t children.\\nSince each node tends to have a large branching factor (a large number of children), it is typically\\nneccessary to traverse relatively few nodes before locating the desired key. If access to each node\\nrequires a disk access, then a B-tree will minimize the number of disk accesses required. The\\nminimzation factor is usually chosen so that the total size of each node corresponds to a multiple\\nof the block size of the underlying storage device. This choice simplifies and optimizes disk\\naccess. Consequently, a B-tree is an ideal data structure for situations where all data cannot\\nreside in primary storage and accesses to secondary storage are comparatively expensive (or time\\nconsuming).\\nTo search the tree, it is similar to binary tree except that the key is compared multiple times in a\\ngiven node because the node contains more than 1 key. If the key is found in the node, the search\\nterminates. Otherwise, it moves down where at child pointed by ci where key k < ki.\\nKey insertions of a B-tree happens from the bottom fasion. This means that it walk down the tree\\nfrom root to the target child node first. If the child is not full, the key is simply inserted. If it is\\nfull, the child node is split in the middle, the median key moves up to the parent, then the new key'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 364, 'file_type': 'pdf'}, page_content='is inserted. When inserting and walking down the tree, if the root node is found to be full, it’s split\\nfirst and we have a new root node. Then the normal insertion operation is performed.\\nKey deletion is more complicated as it needs to maintain the number of keys in each node to meet\\nthe constraint. If a key is found in leaf node and deleting it still keeps the number of keys in the\\nnodes not too low, it’s simply done right away. If it’s done to the inner node, the predecessor of\\nthe key in the corresonding child node is moved to replace the key in the inner node. If moving the\\npredecessor will cause the child node to violate the node count constraint, the sibling child nodes\\nare combined and the key in the inner node is deleted.\\n6.14.4 Augmented Trees\\nIn earlier sections, we have seen various problems like finding the Kth – smallest - element in the\\ntree and other similar ones. Of all the problems the worst complexity is O(n), where n is the\\nnumber of nodes in the tree. To perform such operations in O(logn), augmented trees are useful. In\\nthese trees, extra information is added to each node and that extra data depends on the problem\\nwe are trying to solve.\\nFor example, to find the Kth element in a binary search tree, let us see how augmented trees solve\\nthe problem. Let us assume that we are using Red-Black trees as balanced BST (or any balanced\\nBST) and augmenting the size information in the nodes data. For a given node X in Red-Black tree\\nwith a field size(X) equal to the number of nodes in the subtree and can be calculated as:\\nsize(X) = size(X → left) + size(X → right)) + 1\\nKth - smallest - operation can be defined as:\\nTime Complexity: O(logn). Space Complexity: O(logn).\\nExample: With the extra size information, the augmented tree will look like:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 365, 'file_type': 'pdf'}, page_content='6.14.5 Interval Trees [Segment Trees]\\nWe often face questions that involve queries made in an array based on range. For example, for a\\ngiven array of integers, what is the maximum number in the range α to β, where α and β are of\\ncourse within array limits. To iterate over those entries with intervals containing a particular\\nvalue, we can use a simple array. But if we need more efficient access, we need a more\\nsophisticated data structure.\\nAn array-based storage scheme and a brute-force search through the entire array is acceptable\\nonly if a single search is to be performed, or if the number of elements is small. For example, if\\nyou know all the array values of interest in advance, you need to make only one pass through the\\narray. However, if you can interactively specify different search operations at different times, the\\nbrute-force search becomes impractical because every element in the array must be examined\\nduring each search operation.\\nIf you sort the array in ascending order of the array values, you can terminate the sequential\\nsearch when you reach the object whose low value is greater than the element we are searching.\\nUnfortunately, this technique becomes increasingly ineffective as the low value increases,\\nbecause fewer search operations are eliminated. That means, what if we have to answer a large\\nnumber of queries like this? – is brute force still a good option?\\nAnother example is when we need to return a sum in a given range. We can brute force this too,\\nbut the problem for a large number of queries still remains. So, what can we do? With a bit of\\nthinking we can come up with an approach like maintaining a separate array of n elements, where'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 366, 'file_type': 'pdf'}, page_content='n is the size of the original array, where each index stores the sum of all elements from 0 to that\\nindex. So essentially we have with a bit of preprocessing brought down the query time from a\\nworst case O(n) to O(1). Now this is great as far as static arrays are concerned, but, what if we\\nare required to perform updates on the array too?\\nThe first approach gives us an O(n) query time, but an O(1) update time. The second approach, on\\nthe other hand, gives us O(1) query time, but an O(n) update time. So, which one do we choose?\\nInterval trees are also binary search trees and they store interval information in the node structure.\\nThat means, we maintain a set of n intervals [i1, i2] such that one of the intervals containing a\\nquery point Q (if any) can be found efficiently. Interval trees are used for performing range\\nqueries efficiently.\\nA segment tree is a heap-like data structure that can be used for making update/query operations\\nupon array intervals in logarithmical time. We define the segment tree for the interval [i,j] in the\\nfollowing recursive manner:\\n•\\nThe root (first node in the array) node will hold the information for the interval [i,j]\\n•\\nIf i < y the left and right children will hold the information for the intervals \\nand \\nSegment trees (also called segtrees and interval trees) is a cool data structure, primarily used for\\nrange queries. It is a height balanced binary tree with a static structure. The nodes of a segment\\ntree correspond to various intervals, and can be augmented with appropriate information\\npertaining to those intervals. It is somewhat less powerful than a balanced binary tree because of\\nits static structure, but due to the recursive nature of operations on the segtree, it is incredibly\\neasy to think about and code.\\nWe can use segment trees to solve range minimum/maximum query problems. The time complexity\\nis T(nlogn) where O(n) is the time required to build the tree and each query takes O(logn) time.\\nExample: Given a set of intervals: S= {[2-5], [6-7], [6-10], [8-9], [12-15], [15-23], [25-30]}. A\\nquery with Q = 9 returns [6,10] or [8,9] (assume these are the intervals which contain 9 among\\nall the intervals). A query with Q = 23 returns [15, 23].'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 367, 'file_type': 'pdf'}, page_content='Construction of Interval Trees: Let us assume that we are given a set S of n intervals (called\\nsegments). These n intervals will have 2n endpoints. Now, let us see how to construct the\\ninterval tree.\\nAlgorithm:\\nRecursively build tree on interval set 5 as follows:\\n•\\nSort the 2n endpoints\\n•\\nLet Xmid be the median point\\nTime Complexity for building interval trees: O(nlogn). Since we are choosing the median,\\nInterval Trees will be approximately balanced. This ensures that, we split the set of end points up\\nin half each time. The depth of the tree is O(logn). To simplify the search process, generally Xmid\\nis stored with each node.\\n6.14.6 Scapegoat Trees\\nScapegoat tree is a self-balancing binary search tree, discovered by Arne Andersson. It provides\\nworst-case O(logn) search time, and O(logn) amortized (average) insertion and deletion time.\\nAVL trees rebalance whenever the height of two sibling subtrees differ by more than one;'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 368, 'file_type': 'pdf'}, page_content='scapegoat trees rebalance whenever the size of a child exceeds a certain ratio of its parents, a\\nratio known as a. After inserting the element, we traverse back up the tree. If we find an\\nimbalance where a child’s size exceeds the parent’s size times alpha, we must rebuild the subtree\\nat the parent, the scapegoat.\\nThere might be more than one possible scapegoat, but we only have to pick one. The most optimal\\nscapegoat is actually determined by height balance. When removing it, we see if the total size of\\nthe tree is less than alpha of the largest size since the last rebuilding of the tree. If so, we rebuild\\nthe entire tree. The alpha for a scapegoat tree can be any number between 0.5 and 1.0. The value\\n0.5 will force perfect balance, while 1.0 will cause rebalancing to never occur, effectively\\nturning it into a BST.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 369, 'file_type': 'pdf'}, page_content='7.1 What is a Priority Queue?\\nIn some situations we may need to find the minimum/maximum element among a collection of\\nelements. We can do this with the help of Priority Queue ADT. A priority queue ADT is a data\\nstructure that supports the operations Insert and DeleteMin (which returns and removes the\\nminimum element) or DeleteMax (which returns and removes the maximum element).\\nThese operations are equivalent to EnQueue and DeQueue operations of a queue. The difference\\nis that, in priority queues, the order in which the elements enter the queue may not be the same in\\nwhich they were processed. An example application of a priority queue is job scheduling, which\\nis prioritized instead of serving in first come first serve.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 370, 'file_type': 'pdf'}, page_content='A priority queue is called an ascending – priority queue, if the item with the smallest key has the\\nhighest priority (that means, delete the smallest element always). Similarly, a priority queue is\\nsaid to be a descending –priority queue if the item with the largest key has the highest priority\\n(delete the maximum element always). Since these two types are symmetric we will be\\nconcentrating on one of them: ascending-priority queue.\\n7.2 Priority Queue ADT\\nThe following operations make priority queues an ADT.\\nMain Priority Queues Operations\\nA priority queue is a container of elements, each having an associated key.\\n•\\nInsert (key, data): Inserts data with key to the priority queue. Elements are ordered\\nbased on key.\\n•\\nDeleteMin/DeleteMax: Remove and return the element with the smallest/largest key.\\n•\\nGetMinimum/GetMaximum: Return the element with the smallest/largest key without\\ndeleting it.\\nAuxiliary Priority Queues Operations\\n•\\nkth - Smallest/kth – Largest: Returns the kth -Smallest/kth –Largest key in priority\\nqueue.\\n•\\nSize: Returns number of elements in priority queue.\\n•\\nHeap Sort: Sorts the elements in the priority queue based on priority (key).\\n7.3 Priority Queue Applications\\nPriority queues have many applications - a few of them are listed below:\\n•\\nData compression: Huffman Coding algorithm\\n•\\nShortest path algorithms: Dijkstra’s algorithm\\n•\\nMinimum spanning tree algorithms: Prim’s algorithm\\n•\\nEvent-driven simulation: customers in a line'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 371, 'file_type': 'pdf'}, page_content='•\\nSelection problem: Finding kth- smallest element\\n7.4 Priority Queue Implementations\\nBefore discussing the actual implementation, let us enumerate the possible options.\\nUnordered Array Implementation\\nElements are inserted into the array without bothering about the order. Deletions (DeleteMax) are\\nperformed by searching the key and then deleting.\\nInsertions complexity: O(1). DeleteMin complexity: O(n).\\nUnordered List Implementation\\nIt is very similar to array implementation, but instead of using arrays, linked lists are used.\\nInsertions complexity: O(1). DeleteMin complexity: O(n).\\nOrdered Array Implementation\\nElements are inserted into the array in sorted order based on key field. Deletions are performed at\\nonly one end.\\nInsertions complexity: O(n). DeleteMin complexity: O(1).\\nOrdered List Implementation\\nElements are inserted into the list in sorted order based on key field. Deletions are performed at\\nonly one end, hence preserving the status of the priority queue. All other functionalities associated\\nwith a linked list ADT are performed without modification.\\nInsertions complexity: O(n). DeleteMin complexity: O(1).\\nBinary Search Trees Implementation\\nBoth insertions and deletions take O(logn) on average if insertions are random (refer to Trees\\nchapter).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 372, 'file_type': 'pdf'}, page_content='Balanced Binary Search Trees Implementation\\nBoth insertions and deletion take O(logn) in the worst case (refer to Trees chapter).\\nBinary Heap Implementation\\nIn subsequent sections we will discuss this in full detail. For now, assume that binary heap\\nimplementation gives O(logn) complexity for search, insertions and deletions and O(1) for\\nfinding the maximum or minimum element.\\nComparing Implementations\\n7.5 Heaps and Binary Heaps\\nWhat is a Heap?\\nA heap is a tree with some special properties. The basic requirement of a heap is that the value of\\na node must be ≥ (or ≤) than the values of its children. This is called heap property. A heap also\\nhas the additional property that all leaves should be at h or h – 1 levels (where h is the height of\\nthe tree) for some h > 0 (complete binary trees). That means heap should form a complete binary\\ntree (as shown below).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 373, 'file_type': 'pdf'}, page_content='In the examples below, the left tree is a heap (each element is greater than its children) and the\\nright tree is not a heap (since 11 is greater than 2).\\nTypes of Heaps?\\nBased on the property of a heap we can classify heaps into two types:\\n•\\nMin heap: The value of a node must be less than or equal to the values of its\\nchildren'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 374, 'file_type': 'pdf'}, page_content='•\\nMax heap: The value of a node must be greater than or equal to the values of its\\nchildren\\n7.6 Binary Heaps\\nIn binary heap each node may have up to two children. In practice, binary heaps are enough and\\nwe concentrate on binary min heaps and binary max heaps for the remaining discussion.\\nRepresenting Heaps: Before looking at heap operations, let us see how heaps can be\\nrepresented. One possibility is using arrays. Since heaps are forming complete binary trees, there\\nwill not be any wastage of locations. For the discussion below let us assume that elements are'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 375, 'file_type': 'pdf'}, page_content='stored in arrays, which starts at index 0. The previous max heap can be represented as:\\nNote: For the remaining discussion let us assume that we are doing manipulations in max heap.\\nDeclaration of Heap\\nCreating Heap'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 376, 'file_type': 'pdf'}, page_content='Time Complexity: O(1).\\nParent of a Node\\nFor a node at ith location, its parent is at \\n location. In the previous example, the element 6 is at\\nsecond location and its parent is at 0th location.\\nTime Complexity: O(1).\\nChildren of a Node\\nSimilar to the above discussion, for a node at ith location, its children are at 2 * i + 1 and 2 * i +'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 377, 'file_type': 'pdf'}, page_content='2 locations. For example, in the above tree the element 6 is at second location and its children 2\\nand 5 are at 5 (2 * i + 1 = 2 * 2 + 1) and 6(2 * i + 2 = 2 * 2) locations.\\nGetting the Maximum Element\\nSince the maximum element in max heap is always at root, it will be stored at h→array[O].\\nTime Complexity: O(1).\\nHeapifying an Element\\nAfter inserting an element into heap, it may not satisfy the heap property. In that case we need to\\nadjust the locations of the heap to make it heap again. This process is called heapifying. In max-\\nheap, to heapify an element, we have to find the maximum of its children and swap it with the\\ncurrent element and continue this process until the heap property is satisfied at every node.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 378, 'file_type': 'pdf'}, page_content='Observation: One important property of heap is that, if an element is not satisfying the heap\\nproperty, then all the elements from that element to the root will have the same problem. In the\\nexample below, element 1 is not satisfying the heap property and its parent 31 is also having the\\nissue. Similarly, if we heapify an element, then all the elements from that element to the root will\\nalso satisfy the heap property automatically. Let us go through an example. In the above heap, the\\nelement 1 is not satisfying the heap property. Let us try heapifying this element.\\nTo heapify 1, find the maximum of its children and swap with that.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 379, 'file_type': 'pdf'}, page_content='We need to continue this process until the element satisfies the heap properties. Now, swap 1 with\\n8.\\nNow the tree is satisfying the heap property. In the above heapifying process, since we are'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 380, 'file_type': 'pdf'}, page_content='moving from top to bottom, this process is sometimes called percolate down. Similarly, if we\\nstart heapifying from any other node to root, we can that process percolate up as move from\\nbottom to top.\\nTime Complexity: O(logn). Heap is a complete binary tree and in the worst case we start at the\\nroot and come down to the leaf. This is equal to the height of the complete binary tree. Space\\nComplexity: O(1).\\nDeleting an Element\\nTo delete an element from heap, we just need to delete the element from the root. This is the only\\noperation (maximum element) supported by standard heap. After deleting the root element, copy\\nthe last element of the heap (tree) and delete that last element.\\nAfter replacing the last element, the tree may not satisfy the heap property. To make it heap again,\\ncall the PercolateDown function.\\n•\\nCopy the first element into some variable'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 381, 'file_type': 'pdf'}, page_content='•\\nCopy the last element into first element location\\n•\\nPercolateDown the first element\\nNote: Deleting an element uses PercolateDown, and inserting an element uses PercolateUp.\\nTime Complexity: same as Heapify function and it is O(logn).\\nInserting an Element\\nInsertion of an element is similar to the heapify and deletion process.\\n•\\nIncrease the heap size\\n•\\nKeep the new element at the end of the heap (tree)\\n•\\nHeapify the element from bottom to top (root)\\nBefore going through code, let us look at an example. We have inserted the element 19 at the end\\nof the heap and this is not satisfying the heap property.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 382, 'file_type': 'pdf'}, page_content='In order to heapify this element (19), we need to compare it with its parent and adjust them.\\nSwapping 19 and 14 gives:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 383, 'file_type': 'pdf'}, page_content='Again, swap 19 andl6:\\nNow the tree is satisfying the heap property. Since we are following the bottom-up approach we\\nsometimes call this process percolate up.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 384, 'file_type': 'pdf'}, page_content='Time Complexity: O(logn). The explanation is the same as that of the Heapify function.\\nDestroying Heap'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 385, 'file_type': 'pdf'}, page_content='Heapifying the Array\\nOne simple approach for building the heap is, take n input items and place them into an empty\\nheap. This can be done with n successive inserts and takes O(nlogn) in the worst case. This is\\ndue to the fact that each insert operation takes O(logn).\\nTo finish our discussion of binary heaps, we will look at a method to build an entire heap from a\\nlist of keys. The first method you might think of may be like the following. Given a list of keys,\\nyou could easily build a heap by inserting each key one at a time. Since you are starting with a list\\nof one item, the list is sorted and you could use binary search to find the right position to insert the\\nnext key at a cost of approximately O(logn) operations.\\nHowever, remember that inserting an item in the middle of the list may require O(n) operations to\\nshift the rest of the list over to make room for the new key. Therefore, to insert n keys into the\\nheap would require a total of O(nlogn) operations. However, if we start with an entire list then\\nwe can build the whole heap in O(n) operations.\\nObservation: Leaf nodes always satisfy the heap property and do not need to care for them. The\\nleaf elements are always at the end and to heapify the given array it should be enough if we\\nheapify the non-leaf nodes. Now let us concentrate on finding the first non-leaf node. The last\\nelement of the heap is at location h → count – 1, and to find the first non-leaf node it is enough to\\nfind the parent of the last element.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 386, 'file_type': 'pdf'}, page_content='Time Complexity: The linear time bound of building heap can be shown by computing the sum of\\nthe heights of all the nodes. For a complete binary tree of height h containing n = 2h+1- 1 nodes,\\nthe sum of the heights of the nodes is n – h - 1 = n – logn – 1 (for proof refer to Problems\\nSection). That means, building the heap operation can be done in linear time (O(n)) by applying a\\nPercolateDown function to the nodes in reverse level order.\\n7.7 Heapsort\\nOne main application of heap ADT is sorting (heap sort). The heap sort algorithm inserts all\\nelements (from an unsorted array) into a heap, then removes them from the root of a heap until the\\nheap is empty. Note that heap sort can be done in place with the array to be sorted. Instead of\\ndeleting an element, exchange the first element (maximum) with the last element and reduce the\\nheap size (array size). Then, we heapify the first element. Continue this process until the number\\nof remaining elements is one.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 387, 'file_type': 'pdf'}, page_content='Time complexity: As we remove the elements from the heap, the values become sorted (since\\nmaximum elements are always root only). Since the time complexity of both the insertion\\nalgorithm and deletion algorithm is O(logn) (where n is the number of items in the heap), the time\\ncomplexity of the heap sort algorithm is O(nlogn).\\n7.8 Priority Queues [Heaps]: Problems & Solutions\\nProblem-1\\u2003\\u2003What are the minimum and maximum number of elements in a heap of height h?\\nSolution: Since heap is a complete binary tree (all levels contain full nodes except possibly the\\nlowest level), it has at most 2h+1 – 1 elements (if it is complete). This is because, to get maximum\\nnodes, we need to fill all the h levels completely and the maximum number of nodes is nothing but\\nthe sum of all nodes at all h levels.\\nTo get minimum nodes, we should fill the h – 1 levels fully and the last level with only one\\nelement. As a result, the minimum number of nodes is nothing but the sum of all nodes from h – 1\\nlevels plus 1 (for the last level) and we get 2h – 1 + 1 = 2h elements (if the lowest level has just 1\\nelement and all the other levels are complete).\\nProblem-2\\u2003\\u2003Is there a min-heap with seven distinct elements so that the preorder traversal of\\nit gives the elements in sorted orde?\\nSolution: Yes. For the tree below, preorder traversal produces ascending order.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 388, 'file_type': 'pdf'}, page_content='Problem-3\\u2003\\u2003Is there a max-heap with seven distinct elements so that the preorder traversal of\\nit gives the elements in sorted order?\\nSolution: Yes. For the tree below, preorder traversal produces descending order.\\nProblem-4\\u2003\\u2003Is there a min-heap/max-heap with seven distinct elements so that the inorder\\ntraversal of it gives the elements in sorted order?\\nSolution: No. Since a heap must be either a min-heap or a max-heap, the root will hold the\\nsmallest element or the largest. An inorder traversal will visit the root of the tree as its second\\nstep, which is not the appropriate place if the tree’s root contains the smallest or largest element.\\nProblem-5\\u2003\\u2003Is there a min-heap/max-heap with seven distinct elements so that the postorder\\ntraversal of it gives the elements in sorted order?\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 389, 'file_type': 'pdf'}, page_content='Yes, if the tree is a max-heap and we want descending order (below left), or if the tree is a min-\\nheap and we want ascending order (below right).\\nProblem-6\\u2003\\u2003Show that the height of a heap with n elements is logn?\\nSolution: A heap is a complete binary tree. All the levels, except the lowest, are completely full.\\nA heap has at least 2h elements and at most elements 2h ≤ n ≤ 2h+1 – 1. This implies, h ≤ logn ≤ h\\n+ 1. Since h is an integer, h = logn.\\nProblem-7\\u2003\\u2003Given a min-heap, give an algorithm for finding the maximum element.\\nSolution: For a given min heap, the maximum element will always be at leaf only. Now, the next\\nquestion is how to find the leaf nodes in the tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 390, 'file_type': 'pdf'}, page_content='If we carefully observe, the next node of the last element’s parent is the first leaf node. Since the\\nlast element is always at the h → count – 1th location, the next node of its parent (parent at\\nlocation \\n can be calculated as:\\nNow, the only step remaining is scanning the leaf nodes and finding the maximum among them.\\nTime Complexity: \\n.\\nProblem-8\\u2003\\u2003Give an algorithm for deleting an arbitrary element from min heap.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 391, 'file_type': 'pdf'}, page_content='Solution: To delete an element, first we need to search for an element. Let us assume that we are\\nusing level order traversal for finding the element. After finding the element we need to follow\\nthe DeleteMin process.\\nTime Complexity = Time for finding the element + Time for deleting an element\\n= O(n) + O (logn) ≈ O(n). //Time for searching is\\ndominated.\\nProblem-9\\u2003\\u2003Give an algorithm for deleting the ith indexed element in a given min-heap.\\nSolution:\\nTime Complexity = O(logn).\\nProblem-10\\u2003\\u2003Prove that, for a complete binary tree of height h the sum of the height of all\\nnodes is O(n – h).\\nSolution: A complete binary tree has 2i nodes on level (.Also, a node on level i has depth i and\\nheight h – i. Let us assume that S denotes the sum of the heights of all these nodes and S can be\\ncalculated as:\\nMultiplying with 2 on both sides gives: 2S = 2h + 4(h – 1) + 8(h – 2) + ···+ 2h – 1(1)\\nNow, subtract S from 2S: 2S – S = – h + 2 + 4 + ··· + 2h ⇒ S = (2h+1 – 1) – (h – 1)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 392, 'file_type': 'pdf'}, page_content='But, we already know that the total number of nodes n in a complete binary tree with height h is n\\n= 2h+1 – 1. This gives us: h = log(n + 1).\\nFinally, replacing 2h+1 – 1 with n, gives: S = n – (h – 1) = O(n – logn) = O(n - h).\\nProblem-11\\u2003\\u2003Give an algorithm to find all elements less than some value of k in a binary heap.\\nSolution: Start from the root of the heap. If the value of the root is smaller than k then print its\\nvalue and call recursively once for its left child and once for its right child. If the value of a node\\nis greater or equal than k then the function stops without printing that value.\\nThe complexity of this algorithm is O(n), where n is the total number of nodes in the heap. This\\nbound takes place in the worst case, where the value of every node in the heap will be smaller\\nthan k, so the function has to call each node of the heap.\\nProblem-12\\u2003\\u2003Give an algorithm for merging two binary max-heaps. Let us assume that the size\\nof the first heap is m + n and the size of the second heap is n.\\nSolution: One simple way of solving this problem is:\\n•\\nAssume that the elements of the first array (with size m + n) are at the beginning.\\nThat means, first m cells are filled and remaining n cells are empty.\\n•\\nWithout changing the first heap, just append the second heap and heapify the array.\\n•\\nSince the total number of elements in the new array is m + n, each heapify operation\\ntakes O(log(m + n)).\\nThe complexity of this algorithm is : O((m + n)log(m + n)).\\nProblem-13\\u2003\\u2003Can we improve the complexity of Problem-12?\\nSolution: Instead of heapifying all the elements of the m + n array, we can use the technique of\\n“building heap with an array of elements (heapifying array)”. We can start with non-leaf nodes\\nand heapify them. The algorithm can be given as:\\n•\\nAssume that the elements of the first array (with size m + n) are at the beginning.\\nThat means, the first m cells are filled and the remaining n cells are empty.\\n•\\nWithout changing the first heap, just append the second heap.\\n•\\nNow, find the first non-leaf node and start heapifying from that element.\\nIn the theory section, we have already seen that building a heap with n elements takes O(n)\\ncomplexity. The complexity of merging with this technique is: O(m + n).\\nProblem-14\\u2003\\u2003Is there an efficient algorithm for merging 2 max-heaps (stored as an array)?\\nAssume both arrays have n elements.\\nSolution: The alternative solution for this problem depends on what type of heap it is. If it’s a\\nstandard heap where every node has up to two children and which gets filled up so that the leaves\\nare on a maximum of two different rows, we cannot get better than O(n) for the merge.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 393, 'file_type': 'pdf'}, page_content='There is an O(logm × logn) algorithm for merging two binary heaps with sizes m and n. For m =\\nn, this algorithm takes O(log2n) time complexity. We will be skipping it due to its difficulty and\\nscope.\\nFor better merging performance, we can use another variant of binary heap like a Fibonacci-\\nHeap which can merge in O(1) on average (amortized).\\nProblem-15\\u2003\\u2003Give an algorithm for finding the kth smallest element in min-heap.\\nSolution: One simple solution to this problem is: perform deletion k times from min-heap.\\nTime Complexity: O(klogn). Since we are performing deletion operation k times and each\\ndeletion takes O(logn).\\nProblem-16\\u2003\\u2003For Problem-15, can we improve the time complexity?\\nSolution: Assume that the original min-heap is called HOrig and the auxiliary min-heap is named\\nHAux. Initially, the element at the top of HOrig, the minimum one, is inserted into HAux. Here we\\ndon’t do the operation of DeleteMin with HOrig.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 394, 'file_type': 'pdf'}, page_content='Every while-loop iteration gives the kth smallest element and we need k loops to get the kth\\nsmallest elements. Because the size of the auxiliary heap is always less than k, every while-loop\\niteration the size of the auxiliary heap increases by one, and the original heap HOrig has no\\noperation during the finding, the running time is O(klogk).\\nNote: The above algorithm is useful if the k value is too small compared to n. If the k value is\\napproximately equal to n, then we can simply sort the array (let’s say, using couting sort or any\\nother linear sorting algorithm) and return kth smallest element from the sorted array. This gives\\nO(n) solution.\\nProblem-17\\u2003\\u2003Find k max elements from max heap.\\nSolution: One simple solution to this problem is: build max-heap and perform deletion k times.\\nT(n) = DeleteMin from heap k times = Θ(klogn).\\nProblem-18\\u2003\\u2003For Problem-17, is there any alternative solution?\\nSolution: We can use the Problem-16 solution. At the end, the auxiliary heap contains the k-\\nlargest elements. Without deleting the elements we should keep on adding elements to HAux.\\nProblem-19\\u2003\\u2003How do we implement stack using heap?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 395, 'file_type': 'pdf'}, page_content='Solution: To implement a stack using a priority queue PQ (using min heap), let us assume that we\\nare using one extra integer variable c. Also, assume that c is initialized equal to any known value\\n(e.g., 0). The implementation of the stack ADT is given below. Here c is used as the priority\\nwhile inserting/deleting the elements from PQ.\\nWe could also increment c back when popping.\\nObservation: We could use the negative of the current system time instead of c (to avoid\\noverflow). The implementation based on this can be given as:\\nProblem-20\\u2003\\u2003How do we implement Queue using heap?\\nSolution: To implement a queue using a priority queue PQ (using min heap), as similar to stacks\\nsimulation, let us assume that we are using one extra integer variable, c. Also, assume that c is\\ninitialized equal to any known value (e.g., 0). The implementation of the queue ADT is given\\nbelow. Here the c is used as the priority while inserting/deleting the elements from PQ.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 396, 'file_type': 'pdf'}, page_content='Note: We could also decrement c when popping.\\nObservation: We could use just the negative of the current system time instead of c (to avoid\\noverflow). The implementation based on this can be given as:\\nNote: The only change is that we need to take a positive c value instead of negative.\\nProblem-21\\u2003\\u2003Given a big file containing billions of numbers, how can you find the 10\\nmaximum numbers from that file?\\nSolution: Always remember that when you need to find max n elements, the best data structure to\\nuse is priority queues.\\nOne solution for this problem is to divide the data in sets of 1000 elements (let’s say 1000) and\\nmake a heap of them, and then take 10 elements from each heap one by one. Finally heap sort all\\nthe sets of 10 elements and take the top 10 among those. But the problem in this approach is\\nwhere to store 10 elements from each heap. That may require a large amount of memory as we'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 397, 'file_type': 'pdf'}, page_content='have billions of numbers.\\nReusing the top 10 elements (from the earlier heap) in subsequent elements can solve this\\nproblem. That means take the first block of 1000 elements and subsequent blocks of 990 elements\\neach. Initially, Heapsort the first set of 1000 numbers, take max 10 elements, and mix them with\\n990 elements of the 2nd set. Again, Heapsort these 1000 numbers (10 from the first set and 990\\nfrom the 2nd set), take 10 max elements, and mix them with 990 elements of the 3rd set. Repeat till\\nthe last set of 990 (or less) elements and take max 10 elements from the final heap. These 10\\nelements will be your answer.\\nTime Complexity: O(n) = n/1000 ×(complexity of Heapsort 1000 elements) Since complexity of\\nheap sorting 1000 elements will be a constant so the O(n) = n i.e. linear complexity.\\nProblem-22\\u2003\\u2003Merge k sorted lists with total of n elements: We are given k sorted lists with\\ntotal n inputs in all the lists. Give an algorithm to merge them into one single sorted list.\\nSolution: Since there are k equal size lists with a total of n elements, the size of each list is  One\\nsimple way of solving this problem is:\\n•\\nTake the first list and merge it with the second list. Since the size of each list is ,\\nthis step produces a sorted list with size \\n. This is similar to merge sort logic. The\\ntime complexity of this step is: \\n. This is because we need to scan all the elements\\nof both the lists.\\n•\\nThen, merge the second list output with the third list. As a result, this step produces a\\nsorted list with size \\n. The time complexity of this step is: \\n. This is because we\\nneed to scan all the elements of both lists (one with size \\n and the other with size \\n).\\n•\\nContinue this process until all the lists are merged to one list.\\nTotal \\ntime \\ncomplexity: \\nSpace Complexity: O(1).\\nProblem-23\\u2003\\u2003For Problem-22, can we improve the time complexity?\\nSolution:\\n1\\nDivide the lists into pairs and merge them. That means, first take two lists at a time\\nand merge them so that the total elements parsed for all lists is O(n). This operation\\ngives k/2 lists.\\n2\\nRepeat step-1 until the number of lists becomes one.\\nTime complexity: Step-1 executes logk times and each operation parses all n elements in all the\\nlists for making k/2 lists. For example, if we have 8 lists, then the first pass would make 4 lists by'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 398, 'file_type': 'pdf'}, page_content='parsing all n elements. The second pass would make 2 lists by again parsing n elements and the\\nthird pass would give 1 list by again parsing n elements. As a result the total time complexity is\\nO(nlogn).\\nSpace Complexity: O(n).\\nProblem-24\\u2003\\u2003For Problem-23, can we improve the space complexity?\\nSolution: Let us use heaps for reducing the space complexity.\\n1.\\nBuild the max-heap with all the first elements from each list in O(k).\\n2.\\nIn each step, extract the maximum element of the heap and add it at the end of the\\noutput.\\n3.\\nAdd the next element from the list of the one extracted. That means we need to select\\nthe next element of the list which contains the extracted element of the previous\\nstep.\\n4.\\nRepeat step-2 and step-3 until all the elements are completed from all the lists.\\nTime Complexity = O(nlogk ). At a time we have k elements max-heap and for all n elements we\\nhave to read just the heap in logk time, so total time = O(nlogk).\\nSpace Complexity: O(k) [for Max-heap].\\nProblem-25\\u2003\\u2003Given 2 arrays A and B each with n elements. Give an algorithm for finding\\nlargest n pairs (A[i],B[j]).\\nSolution:\\nAlgorithm:\\n•\\nHeapify A and B. This step takes O(2n) ≈ O(n).\\n•\\nThen keep on deleting the elements from both the heaps. Each step takes O(2logn) ≈\\nO(logn).\\nTotal Time complexity: O(nlogn).\\nProblem-26\\u2003\\u2003Min-Max heap: Give an algorithm that supports min and max in O(1) time,\\ninsert, delete min, and delete max in O(logn) time. That means, design a data structure\\nwhich supports the following operations:\\nOperation\\nComplexity\\nInit\\nO(n)\\nInsert\\nO(logn)\\nFindMin\\nO(1)\\nFindMax\\nO(1)\\nDelete Min\\nO(logn)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 399, 'file_type': 'pdf'}, page_content='Delete Max\\nO(logri)\\nSolution: This problem can be solved using two heaps. Let us say two heaps are: Minimum-Heap\\nHmin and Maximum-Heap Hmax. Also, assume that elements in both the arrays have mutual\\npointers. That means, an element in Hmin will have a pointer to the same element in Hmax and an\\nelement in Hmax will have a pointer to the same element in Hmin.\\nInit\\nBuild Hmin in O(n) and Hmax in O(n)\\nInsert(x)\\nInsert x to Hmin in O(logn). Insert x to Hmax in O(logn). Update the\\npointers in O(1)\\nFindMin()\\nReturn root(Hmin) in O(1)\\nFindMax\\nReturn root(Hmax) in O(1)\\nDelete\\nMin\\nDelete the minimum from Hmin in O(logn). Delete the same element from\\nHmax by using the mutual pointer in O(logn)\\nDeleteMax Delete the maximum from Hmax in O(logn). Delete the same element from\\nHmin by using the mutual pointer in O(logn)\\nProblem-27\\u2003\\u2003Dynamic median finding. Design a heap data structure that supports finding the\\nmedian.\\nSolution: In a set of n elements, median is the middle element, such that the number of elements\\nlesser than the median is equal to the number of elements larger than the median. If n is odd, we\\ncan find the median by sorting the set and taking the middle element. If n is even, the median is\\nusually defined as the average of the two middle elements. This algorithm works even when some\\nof the elements in the list are equal. For example, the median of the multiset {1, 1, 2, 3, 5} is 2,\\nand the median of the multiset {1, 1, 2, 3, 5, 8} is 2.5.\\n“Median heaps” are the variant of heaps that give access to the median element. A median heap\\ncan be implemented using two heaps, each containing half the elements. One is a max-heap,\\ncontaining the smallest elements; the other is a min-heap, containing the largest elements. The size\\nof the max-heap may be equal to the size of the min-heap, if the total number of elements is even.\\nIn this case, the median is the average of the maximum element of the max-heap and the minimum\\nelement of the min-heap. If there is an odd number of elements, the max-heap will contain one\\nmore element than the min-heap. The median in this case is simply the maximum element of the\\nmax-heap.\\nProblem-28\\u2003\\u2003Maximum sum in sliding window: Given array A[] with sliding window of size\\nw which is moving from the very left of the array to the very right. Assume that we can\\nonly see the w numbers in the window. Each time the sliding window moves rightwards by'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 400, 'file_type': 'pdf'}, page_content='one position. For example: The array is [1 3 -1 -3 5 3 6 7], and w is 3.\\nWindow position\\nMax\\n[1 3 -1] -3 5 3 6 7\\n3\\n1 [3 -1 -3] 5 3 6 7\\n3\\n1 3 [-1 -3 5] 3 6 7\\n5\\n1 3 -1 [-3 5 3] 6 7\\n5\\n1 3 -1 -3 [5 3 6] 7\\n6\\n1 3 -1 -3 5 [3 6 7]\\n7\\nInput: A long array A[], and a window width w. Output: An array B[], B[i] is the\\nmaximum value of from A[i] to A[i+w-1]\\nRequirement: Find a good optimal way to get B[i]\\nSolution: Brute force solution is, every time the window is moved we can search for a total of w\\nelements in the window.\\nTime complexity: O(nw).\\nProblem-29\\u2003\\u2003For Problem-28, can we reduce the complexity?\\nSolution: Yes, we can use heap data structure. This reduces the time complexity to O(nlogw).\\nInsert operation takes O(logw) time, where w is the size of the heap. However, getting the\\nmaximum value is cheap; it merely takes constant time as the maximum value is always kept in the\\nroot (head) of the heap. As the window slides to the right, some elements in the heap might not be\\nvalid anymore (range is outside of the current window). How should we remove them? We would\\nneed to be somewhat careful here. Since we only remove elements that are out of the window’s\\nrange, we would need to keep track of the elements’ indices too.\\nProblem-30\\u2003\\u2003For Problem-28, can we further reduce the complexity?\\nSolution: Yes, The double-ended queue is the perfect data structure for this problem. It supports\\ninsertion/deletion from the front and back. The trick is to find a way such that the largest element\\nin the window would always appear in the front of the queue. How would you maintain this\\nrequirement as you push and pop elements in and out of the queue?\\nBesides, you will notice that there are some redundant elements in the queue that we shouldn’t\\neven consider. For example, if the current queue has the elements: [10 5 3], and a new element in\\nthe window has the element 11. Now, we could have emptied the queue without considering\\nelements 10, 5, and 3, and insert only element 11 into the queue.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 401, 'file_type': 'pdf'}, page_content='Typically, most people try to maintain the queue size the same as the window’s size. Try to break\\naway from this thought and think out of the box. Removing redundant elements and storing only\\nelements that need to be considered in the queue is the key to achieving the efficient O(n) solution\\nbelow. This is because each element in the list is being inserted and removed at most once.\\nTherefore, the total number of insert + delete operations is 2n.\\nProblem-31\\u2003\\u2003A priority queue is a list of items in which each item has associated with it a\\npriority. Items are withdrawn from a priority queue in order of their priorities starting with\\nthe highest priority item first. If the maximum priority item is required, then a heap is\\nconstructed such than priority of every node is greater than the priority of its children.\\nDesign such a heap where the item with the middle priority is withdrawn first. If there are\\nn items in the heap, then the number of items with the priority smaller than the middle\\npriority is  if n is odd, else \\n ∓ 1.\\nExplain how withdraw and insert operations work, calculate their complexity, and how the\\ndata structure is constructed.\\nSolution: We can use one min heap and one max heap such that root of the min heap is larger than'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 402, 'file_type': 'pdf'}, page_content='the root of the max heap. The size of the min heap should be equal or one less than the size of the\\nmax heap. So the middle element is always the root of the max heap.\\nFor the insert operation, if the new item is less than the root of max heap, then insert it into the\\nmax heap; else insert it into the min heap. After the withdraw or insert operation, if the size of\\nheaps are not as specified above than transfer the root element of the max heap to min heap or\\nvice-versa.\\nWith this implementation, insert and withdraw operation will be in O(logn) time.\\nProblem-32\\u2003\\u2003Given two heaps, how do you merge (union) them?\\nSolution: Binary heap supports various operations quickly: Find-min, insert, decrease-key. If we\\nhave two min-heaps, H1 and H2, there is no efficient way to combine them into a single min-heap.\\nFor solving this problem efficiently, we can use mergeable heaps. Mergeable heaps support\\nefficient union operation. It is a data structure that supports the following operations:\\n•\\nCreate-Heap(): creates an empty heap\\n•\\nInsert(H,X,K) : insert an item x with key K into a heap H\\n•\\nFind-Min(H) : return item with min key\\n•\\nDelete-Min(H) : return and remove\\n•\\nUnion(H1, H2) : merge heaps H1 and H2\\nExamples of mergeable heaps are:\\n•\\nBinomial Heaps\\n•\\nFibonacci Heaps\\nBoth heaps also support:\\n•\\nDecrease-Key(H,X,K): assign item Y with a smaller key K\\n•\\nDelete(H,X) : remove item X\\nBinomial Heaps: Unlike binary heap which consists of a single tree, a binomial heap consists of\\na small set of component trees and no need to rebuild everything when union is performed. Each\\ncomponent tree is in a special format, called a binomial tree.\\nA binomial tree of order k, denoted by Bk is defined recursively as follows:\\n•\\nB0 is a tree with a single node\\n•\\nFor k ≥ 1, Bk is formed by joining two Bk–1, such that the root of one tree becomes\\nthe leftmost child of the root of the other.\\nExample:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 403, 'file_type': 'pdf'}, page_content='Fibonacci Heaps: Fibonacci heap is another example of mergeable heap. It has no good worst-\\ncase guarantee for any operation (except Insert/Create-Heap). Fibonacci Heaps have excellent\\namortized cost to perform each operation. Like binomial heap, fibonacci heap consists of a set of\\nmin-heap ordered component trees. However, unlike binomial heap, it has\\n•\\nNo limit on number of trees (up to O(n)), and\\n•\\nNo limit on height of a tree (up to O(n))\\nAlso, Find-Min, Delete-Min, Union, Decrease-Key, Delete all have worst-case O(n) running\\ntime. However, in the amortized sense, each operation performs very quickly.\\nProblem-33\\u2003\\u2003Median in an infinite series of integers\\nSolution: Median is the middle number in a sorted list of numbers (if we have odd number of\\nelements). If we have even number of elements, median is the average of two middle numbers in a\\nsorted list of numbers.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 404, 'file_type': 'pdf'}, page_content='We can solve this problem efficiently by using 2 heaps: One MaxHeap and one MinHeap.\\n1.\\nMaxHeap contains the smallest half of the received integers\\n2.\\nMinHeap contains the largest half of the received integers\\nThe integers in MaxHeap are always less than or equal to the integers in MinHeap. Also, the\\nnumber of elements in MaxHeap is either equal to or 1 more than the number of elements in the\\nMinHeap.\\nIn the stream if we get 2n elements (at any point of time), MaxHeap and MinHeap will both\\ncontain equal number of elements (in this case, n elements in each heap). Otherwise, if we have\\nreceived 2n + 1 elements, MaxHeap will contain n + 1 and MinHeap n.\\nLet us find the Median: If we have 2n + 1 elements (odd), the Median of received elements will\\nbe the largest element in the MaxHeap (nothing but the root of MaxHeap). Otherwise, the Median\\nof received elements will be the average of largest element in the MaxHeap (nothing but the root\\nof MaxHeap) and smallest element in the MinHeap (nothing but the root of MinHeap). This can be\\ncalculated in O(1).\\nInserting an element into heap can be done in O(logn). Note that, any heap containing n + 1\\nelements might need one delete operation (and insertion to other heap) as well.\\nExample:\\nInsert 1: Insert to MaxHeap.\\nMaxHeap: {1}, MinHeap:{}\\nInsert 9: Insert to MinHeap. Since 9 is greater than 1 and MinHeap maintains the maximum\\nelements.\\nMaxHeap: {1}, MinHeap:{9}\\nInsert 2: Insert MinHeap. Since 2 is less than all elements of MinHeap.\\nMaxHeap: {1,2}, MinHeap:{9}\\nInsert 0: Since MaxHeap already has more than half; we have to drop the max element\\nfrom MaxHeap and insert it to MinHeap. So, we have to remove 2 and insert into\\nMinHeap. With that it becomes:\\nMaxHeap: {1}, MinHeap:{2,9}\\nNow, insert 0 to MaxHeap.\\nTotal Time Complexity: O(logn).\\nProblem-34\\u2003\\u2003Suppose the elements 7, 2, 10 and 4 are inserted, in that order, into the valid 3-\\nary max heap found in the above question, Which one of the following is the sequence of\\nitems in the array representing the resultant heap?\\n(A)\\n10, 7, 9, 8, 3, 1, 5, 2, 6, 4\\n(B)\\n10, 9, 8, 7, 6, 5, 4, 3, 2, 1'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 405, 'file_type': 'pdf'}, page_content='(C)\\n10, 9, 4, 5, 7, 6, 8, 2, 1, 3\\n(D)\\n10, 8, 6, 9, 7, 2, 3, 4, 1, 5\\nSolution: The 3-ary max heap with elements 9, 5, 6, 8, 3, 1 is:\\nAfter Insertion of 7:\\nAfter Insertion of 2:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 406, 'file_type': 'pdf'}, page_content='After Insertion of 10:\\n\\n) @\\nOOO OO\\n\\nAfter Insertion of 4:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 407, 'file_type': 'pdf'}, page_content='Problem-35\\u2003\\u2003A complete binary min-heap is made by including each integer in [1,1023]\\nexactly once. The depth of a node in the heap is the length of the path from the root of the\\nheap to that node. Thus, the root is at depth 0. The maximum depth at which integer 9 can\\nappear is.\\nSolution: As shown in the figure below, for a given number i, we can fix the element i at ith level\\nand arrange the numbers 1 to i – 1 to the levels above. Since the root is at depth zero, the\\nmaximum depth of the ith element in a min-heap is i – 1. Hence, the maximum depth at which\\ninteger 9 can appear is 8.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 408, 'file_type': 'pdf'}, page_content='Problem-36\\u2003\\u2003A d-ary heap is like a binary heap, but instead of 2 children, nodes have d\\nchildren. How would you represent a d-ary heap with n elements in an array? What are the\\nexpressions for determining the parent of a given element, Parent(i), and a jth child of a\\ngiven element, Child(i,j), where 1 ≤ j ≤ d?\\nSolution: The following expressions determine the parent and jth child of element i (where 1 ≤ j\\n≤ d):'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 409, 'file_type': 'pdf'}, page_content='8.1 Introduction\\nIn this chapter, we will represent an important mathematics concept: sets. This means how to\\nrepresent a group of elements which do not need any order. The disjoint sets ADT is the one used\\nfor this purpose. It is used for solving the equivalence problem. It is very simple to implement. A\\nsimple array can be used for the implementation and each function takes only a few lines of code.\\nDisjoint sets ADT acts as an auxiliary data structure for many other algorithms (for example,\\nKruskal’s algorithm in graph theory). Before starting our discussion on disjoint sets ADT, let us\\nlook at some basic properties of sets.\\n8.2 Equivalence Relations and Equivalence Classes\\nFor the discussion below let us assume that 5 is a set containing the elements and a relation R is\\ndefined on it. That means for every pair of elements in a,b ∈ 5, a R b is either true or false. If a R'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 410, 'file_type': 'pdf'}, page_content='b is true, then we say a is related to b, otherwise a is not related to b. A relation R is called an\\nequivalence relation if it satisfies the following properties:\\n•\\nReflexive: For every element a ∈ S.aR a is true.\\n•\\nSymmetric: For any two elements a, b ∈ S, if a R b is true then b R a is true.\\n•\\nTransitive: For any three elements a, b, c ∈ S, if a R b and b R c are true then a R c\\nis true.\\nAs an example, relations ≤ (less than or equal to) and ≥ (greater than or equal to) on a set of\\nintegers are not equivalence relations. They are reflexive (since a ≤ a) and transitive (a ≤ b and b\\n≤ c implies a ≤ c) but not symmetric (a ≤ b does not imply b ≤ a).\\nSimilarly, rail connectivity is an equivalence relation. This relation is reflexive because any\\nlocation is connected to itself. If there is connectivity from city a to city b, then city b also has\\nconnectivity to city a, so the relation is symmetric. Finally, if city a is connected to city b and city\\nb is connected to city c, then city a is also connected to city c.\\nThe equivalence class of an element a ∈ S is a subset of S that contains all the elements that are\\nrelated to a. Equivalence classes create a partition of S. Every member of S appears in exactly\\none equivalence class. To decide if a R b, we just need to check whether a and b are in the same\\nequivalence class (group) or not.\\nIn the above example, two cities will be in same equivalence class if they have rail connectivity.\\nIf they do not have connectivity then they will be part of different equivalence classes.\\nSince the intersection of any two equivalence classes is empty (ϕ), the equivalence classes are\\nsometimes called disjoint sets. In the subsequent sections, we will try to see the operations that\\ncan be performed on equivalence classes. The possible operations are:\\n•\\nCreating an equivalence class (making a set)\\n•\\nFinding the equivalence class name (Find)\\n•\\nCombining the equivalence classes (Union)\\n8.3 Disjoint Sets ADT\\nTo manipulate the set elements we need basic operations defined on sets. In this chapter, we\\nconcentrate on the following set operations:\\n•\\nMAKESET(X): Creates a new set containing a single element X.\\n•\\nUNION(X, Y): Creates a new set containing the elements X and Y in their union and\\ndeletes the sets containing the elements X and Y.\\n•\\nFIND(X): Returns the name of the set containing the element X.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 411, 'file_type': 'pdf'}, page_content='8.4 Applications\\nDisjoint sets ADT have many applications and a few of them are:\\n•\\nTo represent network connectivity\\n•\\nImage processing\\n•\\nTo find least common ancestor\\n•\\nTo define equivalence of finite state automata\\n•\\nKruskal’s minimum spanning tree algorithm (graph theory)\\n•\\nIn game algorithms\\n8.5 Tradeoffs in Implementing Disjoint Sets ADT\\nLet us see the possibilities for implementing disjoint set operations. Initially, assume the input\\nelements are a collection of n sets, each with one element. That means, initial representation\\nassumes all relations (except reflexive relations) are false. Each set has a different element, so\\nthat Si ∩ Sj= ф. This makes the sets disjoint.\\nTo add the relation a R b (UNION), we first need to check whether a and b are already related or\\nnot. This can be verified by performing FINDs on both a and b and checking whether they are in\\nthe same equivalence class (set) or not.\\nIf they are not, then we apply UNION. This operation merges the two equivalence classes\\ncontaining a and b into a new equivalence class by creating a new set Sk = Si ∪ Sj and deletes Si\\nand Sj. Basically there are two ways to implement the above FIND/UNION operations:\\n•\\nFast FIND implementation (also called Quick FIND)\\n•\\nFast UNION operation implementation (also called Quick UNION)\\n8.6 Fast FIND Implementation (Quick FIND)\\nIn this method, we use an array. As an example, in the representation below the array contains the\\nset name for each element. For simplicity, let us assume that all the elements are numbered\\nsequentially from 0 to n – 1.\\nIn the example below, element 0 has the set name 3, element 1 has the set name 5, and so on. With\\nthis representation FIND takes only O(1) since for any element we can find the set name by\\naccessing its array location in constant time.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 412, 'file_type': 'pdf'}, page_content='In this representation, to perform UNION(a, b) [assuming that a is in set i and b is in set j] we\\nneed to scan the complete array and change all i’s to j. This takes O(n).\\nA sequence of n – 1 unions take O(n2) time in the worst case. If there are O(n2) FIND operations,\\nthis performance is fine, as the average time complexity is O(1) for each UNION or FIND\\noperation. If there are fewer FINDs, this complexity is not acceptable.\\n8.7 Fast UNION Implementation (Quick UNION)\\nIn this and subsequent sections, we will discuss the faster UNION implementations and its\\nvariants. There are different ways of implementing this approach and the following is a list of a\\nfew of them.\\n•\\nFast UNION implementations (Slow FIND)\\n•\\nFast UNION implementations (Quick FIND)\\n•\\nFast UNION implementations with path compression\\n8.8 Fast UNION Implementation (Slow FIND)\\nAs we have discussed, FIND operation returns the same answer (set name) if and only if they are\\nin the same set. In representing disjoint sets, our main objective is to give a different set name for\\neach group. In general we do not care about the name of the set. One possibility for implementing\\nthe set is tree as each element has only one root and we can use it as the set name.\\nHow are these represented? One possibility is using an array: for each element keep the root as\\nits set name. But with this representation, we will have the same problem as that of FIND array\\nimplementation. To solve this problem, instead of storing the root we can keep the parent of the\\nelement. Therefore, using an array which stores the parent of each element solves our problem.\\nTo differentiate the root node, let us assume its parent is the same as that of the element in the\\narray. Based on this representation, MAKESET, FIND, UNION operations can be defined as:\\n•\\n(X): Creates a new set containing a single element X and in the array update the\\nparent of X as X. That means root (set name) of X is X.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 413, 'file_type': 'pdf'}, page_content='•\\nUNION(X, Y): Replaces the two sets containing X and Y by their union and in the\\narray updates the parent of X as Y.\\n•\\nFIND(X): Returns the name of the set containing the element X. We keep on\\nsearching for X’s set name until we come to the root of the tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 414, 'file_type': 'pdf'}, page_content='For the elements 0 to n – 1 the initial representation is:\\nTo perform a UNION on two sets, we merge the two trees by making the root of one tree point to\\nthe root of the other.\\nInitial Configuration for the elements 0 to 6'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 415, 'file_type': 'pdf'}, page_content='oslodeslosiosIosio®\\n\\n0 1 2 3 4 5 6\\n\\nParent Array\\n\\nAfter UNION(,6)\\n\\notosioto®\\n\\nParent Array\\n\\nAfter UNION( 1,2)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 416, 'file_type': 'pdf'}, page_content='After UNION(0,2)\\nOne important thing to observe here is, UNION operation is changing the root’s parent only, but\\nnot for all the elements in the sets. Due to this, the time complexity of UNION operation is O(1).\\nA FIND(X) on element X is performed by returning the root of the tree containing X. The time to\\nperform this operation is proportional to the depth of the node representing X.\\nUsing this method, it is possible to create a tree of depth n - 1 (Skew Trees). The worst-case\\nrunning time of a FIND is O(n) and m consecutive FIND operations take O(mn) time in the worst\\ncase.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 417, 'file_type': 'pdf'}, page_content='MAKESET\\nFIND\\nUNION\\n8.9 Fast UNION Implementations (Quick FIND)\\nThe main problem with the previous approach is that, in the worst case we are getting the skew\\ntrees and as a result the FIND operation is taking O(n) time complexity. There are two ways to\\nimprove it:\\n•\\nUNION by Size (also called UNION by Weight): Make the smaller tree a subtree of\\nthe larger tree\\n•\\nUNION by Height (also called UNION by Rank): Make the tree with less height a\\nsubtree of the tree with more height'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 418, 'file_type': 'pdf'}, page_content='UNION by Size\\nIn the earlier representation, for each element i we have stored i (in the parent array) for the root\\nelement and for other elements we have stored the parent of i. But in this approach we store\\nnegative of the size of the tree (that means, if the size of the tree is 3 then store –3 in the parent\\narray for the root element). For the previous example (after UNION(0,2)), the new representation\\nwill look like:\\nAssume that the size of one element set is 1 and store – 1. Other than this there is no change.\\nMAKESET\\nFIND'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 419, 'file_type': 'pdf'}, page_content='UNION by Size\\nNote: There is no change in FIND operation implementation.\\nUNION by Height (UNION by Rank)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 420, 'file_type': 'pdf'}, page_content='As in UNION by size, in this method we store negative of height of the tree (that means, if the\\nheight of the tree is 3 then we store –3 in the parent array for the root element). We assume the\\nheight of a tree with one element set is 1. For the previous example (after UNION(0,2)), the new\\nrepresentation will look like:\\nUNION by Height'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 421, 'file_type': 'pdf'}, page_content='Note: For FIND operation there is no change in the implementation.\\nComparing UNION by Size and UNION by Height\\nWith UNION by size, the depth of any node is never more than logn. This is because a node is\\ninitially at depth 0. When its depth increases as a result of a UNION, it is placed in a tree that is\\nat least twice as large as before. That means its depth can be increased at most logn times. This\\nmeans that the running time for a FIND operation is O(logn), and a sequence of m operations\\ntakes O(m logn).\\nSimilarly with UNION by height, if we take the UNION of two trees of the same height, the height\\nof the UNION is one larger than the common height, and otherwise equal to the max of the two\\nheights. This will keep the height of tree of n nodes from growing past O(logn). A sequence of m\\nUNIONs and FINDs can then still cost O(m logn).\\nPath Compression\\nFIND operation traverses a list of nodes on the way to the root. We can make later FIND\\noperations efficient by making each of these vertices point directly to the root. This process is\\ncalled path compression. For example, in the FIND(X) operation, we travel from X to the root of\\nthe tree. The effect of path compression is that every node on the path from X to the root has its\\nparent changed to the root.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 422, 'file_type': 'pdf'}, page_content='With path compression the only change to the FIND function is that S[X] is made equal to the\\nvalue returned by FIND. That means, after the root of the set is found recursively, X is made to\\npoint directly to it. This happen recursively to every node on the path to the root.\\nFIND with path compression\\nNote: Path compression is compatible with UNION by size but not with UNION by height as'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 423, 'file_type': 'pdf'}, page_content='there is no efficient way to change the height of the tree.\\n8.10 Summary\\nPerforming m union-find operations on a set of n objects.\\nAlgorithm\\nWorst-case time\\nQuick-Find\\nmn\\nQuick-Union\\nmn\\nQuick-Union by Size/Height\\nn + m logn\\nPath compression\\nn + m logn\\nQuick-Union by Size/Height + Path Compression\\n(m + n) logn\\n8.11 Disjoint Sets: Problems & Solutions\\nProblem-1\\u2003\\u2003Consider a list of cities c1; c2,...,cn. Assume that we have a relation R such that,\\nfor any i,j, R(ci,cj) is 1 if cities ci and cj are in the same state, and 0 otherwise. If R is\\nstored as a table, how much space does it require?\\nSolution: R must have an entry for every pair of cities. There are Θ(n2) of these.\\nProblem-2\\u2003\\u2003For Problem-1, using a Disjoint sets ADT, give an algorithm that puts each city in\\na set such that ci and cj are in the same set if and only if they are in the same state.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 424, 'file_type': 'pdf'}, page_content='Problem-3\\u2003\\u2003For Problem-1, when the cities are stored in the Disjoint sets ADT, if we are\\ngiven two cities ci and cj, how do we check if they are in the same state?\\nSolution: Cities ci and cj are in the same state if and only if FIND(ci) = FIND(cj).\\nProblem-4\\u2003\\u2003For Problem-1, if we use linked-lists with UNION by size to implement the\\nunion-find ADT, how much space do we use to store the cities?\\nSolution: There is one node per city, so the space is Θ(n).\\nProblem-5\\u2003\\u2003For Problem-1, if we use trees with UNION by rank, what is the worst-case\\nrunning time of the algorithm from Problem-2?\\nSolution: Whenever we do a UNION in the algorithm from Problem-2, the second argument is a\\ntree of size 1. Therefore, all trees have height 1, so each union takes time O(1). The worst-case\\nrunning time is then Θ(n2).\\nProblem-6\\u2003\\u2003If we use trees without union-by-rank, what is the worst-case running time of the\\nalgorithm from Problem-2? Are there more worst-case scenarios than Problem-5?\\nSolution: Because of the special case of the unions, union-by-rank does not make a difference for\\nour algorithm. Hence, everything is the same as in Problem-5.\\nProblem-7\\u2003\\u2003With the quick-union algorithm we know that a sequence of n operations (unions\\nand finds) can take slightly more than linear time in the worst case. Explain why if all the\\nfinds are done before all the unions, a sequence of n operations is guaranteed to take O(n)\\ntime.\\nSolution: If the find operations are performed first, then the find operations take O(1) time each\\nbecause every item is the root of its own tree. No item has a parent, so finding the set an item is in\\ntakes a fixed number of operations. Union operations always take O(1) time. Hence, a sequence\\nof n operations with all the finds before the unions takes O(n) time.\\nProblem-8\\u2003\\u2003With reference to Problem-7, explain why if all the unions are done before all the\\nfinds, a sequence of n operations is guaranteed to take O(n) time.\\nSolution: This problem requires amortized analysis. Find operations can be expensive, but this\\nexpensive find operation is balanced out by lots of cheap union operations.\\nThe accounting is as follows. Union operations always take O(1) time, so let’s say they have an\\nactual cost of \\n1. Assign each union operation an amortized cost of \\n2, so every union\\noperation puts \\n1 in the account. Each union operation creates a new child. (Some node that\\nwas not a child of any other node before is a child now.) When all the union operations are done,\\nthere is $1 in the account for every child, or in other words, for every node with a depth of one or\\ngreater. Let’s say that a find(u) operation costs \\n1 if u is a root. For any other node, the find\\noperation costs an additional \\n1 for each parent pointer the find operation traverses. So the\\nactual cost is \\n (1 + d), where d is the depth of u. Assign each find operation an amortized cost'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 425, 'file_type': 'pdf'}, page_content='of \\n2. This covers the case where u is a root or a child of a root. For each additional parent\\npointer traversed, \\n1 is withdrawn from the account to pay for it.\\nFortunately, path compression changes the parent pointers of all the nodes we pay \\n1 to\\ntraverse, so these nodes become children of the root. All of the traversed nodes whose depths are\\n2 or greater move up, so their depths are now 1. We will never have to pay to traverse these\\nnodes again. Say that a node is a grandchild if its depth is 2 or greater.\\nEvery time find(u) visits a grandchild, \\n1 is withdrawn from the account, but the grandchild is\\nno longer a grandchild. So the maximum number of dollars that can ever be withdrawn from the\\naccount is the number of grandchildren. But we initially put $1 in the bank for every child, and\\nevery grandchild is a child, so the bank balance will never drop below zero. Therefore, the\\namortization works out. Union and find operations both have amortized costs of \\n2, so any\\nsequence of n operations where all the unions are done first takes O(n) time.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 426, 'file_type': 'pdf'}, page_content='9.1 Introduction\\nIn the real world, many problems are represented in terms of objects and connections between\\nthem. For example, in an airline route map, we might be interested in questions like: “What’s the\\nfastest way to go from Hyderabad to New York?” or “What is the cheapest way to go from\\nHyderabad to New York?” To answer these questions we need information about connections\\n(airline routes) between objects (towns). Graphs are data structures used for solving these kinds\\nof problems.\\n9.2 Glossary\\nGraph: A graph is a pair (V, E), where V is a set of nodes, called vertices, and £ is a collection\\nof pairs of vertices, called edges.\\n•\\nVertices and edges are positions and store elements'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 427, 'file_type': 'pdf'}, page_content='•\\nDefinitions that we use:\\n○\\nDirected edge:\\n▪\\nordered pair of vertices (u, v)\\n▪\\nfirst vertex u is the origin\\n▪\\nsecond vertex v is the destination\\n▪\\nExample: one-way road traffic\\n○\\nUndirected edge:\\n▪\\nunordered pair of vertices (u, v)\\n▪\\nExample: railway lines\\n○\\nDirected graph:\\n▪\\nall the edges are directed\\n▪\\nExample: route network\\n○ Undirected graph:\\n▪\\nall the edges are undirected\\n▪\\nExample: flight network'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 428, 'file_type': 'pdf'}, page_content='•\\nWhen an edge connects two vertices, the vertices are said to be adjacent to each\\nother and the edge is incident on both vertices.\\n•\\nA graph with no cycles is called a tree. A tree is an acyclic connected graph.\\n•\\nA self loop is an edge that connects a vertex to itself.\\n•\\nTwo edges are parallel if they connect the same pair of vertices.\\n•\\nThe Degree of a vertex is the number of edges incident on it.\\n•\\nA subgraph is a subset of a graph’s edges (with associated vertices) that form a\\ngraph.\\n•\\nA path in a graph is a sequence of adjacent vertices. Simple path is a path with no\\nrepeated vertices. In the graph below, the dotted lines represent a path from G to E.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 429, 'file_type': 'pdf'}, page_content='•\\nA cycle is a path where the first and last vertices are the same. A simple cycle is a\\ncycle with no repeated vertices or edges (except the first and last vertices).\\n•\\nWe say that one vertex is connected to another if there is a path that contains both of\\nthem.\\n•\\nA graph is connected if there is a path from every vertex to every other vertex.\\n•\\nIf a graph is not connected then it consists of a set of connected components.\\n•\\nA directed acyclic graph [DAG] is a directed graph with no cycles.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 430, 'file_type': 'pdf'}, page_content='•\\nA forest is a disjoint set of trees.\\n•\\nA spanning tree of a connected graph is a subgraph that contains all of that graph’s\\nvertices and is a single tree. A spanning forest of a graph is the union of spanning\\ntrees of its connected components.\\n•\\nA bipartite graph is a graph whose vertices can be divided into two sets such that all\\nedges connect a vertex in one set with a vertex in the other set.\\n•\\nIn weighted graphs integers (weights) are assigned to each edge to represent\\n(distances or costs).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 431, 'file_type': 'pdf'}, page_content='•\\nGraphs with all edges present are called complete graphs.\\n•\\nGraphs with relatively few edges (generally if it edges < |V| log |V|) are called\\nsparse graphs.\\n•\\nGraphs with relatively few of the possible edges missing are called dense.\\n•\\nDirected weighted graphs are sometimes called network.\\n•\\nWe will denote the number of vertices in a given graph by |V|, and the number of\\nedges by |E|. Note that E can range anywhere from 0 to |V|(|V| – l)/2 (in undirected\\ngraph). This is because each node can connect to every other node.\\n9.3 Applications of Graphs'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 432, 'file_type': 'pdf'}, page_content='•\\nRepresenting relationships between components in electronic circuits\\n•\\nTransportation networks: Highway network, Flight network\\n•\\nComputer networks: Local area network, Internet, Web\\n•\\nDatabases: For representing ER (Entity Relationship) diagrams in databases, for\\nrepresenting dependency of tables in databases\\n9.4 Graph Representation\\nAs in other ADTs, to manipulate graphs we need to represent them in some useful form. Basically,\\nthere are three ways of doing this:\\n•\\nAdjacency Matrix\\n•\\nAdjacency List\\n•\\nAdjacency Set\\nAdjacency Matrix\\nGraph Declaration for Adjacency Matrix\\nFirst, let us look at the components of the graph data structure. To represent graphs, we need the\\nnumber of vertices, the number of edges and also their interconnections. So, the graph can be\\ndeclared as:\\nDescription\\nIn this method, we use a matrix with size V × V. The values of matrix are boolean. Let us assume\\nthe matrix is Adj. The value Adj[u, v] is set to 1 if there is an edge from vertex u to vertex v and 0\\notherwise.\\nIn the matrix, each edge is represented by two bits for undirected graphs. That means, an edge\\nfrom u to v is represented by 1 value in both Adj[u,v ] and Adj[u,v]. To save time, we can process\\nonly half of this symmetric matrix. Also, we can assume that there is an “edge” from each vertex\\nto itself. So, Adj[u, u] is set to 1 for all vertices.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 433, 'file_type': 'pdf'}, page_content='If the graph is a directed graph then we need to mark only one entry in the adjacency matrix. As an\\nexample, consider the directed graph below.\\nThe adjacency matrix for this graph can be given as:\\nNow, let us concentrate on the implementation. To read a graph, one way is to first read the vertex\\nnames and then read pairs of vertex names (edges). The code below reads an undirected graph.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 434, 'file_type': 'pdf'}, page_content='The adjacency matrix representation is good if the graphs are dense. The matrix requires O(V2)\\nbits of storage and O(V2) time for initialization. If the number of edges is proportional to V2, then\\nthere is no problem because V2 steps are required to read the edges. If the graph is sparse, the\\ninitialization of the matrix dominates the running time of the algorithm as it takes takes O(V2).\\nAdjacency List\\nGraph Declaration for Adjacency List\\nIn this representation all the vertices connected to a vertex v are listed on an adjacency list for\\nthat vertex v. This can be easily implemented with linked lists. That means, for each vertex v we\\nuse a linked list and list nodes represents the connections between v and other vertices to which v\\nhas an edge.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 435, 'file_type': 'pdf'}, page_content='The total number of linked lists is equal to the number of vertices in the graph. The graph ADT\\ncan be declared as:\\nDescription\\nConsidering the same example as that of the adjacency matrix, the adjacency list representation\\ncan be given as:\\nSince vertex A has an edge for B and D, we have added them in the adjacency list for A. The\\nsame is the case with other vertices as well.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 436, 'file_type': 'pdf'}, page_content='| [Nodes of the Linked List\\nstruct ListNode {\\nint vertexNumber;\\nstruct ListNode *next;\\n\\n}\\n}\\n\\n/ his code creates graph with adj ist representation\\nstruct Graph adjistOfGraph)\\ninti, x,y;\\nstruct ListNode “temp;\\nstruct Graph °G = struct Graph ¥) mallo{sizofstruct Graph);\\nig)\\nprint Memory Error)\\nreturn;\\n\\n}\\nscanf{‘Number of Vertices: od, Number of Edges:%d’, &G-»V, &G-B);\\nG-+Adj = malloc(G—V * sizeofistruct ListNode)};\\n\\nforfi= 051 < GV; 4\\nG-+Adji «(struct ListNode * malloszostrct LstNod);\\nG-Adj-vertesNumber*j\\nG-Adji-next = G- Ail\\n\\n}\\nforli=0; i Byit4) {\\n| [Read an edge\\nscanfReading Edge; Yd Yo’, x,y);\\ntemp = (struct ListNode *) malloe|struct ListNode);\\ntemp-vertexNumber = y;\\ntemp-onext = G-rAdjix};\\nG+ Adja|-next = temp;\\ntemp = (struct ListNode*) mallostructListNode};\\ntemp-vertextVumber«y;\\ntemp-snext = G-+ Adj);\\nG-Adjly]-+ next= temp;\\n}\\n\\nretutn G;\\n}'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 437, 'file_type': 'pdf'}, page_content='For this representation, the order of edges in the input is important. This is because they\\ndetermine the order of the vertices on the adjacency lists. The same graph can be represented in\\nmany different ways in an adjacency list. The order in which edges appear on the adjacency list\\naffects the order in which edges are processed by algorithms.\\nDisadvantages of Adjacency Lists\\nUsing adjacency list representation we cannot perform some operations efficiently. As an\\nexample, consider the case of deleting a node. . In adjacency list representation, it is not enugh if\\nwe simply delete a node from the list representation, if we delete a node from the adjacency list\\nthen that is enough. For each node on the adjacency list of that node specifies another vertex. We\\nneed to search other nodes linked list also for deleting it. This problem can be solved by linking\\nthe two list nodes that correspond to a particular edge and making the adjacency lists doubly\\nlinked. But all these extra links are risky to process.\\nAdjacency Set\\nIt is very much similar to adjacency list but instead of using Linked lists, Disjoint Sets [Union-\\nFind] are used. For more details refer to the Disjoint Sets ADT chapter.\\nComparison of Graph Representations\\nDirected and undirected graphs are represented with the same structures. For directed graphs,\\neverything is the same, except that each edge is represented just once. An edge from x to y is\\nrepresented by a 1 value in Agj[x][y] in the adjacency matrix, or by adding y on x’s adjacency list.\\nFor weighted graphs, everything is the same, except fill the adjacency matrix with weights instead\\nof boolean values.\\n9.5 Graph Traversals'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 438, 'file_type': 'pdf'}, page_content='To solve problems on graphs, we need a mechanism for traversing the graphs. Graph traversal\\nalgorithms are also called graph search algorithms. Like trees traversal algorithms (Inorder,\\nPreorder, Postorder and Level-Order traversals), graph search algorithms can be thought of as\\nstarting at some source vertex in a graph and “searching” the graph by going through the edges and\\nmarking the vertices. Now, we will discuss two such algorithms for traversing the graphs.\\n•\\nDepth First Search [DFS]\\n•\\nBreadth First Search [BFS]\\nDepth First Search [DFS]\\nDFS algorithm works in a manner similar to preorder traversal of the trees. Like preorder\\ntraversal, internally this algorithm also uses stack.\\nLet us consider the following example. Suppose a person is trapped inside a maze. To come out\\nfrom that maze, the person visits each path and each intersection (in the worst case). Let us say the\\nperson uses two colors of paint to mark the intersections already passed. When discovering a new\\nintersection, it is marked grey, and he continues to go deeper.\\nAfter reaching a “dead end” the person knows that there is no more unexplored path from the grey\\nintersection, which now is completed, and he marks it with black. This “dead end” is either an\\nintersection which has already been marked grey or black, or simply a path that does not lead to\\nan intersection.\\nThe intersections of the maze are the vertices and the paths between the intersections are the\\nedges of the graph. The process of returning from the “dead end” is called backtracking. We are\\ntrying to go away from the starting vertex into the graph as deep as possible, until we have to\\nbacktrack to the preceding grey vertex. In DFS algorithm, we encounter the following types of\\nedges.\\nTree edge: encounter new vertex\\nBack edge: from descendent to ancestor\\nForward edge: from ancestor to descendent\\nCross edge: between a tree or subtrees\\nFor most algorithms boolean classification, unvisited/visited is enough (for three color\\nimplementation refer to problems section). That means, for some problems we need to use three\\ncolors, but for our discussion two colors are enough.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 439, 'file_type': 'pdf'}, page_content='Initially all vertices are marked unvisited (false). The DFS algorithm starts at a vertex u in the\\ngraph. By starting at vertex u it considers the edges from u to other vertices. If the edge leads to\\nan already visited vertex, then backtrack to current vertex u. If an edge leads to an unvisited\\nvertex, then go to that vertex and start processing from that vertex. That means the new vertex\\nbecomes the current vertex. Follow this process until we reach the dead-end. At this point start\\nbacktracking.\\nThe process terminates when backtracking leads back to the start vertex. The algorithm based on\\nthis mechanism is given below: assume Visited[] is a global array.\\nAs an example, consider the following graph. We can see that sometimes an edge leads to an'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 440, 'file_type': 'pdf'}, page_content='already discovered vertex. These edges are called back edges, and the other edges are called tree\\nedges because deleting the back edges from the graph generates a tree.\\nThe final generated tree is called the DFS tree and the order in which the vertices are processed\\nis called DFS numbers of the vertices. In the graph below, the gray color indicates that the vertex\\nis visited (there is no other significance). We need to see when the Visited table is updated.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 441, 'file_type': 'pdf'}, page_content='Visited Table\\n\\nLL [oo fofoyo yo]\\n\\nVisited Table\\n\\nStarting vertex A is\\nmarked visited Vertex B is visited\\n\\nVisited Table\\n\\nLED D fofojoyo]\\n\\nVisited Table\\n\\nRecursive cal of DFS,\\nvertex Dis visited\\n\\nRecursive cll of\\nDFS, vertex Cis\\nvisited'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 442, 'file_type': 'pdf'}, page_content='Recursive call of DFS,\\n(oO) ® vertex G is visited'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 443, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 444, 'file_type': 'pdf'}, page_content='Visited Table\\n\\nVisited Table\\n\\nBacktrack from H. Edge\\nIR Wien hark elon\\n\\nRecursive call of DFS,\\n© vertex His Visited\\n\\non\\n\\nBacktrack from C\\n\\nVertex A is completed.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 445, 'file_type': 'pdf'}, page_content='From the above diagrams, it can be seen that the DFS traversal creates a tree (without back\\nedges) and we call such tree a DFS tree. The above algorithm works even if the given graph has\\nconnected components.\\nThe time complexity of DFS is O(V + E), if we use adjacency lists for representing the graphs.\\nThis is because we are starting at a vertex and processing the adjacent nodes only if they are not\\nvisited. Similarly, if an adjacency matrix is used for a graph representation, then all edges\\nadjacent to a vertex can’t be found efficiently, and this gives O(V2) complexity.\\nApplications of DFS\\n•\\nTopological sorting\\n•\\nFinding connected components\\n•\\nFinding articulation points (cut vertices) of the graph\\n•\\nFinding strongly connected components\\n•\\nSolving puzzles such as mazes\\nFor algorithms refer to Problems Section.\\nBreadth First Search [BFS]\\nThe BFS algorithm works similar to level – order traversal of the trees. Like level – order\\ntraversal, BFS also uses queues. In fact, level – order traversal got inspired from BFS. BFS\\nworks level by level. Initially, BFS starts at a given vertex, which is at level 0. In the first stage it\\nvisits all vertices at level 1 (that means, vertices whose distance is 1 from the start vertex of the\\ngraph). In the second stage, it visits all vertices at the second level. These new vertices are the\\nones which are adjacent to level 1 vertices.\\nBFS continues this process until all the levels of the graph are completed. Generally queue data\\nstructure is used for storing the vertices of a level.\\nAs similar to DFS, assume that initially all vertices are marked unvisited (false). Vertices that\\nhave been processed and removed from the queue are marked visited (true). We use a queue to\\nrepresent the visited set as it will keep the vertices in the order of when they were first visited.\\nThe implementation for the above discussion can be given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 446, 'file_type': 'pdf'}, page_content='As an example, let us consider the same graph as that of the DFS example. The BFS traversal can\\nbe shown as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 447, 'file_type': 'pdf'}, page_content='Starting vertex Ais marked\\ncunvisited, Assume this is at\\nlevel 0.\\n\\nVertes dis completed. Circle partis level\\n1 and added to Queue,\\n\\nry (Queue: B\\n\\nVisited Table\\n\\n(o] To] [oJofojofo Joo] oJofofo'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 448, 'file_type': 'pdf'}, page_content='Bis completed. Selected part is Vertices C and H are\\nlevel 2 add to Queue). completed. Circled part i\\n— level 3 (add to Queue).\\n\\n© eo\\n\\n.\\nOCLO\\n\\noO\\no\\n\\nVisited Table Visited Table\\n\\nT[1]O]o]o[oyo\\n\\nDand E are completed. F and\\nG are marked with gray color\\n(next level).\\n\\nAll vertices completed and\\nQueue is empty.\\n\\nee ®\\n®@ @\\n\\nVisited Table Visited Table'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 449, 'file_type': 'pdf'}, page_content='Time complexity of BFS is O(V + E), if we use adjacency lists for representing the graphs, and\\nO(V2) for adjacency matrix representation.\\nApplications of BFS\\n•\\nFinding all connected components in a graph\\n•\\nFinding all nodes within one connected component\\n•\\nFinding the shortest path between two nodes\\n•\\nTesting a graph for bipartiteness\\nComparing DFS and BFS\\nComparing BFS and DFS, the big advantage of DFS is that it has much lower memory\\nrequirements than BFS because it’s not required to store all of the child pointers at each level.\\nDepending on the data and what we are looking for, either DFS or BFS can be advantageous. For\\nexample, in a family tree if we are looking for someone who’s still alive and if we assume that\\nperson would be at the bottom of the tree, then DFS is a better choice. BFS would take a very\\nlong time to reach that last level.\\nThe DFS algorithm finds the goal faster. Now, if we were looking for a family member who died\\na very long time ago, then that person would be closer to the top of the tree. In this case, BFS\\nfinds faster than DFS. So, the advantages of either vary depending on the data and what we are\\nlooking for.\\nDFS is related to preorder traversal of a tree. Like preorder traversal, DFS visits each node\\nbefore its children. The BFS algorithm works similar to level – order traversal of the trees.\\nIf someone asks whether DFS is better or BFS is better, the answer depends on the type of the\\nproblem that we are trying to solve. BFS visits each level one at a time, and if we know the\\nsolution we are searching for is at a low depth, then BFS is good. DFS is a better choice if the\\nsolution is at maximum depth. The below table shows the differences between DFS and BFS in\\nterms of their applications.\\n9.6 Topological Sort'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 450, 'file_type': 'pdf'}, page_content='Topological sort is an ordering of vertices in a directed acyclic graph [DAG] in which each node\\ncomes before all nodes to which it has outgoing edges. As an example, consider the course\\nprerequisite structure at universities. A directed edge (v,w) indicates that course v must be\\ncompleted before course w. Topological ordering for this example is the sequence which does not\\nviolate the prerequisite requirement. Every DAG may have one or more topological orderings.\\nTopological sort is not possible if the graph has a cycle, since for two vertices v and w on the\\ncycle, v precedes w and w precedes v.\\nTopological sort has an interesting property. All pairs of consecutive vertices in the sorted order\\nare connected by edges; then these edges form a directed Hamiltonian path [refer to Problems\\nSection] in the DAG. If a Hamiltonian path exists, the topological sort order is unique. If a\\ntopological sort does not form a Hamiltonian path, DAG can have two or more topological\\norderings. In the graph below: 7, 5, 3, 11, 8, 2, 9, 10 and 3, 5, 7, 8, 11, 2, 9, 10 are both\\ntopological orderings.\\nInitially, indegree is computed for all vertices, starting with the vertices which are having\\nindegree 0. That means consider the vertices which do not have any prerequisite. To keep track of\\nvertices with indegree zero we can use a queue.\\nAll vertices of indegree 0 are placed on queue. While the queue is not empty, a vertex v is\\nremoved, and all edges adjacent to v have their indegrees decremented. A vertex is put on the\\nqueue as soon as its indegree falls to 0. The topological ordering is the order in which the\\nvertices DeQueue.\\nThe time complexity of this algorithm is O(|E| + |V|) if adjacency lists are used.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 451, 'file_type': 'pdf'}, page_content='Total running time of topological sort is O(V + E).\\nNote: The Topological sorting problem can be solved with DFS. Refer to the Problems Section\\nfor the algorithm.\\nApplications of Topological Sorting\\n•\\nRepresenting course prerequisites\\n•\\nDetecting deadlocks\\n•\\nPipeline of computing jobs\\n•\\nChecking for symbolic link loop\\n•\\nEvaluating formulae in spreadsheet\\n9.7 Shortest Path Algorithms\\nLet us consider the other important problem of a graph. Given a graph G = (V, E) and a'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 452, 'file_type': 'pdf'}, page_content='distinguished vertex s, we need to find the shortest path from s to every other vertex in G. There\\nare variations in the shortest path algorithms which depend on the type of the input graph and are\\ngiven below.\\nVariations of Shortest Path Algorithms\\nShortest path in unweighted graph\\nShortest path in weighted graph\\nShortest path in weighted graph with negative edges\\nApplications of Shortest Path Algorithms\\n•\\nFinding fastest way to go from one place to another\\n•\\nFinding cheapest way to fly/send data from one city to another\\nShortest Path in Unweighted Graph\\nLet s be the input vertex from which we want to find the shortest path to all other vertices.\\nUnweighted graph is a special case of the weighted shortest-path problem, with all edges a\\nweight of 1. The algorithm is similar to BFS and we need to use the following data structures:\\n•\\nA distance table with three columns (each row corresponds to a vertex):\\n○\\nDistance from source vertex.\\n○\\nPath – contains the name of the vertex through which we get the shortest\\ndistance.\\n•\\nA queue is used to implement breadth-first search. It contains vertices whose\\ndistance from the source node has been computed and their adjacent vertices are to\\nbe examined.\\nAs an example, consider the following graph and its adjacency list representation.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 453, 'file_type': 'pdf'}, page_content='The adjacency list for this graph is:\\nLet s = C. The distance from C to C is 0. Initially, distances to all other nodes are not computed,\\nand we initialize the second column in the distance table for all vertices (except C) with -1 as\\nbelow.\\nAlgorithm'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 454, 'file_type': 'pdf'}, page_content='Running time: O(|E| + |V|), if adjacency lists are used. In for loop, we are checking the outgoing\\nedges for a given vertex and the sum of all examined edges in the while loop is equal to the\\nnumber of edges which gives O(|E|).\\nIf we use matrix representation the complexity is O(|V|2), because we need to read an entire row\\nin the matrix of length |V| in order to find the adjacent vertices for a given vertex.\\nShortest path in Weighted Graph [Dijkstra’s]\\nA famous solution for the shortest path problem was developed by Dijkstra. Dijkstra’s algorithm\\nis a generalization of the BFS algorithm. The regular BFS algorithm cannot solve the shortest path\\nproblem as it cannot guarantee that the vertex at the front of the queue is the vertex closest to\\nsource s.\\nBefore going to code let us understand how the algorithm works. As in unweighted shortest path\\nalgorithm, here too we use the distance table. The algorithm works by keeping the shortest\\ndistance of vertex v from the source in the Distance table. The value Distance[v] holds the\\ndistance from s to v. The shortest distance of the source to itself is zero. The Distance table for\\nall other vertices is set to –1 to indicate that those vertices are not already processed.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 455, 'file_type': 'pdf'}, page_content='After the algorithm finishes, the Distance table will have the shortest distance from source s to\\neach other vertex v. To simplify the understanding of Dijkstra’s algorithm, let us assume that the\\ngiven vertices are maintained in two sets. Initially the first set contains only the source element\\nand the second set contains all the remaining elements. After the kth iteration, the first set contains\\nk vertices which are closest to the source. These k vertices are the ones for which we have\\nalready computed the shortest distances from source.\\nNotes on Dijkstra’s Algorithm\\n•\\nIt uses greedy method: Always pick the next closest vertex to the source.\\n•\\nIt uses priority queue to store unvisited vertices by distance from s.\\n•\\nIt does not work with negative weights.\\nDifference between Unweighted Shortest Path and Dijkstra’s Algorithm\\n1)\\nTo represent weights in the adjacency list, each vertex contains the weights of the\\nedges (in addition to their identifier).\\n2)\\nInstead of ordinary queue we use priority queue [distances are the priorities] and the\\nvertex with the smallest distance is selected for processing.\\n3)\\nThe distance to a vertex is calculated by the sum of the weights of the edges on the\\npath from the source to that vertex.\\n4)\\nWe update the distances in case the newly computed distance is smaller than the old\\ndistance which we have already computed.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 456, 'file_type': 'pdf'}, page_content='The above algorithm can be better understood through an example, which will explain each step\\nthat is taken and how Distance is calculated. The weighted graph below has 5 vertices from A –\\nE.\\nThe value between the two vertices is known as the edge cost between two vertices. For\\nexample, the edge cost between A and C is 1. Dijkstra’s algorithm can be used to find the shortest\\npath from source A to the remaining vertices in the graph.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 457, 'file_type': 'pdf'}, page_content='Initially the Distance table is:\\nAfter the first step, from vertex A, we can reach B and C. So, in the Distance table we update the\\nreachability of B and C with their costs and the same is shown below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 458, 'file_type': 'pdf'}, page_content='Now, let us select the minimum distance among all. The minimum distance vertex is C. That\\nmeans, we have to reach other vertices from these two vertices (A and C). For example, B can be\\nreached from A and also from C. In this case we have to select the one which gives the lowest\\ncost. Since reaching B through C is giving the minimum cost (1 + 2), we update the Distance table\\nfor vertex B with cost 3 and the vertex from which we got this cost as C.\\nThe only vertex remaining is E. To reach E, we have to see all the paths through which we can\\nreach E and select the one which gives the minimum cost. We can see that if we use B as the\\nintermediate vertex through C we get the minimum cost.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 459, 'file_type': 'pdf'}, page_content='The final minimum cost tree which Dijkstra’s algorithm generates is:\\nPerformance\\nIn Dijkstra’s algorithm, the efficiency depends on the number of DeleteMins (V DeleteMins) and\\nupdates for priority queues (E updates) that are used. If a standard binary heap is used then the\\ncomplexity is O(ElogV).\\nThe term ElogV comes from E updates (each update takes logV) for the standard heap. If the set\\nused is an array then the complexity is O(E + V2).\\nDisadvantages of Dijkstra’s Algorithm'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 460, 'file_type': 'pdf'}, page_content='•\\nAs discussed above, the major disadvantage of the algorithm is that it does a blind\\nsearch, thereby wasting time and necessary resources.\\n•\\nAnother disadvantage is that it cannot handle negative edges. This leads to acyclic\\ngraphs and most often cannot obtain the right shortest path.\\nRelatives of Dijkstra’s Algorithm\\n•\\nThe Bellman- Ford algorithm computes single-source shortest paths in a weighted\\ndigraph. It uses the same concept as that of Dijkstra’s algorithm but can handle\\nnegative edges as well. It has more running time than Dijkstra’s algorithm.\\n•\\nPrim’s algorithm finds a minimum spanning tree for a connected weighted graph. It\\nimplies that a subset of edges that form a tree where the total weight of all the edges\\nin the tree is minimized.\\nBellman-Ford Algorithm\\nIf the graph has negative edge costs, then Dijkstra’s algorithm does not work. The problem is that\\nonce a vertex u is declared known, it is possible that from some other, unknown vertex v there is a\\npath back to u that is very negative. In such a case, taking a path from s to v back to u is better\\nthan going from s to u without using v. A combination of Dijkstra’s algorithm and unweighted\\nalgorithms will solve the problem. Initialize the queue with s. Then, at each stage, we DeQueue a\\nvertex v. We find all vertices W adjacent to v such that,\\ndistance to v + weight (v,w) < old distance to w\\nWe update w old distance and path, and place w on a queue if it is not already there. A bit can be\\nset for each vertex to indicate presence in the queue. We repeat the process until the queue is\\nempty.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 461, 'file_type': 'pdf'}, page_content='This algorithm works if there are no negative-cost cycles. Each vertex can DeQueue at most | V|\\ntimes, so the running time is O(|E|. |V|) if adjacency lists are used.\\nOverview of Shortest Path Algorithms\\nShortest path in unweighted graph [Modified BFS]\\nO(|E| + |V|)\\nShortest path in weighted graph [Dijkstra’s]\\nO(|E| log |V|)\\nShortest path in weighted graph with negative edges [Bellman – Ford] O(|E|.|V|)\\nShortest path in weighted acyclic graph\\nO(|E| + |V|)\\n9.8 Minimal Spanning Tree\\nThe Spanning tree of a graph is a subgraph that contains all the vertices and is also a tree. A\\ngraph may have many spanning trees. As an example, consider a graph with 4 vertices as shown\\nbelow. Let us assume that the corners of the graph are vertices.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 462, 'file_type': 'pdf'}, page_content='For this simple graph, we can have multiple spanning trees as shown below.\\nThe algorithm we will discuss now is minimum spanning tree in an undirected graph. We assume\\nthat the given graphs are weighted graphs. If the graphs are unweighted graphs then we can still\\nuse the weighted graph algorithms by treating all weights as equal. A minimum spanning tree of\\nan undirected graph G is a tree formed from graph edges that connect all the vertices of G with\\nminimum total cost (weights). A minimum spanning tree exists only if the graph is connected.\\nThere are two famous algorithms for this problem:\\n•\\nPrim’s Algorithm\\n•\\nKruskal’s Algorithm\\nPrim’s Algorithm\\nPrim’s algorithm is almost the same as Dijkstra’s algorithm. As in Dijkstra’s algorithm, in Prim’s\\nalgorithm we keep the values distance and paths in the distance table. The only exception is that\\nsince the definition of distance is different, the updating statement also changes a little. The\\nupdate statement is simpler than before.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 463, 'file_type': 'pdf'}, page_content='The entire implementation of this algorithm is identical to that of Dijkstra’s algorithm. The\\nrunning time is O(|V|2) without heaps [good for dense graphs], and O (ElogV) using binary heaps\\n[good for sparse graphs].\\nKruskal’s Algorithm\\nThe algorithm starts with V different trees (V is the vertices in the graph). While constructing the\\nminimum spanning tree, every time Kruskal’s alorithm selects an edge that has minimum weight\\nand then adds that edge if it doesn’t create a cycle. So, initially, there are | V | single-node trees in\\nthe forest. Adding an edge merges two trees into one. When the algorithm is completed, there will\\nbe only one tree, and that is the minimum spanning tree. There are two ways of implementing\\nKruskal’s algorithm:\\n•\\nBy using Disjoint Sets: Using UNION and FIND operations\\n•\\nBy using Priority Queues: Maintains weights in priority queue'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 464, 'file_type': 'pdf'}, page_content='The appropriate data structure is the UNION/FIND algorithm [for implementing forests]. Two\\nvertices belong to the same set if and only if they are connected in the current spanning forest.\\nEach vertex is initially in its own set. If u and v are in the same set, the edge is rejected because it\\nforms a cycle. Otherwise, the edge is accepted, and a UNION is performed on the two sets\\ncontaining u and v. As an example, consider the following graph (the edges show the weights).\\nNow let us perform Kruskal’s algorithm on this graph. We always select the edge which has\\nminimum weight.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 465, 'file_type': 'pdf'}, page_content='From the above graph, the edges which have minimum weight (cost) are: AD and BE. From these two we can\\nselect one of them and let us assume that we select AD (dotted line).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 466, 'file_type': 'pdf'}, page_content='11\\n\\nDDFs the next edge that has the west cost (6).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 467, 'file_type': 'pdf'}, page_content='BE now has the lowest cost and we select it (dotted lines indicate selected edges).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 468, 'file_type': 'pdf'}, page_content='Next, AC and CE have the low cost of 7 and we select AC.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 469, 'file_type': 'pdf'}, page_content='Then we select CE as its cost is 7 and it does not form a cycle.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 470, 'file_type': 'pdf'}, page_content='The next low cost edges are CB and EF. But if we select CB, then it forms a cycle. So we discard it. This is also\\nthe case with EF. So we should not select those two. And the next low cost is 9 (BD and EG). Selecting BD\\nforms a cycle so we discard it. Adding EG will not form a cycle and therefore with this edge we complete all\\nvertices of the graph.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 471, 'file_type': 'pdf'}, page_content='Note: For implementation of UNION and FIND operations, refer to the Disjoint Sets ADT\\nchapter.\\nThe worst-case running time of this algorithm is O(ElogE), which is dominated by the heap\\noperations. That means, since we are constructing the heap with E edges, we need O(ElogE) time\\nto do that.\\n9.9 Graph Algorithms: Problems & Solutions\\nProblem-1\\u2003\\u2003In an undirected simple graph with n vertices, what is the maximum number of\\nedges? Self-loops are not allowed.\\nSolution: Since every node can connect to all other nodes, the first node can connect to n – 1\\nnodes. The second node can connect to n – 2 nodes [since one edge is already there from the first\\nnode]. The total number of edges is: 1 + 2 + 3 + ··· + n – \\n edges.\\nProblem-2\\u2003\\u2003How many different adjacency matrices does a graph with n vertices and E edges\\nhave?\\nSolution: It’s equal to the number of permutations of n elements, i.e., n!.\\nProblem-3\\u2003\\u2003How many different adjacency lists does a graph with n vertices have?\\nSolution: It’s equal to the number of permutations of edges, i.e., E!.\\nProblem-4\\u2003\\u2003Which undirected graph representation is most appropriate for determining\\nwhether or not a vertex is isolated (is not connected to any other vertex)?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 472, 'file_type': 'pdf'}, page_content='Solution: Adjacency List. If we use the adjacency matrix, then we need to check the complete\\nrow to determine whether that vertex has edges or not. By using the adjacency list, it is very easy\\nto check, and it can be done just by checking whether that vertex has NULL for next pointer or not\\n[NULL indicates that the vertex is not connected to any other vertex].\\nProblem-5\\u2003\\u2003For checking whether there is a path from source s to target t, which one is best\\nbetween disjoint sets and DFS?\\nSolution: The table below shows the comparison between disjoint sets and DFS. The entries in\\nthe table represent the case for any pair of nodes (for s and t).\\nProblem-6\\u2003\\u2003What is the maximum number of edges a directed graph with n vertices can have\\nand still not contain a directed cycle?\\nSolution: The number is V (V – 1)/2. Any directed graph can have at most n2 edges. However,\\nsince the graph has no cycles it cannot contain a self loop, and for any pair x,y of vertices, at most\\none edge from (x,y) and (y,x) can be included. Therefore the number of edges can be at most (V2 –\\nV)/2 as desired. It is possible to achieve V(V – 1)/2 edges. Label n nodes 1,2... n and add an\\nedge (x, y) if and only if x < y. This graph has the appropriate number of edges and cannot contain\\na cycle (any path visits an increasing sequence of nodes).\\nProblem-7\\u2003\\u2003How many simple directed graphs with no parallel edges and self-loops are\\npossible in terms of V?\\nSolution: (V) × (V – 1). Since, each vertex can connect to V – 1 vertices without self-loops.\\nProblem-8\\u2003\\u2003What are the differences between DFS and BFS?\\nSolution:\\nDFS\\nBFS\\nBacktracking is possible from a dead end.\\nBacktracking is not possible.\\nVertices from which exploration is\\nincomplete are processed in a LIFO order\\nThe vertices to be explored are organized\\nas a FIFO queue.\\nThe search is done in one particular\\ndirection\\nThe vertices at the same level are\\nmaintained in parallel.\\nProblem-9\\u2003\\u2003Earlier in this chapter, we discussed minimum spanning tree algorithms. Now,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 473, 'file_type': 'pdf'}, page_content='give an algorithm for finding the maximum-weight spanning tree in a graph.\\nSolution:\\nUsing the given graph, construct a new graph with the same nodes and edges. But instead of using\\nthe same weights, take the negative of their weights. That means, weight of an edge = negative of\\nweight of the corresponding edge in the given graph. Now, we can use existing minimum\\nspanning tree algorithms on this new graph. As a result, we will get the maximum-weight\\nspanning tree in the original one.\\nProblem-10\\u2003\\u2003Give an algorithm for checking whether a given graph G has simple path from\\nsource s to destination d. Assume the graph G is represented using the adjacent matrix.\\nSolution: Let us assume that the structure for the graph is:\\nFor each vertex call DFS and check whether the current vertex is the same as the destination\\nvertex or not. If they are the same, then return 1. Otherwise, call the DFS on its unvisited\\nneighbors. One important thing to note here is that, we are calling the DFS algorithm on vertices\\nwhich are not yet visited.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 474, 'file_type': 'pdf'}, page_content='Time Complexity: O(E). In the above algorithm, for each node, since we are not calling DFS on\\nall of its neighbors (discarding through if condition), Space Complexity: O(V).\\nProblem-11\\u2003\\u2003Count simple paths for a given graph G has simple path from source s to\\ndestination d? Assume the graph is represented using the adjacent matrix.\\nSolution: Similar to the discussion in Problem-10, start at one node and call DFS on that node.\\nAs a result of this call, it visits all the nodes that it can reach in the given graph. That means it\\nvisits all the nodes of the connected component of that node. If there are any nodes that have not\\nbeen visited, then again start at one of those nodes and call DFS.\\nBefore the first DFS in each connected component, increment the connected components count.\\nContinue this process until all of the graph nodes are visited. As a result, at the end we will get\\nthe total number of connected components. The implementation based on this logic is given\\nbelow:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 475, 'file_type': 'pdf'}, page_content='Problem-12\\u2003\\u2003All pairs shortest path problem: Find the shortest graph distances between\\nevery pair of vertices in a given graph. Let us assume that the given graph does not have\\nnegative edges.\\nSolution: The problem can be solved using n applications of Dijkstra’s algorithm. That means we\\napply Dijkstra’s algorithm on each vertex of the given graph. This algorithm does not work if the\\ngraph has edges with negative weights.\\nProblem-13\\u2003\\u2003In Problem-12, how do we solve the all pairs shortest path problem if the graph\\nhas edges with negative weights?\\nSolution: This can be solved by using the Floyd – Warshall algorithm. This algorithm also\\nworks in the case of a weighted graph where the edges have negative weights. This algorithm is\\nan example of Dynamic Programming -refer to the Dynamic Programming chapter.\\nProblem-14\\u2003\\u2003DFS Application: Cut Vertex or Articulation Points\\nSolution: In an undirected graph, a cut vertex (or articulation point) is a vertex, and if we remove\\nit, then the graph splits into two disconnected components. As an example, consider the following\\nfigure. Removal of the “D” vertex divides the graph into two connected components ({E,F} and\\n{A,B, C, G}).\\nSimilarly, removal of the “C” vertex divides the graph into ({G} and {A, B,D,E,F}). For this\\ngraph, A and C are the cut vertices.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 476, 'file_type': 'pdf'}, page_content='Note: A connected, undirected graph is called bi – connected if the graph is still connected after\\nremoving any vertex.\\nDFS provides a linear-time algorithm (O(n)) to find all cut vertices in a connected graph. Starting\\nat any vertex, call a DFS and number the nodes as they are visited. For each vertex v, we call this\\nDFS number dfsnum(v). The tree generated with DFS traversal is called DFS spanning tree.\\nThen, for every vertex v in the DFS spanning tree, we compute the lowest-numbered vertex,\\nwhich we call low(v), that is reachable from v by taking zero or more tree edges and then\\npossibly one back edge (in that order).\\nBased on the above discussion, we need the following information for this algorithm: the dfsnum\\nof each vertex in the DFS tree (once it gets visited), and for each vertex v, the lowest depth of\\nneighbors of all descendants of v in the DFS tree, called the low.\\nThe dfsnum can be computed during DFS. The low of v can be computed after visiting all\\ndescendants of v (i.e., just before v gets popped off the DFS stack) as the minimum of the dfsnum\\nof all neighbors of v (other than the parent of v in the DFS tree) and the low of all children of v in\\nthe DFS tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 477, 'file_type': 'pdf'}, page_content='The root vertex is a cut vertex if and only if it has at least two children. A non-root vertex u is a\\ncut vertex if and only if there is a son v of u such that low(v) ≥ dfsnum(u). This property can be\\ntested once the DFS is returned from every child of u (that means, just before u gets popped off\\nthe DFS stack), and if true, u separates the graph into different bi-connected components. This can\\nbe represented by computing one bi-connected component out of every such v (a component\\nwhich contains v will contain the sub-tree of v, plus u), and then erasing the sub-tree of v from the\\ntree.\\nFor the given graph, the DFS tree with dfsnum/low can be given as shown in the figure below.\\nThe implementation for the above discussion is:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 478, 'file_type': 'pdf'}, page_content='Problem-15\\u2003\\u2003Let G be a connected graph of order n. What is the maximum number of cut-\\nvertices that G can contain?\\nSolution: n – 2. As an example, consider the following graph. In the graph below, except for the\\nvertices 1 and n, all the remaining vertices are cut vertices. This is because removing 1 and n\\nvertices does not split the graph into two. This is a case where we can get the maximum number\\nof cut vertices.\\nProblem-16\\u2003\\u2003DFS Application: Cut Bridges or Cut Edges\\nSolution:\\nDefinition: Let G be a connected graph. An edge uv in G is called a bridge of G if G – uv is\\ndisconnected.\\nAs an example, consider the following graph.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 479, 'file_type': 'pdf'}, page_content='In the above graph, if we remove the edge uv then the graph splits into two components. For this\\ngraph, uv is a bridge. The discussion we had for cut vertices holds good for bridges also. The\\nonly change is, instead of printing the vertex, we give the edge. The main observation is that an\\nedge (u, v) cannot be a bridge if it is part of a cycle. If (u, v) is not part of a cycle, then it is a\\nbridge.\\nWe can detect cycles in DFS by the presence of back edges, (u, v) is a bridge if and only if none\\nof v or v’s children has a back edge to u or any of u’s ancestors. To detect whether any of v’s\\nchildren has a back edge to u’s parent, we can use a similar idea as above to see what is the\\nsmallest dfsnum reachable from the subtree rooted at v.\\nProblem-17\\u2003\\u2003DFS Application: Discuss Euler Circuits'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 480, 'file_type': 'pdf'}, page_content='Solution: Before discussing this problem let us see the terminology:\\n•\\nEulerian tour- a path that contains all edges without repetition.\\n•\\nEulerian circuit – a path that contains all edges without repetition and starts and\\nends in the same vertex.\\n•\\nEulerian graph – a graph that contains an Eulerian circuit.\\n•\\nEven vertex: a vertex that has an even number of incident edges.\\n•\\nOdd vertex: a vertex that has an odd number of incident edges.\\nEuler circuit: For a given graph we have to reconstruct the circuits using a pen, drawing each line\\nexactly once. We should not lift the pen from the paper while drawing. That means, we must find a\\npath in the graph that visits every edge exactly once and this problem is called an Euler path\\n(also called Euler tour) or Euler circuit problem. This puzzle has a simple solution based on\\nDFS.\\nAn Euler circuit exists if and only if the graph is connected and the number of neighbors of each\\nvertex is even. Start with any node, select any untraversed outgoing edge, and follow it. Repeat\\nuntil there are no more remaining unselected outgoing edges. For example, consider the following\\ngraph: A legal Euler Circuit of this graph is 0 1 3 4 1 2 3 5 4 2 0.\\nIf we start at vertex 0, we can select the edge to vertex 1, then select the edge to vertex 2, then\\nselect the edge to vertex 0. There are now no remaining unchosen edges from vertex 0:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 481, 'file_type': 'pdf'}, page_content='We now have a circuit 0,1,2,0 that does not traverse every edge. So, we pick some other vertex\\nthat is on that circuit, say vertex 1. We then do another depth first search of the remaining edges.\\nSay we choose the edge to node 3, then 4, then 1. Again we are stuck. There are no more\\nunchosen edges from node 1. We now splice this path 1,3,4,1 into the old path 0,1,2,0 to get:\\n0,1,3,4,1,2,0. The unchosen edges now look like this:\\nWe can pick yet another vertex to start another DFS. If we pick vertex 2, and splice the path\\n2,3,5,4,2, then we get the final circuit 0,1,3,4,1,2,3,5,4,2,0.\\nA similar problem is to find a simple cycle in an undirected graph that visits every vertex. This is\\nknown as the Hamiltonian cycle problem. Although it seems almost identical to the Euler circuit\\nproblem, no efficient algorithm for it is known.\\nNotes:\\n•\\nA connected undirected graph is Eulerian if and only if every graph vertex has an\\neven degree, or exactly two vertices with an odd degree.\\n•\\nA directed graph is Eulerian if it is strongly connected and every vertex has an equal\\nin and out degree.\\nApplication: A postman has to visit a set of streets in order to deliver mails and packages. He\\nneeds to find a path that starts and ends at the post-office, and that passes through each street'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 482, 'file_type': 'pdf'}, page_content='(edge) exactly once. This way the postman will deliver mails and packages to all the necessary\\nstreets, and at the same time will spend minimum time/effort on the road.\\nProblem-18\\u2003\\u2003DFS Application: Finding Strongly Connected Components.\\nSolution: This is another application of DFS. In a directed graph, two vertices u and v are\\nstrongly connected if and only if there exists a path from u to v and there exists a path from v to u.\\nThe strong connectedness is an equivalence relation.\\n•\\nA vertex is strongly connected with itself\\n•\\nIf a vertex u is strongly connected to a vertex v, then v is strongly connected to u\\n•\\nIf a vertex u is strongly connected to a vertex v, and v is strongly connected to a\\nvertex x, then u is strongly connected to x\\nWhat this says is, for a given directed graph we can divide it into strongly connected components.\\nThis problem can be solved by performing two depth-first searches. With two DFS searches we\\ncan test whether a given directed graph is strongly connected or not. We can also produce the\\nsubsets of vertices that are strongly connected.\\nAlgorithm\\n•\\nPerform DFS on given graph G.\\n•\\nNumber vertices of given graph G according to a post-order traversal of depth-first\\nspanning forest.\\n•\\nConstruct graph Gr by reversing all edges in G.\\n•\\nPerform DFS on Gr: Always start a new DFS (initial call to Visit) at the highest-\\nnumbered vertex.\\n•\\nEach tree in the resulting depth-first spanning forest corresponds to a strongly-\\nconnected component.\\nWhy this algorithm works?\\nLet us consider two vertices, v and w. If they are in the same strongly connected component, then\\nthere are paths from v to W and from w to v in the original graph G, and hence also in Gr. If two\\nvertices v and w are not in the same depth-first spanning tree of Gr, clearly they cannot be in the\\nsame strongly connected component. As an example, consider the graph shown below on the left.\\nLet us assume this graph is G.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 483, 'file_type': 'pdf'}, page_content='Now, as per the algorithm, performing DFS on this G graph gives the following diagram. The\\ndotted line from C to A indicates a back edge.\\nNow, performing post order traversal on this tree gives: D,C,B and A.\\nVertex\\nPost Order Number\\nA\\n4\\nB\\n3\\nC\\n2\\nD\\n1\\nNow reverse the given graph G and call it Gr and at the same time assign postorder numbers to\\nthe vertices. The reversed graph Gr will look like:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 484, 'file_type': 'pdf'}, page_content='The last step is performing DFS on this reversed graph Gr. While doing DFS, we need to\\nconsider the vertex which has the largest DFS number. So, first we start at A and with DFS we go\\nto C and then B. At B, we cannot move further. This says that {A, B, C} is a strongly connected\\ncomponent. Now the only remaining element is D and we end our second DFS at D. So the\\nconnected components are: {A, B, C} and {D}.\\nThe implementation based on this discussion can be shown as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 485, 'file_type': 'pdf'}, page_content='Problem-19\\u2003\\u2003Count the number of connected components of Graph G which is represented in\\nthe adjacent matrix.\\nSolution: This problem can be solved with one extra counter in DFS.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 486, 'file_type': 'pdf'}, page_content='Time Complexity: Same as that of DFS and it depends on implementation. With adjacency matrix\\nthe complexity is O(|E| + |V|) and with adjacency matrix the complexity is O(|V|2).\\nProblem-20\\u2003\\u2003Can we solve the Problem-19, using BFS?\\nSolution: Yes. This problem can be solved with one extra counter in BFS.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 487, 'file_type': 'pdf'}, page_content='Time Complexity: Same as that of BFS and it depends on implementation. With adjacency matrix\\nthe complexity is O(|E| + |V|) and with adjacency matrix the complexity is O(|V|2).\\nProblem-21\\u2003\\u2003Let us assume that G(V,E) is an undirected graph. Give an algorithm for finding a\\nspanning tree which takes O(|E|) time complexity (not necessarily a minimum spanning\\ntree).\\nSolution: The test for a cycle can be done in constant time, by marking vertices that have been\\nadded to the set S. An edge will introduce a cycle, if both its vertices have already been marked.\\nAlgorithm:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 488, 'file_type': 'pdf'}, page_content='Problem-22\\u2003\\u2003Is there any other way of solving 0?\\nSolution: Yes. We can run BFS and find the BFS tree for the graph (level order tree of the graph).\\nThen start at the root element and keep moving to the next levels and at the same time we have to\\nconsider the nodes in the next level only once. That means, if we have a node with multiple input\\nedges then we should consider only one of them; otherwise they will form a cycle.\\nProblem-23\\u2003\\u2003Detecting a cycle in an undirected graph\\nSolution: An undirected graph is acyclic if and only if a DFS yields no back edges, edges (u, v)\\nwhere v has already been discovered and is an ancestor of u.\\n•\\nExecute DFS on the graph.\\n•\\nIf there is a back edge – the graph has a cycle.\\nIf the graph does not contain a cycle, then |E| < |V| and DFS cost O(|V|). If the graph contains a\\ncycle, then a back edge is discovered after 2|V| steps at most.\\nProblem-24\\u2003\\u2003Detecting a cycle in DAG\\nSolution:\\nCycle detection on a graph is different than on a tree. This is because in a graph, a node can have\\nmultiple parents. In a tree, the algorithm for detecting a cycle is to do a depth first search, marking\\nnodes as they are encountered. If a previously marked node is seen again, then a cycle exists. This\\nwon’t work on a graph. Let us consider the graph shown in the figure below. If we use a tree\\ncycle detection algorithm, then it will report the wrong result. That means that this graph has a\\ncycle in it. But the given graph does not have a cycle in it. This is because node 3 will be seen\\ntwice in a DFS starting at node 1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 489, 'file_type': 'pdf'}, page_content='The cycle detection algorithm for trees can easily be modified to work for graphs. The key is that\\nin a DFS of an acyclic graph, a node whose descendants have all been visited can be seen again\\nwithout implying a cycle. But, if a node is seen for the second time before all its descendants have\\nbeen visited, then there must be a cycle. Can you see why this is? Suppose there is a cycle\\ncontaining node A. This means that A must be reachable from one of its descendants. So when the\\nDFS is visiting that descendant, it will see A again, before it has finished visiting all of A’s\\ndescendants. So there is a cycle. In order to detect cycles, we can modify the depth first search.\\nTime Complexity: O(V + E).\\nProblem-25\\u2003\\u2003Given a directed acyclic graph, give an algorithm for finding its depth.\\nSolution: If it is an undirected graph, we can use the simple unweighted shortest path algorithm\\n(check Shortest Path Algorithms section). We just need to return the highest number among all\\ndistances. For directed acyclic graph, we can solve by following the similar approach which we\\nused for finding the depth in trees. In trees, we have solved this problem using level order'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 490, 'file_type': 'pdf'}, page_content='traversal (with one extra special symbol to indicate the end of the level).\\nTotal running time is O(V + E).\\nProblem-26\\u2003\\u2003How many topological sorts of the following dag are there?\\nSolution: If we observe the above graph there are three stages with 2 vertices. In the early'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 491, 'file_type': 'pdf'}, page_content='discussion of this chapter, we saw that topological sort picks the elements with zero indegree at\\nany point of time. At each of the two vertices stages, we can first process either the top vertex or\\nthe bottom vertex. As a result, at each of these stages we have two possibilities. So the total\\nnumber of possibilities is the multiplication of possibilities at each stage and that is, 2 × 2 × 2 =\\n8.\\nProblem-27\\u2003\\u2003Unique topological ordering: Design an algorithm to determine whether a\\ndirected graph has a unique topological ordering.\\nSolution: A directed graph has a unique topological ordering if and only if there is a directed\\nedge between each pair of consecutive vertices in the topological order. This can also be defined\\nas: a directed graph has a unique topological ordering if and only if it has a Hamiltonian path. If\\nthe digraph has multiple topological orderings, then a second topological order can be obtained\\nby swapping a pair of consecutive vertices.\\nProblem-28\\u2003\\u2003Let us consider the prerequisites for courses at IIT Bombay. Suppose that all\\nprerequisites are mandatory, every course is offered every semester, and there is no limit\\nto the number of courses we can take in one semester. We would like to know the minimum\\nnumber of semesters required to complete the major. Describe the data structure we would\\nuse to represent this problem, and outline a linear time algorithm for solving it.\\nSolution: Use a directed acyclic graph (DAG). The vertices represent courses and the edges\\nrepresent the prerequisite relation between courses at IIT Bombay. It is a DAG, because the\\nprerequisite relation has no cycles.\\nThe number of semesters required to complete the major is one more than the longest path in the\\ndag. This can be calculated on the DFS tree recursively in linear time. The longest path out of a\\nvertex x is 0 if x has outdegree 0, otherwise it is 1 + max {longest path out of y | (x,y) is an edge\\nof G}.\\nProblem-29\\u2003\\u2003At a university let’s say IIT Bombay), there is a list of courses along with their\\nprerequisites. That means, two lists are given:\\nA – Courses list\\nB – Prerequisites: B contains couples (x,y) where x,y ∈ A indicating that course x can’t be\\ntaken before course y.\\nLet us consider a student who wants to take only one course in a semester. Design a schedule\\nfor this student.\\nExample: A = {C-Lang, Data Structures, OS, CO, Algorithms, Design Patterns,\\nProgramming}. B = { (C-Lang, CO), (OS, CO), (Data Structures, Algorithms), (Design\\nPatterns, Programming) }. One possible schedule could be:\\nSemester 1:\\nData Structures\\nSemester 2:\\nAlgorithms\\nSemester 3:\\nC-Lang'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 492, 'file_type': 'pdf'}, page_content='Semester 4:\\nOS\\nSemester 5:\\nCO\\nSemester 6:\\nDesign Patterns\\nSemester 7:\\nProgramming\\nSolution: The solution to this problem is exactly the same as that of topological sort. Assume that\\nthe courses names are integers in the range [1..n], n is known (n is not constant). The relations\\nbetween the courses will be represented by a directed graph G = (V,E), where V are the set of\\ncourses and if course i is prerequisite of course j, E will contain the edge (i,j). Let us assume that\\nthe graph will be represented as an Adjacency list.\\nFirst, let’s observe another algorithm to topologically sort a DAG in O(|V| + |E|).\\n•\\nFind in-degree of all the vertices - O(|V| + |E|)\\n•\\nRepeat:\\nFind a vertex v with in-degree=0 - O(|V|)\\nOutput v and remove it from G, along with its edges - O(|V|)\\nReduce the in-degree of each node u such as (v, u) was an edge in G and keep a list\\nof vertices with in-degree=0 – O(degree(v))\\nRepeat the process until all the vertices are removed\\nThe time complexity of this algorithm is also the same as that of the topological sort and it is O(|V|\\n+ |E|).\\nProblem-30\\u2003\\u2003In Problem-29, a student wants to take all the courses in A, in the minimal\\nnumber of semesters. That means the student is ready to take any number of courses in a\\nsemester. Design a schedule for this scenario. One possible schedule is:\\nSemester 1: C-Lang, OS, Design Patterns\\nSemester 2: Data Structures, CO, Programming\\nSemester 3: Algorithms\\nSolution: A variation of the above topological sort algorithm with a slight change: In each\\nsemester, instead of taking one subject, take all the subjects with zero indegree. That means,\\nexecute the algorithm on all the nodes with degree 0 (instead of dealing with one source in each\\nstage, all the sources will be dealt and printed).\\nTime Complexity: O(|V| + |E|).\\nProblem-31\\u2003\\u2003LCA of a DAG: Given a DAG and two vertices v and w, find the lowest\\ncommon ancestor (LCA) of v and w. The LCA of v and w is an ancestor of v and w that\\nhas no descendants that are also ancestors of v and w.\\nHint: Define the height of a vertex v in a DAG to be the length of the longest path from root to v.\\nAmong the vertices that are ancestors of both v and w, the one with the greatest height is an LCA'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 493, 'file_type': 'pdf'}, page_content='of v and w.\\nProblem-32\\u2003\\u2003Shortest ancestral path: Given a DAG and two vertices v and w, find the\\nshortest ancestral path between v and w. An ancestral path between v and w is a common\\nancestor x along with a shortest path from v to x and a shortest path from w to x. The\\nshortest ancestral path is the ancestral path whose total length is minimized.\\nHint: Run BFS two times. First run from v and second time from w. Find a DAG where the\\nshortest ancestral path goes to a common ancestor x that is not an LCA.\\nProblem-33\\u2003\\u2003Let us assume that we have two graphs G1 and G2. How do we check whether\\nthey are isomorphic or not?\\nSolution: There are many ways of representing the same graph. As an example, consider the\\nfollowing simple graph. It can be seen that all the representations below have the same number of\\nvertices and the same number of edges.\\nDefinition: Graphs G1 = {V1, E1} and G2 = {V2, E2} are isomorphic if\\n1)\\nThere is a one-to-one correspondence from V1 to V2 and\\n2)\\nThere is a one-to-one correspondence from E1 to E2 that map each edge of G1 to G2.\\nNow, for the given graphs how do we check whether they are isomorphic or not?\\nIn general, it is not a simple task to prove that two graphs are isomorphic. For that reason we\\nmust consider some properties of isomorphic graphs. That means those properties must be\\nsatisfied if the graphs are isomorphic. If the given graph does not satisfy these properties then we\\nsay they are not isomorphic graphs.\\nProperty: Two graphs are isomorphic if and only if for some ordering of their vertices their\\nadjacency matrices are equal.\\nBased on the above property we decide whether the given graphs are isomorphic or not. I order\\nto check the property, we need to do some matrix transformation operations.\\nProblem-34\\u2003\\u2003How many simple undirected non-isomorphic graphs are there with n vertices?\\nSolution: We will try to answer this question in two steps. First, we count all labeled graphs.\\nAssume all the representations below are labeled with {1,2,3} as vertices. The set of all such\\ngraphs for n = 3 are:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 494, 'file_type': 'pdf'}, page_content='There are only two choices for each edge: it either exists or it does not. Therefore, since the\\nmaximum number of edges is \\n (and since the maximum number of edges in an undirected\\ngraph with n vertices is \\n, the total number of undirected labeled graphs is \\n.\\nProblem-35\\u2003\\u2003Hamiltonian path in DAGs: Given a DAG, design a linear time algorithm to\\ndetermine whether there is a path that visits each vertex exactly once.\\nSolution: The Hamiltonian path problem is an NP-Complete problem (for more details ref\\nComplexity Classes chapter). To solve this problem, we will try to give the approximation\\nalgorithm (which solves the problem, but it may not always produce the optimal solution).\\nLet us consider the topological sort algorithm for solving this problem. Topological sort has an\\ninteresting property: that if all pairs of consecutive vertices in the sorted order are connected by\\nedges, then these edges form a directed Hamiltonian path in the DAG. If a Hamiltonian path\\nexists, the topological sort order is unique. Also, if a topological sort does not form a\\nHamiltonian path, the DAG will have two or more topological orderings.\\nApproximation Algorithm: Compute a topological sort and check if there is an edge between each\\nconsecutive pair of vertices in the topological order.\\nIn an unweighted graph, find a path from s to t that visits each vertex exactly once. The basic\\nsolution based on backtracking is, we start at s and try all of its neighbors recursively, making\\nsure we never visit the same vertex twice. The algorithm based on this implementation can be\\ngiven as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 495, 'file_type': 'pdf'}, page_content='Note that if we have a partial path from s to u using vertices s = v1, v2,..., vk = u, then we don’t\\ncare about the order in which we visited these vertices so as to figure out which vertex to visit\\nnext. All that we need to know is the set of vertices we have seen (the seenTable[] array) and\\nwhich vertex we are at right now (u). There are 2n possible sets of vertices and n choices for u. In\\nother words, there are 2n possible seenTable[] arrays and n different parameters to\\nHamiltonian_path(). What Hamiltonian_path() does during any particular recursive call is\\ncompletely determined by the seenTable[ ] array and the parameter u.\\nProblem-36\\u2003\\u2003For a given graph G with n vertices how many trees we can construct?\\nSolution: There is a simple formula for this problem and it is named after Arthur Cayley. For a\\ngiven graph with n labeled vertices the formula for finding number of trees on is nn–2. Below, the\\nnumber of trees with different n values is shown.\\nProblem-37\\u2003\\u2003For a given graph G with n vertices how many spanning trees can we construct?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 496, 'file_type': 'pdf'}, page_content='Solution: The solution to this problem is the same as that of Problem-36. It is just another way of\\nasking the same question. Because the number of edges in both regular tree and spanning tree are\\nthe same.\\nProblem-38\\u2003\\u2003The Hamiltonian cycle problem: Is it possible to traverse each of the vertices\\nof a graph exactly once, starting and ending at the same vertex?\\nSolution: Since the Hamiltonian path problem is an NP-Complete problem, the Hamiltonian\\ncycle problem is an NP-Complete problem. A Hamiltonian cycle is a cycle that traverses every\\nvertex of a graph exactly once. There are no known conditions in which are both necessary and\\nsufficient, but there are a few sufficient conditions.\\n•\\nFor a graph to have a Hamiltonian cycle the degree of each vertex must be two or\\nmore.\\n•\\nThe Petersen graph does not have a Hamiltonian cycle and the graph is given below.\\n•\\nIn general, the more edges a graph has, the more likely it is to have a Hamiltonian\\ncycle.\\n•\\nLet G be a simple graph with n ≥ 3 vertices. If every vertex has a degree of at least \\n, then G has a Hamiltonian cycle.\\n•\\nThe best known algorithm for finding a Hamiltonian cycle has an exponential worst-\\ncase complexity.\\nNote: For the approximation algorithm of Hamiltonian path, refer to the Dynamic Programming\\nchapter.\\nProblem-39\\u2003\\u2003What is the difference between Dijkstra’s and Prim’s algorithm?\\nSolution: Dijkstra’s algorithm is almost identical to that of Prim’s. The algorithm begins at a\\nspecific vertex and extends outward within the graph until all vertices have been reached. The\\nonly distinction is that Prim’s algorithm stores a minimum cost edge whereas Dijkstra’s algorithm\\nstores the total cost from a source vertex to the current vertex. More simply, Dijkstra’s algorithm\\nstores a summation of minimum cost edges whereas Prim’s algorithm stores at most one minimum\\ncost edge.\\nProblem-40\\u2003\\u2003Reversing Graph: : Give an algorithm that returns the reverse of the directed\\ngraph (each edge from v to w is replaced by an edge from w to v).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 497, 'file_type': 'pdf'}, page_content='Solution: In graph theory, the reverse (also called transpose) of a directed graph G is another\\ndirected graph on the same set of vertices with all the edges reversed. That means, if G contains\\nan edge (u, v) then the reverse of G contains an edge (v, u) and vice versa.\\nAlgorithm:\\nProblem-41\\u2003\\u2003Travelling Sales Person Problem: Find the shortest path in a graph that visits\\neach vertex at least once, starting and ending at the same vertex?\\nSolution: The Traveling Salesman Problem (TSP) is related to finding a Hamiltonian cycle.\\nGiven a weighted graph G, we want to find the shortest cycle (may be non-simple) that visits all\\nthe vertices.\\nApproximation algorithm: This algorithm does not solve the problem but gives a solution which\\nis within a factor of 2 of optimal (in the worst-case).\\n1)\\nFind a Minimal Spanning Tree (MST).\\n2)\\nDo a DFS of the MST.\\nFor details, refer to the chapter on Complexity Classes.\\nProblem-42\\u2003\\u2003Discuss Bipartite matchings?\\nSolution: In Bipartite graphs, we divide the graphs in to two disjoint sets, and each edge connects\\na vertex from one set to a vertex in another subset (as shown in figure).\\nDefinition: A simple graph G = (V, E) is called a bipartite graph if its vertices can be divided\\ninto two disjoint sets V = V1 ⋃ V2, such that every edge has the form e = (a,b) where a ∈ V1 and\\nb ∈ V2. One important condition is that no vertices both in V1 or both in V2 are connected.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 498, 'file_type': 'pdf'}, page_content='Properties of Bipartite Graphs\\n•\\nA graph is called bipartite if and only if the given graph does not have an odd length\\ncycle.\\n•\\nA complete bipartite graph Km,n is a bipartite graph that has each vertex from one\\nset adjacent to each vertex from another set.\\n•\\nA subset of edges M ⊂ E is a matching if no two edges have a common vertex. As\\nan example, matching sets of edges are represented with dotted lines. A matching M\\nis called maximum if it has the largest number of possible edges. In the graphs, the\\ndotted edges represent the alternative matching for the given graph.\\n•\\nA matching M is perfect if it matches all vertices. We must have V1 = V2 in order to\\nhave perfect matching.\\n•\\nAn alternating path is a path whose edges alternate between matched and\\nunmatched edges. If we find an alternating path, then we can improve the matching.\\nThis is because an alternating path consists of matched and unmatched edges. The\\nnumber of unmatched edges exceeds the number of matched edges by one.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 499, 'file_type': 'pdf'}, page_content='Therefore, an alternating path always increases the matching by one.\\nThe next question is, how do we find a perfect matching? Based on the above theory and\\ndefinition, we can find the perfect matching with the following approximation algorithm.\\nMatching Algorithm (Hungarian algorithm)\\n1)\\nStart at unmatched vertex.\\n2)\\nFind an alternating path.\\n3)\\nIf it exists, change matching edges to no matching edges and conversely. If it does not\\nexist, choose another unmatched vertex.\\n4)\\nIf the number of edges equals V/2, stop. Otherwise proceed to step 1 and repeat, as\\nlong as all vertices have been examined without finding any alternating paths.\\nTime Complexity of the Matching Algorithm: The number of iterations is in O(V). The\\ncomplexity of finding an alternating path using BFS is O(E). Therefore, the total time complexity\\nis O(V × E).\\nProblem-43\\u2003\\u2003Marriage and Personnel Problem?\\nMarriage Problem: There are X men and Y women who desire to get married. Participants\\nindicate who among the opposite sex could be a potential spouse for them. Every woman can be\\nmarried to at most one man, and every man to at most one woman. How can we marry everybody\\nto someone they like?\\nPersonnel Problem: You are the boss of a company. The company has M workers and N jobs.\\nEach worker is qualified to do some jobs, but not others. How will you assign jobs to each\\nworker?\\nSolution: These two cases are just another way of asking about bipartite graphs, and the solution\\nis the same as that of Problem-42.\\nProblem-44\\u2003\\u2003How many edges will be there in complete bipartite graph Km,n?\\nSolution: m × n. This is because each vertex in the first set can connect all vertices in the second\\nset.\\nProblem-45\\u2003\\u2003A graph is called a regular graph if it has no loops and multiple edges where\\neach vertex has the same number of neighbors; i.e., every vertex has the same degree.\\nNow, if Km,n is a regular graph, what is the relation between m and n?\\nSolution: Since each vertex should have the same degree, the relation should be m = n.\\nProblem-46\\u2003\\u2003What is the maximum number of edges in the maximum matching of a bipartite\\ngraph with n vertices?\\nSolution: From the definition of matching, we should not have edges with common vertices. So'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 500, 'file_type': 'pdf'}, page_content='in a bipartite graph, each vertex can connect to only one vertex. Since we divide the total vertices\\ninto two sets, we can get the maximum number of edges if we divide them in half. Finally the\\nanswer is .\\nProblem-47\\u2003\\u2003Discuss Planar Graphs. Planar graph: Is it possible to draw the edges of a\\ngraph in such a way that the edges do not cross?\\nSolution: A graph G is said to be planar if it can be drawn in the plane in such a way that no two\\nedges meet each other except at a vertex to which they are incident. Any such drawing is called a\\nplane drawing of G. As an example consider the below graph:\\nThis graph we can easily convert to a planar graph as below (without any crossed edges).\\nHow do we decide whether a given graph is planar or not?\\nThe solution to this problem is not simple, but researchers have found some interesting properties\\nthat we can use to decide whether the given graph is a planar graph or not.\\nProperties of Planar Graphs\\n•\\nIf a graph G is a connected planar simple graph with V vertices, where V = 3 and E\\nedges, then E = 3V – 6.\\n•\\nK5 is non-planar. [K5 stands for complete graph with 5 vertices].\\n•\\nIf a graph G is a connected planar simple graph with V vertices and E edges, and no'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 501, 'file_type': 'pdf'}, page_content='triangles, then E = 2V – 4.\\n•\\nK3,3 is non-planar. [K3,3 stands for bipartite graph with 3 vertices on one side and\\nthe other 3 vertices on the other side. K3,3 contains 6 vertices].\\n•\\nIf a graph G is a connected planar simple graph, then G contains at least one vertex\\nof 5 degrees or less.\\n•\\nA graph is planar if and only if it does not contain a subgraph that has K5 and K3,3 as\\na contraction.\\n•\\nIf a graph G contains a nonplanar graph as a subgraph, then G is non-planar.\\n•\\nIf a graph G is a planar graph, then every subgraph of G is planar.\\n•\\nFor any connected planar graph G = (V,E), the following formula should hold: V + F\\n– E = 2, where F stands for the number of faces.\\n•\\nFor any planar graph G = (V, E) with K components, the following formula holds: V\\n+ F – E = 1 + K.\\nIn order to test the planarity of a given graph, we use these properties and decide whether it is a\\nplanar graph or not. Note that all the above properties are only the necessary conditions but not\\nsufficient.\\nProblem-48\\u2003\\u2003How many faces does K2,3 have?\\nSolution: From the above discussion, we know that V + F – E = 2, and from an earlier problem\\nwe know that E = m × n = 2 × 3 = 6 and V = m + n = 5. ∴ 5 + F – 6 = 2 ⇒ F = 3.\\nProblem-49\\u2003\\u2003Discuss Graph Coloring\\nSolution: A k –coloring of a graph G is an assignment of one color to each vertex of G such that\\nno more than k colors are used and no two adjacent vertices receive the same color. A graph is\\ncalled k –colorable if and only if it has a k –coloring.\\nApplications of Graph Coloring: The graph coloring problem has many applications such as\\nscheduling, register allocation in compilers, frequency assignment in mobile radios, etc.\\nClique: A clique in a graph G is the maximum complete subgraph and is denoted by ω(G).\\nChromatic number: The chromatic number of a graph G is the smallest number k such that G is k\\n–colorable, and it is denoted by X (G).\\nThe lower bound for X (G) is ω(G), and that means ω(G) ≤ X (G).\\nProperties of Chromatic number: Let G be a graph with n vertices and G′ is its complement.\\nThen,\\n•\\nX (G) ≤ ∆ (G) + 1, where ∆ (G) is the maximum degree of G.\\n•\\nX(G) ω(G′) ≥ n\\n•\\nX(G) + ω(G′) ≤ n + 1'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 502, 'file_type': 'pdf'}, page_content='•\\nX(G) + (G′) ≤ n + 1\\nK-colorability problem: Given a graph G = (V,E) and a positive integer k ≤ V. Check whether G\\nis k –colorable?\\nThis problem is NP-complete and will be discussed in detail in the chapter on Complexity\\nClasses.\\nGraph coloring algorithm: As discussed earlier, this problem is NP-Complete. So we do not\\nhave a polynomial time algorithm to determine X(G). Let us consider the following approximation\\n(no efficient) algorithm.\\n•\\nConsider a graph G with two non-adjacent vertices a and b. The connection G1 is\\nobtained by joining the two non-adjacent vertices a and b with an edge. The\\ncontraction G2 is obtained by shrinking {a,b} into a single vertex c(a, b) and by\\njoining it to each neighbor in G of vertex a and of vertex b (and eliminating multiple\\nedges).\\n•\\nA coloring of G in which a and b have the same color yields a coloring of G1. A\\ncoloring of G in which a and b have different colors yields a coloring of G2.\\n•\\nRepeat the operations of connection and contraction in each graph generated, until\\nthe resulting graphs are all cliques. If the smallest resulting clique is a K –clique,\\nthen (G) = K.\\nImportant notes on Graph Coloring\\n•\\nAny simple planar graph G can be colored with 6 colors.\\n•\\nEvery simple planar graph can be colored with less than or equal to 5 colors.\\nProblem-50\\u2003\\u2003What is the four coloring problem?\\nSolution: A graph can be constructed from any map. The regions of the map are represented by\\nthe vertices of the graph, and two vertices are joined by an edge if the regions corresponding to\\nthe vertices are adjacent. The resulting graph is planar. That means it can be drawn in the plane\\nwithout any edges crossing.\\nThe Four Color Problem is whether the vertices of a planar graph can be colored with at most\\nfour colors so that no two adjacent vertices use the same color.\\nHistory: The Four Color Problem was first given by Francis Guthrie. He was a student at\\nUniversity College London where he studied under Augusts De Morgan. After graduating from\\nLondon he studied law, but some years later his brother Frederick Guthrie had become a student\\nof De Morgan. One day Francis asked his brother to discuss this problem with De Morgan.\\nProblem-51\\u2003\\u2003When an adjacency-matrix representation is used, most graph algorithms require\\ntime O(V2). Show that determining whether a directed graph, represented in an adjacency-'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 503, 'file_type': 'pdf'}, page_content='matrix that contains a sink can be done in time O(V). A sink is a vertex with in-degree |V|\\n– 1 and out-degree 0 (Only one can exist in a graph).\\nSolution: A vertex i is a sink if and only if M[i,j] = 0 for all j and M[j, i] = 1 for all j ≠ i. For any\\npair of vertices i and j:\\nAlgorithm:\\n•\\nStart at i = 1,j = 1\\n•\\nIf M[i,j] = 0 → i wins, j + +\\n•\\nIf M[i,j] = 1 → j wins, i + +\\n•\\nProceed with this process until j = n or i = n + 1\\n•\\nIf i == n + 1, the graph does not contain a sink\\n•\\nOtherwise, check row i – it should be all zeros; and check column i – it should be all\\nbut M[i, i] ones; – if so, t is a sink.\\nTime Complexity: O(V), because at most 2|V| cells in the matrix are examined.\\nProblem-52\\u2003\\u2003What is the worst – case memory usage of DFS?\\nSolution: It occurs when the O(|V|), which happens if the graph is actually a list. So the algorithm\\nis memory efficient on graphs with small diameter.\\nProblem-53\\u2003\\u2003Does DFS find the shortest path from start node to some node w ?\\nSolution: No. In DFS it is not compulsory to select the smallest weight edge.\\nProblem-54\\u2003\\u2003True or False: Dijkstra’s algorithm does not compute the “all pairs” shortest\\npaths in a directed graph with positive edge weights because, running the algorithm a\\nsingle time, starting from some single vertex x, it will compute only the min distance from\\nx to y for all nodes y in the graph.\\nSolution: True.\\nProblem-55\\u2003\\u2003True or False: Prim’s and Kruskal’s algorithms may compute different minimum\\nspanning trees when run on the same graph.\\nSolution: True.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 504, 'file_type': 'pdf'}, page_content='10.1 What is Sorting?\\nSorting is an algorithm that arranges the elements of a list in a certain order [either ascending or\\ndescending]. The output is a permutation or reordering of the input.\\n10.2 Why is Sorting Necessary?\\nSorting is one of the important categories of algorithms in computer science and a lot of research\\nhas gone into this category. Sorting can significantly reduce the complexity of a problem, and is\\noften used for database algorithms and searches.\\n10.3 Classification of Sorting Algorithms\\nSorting algorithms are generally categorized based on the following parameters.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 505, 'file_type': 'pdf'}, page_content='By Number of Comparisons\\nIn this method, sorting algorithms are classified based on the number of comparisons. For\\ncomparison based sorting algorithms, best case behavior is O(nlogn) and worst case behavior is\\nO(n2). Comparison-based sorting algorithms evaluate the elements of the list by key comparison\\noperation and need at least O(nlogn) comparisons for most inputs.\\nLater in this chapter we will discuss a few non – comparison (linear) sorting algorithms like\\nCounting sort, Bucket sort, Radix sort, etc. Linear Sorting algorithms impose few restrictions on\\nthe inputs to improve the complexity.\\nBy Number of Swaps\\nIn this method, sorting algorithms are categorized by the number of swaps (also called\\ninversions).\\nBy Memory Usage\\nSome sorting algorithms are “in place” and they need O(1) or O(logn) memory to create\\nauxiliary locations for sorting the data temporarily.\\nBy Recursion\\nSorting algorithms are either recursive [quick sort] or non-recursive [selection sort, and insertion\\nsort], and there are some algorithms which use both (merge sort).\\nBy Stability\\nSorting algorithm is stable if for all indices i and j such that the key A[i] equals key A[j], if record\\nR[i] precedes record R[j] in the original file, record R[i] precedes record R[j] in the sorted list.\\nFew sorting algorithms maintain the relative order of elements with equal keys (equivalent\\nelements retain their relative positions even after sorting).\\nBy Adaptability\\nWith a few sorting algorithms, the complexity changes based on pre-sortedness [quick sort]: pre-\\nsortedness of the input affects the running time. Algorithms that take this into account are known to\\nbe adaptive.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 506, 'file_type': 'pdf'}, page_content='10.4 Other Classifications\\nAnother method of classifying sorting algorithms is:\\n•\\nInternal Sort\\n•\\nExternal Sort\\nInternal Sort\\nSort algorithms that use main memory exclusively during the sort are called internal sorting\\nalgorithms. This kind of algorithm assumes high-speed random access to all memory.\\nExternal Sort\\nSorting algorithms that use external memory, such as tape or disk, during the sort come under this\\ncategory.\\n10.5 Bubble Sort\\nBubble sort is the simplest sorting algorithm. It works by iterating the input array from the first\\nelement to the last, comparing each pair of elements and swapping them if needed. Bubble sort\\ncontinues its iterations until no more swaps are needed. The algorithm gets its name from the way\\nsmaller elements “bubble” to the top of the list. Generally, insertion sort has better performance\\nthan bubble sort. Some researchers suggest that we should not teach bubble sort because of its\\nsimplicity and high time complexity.\\nThe only significant advantage that bubble sort has over other implementations is that it can detect\\nwhether the input list is already sorted or not.\\nImplementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 507, 'file_type': 'pdf'}, page_content='Algorithm takes O(n2) (even in best case). We can improve it by using one extra flag. No more\\nswaps indicate the completion of sorting. If the list is already sorted, we can use this flag to skip\\nthe remaining passes.\\nThis modified version improves the best case of bubble sort to O(n).\\nPerformance\\nWorst case complexity : O(n2)\\nBest case complexity (Improved version) : O(n)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 508, 'file_type': 'pdf'}, page_content='Average case complexity (Basic version) : O(n2)\\nWorst case space complexity : O(1) auxiliary\\n10.6 Selection Sort\\nSelection sort is an in-place sorting algorithm. Selection sort works well for small files. It is used\\nfor sorting the files with very large values and small keys. This is because selection is made\\nbased on keys and swaps are made only when required.\\nAdvantages\\n•\\nEasy to implement\\n•\\nIn-place sort (requires no additional storage space)\\nDisadvantages\\n•\\nDoesn’t scale well: O(n2)\\nAlgorithm\\n1.\\nFind the minimum value in the list\\n2.\\nSwap it with the value in the current position\\n3.\\nRepeat this process for all the elements until the entire array is sorted\\nThis algorithm is called selection sort since it repeatedly selects the smallest element.\\nImplementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 509, 'file_type': 'pdf'}, page_content='Performance\\nWorst case complexity : O(n2)\\nBest case complexity : O(n2)\\nAverage case complexity : O(n2)\\nWorst case space complexity: O(1) auxiliary\\n10.7 Insertion Sort\\nInsertion sort is a simple and efficient comparison sort. In this algorithm, each iteration removes\\nan element from the input data and inserts it into the correct position in the list being sorted. The\\nchoice of the element being removed from the input is random and this process is repeated until\\nall input elements have gone through.\\nAdvantages\\n•\\nSimple implementation\\n•\\nEfficient for small data\\n•\\nAdaptive: If the input list is presorted [may not be completely] then insertions sort\\ntakes O(n + d), where d is the number of inversions\\n•\\nPractically more efficient than selection and bubble sorts, even though all of them\\nhave O(n2) worst case complexity'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 510, 'file_type': 'pdf'}, page_content='•\\nStable: Maintains relative order of input data if the keys are same\\n•\\nIn-place: It requires only a constant amount O(1) of additional memory space\\n•\\nOnline: Insertion sort can sort the list as it receives it\\nAlgorithm\\nEvery repetition of insertion sort removes an element from the input data, and inserts it into the\\ncorrect position in the already-sorted list until no input elements remain. Sorting is typically done\\nin-place. The resulting array after k iterations has the property where the first k + 1 entries are\\nsorted.\\nEach element greater than x is copied to the right as it is compared against x.\\nImplementation\\nExample'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 511, 'file_type': 'pdf'}, page_content='Given an array: 6 8 1 4 5 3 7 2 and the goal is to put them in ascending order.\\nAnalysis\\nWorst case analysis\\nWorst case occurs when for every i the inner loop has to move all elements A[1], . . . , A[i – 1]\\n(which happens when A[i] = key is smaller than all of them), that takes Θ(i – 1) time.\\nAverage case analysis\\nFor the average case, the inner loop will insert A[i] in the middle of A[1], . . . , A[i – 1]. This\\ntakes Θ(i/2) time.\\nPerformance\\nIf every element is greater than or equal to every element to its left, the running time of insertion\\nsort is Θ(n). This situation occurs if the array starts out already sorted, and so an already-sorted\\narray is the best case for insertion sort.\\nWorst case complexity: Θ(n2)\\nBest case complexity: Θ(n)\\nAverage case complexity: Θ(n2)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 512, 'file_type': 'pdf'}, page_content='Worst case space complexity: O(n2) total, O(1) auxiliary\\nComparisons to Other Sorting Algorithms\\nInsertion sort is one of the elementary sorting algorithms with O(n2) worst-case time. Insertion\\nsort is used when the data is nearly sorted (due to its adaptiveness) or when the input size is small\\n(due to its low overhead). For these reasons and due to its stability, insertion sort is used as the\\nrecursive base case (when the problem size is small) for higher overhead divide-and-conquer\\nsorting algorithms, such as merge sort or quick sort.\\nNotes:\\n•\\nBubble sort takes \\n comparisons and \\n swaps (inversions) in both average case\\nand in worst case.\\n•\\nSelection sort takes \\n comparisons and n swaps.\\n•\\nInsertion sort takes \\n comparisons and \\n swaps in average case and in the worst\\ncase they are double.\\n•\\nInsertion sort is almost linear for partially sorted input.\\n•\\nSelection sort is best suits for elements with bigger values and small keys.\\n10.8 Shell Sort\\nShell sort (also called diminishing increment sort) was invented by Donald Shell. This sorting\\nalgorithm is a generalization of insertion sort. Insertion sort works efficiently on input that is\\nalready almost sorted. Shell sort is also known as n-gap insertion sort. Instead of comparing only\\nthe adjacent pair, shell sort makes several passes and uses various gaps between adjacent\\nelements (ending with the gap of 1 or classical insertion sort).\\nIn insertion sort, comparisons are made between the adjacent elements. At most 1 inversion is\\neliminated for each comparison done with insertion sort. The variation used in shell sort is to\\navoid comparing adjacent elements until the last step of the algorithm. So, the last step of shell\\nsort is effectively the insertion sort algorithm. It improves insertion sort by allowing the\\ncomparison and exchange of elements that are far away. This is the first algorithm which got less\\nthan quadratic complexity among comparison sort algorithms.\\nShellsort is actually a simple extension for insertion sort. The primary difference is its capability\\nof exchanging elements that are far apart, making it considerably faster for elements to get to\\nwhere they should be. For example, if the smallest element happens to be at the end of an array,\\nwith insertion sort it will require the full array of steps to put this element at the beginning of the\\narray. However, with shell sort, this element can jump more than one step a time and reach the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 513, 'file_type': 'pdf'}, page_content='proper destination in fewer exchanges.\\nThe basic idea in shellsort is to exchange every hth element in the array. Now this can be\\nconfusing so we’ll talk more about this, h determines how far apart element exchange can happen,\\nsay for example take h as 13, the first element (index-0) is exchanged with the 14th element\\n(index-13) if necessary (of course). The second element with the 15th element, and so on. Now if\\nwe take has 1, it is exactly the same as a regular insertion sort.\\nShellsort works by starting with big enough (but not larger than the array size) h so as to allow\\neligible element exchanges that are far apart. Once a sort is complete with a particular h, the\\narray can be said as h-sorted. The next step is to reduce h by a certain sequence, and again\\nperform another complete h-sort. Once h is 1 and h-sorted, the array is completely sorted. Notice\\nthat the last sequence for ft is 1 so the last sort is always an insertion sort, except by this time the\\narray is already well-formed and easier to sort.\\nShell sort uses a sequence h1,h2, ...,ht called the increment sequence. Any increment sequence is\\nfine as long as h1 = 1, and some choices are better than others. Shell sort makes multiple passes\\nthrough the input list and sorts a number of equally sized sets using the insertion sort. Shell sort\\nimproves the efficiency of insertion sort by quickly shifting values to their destination.\\nImplementation\\nNote that when h == 1, the algorithm makes a pass over the entire list, comparing adjacent\\nelements, but doing very few element exchanges. For h == 1, shell sort works just like insertion\\nsort, except the number of inversions that have to be eliminated is greatly reduced by the previous'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 514, 'file_type': 'pdf'}, page_content='steps of the algorithm with h > 1.\\nAnalysis\\nShell sort is efficient for medium size lists. For bigger lists, the algorithm is not the best choice. It\\nis the fastest of all O(n2) sorting algorithms.\\nThe disadvantage of Shell sort is that it is a complex algorithm and not nearly as efficient as the\\nmerge, heap, and quick sorts. Shell sort is significantly slower than the merge, heap, and quick\\nsorts, but is a relatively simple algorithm, which makes it a good choice for sorting lists of less\\nthan 5000 items unless speed is important. It is also a good choice for repetitive sorting of\\nsmaller lists.\\nThe best case in Shell sort is when the array is already sorted in the right order. The number of\\ncomparisons is less. The running time of Shell sort depends on the choice of increment sequence.\\nPerformance\\nWorst case complexity depends on gap sequence. Best known: O(nlog2n)\\nBest case complexity: O(n)\\nAverage case complexity depends on gap sequence\\nWorst case space complexity: O(n)\\n10.9 Merge Sort\\nMerge sort is an example of the divide and conquer strategy.\\nImportant Notes\\n•\\nMerging is the process of combining two sorted files to make one bigger sorted file.\\n•\\nSelection is the process of dividing a file into two parts: k smallest elements and n –\\nk largest elements.\\n•\\nSelection and merging are opposite operations\\n○\\nselection splits a list into two lists\\n○\\nmerging joins two files to make one file\\n•\\nMerge sort is Quick sort’s complement\\n•\\nMerge sort accesses the data in a sequential manner\\n•\\nThis algorithm is used for sorting a linked list'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 515, 'file_type': 'pdf'}, page_content='•\\nMerge sort is insensitive to the initial order of its input\\n•\\nIn Quick sort most of the work is done before the recursive calls. Quick sort starts\\nwith the largest subfile and finishes with the small ones and as a result it needs\\nstack. Moreover, this algorithm is not stable. Merge sort divides the list into two\\nparts; then each part is conquered individually. Merge sort starts with the small\\nsubfiles and finishes with the largest one. As a result it doesn’t need stack. This\\nalgorithm is stable.\\nImplementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 516, 'file_type': 'pdf'}, page_content='void Mergesortin A] int temp), int lf, int right) {\\nint mid;\\niffight > let (\\nmi» (right + left) / 2;\\nMergesort{, temp, lef, mi);\\nMergesort{, temp, mid, right)\\n‘Merge(A, temp, left, mid+1, right);\\n}\\n|\\nvoid Merge(nt Af, int tempi), int eft, int mid, int right {\\nint let_end, size, temp_pos;\\nleft end = ce “Ii\\ntemp_pos =\\nsize = right its\\nwhile (eft <= let_end) € mid <= right) {\\niff] <= Amid) (\\ntempltemp_pos| = Alle];\\n‘temp_pos = temp_pos + 1;\\nleft = left +1;\\n\\nelse {\\ntempltemp_pos) = A{midl\\n‘temp_pos = temp_pos + 1;\\nmid = mid + 1;\\n}\\n}\\nwhile (left <= Let end) {\\ntempltemp_pos| = Afleft);\\nleft= let +1;\\n\\ntemp_pos = temp_pos + 1;\\n\\nwhile (mid <= right) {\\ntemptemp_pos| = A{midl\\nmid = mid + 1;\\ntemp_pos «temp. pos +1;\\n\\n}\\n\\nfor (i= 0; <= sie; t+) (\\nAlright] = templright)\\nright = right - 1;'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 517, 'file_type': 'pdf'}, page_content='Analysis\\nIn Merge sort the input list is divided into two parts and these are solved recursively. After\\nsolving the sub problems, they are merged by scanning the resultant sub problems. Let us assume\\nT(n) is the complexity of Merge sort with n elements. The recurrence for the Merge Sort can be\\ndefined as:\\nNote: For more details, refer to Divide and Conquer chapter.\\nPerformance\\nWorst case complexity : Θ(nlogn)\\nBest case complexity : Θ(nlogn)\\nAverage case complexity : Θ(nlogn)\\nWorst case space complexity: Θ(n) auxiliary\\n10.10 Heap Sort\\nHeapsort is a comparison-based sorting algorithm and is part of the selection sort family.\\nAlthough somewhat slower in practice on most machines than a good implementation of Quick\\nsort, it has the advantage of a more favorable worst-case Θ(nlogn) runtime. Heapsort is an in-\\nplace algorithm but is not a stable sort.\\nPerformance\\nWorst case performance: Θ(nlogn)\\nBest case performance: Θ(nlogn)\\nAverage case performance: Θ(nlogn)\\nWorst case space complexity: Θ(n) total, Θ(1) auxiliary\\nFor other details on Heapsort refer to the Priority Queues chapter.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 518, 'file_type': 'pdf'}, page_content='10.11 Quicksort\\nQuick sort is an example of a divide-and-conquer algorithmic technique. It is also called\\npartition exchange sort. It uses recursive calls for sorting the elements, and it is one of the\\nfamous algorithms among comparison-based sorting algorithms.\\nDivide: The array A[low ...high] is partitioned into two non-empty sub arrays A[low ...q] and A[q\\n+ 1... high], such that each element of A[low ... high] is less than or equal to each element of A[q\\n+ 1... high]. The index q is computed as part of this partitioning procedure.\\nConquer: The two sub arrays A[low ...q] and A[q + 1 ...high] are sorted by recursive calls to\\nQuick sort.\\nAlgorithm\\nThe recursive algorithm consists of four steps:\\n1)\\nIf there are one or no elements in the array to be sorted, return.\\n2)\\nPick an element in the array to serve as the “pivot” point. (Usually the left-most\\nelement in the array is used.)\\n3)\\nSplit the array into two parts – one with elements larger than the pivot and the other\\nwith elements smaller than the pivot.\\n4)\\nRecursively repeat the algorithm for both halves of the original array.\\nImplementation'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 519, 'file_type': 'pdf'}, page_content='Analysis\\nLet us assume that T(n) be the complexity of Quick sort and also assume that all elements are\\ndistinct. Recurrence for T(n) depends on two subproblem sizes which depend on partition\\nelement. If pivot is ith smallest element then exactly (i – 1) items will be in left part and (n – i) in\\nright part. Let us call it as i –split. Since each element has equal probability of selecting it as\\npivot the probability of selecting ith element is .\\nBest Case: Each partition splits array in halves and gives'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 520, 'file_type': 'pdf'}, page_content='T(n) = 2T(n/2) + Θ(n) = Θ(nlogn), [using Divide and Conquer master theorem]\\nWorst Case: Each partition gives unbalanced splits and we get\\nT(n) = T(n – 1) + Θ(n) = Θ(n2)[using Subtraction and Conquer master theorem]\\nThe worst-case occurs when the list is already sorted and last element chosen as pivot.\\nAverage Case: In the average case of Quick sort, we do not know where the split happens. For\\nthis reason, we take all possible values of split locations, add all their complexities and divide\\nwith n to get the average case complexity.\\nMultiply both sides by n.\\nSame formula for n – 1.\\nSubtract the n – 1 formula from n.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 521, 'file_type': 'pdf'}, page_content='Divide with n(n + 1).\\nTime Complexity, T(n) = O(nlogn).\\nPerformance\\nWorst case Complexity: O(n2)\\nBest case Complexity: O(nlogn)\\nAverage case Complexity: O(nlogn)\\nWorst case space Complexity: O(1)\\nRandomized Quick sort\\nIn average-case behavior of Quick sort, we assume that all permutations of the input numbers are\\nequally likely. However, we cannot always expect it to hold. We can add randomization to an\\nalgorithm in order to reduce the probability of getting worst case in Quick sort.\\nThere are two ways of adding randomization in Quick sort: either by randomly placing the input\\ndata in the array or by randomly choosing an element in the input data for pivot. The second\\nchoice is easier to analyze and implement. The change will only be done at the partition\\nalgorithm.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 522, 'file_type': 'pdf'}, page_content='In normal Quick sort, pivot element was always the leftmost element in the list to be sorted.\\nInstead of always using A[low] as pivot, we will use a randomly chosen element from the\\nsubarray A[low..high] in the randomized version of Quick sort. It is done by exchanging element\\nA[low] with an element chosen at random from A[low..high]. This ensures that the pivot element is\\nequally likely to be any of the high – low + 1 elements in the subarray.\\nSince the pivot element is randomly chosen, we can expect the split of the input array to be\\nreasonably well balanced on average. This can help in preventing the worst-case behavior of\\nquick sort which occurs in unbalanced partitioning. Even though the randomized version improves\\nthe worst case complexity, its worst case complexity is still O(n2). One way to improve\\nRandomized – Quick sort is to choose the pivot for partitioning more carefully than by picking a\\nrandom element from the array. One common approach is to choose the pivot as the median of a\\nset of 3 elements randomly selected from the array.\\n10.12 Tree Sort\\nTree sort uses a binary search tree. It involves scanning each element of the input and placing it\\ninto its proper position in a binary search tree. This has two phases:\\n•\\nFirst phase is creating a binary search tree using the given array elements.\\n•\\nSecond phase is traversing the given binary search tree in inorder, thus resulting in a\\nsorted array.\\nPerformance\\nThe average number of comparisons for this method is O(nlogn). But in worst case, the number of\\ncomparisons is reduced by O(n2), a case which arises when the sort tree is skew tree.\\n10.13 Comparison of Sorting Algorithms'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 523, 'file_type': 'pdf'}, page_content='Note: n denotes the number of elements in the input.\\n10.14 Linear Sorting Algorithms\\nIn earlier sections, we have seen many examples of comparison-based sorting algorithms. Among\\nthem, the best comparison-based sorting has the complexity O(nlogn). In this section, we will\\ndiscuss other types of algorithms: Linear Sorting Algorithms. To improve the time complexity of\\nsorting these algorithms, we make some assumptions about the input. A few examples of Linear\\nSorting Algorithms are:\\n•\\nCounting Sort\\n•\\nBucket Sort\\n•\\nRadix Sort\\n10.15 Counting Sort\\nCounting sort is not a comparison sort algorithm and gives O(n) complexity for sorting. To\\nachieve O(n) complexity, counting sort assumes that each of the elements is an integer in the\\nrange 1 to K, for some integer K. When if = O(n), the counting sort runs in O(n) time. The basic\\nidea of Counting sort is to determine, for each input element X, the number of elements less than\\nX. This information can be used to place it directly into its correct position. For example, if 10\\nelements are less than X, then X belongs to position 11 in the output.\\nIn the code below, A[0 ..n – 1] is the input array with length n. In Counting sort we need two more\\narrays: let us assume array B[0 ..n – 1] contains the sorted output and the array C[0 ..K – 1]\\nprovides temporary storage.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 524, 'file_type': 'pdf'}, page_content='Total Complexity: O(K) + O(n) + O(K) + O(n) = O(n) if K =O(n). Space Complexity: O(n) if K\\n=O(n).\\nNote: Counting works well if K =O(n). Otherwise, the complexity will be greater.\\n10.16 Bucket Sort (or Bin Sort)\\nLike Counting sort, Bucket sort also imposes restrictions on the input to improve the\\nperformance. In other words, Bucket sort works well if the input is drawn from fixed set. Bucket\\nsort is the generalization of Counting Sort. For example, assume that all the input elements from\\n{0, 1, . . . , K – 1}, i.e., the set of integers in the interval [0, K – 1]. That means, K is the number\\nof distant elements in the input. Bucket sort uses K counters. The ith counter keeps track of the\\nnumber of occurrences of the ith element. Bucket sort with two buckets is effectively a version of\\nQuick sort with two buckets.\\nFor bucket sort, the hash function that is used to partition the elements need to be very good and\\nmust produce ordered hash: if i < k then hash(i) < hash(k). Second, the elements to be sorted must\\nbe uniformly distributed.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 525, 'file_type': 'pdf'}, page_content='The aforementioned aside, bucket sort is actually very good considering that counting sort is\\nreasonably speaking its upper bound. And counting sort is very fast. The particular distinction for\\nbucket sort is that it uses a hash function to partition the keys of the input array, so that multiple\\nkeys may hash to the same bucket. Hence each bucket must effectively be a growable list; similar\\nto radix sort.\\nIn the below code insertionsort is used to sort each bucket. This is to inculcate that the bucket sort\\nalgorithm does not specify which sorting technique to use on the buckets. A programmer may\\nchoose to continuously use bucket sort on each bucket until the collection is sorted (in the manner\\nof the radix sort program below). Whichever sorting method is used on the , bucket sort still tends\\ntoward O(n).\\nTime Complexity: O(n). Space Complexity: O(n).\\n10.17 Radix Sort\\nSimilar to Counting sort and Bucket sort, this sorting algorithm also assumes some kind of\\ninformation about the input elements. Suppose that the input values to be sorted are from base d.\\nThat means all numbers are d-digit numbers.\\nIn Radix sort, first sort the elements based on the last digit [the least significant digit]. These\\nresults are again sorted by second digit [the next to least significant digit]. Continue this process\\nfor all digits until we reach the most significant digits. Use some stable sort to sort them by last\\ndigit. Then stable sort them by the second least significant digit, then by the third, etc. If we use\\nCounting sort as the stable sort, the total time is O(nd) ≈ O(n).\\nAlgorithm:\\n1)\\nTake the least significant digit of each element.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 526, 'file_type': 'pdf'}, page_content='2)\\nSort the list of elements based on that digit, but keep the order of elements with the\\nsame digit (this is the definition of a stable sort).\\n3)\\nRepeat the sort with each more significant digit.\\nThe speed of Radix sort depends on the inner basic operations. If the operations are not efficient\\nenough, Radix sort can be slower than other algorithms such as Quick sort and Merge sort. These\\noperations include the insert and delete functions of the sub-lists and the process of isolating the\\ndigit we want. If the numbers are not of equal length then a test is needed to check for additional\\ndigits that need sorting. This can be one of the slowest parts of Radix sort and also one of the\\nhardest to make efficient.\\nSince Radix sort depends on the digits or letters, it is less flexible than other sorts. For every\\ndifferent type of data, Radix sort needs to be rewritten, and if the sorting order changes, the sort\\nneeds to be rewritten again. In short, Radix sort takes more time to write, and it is very difficult to\\nwrite a general purpose Radix sort that can handle all kinds of data.\\nFor many programs that need a fast sort, Radix sort is a good choice. Still, there are faster sorts,\\nwhich is one reason why Radix sort is not used as much as some other sorts.\\nTime Complexity: O(nd) ≈ O(n), if d is small.\\n10.18 Topological Sort\\nRefer to Graph Algorithms Chapter.\\n10.19 External Sorting\\nExternal sorting is a generic term for a class of sorting algorithms that can handle massive\\namounts of data. These external sorting algorithms are useful when the files are too big and cannot\\nfit into main memory.\\nAs with internal sorting algorithms, there are a number of algorithms for external sorting. One\\nsuch algorithm is External Mergesort. In practice, these external sorting algorithms are being\\nsupplemented by internal sorts.\\nSimple External Mergesort\\nA number of records from each tape are read into main memory, sorted using an internal sort, and\\nthen output to the tape. For the sake of clarity, let us assume that 900 megabytes of data needs to\\nbe sorted using only 100 megabytes of RAM.\\n1)\\nRead 100MB of the data into main memory and sort by some conventional method'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 527, 'file_type': 'pdf'}, page_content='(let us say Quick sort).\\n2)\\nWrite the sorted data to disk.\\n3)\\nRepeat steps 1 and 2 until all of the data is sorted in chunks of 100MB. Now we need\\nto merge them into one single sorted output file.\\n4)\\nRead the first 10MB of each sorted chunk (call them input buffers) in main memory\\n(90MB total) and allocate the remaining 10MB for output buffer.\\n5)\\nPerform a 9-way Mergesort and store the result in the output buffer. If the output\\nbuffer is full, write it to the final sorted file. If any of the 9 input buffers gets empty,\\nfill it with the next 10MB of its associated 100MB sorted chunk; or if there is no\\nmore data in the sorted chunk, mark it as exhausted and do not use it for merging.\\nThe above algorithm can be generalized by assuming that the amount of data to be sorted exceeds\\nthe available memory by a factor of K. Then, K chunks of data need to be sorted and a K -way\\nmerge has to be completed.\\nIf X is the amount of main memory available, there will be K input buffers and 1 output buffer of\\nsize X/(K + 1) each. Depending on various factors (how fast is the hard drive?) better\\nperformance can be achieved if the output buffer is made larger (for example, twice as large as\\none input buffer).\\nComplexity of the 2-way External Merge sort: In each pass we read + write each page in file. Let'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 528, 'file_type': 'pdf'}, page_content='us assume that there are n pages in file. That means we need ⌈logn⌉ + 1 number of passes. The\\ntotal cost is 2n(⌈logn⌉ + 1).\\n10.20 Sorting: Problems & Solutions\\nProblem-1\\u2003\\u2003Given an array A[0...n– 1] of n numbers containing the repetition of some number.\\nGive an algorithm for checking whether there are repeated elements or not. Assume that\\nwe are not allowed to use additional space (i.e., we can use a few temporary variables,\\nO(1) storage).\\nSolution: Since we are not allowed to use extra space, one simple way is to scan the elements\\none-by-one and for each element check whether that element appears in the remaining elements. If\\nwe find a match we return true.\\nEach iteration of the inner, j-indexed loop uses O(1) space, and for a fixed value of i, the j loop\\nexecutes n – i times. The outer loop executes n – 1 times, so the entire function uses time\\nproportional to\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-2\\u2003\\u2003Can we improve the time complexity of Problem-1?\\nSolution: Yes, using sorting technique.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 529, 'file_type': 'pdf'}, page_content='Heapsort function takes O(nlogn) time, and requires O(1) space. The scan clearly takes n – 1\\niterations, each iteration using O(1) time. The overall time is O(nlogn + n) = O(nlogn).\\nTime Complexity: O(nlogn). Space Complexity: O(1).\\nNote: For variations of this problem, refer Searching chapter.\\nProblem-3\\u2003\\u2003Given an array A[0 ...n – 1], where each element of the array represents a vote in\\nthe election. Assume that each vote is given as an integer representing the ID of the chosen\\ncandidate. Give an algorithm for determining who wins the election.\\nSolution: This problem is nothing but finding the element which repeated the maximum number of\\ntimes. The solution is similar to the Problem-1 solution: keep track of counter.\\nTime Complexity: O(n2). Space Complexity: O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 530, 'file_type': 'pdf'}, page_content='Note: For variations of this problem, refer to Searching chapter.\\nProblem-4\\u2003\\u2003Can we improve the time complexity of Problem-3? Assume we don’t have any\\nextra space.\\nSolution: Yes. The approach is to sort the votes based on candidate ID, then scan the sorted array\\nand count up which candidate so far has the most votes. We only have to remember the winner, so\\nwe don’t need a clever data structure. We can use Heapsort as it is an in-place sorting algorithm.\\nSince Heapsort time complexity is O(nlogn) and in-place, it only uses an additional O(1) of\\nstorage in addition to the input array. The scan of the sorted array does a constant-time\\nconditional n – 1 times, thus using O(n) time. The overall time bound is O(nlogn).\\nProblem-5\\u2003\\u2003Can we further improve the time complexity of Problem-3?\\nSolution: In the given problem, the number of candidates is less but the number of votes is\\nsignificantly large. For this problem we can use counting sort.\\nTime Complexity: O(n), n is the number of votes (elements) in the array. Space Complexity: O(k),'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 531, 'file_type': 'pdf'}, page_content='k is the number of candidates participating in the election.\\nProblem-6\\u2003\\u2003Given an array A of n elements, each of which is an integer in the range [1, n2],\\nhow do we sort the array in O(n) time?\\nSolution: If we subtract each number by 1 then we get the range [0, n2 – 1]. If we consider all\\nnumbers as 2 –digit base n. Each digit ranges from 0 to n2 – 1. Sort this using radix sort. This uses\\nonly two calls to counting sort. Finally, add 1 to all the numbers. Since there are 2 calls, the\\ncomplexity is O(2n) ≈ O(n).\\nProblem-7\\u2003\\u2003For Problem-6, what if the range is [1... n3]?\\nSolution: If we subtract each number by 1 then we get the range [0, n3 – 1]. Considering all\\nnumbers as 3-digit base n: each digit ranges from 0 to n3 – 1. Sort this using radix sort. This uses\\nonly three calls to counting sort. Finally, add 1 to all the numbers. Since there are 3 calls, the\\ncomplexity is O(3n) ≈ O(n).\\nProblem-8\\u2003\\u2003Given an array with n integers, each of value less than n100, can it be sorted in\\nlinear time?\\nSolution: Yes. The reasoning is same as in of Problem-6 and Problem-7.\\nProblem-9\\u2003\\u2003Let A and B be two arrays of n elements each. Given a number K, give an\\nO(nlogn) time algorithm for determining whether there exists a ∈ A and b ∈ B such that a\\n+ b = K.\\nSolution: Since we need O(nlogn), it gives us a pointer that we need to sort. So, we will do that.\\nNote: For variations of this problem, refer to Searching chapter.\\nProblem-10\\u2003\\u2003Let A,B and C be three arrays of n elements each. Given a number K, give an\\nO(nlogn) time algorithm for determining whether there exists a ∈ A, b ∈ B and c ∈ C such\\nthat a + b + c = K.\\nSolution: Refer to Searching chapter.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 532, 'file_type': 'pdf'}, page_content='Problem-11\\u2003\\u2003Given an array of n elements, can we output in sorted order the K elements\\nfollowing the median in sorted order in time O(n + KlogK).\\nSolution: Yes. Find the median and partition the median. With this we can find all the elements\\ngreater than it. Now find the Kth largest element in this set and partition it; and get all the elements\\nless than it. Output the sorted list of the final set of elements. Clearly, this operation takes O(n +\\nKlogK) time.\\nProblem-12\\u2003\\u2003Consider the sorting algorithms: Bubble sort, Insertion sort, Selection sort,\\nMerge sort, Heap sort, and Quick sort. Which of these are stable?\\nSolution: Let us assume that A is the array to be sorted. Also, let us say R and S have the same key\\nand R appears earlier in the array than S. That means, R is at A[i] and S is at A[j], with i < j. To\\nshow any stable algorithm, in the sorted output R must precede S.\\nBubble sort: Yes. Elements change order only when a smaller record follows a larger. Since S is\\nnot smaller than R it cannot precede it.\\nSelection sort: No. It divides the array into sorted and unsorted portions and iteratively finds the\\nminimum values in the unsorted portion. After finding a minimum x, if the algorithm moves x into\\nthe sorted portion of the array by means of a swap, then the element swapped could be R which\\nthen could be moved behind S. This would invert the positions of R and S, so in general it is not\\nstable. If swapping is avoided, it could be made stable but the cost in time would probably be\\nvery significant.\\nInsertion sort: Yes. As presented, when S is to be inserted into sorted subarray A[1..j – 1], only\\nrecords larger than S are shifted. Thus R would not be shifted during S’s insertion and hence\\nwould always precede it.\\nMerge sort: Yes, In the case of records with equal keys, the record in the left subarray gets\\npreference. Those are the records that came first in the unsorted array. As a result, they will\\nprecede later records with the same key.\\nHeap sort: No. Suppose i = 1 and R and S happen to be the two records with the largest keys in\\nthe input. Then R will remain in location 1 after the array is heapified, and will be placed in\\nlocation n in the first iteration of Heapsort. Thus S will precede R in the output.\\nQuick sort: No. The partitioning step can swap the location of records many times, and thus two\\nrecords with equal keys could swap position in the final output.\\nProblem-13\\u2003\\u2003Consider the same sorting algorithms as that of Problem-12. Which of them are\\nin-place?\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 533, 'file_type': 'pdf'}, page_content='Bubble sort: Yes, because only two integers are required.\\nInsertion sort: Yes, since we need to store two integers and a record.\\nSelection sort: Yes. This algorithm would likely need space for two integers and one record.\\nMerge sort: No. Arrays need to perform the merge. (If the data is in the form of a linked list, the\\nsorting can be done in-place, but this is a nontrivial modification.)\\nHeap sort: Yes, since the heap and partially-sorted array occupy opposite ends of the input array.\\nQuicksort: No, since it is recursive and stores O(logn) activation records on the stack.\\nModifying it to be non-recursive is feasible but nontrivial.\\nProblem-14\\u2003\\u2003Among Quick sort, Insertion sort, Selection sort, and Heap sort algorithms,\\nwhich one needs the minimum number of swaps?\\nSolution: Selection sort – it needs n swaps only (refer to theory section).\\nProblem-15\\u2003\\u2003What is the minimum number of comparisons required to determine if an integer\\nappears more than n/2 times in a sorted array of n integers?\\nSolution: Refer to Searching chapter.\\nProblem-16\\u2003\\u2003Sort an array of 0’s, 1’s and 2’s: Given an array A[] consisting of 0’s, 1’s and\\n2’s, give an algorithm for sorting A[]. The algorithm should put all 0’s first, then all 1’s and\\nall 2’s last.\\nExample: Input = {0,1,1,0,1,2,1,2,0,0,0,1}, Output = {0,0,0,0,0,1,1,1,1,1,2,2}\\nSolution: Use Counting sort. Since there are only three elements and the maximum value is 2, we\\nneed a temporary array with 3 elements.\\nTime Complexity: O(n). Space Complexity: O(1).\\nNote: For variations of this problem, refer to Searching chapter.\\nProblem-17\\u2003\\u2003Is there any other way of solving Problem-16?\\nSolution: Using Quick dort. Since we know that there are only 3 elements, 0,1 and 2 in the array,\\nwe can select 1 as a pivot element for Quick sort. Quick sort finds the correct place for 1 by\\nmoving all 0’s to the left of 1 and all 2’s to the right of 1. For doing this it uses only one scan.\\nTime Complexity: O(n). Space Complexity: O(1).\\nNote: For efficient algorithm, refer to Searching chapter.\\nProblem-18\\u2003\\u2003How do we find the number that appeared the maximum number of times in an\\narray?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 534, 'file_type': 'pdf'}, page_content='Solution: One simple approach is to sort the given array and scan the sorted array. While\\nscanning, keep track of the elements that occur the maximum number of times.\\nAlgorithm:\\nTime Complexity = Time for Sorting + Time for Scan = O(nlogn) +O(n) = O(nlogn). Space\\nComplexity: O(1).\\nNote: For variations of this problem, refer to Searching chapter.\\nProblem-19\\u2003\\u2003Is there any other way of solving Problem-18?\\nSolution: Using Binary Tree. Create a binary tree with an extra field count which indicates the\\nnumber of times an element appeared in the input. Let us say we have created a Binary Search\\nTree [BST]. Now, do the In-Order traversal of the tree. The In-Order traversal of BST produces\\nthe sorted list. While doing the In-Order traversal keep track of the maximum element.\\nTime Complexity: O(n) + O(n) ≈ O(n). The first parameter is for constructing the BST and the\\nsecond parameter is for Inorder Traversal. Space Complexity: O(2n) ≈ O(n), since every node in\\nBST needs two extra pointers.\\nProblem-20\\u2003\\u2003Is there yet another way of solving Problem-18?\\nSolution: Using Hash Table. For each element of the given array we use a counter, and for each\\noccurrence of the element we increment the corresponding counter. At the end we can just return\\nthe element which has the maximum counter.\\nTime Complexity: O(n). Space Complexity: O(n). For constructing the hash table we need O(n).\\nNote: For the efficient algorithm, refer to the Searching chapter.\\nProblem-21\\u2003\\u2003Given a 2 GB file with one string per line, which sorting algorithm would we\\nuse to sort the file and why?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 535, 'file_type': 'pdf'}, page_content='Solution: When we have a size limit of 2GB, it means that we cannot bring all the data into the\\nmain memory.\\nAlgorithm: How much memory do we have available? Let’s assume we have X MB of memory\\navailable. Divide the file into K chunks, where X * K ~ 2 GB.\\n•\\nBring each chunk into memory and sort the lines as usual (any O(nlogn) algorithm).\\n•\\nSave the lines back to the file.\\n•\\nNow bring the next chunk into memory and sort.\\n•\\nOnce we’re done, merge them one by one; in the case of one set finishing, bring more\\ndata from the particular chunk.\\nThe above algorithm is also known as external sort. Step 3 – 4 is known as K-way merge. The\\nidea behind going for an external sort is the size of data. Since the data is huge and we can’t bring\\nit to the memory, we need to go for a disk-based sorting algorithm.\\nProblem-22\\u2003\\u2003Nearly sorted: Given an array of n elements, each which is at most K positions\\nfrom its target position, devise an algorithm that sorts in O(n logK) time.\\nSolution: Divide the elements into n/K groups of size K, and sort each piece in O(KlogK) time,\\nlet’s say using Mergesort. This preserves the property that no element is more than K elements out\\nof position. Now, merge each block of K elements with the block to its left.\\nProblem-23\\u2003\\u2003Is there any other way of solving Problem-22?\\nSolution: Insert the first K elements into a binary heap. Insert the next element from the array into\\nthe heap, and delete the minimum element from the heap. Repeat.\\nProblem-24\\u2003\\u2003Merging K sorted lists: Given K sorted lists with a total of n elements, give an\\nO(nlogK) algorithm to produce a sorted list of all n elements.\\nSolution: Simple Algorithm for merging K sorted lists: Consider groups each having  elements.\\nTake the first list and merge it with the second list using a linear-time algorithm for merging two\\nsorted lists, such as the merging algorithm used in merge sort. Then, merge the resulting list of \\nelements with the third list, and then merge the resulting list of \\n elements with the fourth list.\\nRepeat this until we end up with a single sorted list of all n elements.\\nTime Complexity: In each iteration we are merging K elements.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 536, 'file_type': 'pdf'}, page_content='Problem-25\\u2003\\u2003Can we improve the time complexity of Problem-24?\\nSolution: One method is to repeatedly pair up the lists and then merge each pair. This method can\\nalso be seen as a tail component of the execution merge sort, where the analysis is clear. This is\\ncalled the Tournament Method. The maximum depth of the Tournament Method is logK and in\\neach iteration we are scanning all the n elements.\\nTime Complexity; O(nlogK).\\nProblem-26\\u2003\\u2003Is there any other way of solving Problem-24?\\nSolution: The other method is to use a rain priority queue for the minimum elements of each of\\nthe if lists. At each step, we output the extracted minimum of the priority queue, determine from\\nwhich of the K lists it came, and insert the next element from that list into the priority queue. Since\\nwe are using priority queue, that maximum depth of priority queue is logK.\\nTime Complexity; O(nlogK).\\nProblem-27\\u2003\\u2003Which sorting method is better for Linked Lists?\\nSolution: Merge Sort is a better choice. At first appearance, merge sort may not be a good\\nselection since the middle node is required to subdivide the given list into two sub-lists of equal\\nlength. We can easily solve this problem by moving the nodes alternatively to two lists (refer to\\nLinked Lists chapter). Then, sorting these two lists recursively and merging the results into a\\nsingle list will sort the given one.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 537, 'file_type': 'pdf'}, page_content='Note: Append() appends the first argument to the tail of a singly linked list whose head and tail\\nare defined by the second and third arguments.\\nAll external sorting algorithms can be used for sorting linked lists since each involved file can be\\nconsidered as a linked list that can only be accessed sequentially. We can sort a doubly linked list\\nusing its next fields as if it was a singly linked one and reconstruct the prev fields after sorting\\nwith an additional scan.\\nProblem-28\\u2003\\u2003Can we implement Linked Lists Sorting with Quick Sort?\\nSolution: The original Quick Sort cannot be used for sorting Singly Linked Lists. This is because\\nwe cannot move backward in Singly Linked Lists. But we can modify the original Quick Sort and\\nmake it work for Singly Linked Lists.\\nLet us consider the following modified Quick Sort implementation. The first node of the input list\\nis considered a pivot and is moved to equal. The value of each node is compared with the pivot\\nand moved to less (respectively, equal or larger) if the nodes value is smaller than (respectively,\\nequal to or larger than) the pivot. Then, less and larger are sorted recursively. Finally, joining\\nless, equal and larger into a single list yields a sorted one.\\nAppend() appends the first argument to the tail of a singly linked list whose head and tail are\\ndefined by the second and third arguments. On return, the first argument will be modified so that it'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 538, 'file_type': 'pdf'}, page_content='points to the next node of the list. Join() appends the list whose head and tail are defined by the\\nthird and fourth arguments to the list whose head and tail are defined by the first and second\\narguments. For simplicity, the first and fourth arguments become the head and tail of the resulting\\nlist.\\nProblem-29\\u2003\\u2003Given an array of 100,000 pixel color values, each of which is an integer in the\\nrange [0,255]. Which sorting algorithm is preferable for sorting them?\\nSolution: Counting Sort. There are only 256 key values, so the auxiliary array would only be of\\nsize 256, and there would be only two passes through the data, which would be very efficient in\\nboth time and space.\\nProblem-30\\u2003\\u2003Similar to Problem-29, if we have a telephone directory with 10 million entries,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 539, 'file_type': 'pdf'}, page_content='which sorting algorithm is best?\\nSolution: Bucket Sort. In Bucket Sort the buckets are defined by the last 7 digits. This requires an\\nauxiliary array of size 10 million and has the advantage of requiring only one pass through the\\ndata on disk. Each bucket contains all telephone numbers with the same last 7 digits but with\\ndifferent area codes. The buckets can then be sorted by area code with selection or insertion sort;\\nthere are only a handful of area codes.\\nProblem-31\\u2003\\u2003Give an algorithm for merging K-sorted lists.\\nSolution: Refer to Priority Queues chapter.\\nProblem-32\\u2003\\u2003Given a big file containing billions of numbers. Find maximum 10 numbers from\\nthis file.\\nSolution: Refer to Priority Queues chapter.\\nProblem-33\\u2003\\u2003There are two sorted arrays A and B. The first one is of size m + n containing\\nonly m elements. Another one is of size n and contains n elements. Merge these two arrays\\ninto the first array of size m + n such that the output is sorted.\\nSolution: The trick for this problem is to start filling the destination array from the back with the\\nlargest elements. We will end up with a merged and sorted destination array.\\nTime Complexity: O(m + n). Space Complexity: O(1).\\nProblem-34\\u2003\\u2003Nuts and Bolts Problem: Given a set of n nuts of different sizes and n bolts\\nsuch that there is a one-to-one correspondence between the nuts and the bolts, find for each\\nnut its corresponding bolt. Assume that we can only compare nuts to bolts: we cannot'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 540, 'file_type': 'pdf'}, page_content='compare nuts to nuts and bolts to bolts.\\nAlternative way of framing the question: We are given a box which contains bolts and\\nnuts. Assume there are n nuts and n bolts and that each nut matches exactly one bolt (and\\nvice versa). By trying to match a bolt and a nut we can see which one is bigger, but we\\ncannot compare two bolts or two nuts directly. Design an efficient algorithm for matching\\nthe nuts and bolts.\\nSolution: Brute Force Approach: Start with the first bolt and compare it with each nut until we\\nfind a match. In the worst case, we require n comparisons. Repeat this for successive bolts on all\\nremaining gives O(n2) complexity.\\nProblem-35\\u2003\\u2003For Problem-34, can we improve the complexity?\\nSolution: In Problem-34, we got O(n2) complexity in the worst case (if bolts are in ascending\\norder and nuts are in descending order). Its analysis is the same as that of Quick Sort. The\\nimprovement is also along the same lines. To reduce the worst case complexity, instead of\\nselecting the first bolt every time, we can select a random bolt and match it with nuts. This\\nrandomized selection reduces the probability of getting the worst case, but still the worst case is\\nO(n2).\\nProblem-36\\u2003\\u2003For Problem-34, can we further improve the complexity?\\nSolution: We can use a divide-and-conquer technique for solving this problem and the solution is\\nvery similar to randomized Quick Sort. For simplicity let us assume that bolts and nuts are\\nrepresented in two arrays B and N.\\nThe algorithm first performs a partition operation as follows: pick a random boltB[t]. Using this\\nbolt, rearrange the array of nuts into three groups of elements:\\n•\\nFirst the nuts smaller than B[i]\\n•\\nThen the nut that matches B[i], and\\n•\\nFinally, the nuts larger than B[i].\\nNext, using the nut that matches B[i], perform a similar partition on the array of bolts. This pair of\\npartitioning operations can easily be implemented in O(n) time, and it leaves the bolts and nuts\\nnicely partitioned so that the “pivot” bolt and nut are aligned with each other and all other bolts\\nand nuts are on the correct side of these pivots – smaller nuts and bolts precede the pivots, and\\nlarger nuts and bolts follow the pivots. Our algorithm then completes by recursively applying\\nitself to the subarray to the left and right of the pivot position to match these remaining bolts and\\nnuts. We can assume by induction on n that these recursive calls will properly match the\\nremaining bolts.\\nTo analyze the running time of our algorithm, we can use the same analysis as that of randomized\\nQuick Sort. Therefore, applying the analysis from Quick Sort, the time complexity of our\\nalgorithm is O(nlogn).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 541, 'file_type': 'pdf'}, page_content='Alternative Analysis: We can solve this problem by making a small change to Quick Sort. Let us\\nassume that we pick the last element as the pivot, say it is a nut. Compare the nut with only bolts\\nas we walk down the array. This will partition the array for the bolts. Every bolt less than the\\npartition nut will be on the left. And every bolt greater than the partition nut will be on the right.\\nWhile traversing down the list, find the matching bolt for the partition nut. Now we do the\\npartition again using the matching bolt. As a result, all the nuts less than the matching bolt will be\\non the left side and all the nuts greater than the matching bolt will be on the right side.\\nRecursively call on the left and right arrays.\\nThe time complexity is O(2nlogn) ≈ O(nlogn).\\nProblem-37\\u2003\\u2003Given a binary tree, can we print its elements in sorted order in O(n) time by\\nperforming an In-order tree traversal?\\nSolution: Yes, if the tree is a Binary Search Tree [BST]. For more details refer to Trees chapter.\\nProblem-38\\u2003\\u2003Given an array of elements, convert it into an array such that A < B > C < D > E\\n< F and so on.\\nSolution: Sort the array, then swap every adjacent element to get the final result.\\nThe time complexity is O(nlogn+n) ≈ O(nlogn), for sorting and a scan.\\nProblem-39\\u2003\\u2003Can we do Problem-38 with O(n) time?\\nSolution: Make sure all even positioned elements are greater than their adjacent odd elements,\\nand we don’t need to worry about odd positioned elements. Traverse all even positioned\\nelements of input array, and do the following:\\n•\\nIf the current element is smaller than the previous odd element, swap previous and'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 542, 'file_type': 'pdf'}, page_content='current.\\n•\\nIf the current element is smaller than the next odd element, swap next and current.\\nThe time complexity is O(n).\\nProblem-40\\u2003\\u2003Merge sort uses\\n(a)\\nDivide and conquer strategy\\n(b)\\nBacktracking approach\\n(c)\\nHeuristic search\\n(d)\\nGreedy approach\\nSolution: (a). Refer theory section.\\nProblem-41\\u2003\\u2003Which of the following algorithm design techniques is used in the quicksort\\nalgorithm?\\n(a)\\nDynamic programming\\n(b)\\nBacktracking\\n(c)\\nDivide and conquer\\n(d)\\nGreedy method\\nSolution: (c). Refer theory section.\\nProblem-42\\u2003\\u2003For merging two sorted lists of sizes m and n into a sorted list of size m+n, we\\nrequired comparisons of\\n(a)\\nO(m)\\n(b)\\nO(n)\\n(c)\\nO(m + n)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 543, 'file_type': 'pdf'}, page_content='(d)\\nO(logm + logn)\\nSolution: (c). We can use merge sort logic. Refer theory section.\\nProblem-43\\u2003\\u2003Quick-sort is run on two inputs shown below to sort in ascending order\\n(i)\\n1,2,3 ....n\\n(ii)\\u2003n, n- 1, n-2, .... 2, 1\\nLet C1 and C2 be the number of comparisons made for the inputs (i) and (ii) respectively.\\nThen,\\n(a)\\nC1 < C2\\n(b)\\nC1 > C2\\n(c)\\nC1 = C2\\n(d)\\nwe cannot say anything for arbitrary n.\\nSolution: (b). Since the given problems needs the output in ascending order, Quicksort on already\\nsorted order gives the worst case (O(n2)). So, (i) generates worst case and (ii) needs fewer\\ncomparisons.\\nProblem-44\\u2003\\u2003Give the correct matching for the following pairs:\\n(A)\\nO(logn)\\n(B)\\nO(n)\\n(C)\\nO(nlogn)\\n(D)\\nO(n2)\\n(P)\\nSelection\\n(Q)\\nInsertion sort\\n(R)\\nBinary search\\n(S)\\nMerge sort\\n(a)\\nA – R B – P C – Q – D – S\\n(b)\\nA – R B – P C – S D – Q\\n(c)\\nA – P B – R C – S D – Q\\n(d)\\nA – P B – S C – R D – Q\\nSolution: (b). Refer theory section.\\nProblem-45\\u2003\\u2003Let s be a sorted array of n integers. Let t(n) denote the time taken for the most\\nefficient algorithm to determine if there are two elements with sum less than 1000 in s.\\nwhich of the following statements is true?\\na)\\nt(n) is O(1)\\nb)\\nn < t(n) < \\nc)\\nd)\\nSolution: (a). Since the given array is already sorted it is enough if we check the first two\\nelements of the array.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 544, 'file_type': 'pdf'}, page_content='Problem-46\\u2003\\u2003The usual Θ(n2) implementation of Insertion Sort to sort an array uses linear\\nsearch to identify the position where an element is to be inserted into the already sorted\\npart of the array. If, instead, we use binary search to identify the position, the worst case\\nrunning time will\\n(a)\\nremain Θ(n2)\\n(b)\\nbecome Θ(n(log n)2)\\n(c)\\nbecome Θ(nlogn)\\n(d)\\nbecome Θ(n)\\nSolution: (a). If we use binary search then there will be \\n comparisons in the worst case,\\nwhich is Θ(nlogn). But the algorithm as a whole will still have a running time of Θ(n2) on\\naverage because of the series of swaps required for each insertion.\\nProblem-47\\u2003\\u2003In quick sort, for sorting n elements, the n/4th smallest element is selected as\\npivot using an O(n) time algorithm. What is the worst case time complexity of the quick\\nsort?\\n(A)\\nΘ(n)\\n(B)\\nΘ(nLogn)\\n(C)\\nΘ(n2)\\n(D)\\nΘ(n2logn)\\nSolution: The recursion expression becomes: T(n) = T(n/4) + T(3n/4) + en. Solving the recursion\\nusing variant of master theorem, we get Θ(nLogn).\\nProblem-48\\u2003\\u2003Consider the Quicksort algorithm. Suppose there is a procedure for finding a\\npivot element which splits the list into two sub-lists each of which contains at least one-\\nfifth of the elements. Let T(n) be the number of comparisons required to sort n elements.\\nThen\\nA)\\nT (n) ≤ 2T (n /5) + n\\nB)\\nT (n) ≤ T (n /5) + T (4n /5) + n\\nC)\\nT (n) ≤ 2T (4n /5) + n\\nD)\\nT (n) ≤ 2T (n /2) + n\\nSolution: (C). For the case where n/5 elements are in one subset, T(n/5) comparisons are needed\\nfor the first subset with n/5 elements, T(4n/5) is for the rest 4n/5 elements, and n is for finding the\\npivot. If there are more than n/5 elements in one set then other set will have less than 4n/5\\nelements and time complexity will be less than T(n/5) + T(4n/5) + n.\\nProblem-49\\u2003\\u2003Which of the following sorting algorithms has the lowest worst-case\\ncomplexity?\\n(A)\\nMerge sort\\n(B)\\nBubble sort\\n(C)\\nQuick sort\\n(D)\\nSelection sort'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 545, 'file_type': 'pdf'}, page_content='Solution: (A). Refer theory section.\\nProblem-50\\u2003\\u2003Which one of the following in place sorting algorithms needs the minimum\\nnumber of swaps?\\n(A)\\nQuick sort\\n(B)\\nInsertion sort\\n(C)\\nSelection sort\\n(D)\\nHeap sort\\nSolution: (C). Refer theory section.\\nProblem-51\\u2003\\u2003You have an array of n elements. Suppose you implement quicksort by always\\nchoosing the central element of the array as the pivot. Then the tightest upper bound for the\\nworst case performance is\\n(A)\\nO(n2)\\n(B)\\nO(nlogn)\\n(C)\\nΘ(nlogn)\\n(D)\\nO(n3)\\nSolution: (A). When we choose the first element as the pivot, the worst case of quick sort comes\\nif the input is sorted- either in ascending or descending order.\\nProblem-52\\u2003\\u2003Let P be a Quicksort Program to sort numbers in ascending order using the first\\nelement as pivot. Let t1 and t2 be the number of comparisons made by P for the inputs {1,\\n2, 3, 4, 5} and {4, 1, 5, 3, 2} respectively. Which one of the following holds?\\n(A)\\nt1 = 5\\n(B)\\nt1 < t2\\n(C)\\nt1 > t2\\n(D)\\nt1 = t2\\nSolution: (C). Quick Sort’s worst case occurs when first (or last) element is chosen as pivot with\\nsorted arrays.\\nProblem-53\\u2003\\u2003The minimum number of comparisons required to find the minimum and the\\nmaximum of 100 numbers is ——\\nSolution: 147 (Formula for the minimum number of comparisons required is 3n/2 – 3 with n\\nnumbers).\\nProblem-54\\u2003\\u2003The number of elements that can be sorted in T(logn) time using heap sort is\\n(A)\\nΘ(1)\\n(B)\\nΘ(sqrt(logn))\\n(C)\\nΘ(log n/(log log n))\\n(D)\\nΘ(logn)\\nSolution: (D). Sorting an array with k elements takes time Θ(k log k) as k grows. We want to\\nchoose k such that Θ(k log k) = Θ(logn). Choosing k = Θ(logn) doesn’t necessarily work, since'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 546, 'file_type': 'pdf'}, page_content='Θ(k log k) = Θ(logn loglogn) ≠ Θ(logn). On the other hand, if you choose k = T(log n / log log n),\\nthen the runtime of the sort will be\\nNotice that 1 – logloglogn / loglogn tends toward 1 as n goes to infinity, so the above expression\\nactually is Θ(log n), as required. Therefore, if you try to sort an array of size Θ(logn / loglogn)\\nusing heap sort, as a function of n, the runtime is Θ(logn).\\nProblem-55\\u2003\\u2003Which one of the following is the tightest upper bound that represents the\\nnumber of swaps required to sort n numbers using selection sort?\\n(A)\\nO(logn)\\n(B)\\nO(n)\\n(C)\\nO(nlogn)\\n(D)\\nO(n2)\\nSolution: (B). Selection sort requires only O(n) swaps.\\nProblem-56\\u2003\\u2003Which one of the following is the recurrence equation for the worst case time\\ncomplexity of the Quicksort algorithm for sorting n(≥ 2) numbers? In the recurrence\\nequations given in the options below, c is a constant.\\n(A)T(n) = 2T (n/2) + cn\\n(B)\\nT(n) = T(n – 1) + T(0) + cn\\n(C)\\nT(n) = 2T (n – 2) + cn\\n(D)\\nT(n) = T(n/2) + cn\\nSolution: (B). When the pivot is the smallest (or largest) element at partitioning on a block of size\\nn the result yields one empty sub-block, one element (pivot) in the correct place and sub block of\\nsize n – 1.\\nProblem-57\\u2003\\u2003True or False. In randomized quicksort, each key is involved in the same number\\nof comparisons.\\nSolution: False.\\nProblem-58\\u2003\\u2003True or False: If Quicksort is written so that the partition algorithm always uses\\nthe median value of the segment as the pivot, then the worst-case performance is O(nlogn).\\nSoution: True.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 547, 'file_type': 'pdf'}, page_content='11.1 What is Searching?\\nIn computer science, searching is the process of finding an item with specified properties from a\\ncollection of items. The items may be stored as records in a database, simple data elements in\\narrays, text in files, nodes in trees, vertices and edges in graphs, or they may be elements of other\\nsearch spaces.\\n11.2 Why do we need Searching?\\nSearching is one of the core computer science algorithms. We know that today’s computers store\\na lot of information. To retrieve this information proficiently we need very efficient searching\\nalgorithms. There are certain ways of organizing the data that improves the searching process.\\nThat means, if we keep the data in proper order, it is easy to search the required element. Sorting\\nis one of the techniques for making the elements ordered. In this chapter we will see different\\nsearching algorithms.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 548, 'file_type': 'pdf'}, page_content='11.3 Types of Searching\\nFollowing are the types of searches which we will be discussing in this book.\\n•\\nUnordered Linear Search\\n•\\nSorted/Ordered Linear Search\\n•\\nBinary Search\\n•\\nInterpolation search\\n•\\nBinary Search Trees (operates on trees and refer Trees chapter)\\n•\\nSymbol Tables and Hashing\\n•\\nString Searching Algorithms: Tries, Ternary Search and Suffix Trees\\n11.4 Unordered Linear Search\\nLet us assume we are given an array where the order of the elements is not known. That means the\\nelements of the array are not sorted. In this case, to search for an element we have to scan the\\ncomplete array and see if the element is there in the given list or not.\\nTime complexity: O(n), in the worst case we need to scan the complete array. Space complexity:\\nO(1).\\n11.5 Sorted/Ordered Linear Search\\nIf the elements of the array are already sorted, then in many cases we don’t have to scan the\\ncomplete array to see if the element is there in the given array or not. In the algorithm below, it\\ncan be seen that, at any point if the value at A[i] is greater than the data to be searched, then we\\njust return –1 without searching the remaining array.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 549, 'file_type': 'pdf'}, page_content='Time complexity of this algorithm is O(n).This is because in the worst case we need to scan the\\ncomplete array. But in the average case it reduces the complexity even though the growth rate is\\nthe same.\\nSpace complexity: O(1).\\nNote: For the above algorithm we can make further improvement by incrementing the index at a\\nfaster rate (say, 2). This will reduce the number of comparisons for searching in the sorted list.\\n11.6 Binary Search\\nLet us consider the problem of searching a word in a dictionary. Typically, we directly go to\\nsome approximate page [say, middle page] and start searching from that point. If the name that we\\nare searching is the same then the search is complete. If the page is before the selected pages then\\napply the same process for the first half; otherwise apply the same process to the second half.\\nBinary search also works in the same way. The algorithm applying such a strategy is referred to\\nas binary search algorithm.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 550, 'file_type': 'pdf'}, page_content='Recurrence for binary search is \\n. This is because we are always\\nconsidering only half of the input list and throwing out the other half. Using Divide and Conquer\\nmaster theorem, we get, T(n) = O(logn).\\nTime Complexity: O(logn). Space Complexity: O(1) [for iterative algorithm].\\n11.7 Interpolation Search\\nUndoubtedly binary search is a great algorithm for searching with average running time\\ncomplexity of logn. It always chooses the middle of the remaining search space, discarding one\\nhalf or the other, again depending on the comparison between the key value found at the estimated\\n(middle) position and the key value sought. The remaining search space is reduced to the part'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 551, 'file_type': 'pdf'}, page_content='before or after the estimated position.\\nIn the mathematics, interpolation is a process of constructing new data points within the range of a\\ndiscrete set of known data points. In computer science, one often has a number of data points\\nwhich represent the values of a function for a limited number of values of the independent\\nvariable. It is often required to interpolate (i.e. estimate) the value of that function for an\\nintermediate value of the independent variable.\\nFor example, suppose we have a table like this, which gives some values of an unknown function\\nf. Interpolation provides a means of estimating the function at intermediate points, such as x = 55.\\nx\\nf(x)\\n1\\n10\\n2\\n20\\n3\\n30\\n4\\n40\\n5\\n50\\n6\\n60\\n7\\n70\\nThere are many different interpolation methods, and one of the simplest methods is linear\\ninterpolation. Since 55 is midway between 50 and 60, it is reasonable to take f(55) midway\\nbetween f(5) = 50 and f(6) = 60, which yields 55.\\nLinear interpolation takes two data points, say (x1; y2) and (x2, y2), and the interpolant is given by:\\nWith above inputs, what will happen if we don’t use the constant ½, but another more accurate\\nconstant “K”, that can lead us closer to the searched item.\\nThis algorithm tries to follow the way we search a name in a phone book, or a word in the\\ndictionary. We, humans, know in advance that in case the name we’re searching starts with a “m”,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 552, 'file_type': 'pdf'}, page_content='like “monk” for instance, we should start searching near the middle of the phone book. Thus if\\nwe’re searching the word “career” in the dictionary, you know that it should be placed\\nsomewhere at the beginning. This is because we know the order of the letters, we know the\\ninterval (a-z), and somehow we intuitively know that the words are dispersed equally. These\\nfacts are enough to realize that the binary search can be a bad choice. Indeed the binary search\\nalgorithm divides the list in two equal sub-lists, which is useless if we know in advance that the\\nsearched item is somewhere in the beginning or the end of the list. Yes, we can use also jump\\nsearch if the item is at the beginning, but not if it is at the end, in that case this algorithm is not so\\neffective.\\nThe interpolation search algorithm tries to improve the binary search. The question is how to find\\nthis value? Well, we know bounds of the interval and looking closer to the image above we can\\ndefine the following formula.\\nThis constant K is used to narrow down the search space. For binary search, this constant K is\\n(low + high)/2.\\nNow we can be sure that we’re closer to the searched value. On average the interpolation search\\nmakes about log (logn) comparisons (if the elements are uniformly distributed), where n is the\\nnumber of elements to be searched. In the worst case (for instance where the numerical values of\\nthe keys increase exponentially) it can make up to O(n) comparisons. In interpolation-sequential\\nsearch, interpolation is used to find an item near the one being searched for, then linear search is\\nused to find the exact item. For this algorithm to give best results, the dataset should be ordered\\nand uniformly distributed.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 553, 'file_type': 'pdf'}, page_content='11.8 Comparing Basic Searching Algorithms\\nNote: For discussion on binary search trees refer Trees chapter.\\n11.9 Symbol Tables and Hashing\\nRefer to Symbol Tables and Hashing chapters.\\n11.10 String Searching Algorithms\\nRefer to String Algorithms chapter.\\n11.11 Searching: Problems & Solutions\\nProblem-1\\u2003\\u2003Given an array of n numbers, give an algorithm for checking whether there are\\nany duplicate elements in the array or no?\\nSolution: This is one of the simplest problems. One obvious answer to this is exhaustively\\nsearching for duplicates in the array. That means, for each input element check whether there is\\nany element with the same value. This we can solve just by using two simple for loops. The code\\nfor this solution can be given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 554, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2), for two nested for loops. Space Complexity: O(1).\\nProblem-2\\u2003\\u2003Can we improve the complexity of Problem-1’s solution?\\nSolution: Yes. Sort the given array. After sorting, all the elements with equal values will be\\nadjacent. Now, do another scan on this sorted array and see if there are elements with the same\\nvalue and adjacent.\\nTime Complexity: O(nlogn), for sorting (assuming nlogn sorting algorithm). Space Complexity:\\nO(1).\\nProblem-3\\u2003\\u2003Is there any alternative way of solving Problem-1?\\nSolution: Yes, using hash table. Hash tables are a simple and effective method used to implement\\ndictionaries. Average time to search for an element is O(1), while worst-case time is O(n). Refer\\nto Hashing chapter for more details on hashing algorithms. As an example, consider the array, A =\\n{3,2,1,2,2,3}.\\nScan the input array and insert the elements into the hash. For each inserted element, keep the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 555, 'file_type': 'pdf'}, page_content='counter as 1 (assume initially all entires are filled with zeros). This indicates that the\\ncorresponding element has occurred already. For the given array, the hash table will look like\\n(after inserting the first three elements 3,2 and 1):\\nNow if we try inserting 2, since the counter value of 2 is already 1, we can say the element has\\nappeared twice.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-4\\u2003\\u2003Can we further improve the complexity of Problem-1 solution?\\nSolution: Let us assume that the array elements are positive numbers and all the elements are in\\nthe range 0 to n – 1. For each element A[i], go to the array element whose index is A[i]. That\\nmeans select A[A[i]] and mark - A[A[i]] (negate the value at A[A[i]]). Continue this process until\\nwe encounter the element whose value is already negated. If one such element exists then we say\\nduplicate elements exist in the given array. As an example, consider the array, A = {3,2,1,2,2,3}.\\nInitially,\\nAt step-1, negate A[abs(A[0])],\\nAt step-2, negate A[abs(A[l])],\\nAt step-3, negate A[abs(A[2])],'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 556, 'file_type': 'pdf'}, page_content='At step-4, negate A[abs(A[3])],\\nAt step-4, observe that A[abs(A[3])] is already negative. That means we have encountered the\\nsame value twice.\\nTime Complexity: O(n). Since only one scan is required. Space Complexity: O(1).\\nNotes:\\n•\\nThis solution does not work if the given array is read only.\\n•\\nThis solution will work only if all the array elements are positive.\\n•\\nIf the elements range is not in 0 to n – 1 then it may give exceptions.\\nProblem-5\\u2003\\u2003Given an array of n numbers. Give an algorithm for finding the element which\\nappears the maximum number of times in the array?\\nBrute Force Solution: One simple solution to this is, for each input element check whether there\\nis any element with the same value, and for each such occurrence, increment the counter. Each\\ntime, check the current counter with the max counter and update it if this value is greater than max\\ncounter. This we can solve just by using two simple for loops.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 557, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2), for two nested for loops. Space Complexity: O(1).\\nProblem-6\\u2003\\u2003Can we improve the complexity of Problem-5 solution?\\nSolution: Yes. Sort the given array. After sorting, all the elements with equal values come\\nadjacent. Now, just do another scan on this sorted array and see which element is appearing the\\nmaximum number of times.\\nTime Complexity: O(nlogn). (for sorting). Space Complexity: O(1).\\nProblem-7\\u2003\\u2003Is there any other way of solving Problem-5?\\nSolution: Yes, using hash table. For each element of the input, keep track of how many times that\\nelement appeared in the input. That means the counter value represents the number of occurrences\\nfor that element.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-8\\u2003\\u2003or Problem-5, can we improve the time complexity? Assume that the elements’\\nrange is 1 to n. That means all the elements are within this range only.\\nSolution: Yes. We can solve this problem in two scans. We cannot use the negation technique of\\nProblem-3 for this problem because of the number of repetitions. In the first scan, instead of\\nnegating, add the value n. That means for each occurrence of an element add the array size to that\\nelement. In the second scan, check the element value by dividing it by n and return the element\\nwhich gives the maximum value. The code based on this method is given below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 558, 'file_type': 'pdf'}, page_content='Notes:\\n•\\nThis solution does not work if the given array is read only.\\n•\\nThis solution will work only if the array elements are positive.\\n•\\nIf the elements range is not in 1 to n then it may give exceptions.\\nTime Complexity: O(n). Since no nested for loops are required. Space Complexity: O(1).\\nProblem-9\\u2003\\u2003Given an array of n numbers, give an algorithm for finding the first element in the\\narray which is repeated. For example, in the array A = {3,2,1,2,2,3}, the first repeated\\nnumber is 3 (not 2). That means, we need to return the first element among the repeated\\nelements.\\nSolution: We can use the brute force solution that we used for Problem-1. For each element, since\\nit checks whether there is a duplicate for that element or not, whichever element duplicates first\\nwill be returned.\\nProblem-10\\u2003\\u2003For Problem-9, can we use the sorting technique?\\nSolution: No. For proving the failed case, let us consider the following array. For example, A =\\n{3, 2, 1, 2, 2, 3}. After sorting we get A = {1,2,2,2,3,3}. In this sorted array the first repeated\\nelement is 2 but the actual answer is 3.\\nProblem-11\\u2003\\u2003For Problem-9, can we use hashing technique?\\nSolution: Yes. But the simple hashing technique which we used for Problem-3 will not work. For\\nexample, if we consider the input array as A = {3,2,1,2,3}, then the first repeated element is 3,\\nbut using our simple hashing technique we get the answer as 2. This is because 2 is coming twice\\nbefore 3. Now let us change the hashing table behavior so that we get the first repeated element.\\nLet us say, instead of storing 1 value, initially we store the position of the element in the array. As\\na result the hash table will look like (after inserting 3,2 and 1):'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 559, 'file_type': 'pdf'}, page_content='Now, if we see 2 again, we just negate the current value of 2 in the hash table. That means, we\\nmake its counter value as –2. The negative value in the hash table indicates that we have seen the\\nsame element two times. Similarly, for 3 (the next element in the input) also, we negate the current\\nvalue of the hash table and finally the hash table will look like:\\nAfter processing the complete input array, scan the hash table and return the highest negative\\nindexed value from it (i.e., –1 in our case). The highest negative value indicates that we have seen\\nthat element first (among repeated elements) and also repeating.\\nWhat if the element is repeated more than twice? In this case, just skip the element if the\\ncorresponding value i is already negative.\\nProblem-12\\u2003\\u2003For Problem-9, can we use the technique that we used for Problem-3 (negation\\ntechnique)?\\nSolution: No. As an example of contradiction, for the array A = {3,2,1,2,2,3} the first repeated\\nelement is 3. But with negation technique the result is 2.\\nProblem-13\\u2003\\u2003Finding the Missing Number: We are given a list of n – 1 integers and these\\nintegers are in the range of 1 to n. There are no duplicates in the list. One of the integers is\\nmissing in the list. Given an algorithm to find the missing integer. Example: I/P:\\n[1,2,4,6,3,7,8] O/P: 5\\nBrute Force Solution: One simple solution to this is, for each number in 1 to n, check whether\\nthat number is in the given array or not.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 560, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). Space Complexity: O(1).\\nProblem-14\\u2003\\u2003For Problem-13, can we use sorting technique?\\nSolution: Yes. Sorting the list will give the elements in increasing order and with another scan we\\ncan find the missing number.\\nTime Complexity: O(nlogn), for sorting. Space Complexity: O(1).\\nProblem-15\\u2003\\u2003For Problem-13, can we use hashing technique?\\nSolution: Yes. Scan the input array and insert elements into the hash. For inserted elements, keep\\ncounter as 1 (assume initially all entires are filled with zeros). This indicates that the\\ncorresponding element has occurred already. Now, scan the hash table and return the element\\nwhich has counter value zero.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-16\\u2003\\u2003For Problem-13, can we improve the complexity?\\nSolution: Yes. We can use summation formula.\\n1)\\nGet the sum of numbers, sum = n × (n + l)/2.\\n2)\\nSubtract all the numbers from sum and you will get the missing number.\\nTime Complexity: O(n), for scanning the complete array.\\nProblem-17\\u2003\\u2003In Problem-13, if the sum of the numbers goes beyond the maximum allowed\\ninteger, then there can be integer overflow and we may not get the correct answer. Can we\\nsolve this problem?\\nSolution:\\n1)\\nXOR all the array elements, let the result of XOR be X.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 561, 'file_type': 'pdf'}, page_content='2)\\nXOR all numbers from 1 to n, let XOR be Y.\\n3)\\nXOR of X and Y gives the missing number.\\nTime Complexity: O(n), for scanning the complete array. Space Complexity: O(1).\\nProblem-18\\u2003\\u2003Find the Number Occurring an Odd Number of Times: Given an array of\\npositive integers, all numbers occur an even number of times except one number which\\noccurs an odd number of times. Find the number in O(n) time & constant space. Example :\\nI/P = [1,2,3,2,3,1,3] O/P = 3\\nSolution: Do a bitwise XOR of all the elements. We get the number which has odd occurrences.\\nThis is because, A XOR A = 0.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-19\\u2003\\u2003Find the two repeating elements in a given array: Given an array with size,\\nall elements of the array are in range 1 to n and also all elements occur only once except\\ntwo numbers which occur twice. Find those two repeating numbers. For example: if the\\narray is 4,2,4,5,2,3,1 with size = 7 and n = 5. This input has n + 2 = 7 elements with all\\nelements occurring once except 2 and 4 which occur twice. So the output should be 4 2.\\nSolution: One simple way is to scan the complete array for each element of the input elements.\\nThat means use two loops. In the outer loop, select elements one by one and count the number of\\noccurrences of the selected element in the inner loop. For the code below, assume that\\nPrintRepeatedElements is called with n + 2 to indicate the size.\\nTime Complexity: O(n2). Space Complexity: O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 562, 'file_type': 'pdf'}, page_content='Problem-20\\u2003\\u2003For Problem-19, can we improve the time complexity?\\nSolution: Sort the array using any comparison sorting algorithm and see if there are any elements\\nwhich are contiguous with the same value.\\nTime Complexity: O(nlogn). Space Complexity: O(1).\\nProblem-21\\u2003\\u2003For Problem-19, can we improve the time complexity?\\nSolution: Use Count Array. This solution is like using a hash table. For simplicity we can use\\narray for storing the counts. Traverse the array once and keep track of the count of all elements in\\nthe array using a temp array count[] of size n. When we see an element whose count is already\\nset, print it as duplicate. For the code below assume that PrintRepeatedElements is called with n\\n+ 2 to indicate the size.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-22\\u2003\\u2003Consider Problem-19. Let us assume that the numbers are in the range 1 to n. Is\\nthere any other way of solving the problem?\\nSolution: Yes, by using XOR Operation. Let the repeating numbers be X and Y, if we XOR all\\nthe elements in the array and also all integers from 1 to n, then the result will be X XOR Y. The 1’s\\nin binary representation of X XOR Y correspond to the different bits between X and Y. If the kth bit\\nof X XOR Y is 1, we can XOR all the elements in the array and also all integers from 1 to n whose\\nkth bits are 1. The result will be one of X and Y.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 563, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-23\\u2003\\u2003Consider Problem-19. Let us assume that the numbers are in the range 1 to n. Is\\nthere yet other way of solving the problem?\\nSolution: We can solve this by creating two simple mathematical equations. Let us assume that\\ntwo numbers we are going to find are X and Y. We know the sum of n numbers is n(n + l)/2 and\\nthe product is n!. Make two equations using these sum and product formulae, and get values of\\ntwo unknowns using the two equations. Let the summation of all numbers in array be S and\\nproduct be P and the numbers which are being repeated are X and Y.\\nUsing the above two equations, we can find out X and Y. There can be an addition and\\nmultiplication overflow problem with this approach.\\nTime Complexity: O(n). Space Complexity: O(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 564, 'file_type': 'pdf'}, page_content='Problem-24\\u2003\\u2003Similar to Problem-19, let us assume that the numbers are in the range 1 to n.\\nAlso, n – 1 elements are repeating thrice and remaining element repeated twice. Find the\\nelement which repeated twice.\\nSolution: If we XOR all the elements in the array and all integers from 1 to n, then all the\\nelements which are repeated thrice will become zero. This is because, since the element is\\nrepeating thrice and XOR another time from range makes that element appear four times. As a\\nresult, the output of a XOR a XOR a XOR a = 0. It is the same case with all elements that are\\nrepeated three times.\\nWith the same logic, for the element which repeated twice, if we XOR the input elements and also\\nthe range, then the total number of appearances for that element is 3. As a result, the output of a\\nXOR a XOR a = a. Finally, we get the element which repeated twice.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-25\\u2003\\u2003Given an array of n elements. Find two elements in the array such that their sum\\nis equal to given element K.\\nBrute Force Solution: One simple solution to this is, for each input element, check whether there\\nis any element whose sum is K. This we can solve just by using two simple for loops. The code\\nfor this solution can be given as:\\nTime Complexity: O(n2). This is because of two nested for loops. Space Complexity: O(1).\\nProblem-26\\u2003\\u2003For Problem-25, can we improve the time complexity?\\nSolution: Yes. Let us assume that we have sorted the given array. This operation takes O(nlogn).\\nOn the sorted array, maintain indices loIndex = 0 and hiIndex = n – 1 and compute A[loIndex] +\\nA[hiIndex]. If the sum equals K, then we are done with the solution. If the sum is less than K,\\ndecrement hiIndex, if the sum is greater than K, increment loIndex.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 565, 'file_type': 'pdf'}, page_content='Time Complexity: O(nlogn). If the given array is already sorted then the complexity is O(n).\\nSpace Complexity: O(1).\\nProblem-27\\u2003\\u2003Does the solution of Problem-25 work even if the array is not sorted?\\nSolution: Yes. Since we are checking all possibilities, the algorithm ensures that we get the pair\\nof numbers if they exist.\\nProblem-28\\u2003\\u2003Is there any other way of solving Problem-25?\\nSolution: Yes, using hash table. Since our objective is to find two indexes of the array whose sum\\nis K. Let us say those indexes are X and Y. That means, A[X] + A[Y] = K. What we need is, for\\neach element of the input array A[X], check whether K – A[X] also exists in the input array. Now,\\nlet us simplify that searching with hash table.\\nAlgorithm:\\n•\\nFor each element of the input array, insert it into the hash table. Let us say the current\\nelement is A[X].\\n•\\nBefore proceeding to the next element we check whether K – A[X] also exists in the\\nhash table or not.\\n•\\nTher existence of such number indicates that we are able to find the indexes.\\n•\\nOtherwise proceed to the next input element.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-29\\u2003\\u2003Given an array A of n elements. Find three indices, i,j & k such that A[i]2 + A[j]2'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 566, 'file_type': 'pdf'}, page_content='= A[k]2?\\nSolution:\\nAlgorithm:\\n•\\nSort the given array in-place.\\n•\\nFor each array index i compute A[i]2 and store in array.\\n•\\nSearch for 2 numbers in array from 0 to i – 1 which adds to A[i] similar to Problem-\\n25. This will give us the result in O(n) time. If we find such a sum, return true,\\notherwise continue.\\nTime Complexity: Time for sorting + n × (Time for finding the sum) = O(nlogn) + n × O(n)= n2.\\nSpace Complexity: O(1).\\nProblem-30\\u2003\\u2003Two elements whose sum is closest to zero. Given an array with both positive\\nand negative numbers, find the two elements such that their sum is closest to zero. For the\\nbelow array, algorithm should give -80 and 85. Example: 1 60 – 10 70 – 80 85\\nBrute Force Solution: For each element, find the sum with every other element in the array and\\ncompare sums. Finally, return the minimum sum.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 567, 'file_type': 'pdf'}, page_content='Time complexity: O(n2). Space Complexity: O(1).\\nProblem-31\\u2003\\u2003Can we improve the time complexity of Problem-30?\\nSolution: Use Sorting.\\nAlgorithm:\\n1.\\nSort all the elements of the given input array.\\n2.\\nMaintain two indexes, one at the beginning (i = 0) and the other at the ending (j = n –\\n1). Also, maintain two variables to keep track of the smallest positive sum closest\\nto zero and the smallest negative sum closest to zero.\\n3.\\nWhile i < j:\\na.\\nIf the current pair sum is > zero and < postiveClosest then update the\\npostiveClosest. Decrement j.\\nb.\\nIf the current pair sum is < zero and > negativeClosest then update the\\nnegativeClosest. Increment i.\\nc.\\nElse, print the pair'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 568, 'file_type': 'pdf'}, page_content='Time Complexity: O(nlogn), for sorting. Space Complexity: O(1).\\nProblem-32\\u2003\\u2003Given an array of n elements. Find three elements in the array such that their sum\\nis equal to given element K?\\nBrute Force Solution: The default solution to this is, for each pair of input elements check\\nwhether there is any element whose sum is K. This we can solve just by using three simple for\\nloops. The code for this solution can be given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 569, 'file_type': 'pdf'}, page_content='Time Complexity: O(n3), for three nested for loops. Space Complexity: O(1).\\nProblem-33\\u2003\\u2003Does the solution of Problem-32 work even if the array is not sorted?\\nSolution: Yes. Since we are checking all possibilities, the algorithm ensures that we can find\\nthree numbers whose sum is K if they exist.\\nProblem-34\\u2003\\u2003Can we use sorting technique for solving Problem-32?\\nSolution: Yes.\\nTime Complexity: Time for sorting + Time for searching in sorted list = O(nlogn) + O(n2) ≈\\nO(n2). This is because of two nested for loops. Space Complexity: O(1).\\nProblem-35\\u2003\\u2003Can we use hashing technique for solving Problem-32?\\nSolution: Yes. Since our objective is to find three indexes of the array whose sum is K. Let us say\\nthose indexes are X,Y and Z. That means, A[X] + A[Y] + A[Z] = K.\\nLet us assume that we have kept all possible sums along with their pairs in hash table. That means\\nthe key to hash table is K – A[X] and values for K – A[X] are all possible pairs of input whose\\nsum is if – A[X].\\nAlgorithm:\\n•\\nBefore starting the search, insert all possible sums with pairs of elements into the\\nhash table.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 570, 'file_type': 'pdf'}, page_content='•\\nFor each element of the input array, insert into the hash table. Let us say the current\\nelement is A[X].\\n•\\nCheck whether there exists a hash entry in the table with key: K – A[X].\\n•\\nIf such element exists then scan the element pairs of K – A[X] and return all possible\\npairs by including A[X] also.\\n•\\nIf no such element exists (with K – A[X] as key) then go to next element.\\nTime Complexity: The time for storing all possible pairs in Hash table + searching = O(n2) +\\nO(n2) ≈ O(n2). Space Complexity: O(n).\\nProblem-36\\u2003\\u2003Given an array of n integers, the 3 – sum problem is to find three integers whose\\nsum is closest to zero.\\nSolution: This is the same as that of Problem-32 with K value is zero.\\nProblem-37\\u2003\\u2003Let A be an array of n distinct integers. Suppose A has the following property:\\nthere exists an index 1 ≤ k ≤ n such that A[l],..., A[k] is an increasing sequence and A[k +\\n1],..., A[n] is a decreasing sequence. Design and analyze an efficient algorithm for finding\\nk.\\nSimilar question: Let us assume that the given array is sorted but starts with negative\\nnumbers and ends with positive numbers [such functions are called monotonically\\nincreasing functions]. In this array find the starting index of the positive numbers. Assume\\nthat we know the length of the input array. Design a O(logn) algorithm.\\nSolution: Let us use a variant of the binary search.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 571, 'file_type': 'pdf'}, page_content='The recursion equation is T(n) = 2T(n/2) + c. Using master theorem, we get O(logn).\\nProblem-38\\u2003\\u2003If we don’t know n, how do we solve the Problem-37?\\nSolution: Repeatedly compute A[1],A[2],A[4],A[8],A[16] and so on, until we find a value of n\\nsuch that A[n] > 0.\\nTime Complexity: O(logn), since we are moving at the rate of 2. Refer to Introduction to\\nAnalysis of Algorithms chapter for details on this.\\nProblem-39\\u2003\\u2003Given an input array of size unknown with all 1’s in the beginning and 0’s in the\\nend. Find the index in the array from where 0’s start. Consider there are millions of 1’s and\\n0’s in the array. E.g. array contents 1111111……..1100000……..0000000.\\nSolution: This problem is almost similar to Problem-38. Check the bits at the rate of 2Kwhere k\\n= 0,1,2 .... Since we are moving at the rate of 2, the complexity is O(logn).\\nProblem-40\\u2003\\u2003Given a sorted array of n integers that has been rotated an unknown number of\\ntimes, give a O(logn) algorithm that finds an element in the array.\\nExample: Find 5 in array (15 16 19 20 25 1 3 4 5 7 10 14) Output: 8 (the index of 5 in\\nthe array)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 572, 'file_type': 'pdf'}, page_content='Solution: Let us assume that the given array is A[]and use the solution of Problem-37 with an\\nextension. The function below FindPivot returns the k value (let us assume that this function\\nreturns the index instead of the value). Find the pivot point, divide the array into two sub-arrays\\nand call binary search.\\nThe main idea for finding the pivot point is – for a sorted (in increasing order) and pivoted array,\\nthe pivot element is the only element for which the next element to it is smaller than it. Using the\\nabove criteria and the binary search methodology we can get pivot element in O(logn) time.\\nAlgorithm:\\n1)\\nFind out the pivot point and divide the array into two sub-arrays.\\n2)\\nNow call binary search for one of the two sub-arrays.\\na.\\nif the element is greater than the first element then search in left\\nsubarray.\\nb.\\nelse search in right subarray.\\n3)\\nIf element is found in selected sub-array, then return index else return –1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 573, 'file_type': 'pdf'}, page_content='Time complexity: O(logn).\\nProblem-41\\u2003\\u2003For Problem-40, can we solve with recursion?\\nSolution: Yes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 574, 'file_type': 'pdf'}, page_content='Time complexity: O(logn).\\nProblem-42\\u2003\\u2003Bitonic search: An array is bitonic if it is comprised of an increasing sequence\\nof integers followed immediately by a decreasing sequence of integers. Given a bitonic\\narray A of n distinct integers, describe how to determine whether a given integer is in the\\narray in O(logn) steps.\\nSolution: The solution is the same as that for Problem-37.\\nProblem-43\\u2003\\u2003Yet, other way of framing Problem-37.\\nLet A[] be an array that starts out increasing, reaches a maximum, and then decreases.\\nDesign an O(logn) algorithm to find the index of the maximum value.\\nProblem-44\\u2003\\u2003Give an O(nlogn) algorithm for computing the median of a sequence of n\\nintegers.\\nSolution: Sort and return element at .\\nProblem-45\\u2003\\u2003Given two sorted lists of size m and n, find median of all elements in O(log (m\\n+ n)) time.\\nSolution: Refer to Divide and Conquer chapter.\\nProblem-46\\u2003\\u2003Given a sorted array A of n elements, possibly with duplicates, find the index of\\nthe first occurrence of a number in O(logn) time.\\nSolution: To find the first occurrence of a number we need to check for the following condition.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 575, 'file_type': 'pdf'}, page_content='Return the position if any one of the following is true:\\nTime Complexity: O(logn).\\nProblem-47\\u2003\\u2003Given a sorted array A of n elements, possibly with duplicates. Find the index of\\nthe last occurrence of a number in O(logn) time.\\nSolution: To find the last occurrence of a number we need to check for the following condition.\\nReturn the position if any one of the following is true:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 576, 'file_type': 'pdf'}, page_content='Time Complexity: O(logn).\\nProblem-48\\u2003\\u2003Given a sorted array of n elements, possibly with duplicates. Find the number of\\noccurrences of a number.\\nBrute Force Solution: Do a linear search of the array and increment count as and when we find\\nthe element data in the array.\\nTime Complexity: O(n).\\nProblem-49\\u2003\\u2003Can we improve the time complexity of Problem-48?\\nSolution: Yes. We can solve this by using one binary search call followed by another small scan.\\nAlgorithm:\\n•\\nDo a binary search for the data in the array. Let us assume its position is K.\\n•\\nNow traverse towards the left from K and count the number of occurrences of data.\\nLet this count be leftCount.\\n•\\nSimilarly, traverse towards right and count the number of occurrences of data. Let\\nthis count be rightCount.\\n•\\nTotal number of occurrences = leftCount + 1 + rightCount\\nTime Complexity – O(logn + S) where 5 is the number of occurrences of data.\\nProblem-50\\u2003\\u2003Is there any alternative way of solving Problem-48?\\nSolution:\\nAlgorithm:\\n•\\nFind first occurrence of data and call its index as firstOccurrence (for algorithm\\nrefer to Problem-46)\\n•\\nFind last occurrence of data and call its index as lastOccurrence (for algorithm\\nrefer to Problem-47)\\n•\\nReturn lastOccurrence – firstOccurrence + 1\\nTime Complexity = O(logn + logn) = O(logn).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 577, 'file_type': 'pdf'}, page_content='Problem-51\\u2003\\u2003What is the next number in the sequence 1,11,21 and why?\\nSolution: Read the given number loudly. This is just a fun problem.\\nSo the answer is: the next number is the representation of the previous number by reading it\\nloudly.\\nProblem-52\\u2003\\u2003Finding second smallest number efficiently.\\nSolution: We can construct a heap of the given elements using up just less than n comparisons\\n(Refer to the Priority Queues chapter for the algorithm). Then we find the second smallest using\\nlogn comparisons for the GetMax() operation. Overall, we get n + logn + constant.\\nProblem-53\\u2003\\u2003Is there any other solution for Problem-52?\\nSolution: Alternatively, split the n numbers into groups of 2, perform n/2 comparisons\\nsuccessively to find the largest, using a tournament-like method. The first round will yield the\\nmaximum in n – 1 comparisons. The second round will be performed on the winners of the first\\nround and the ones that the maximum popped. This will yield logn – 1 comparison for a total of n\\n+ logn – 2. The above solution is called the tournament problem.\\nProblem-54\\u2003\\u2003An element is a majority if it appears more than n/2 times. Give an algorithm\\ntakes an array of n element as argument and identifies a majority (if it exists).\\nSolution: The basic solution is to have two loops and keep track of the maximum count for all\\ndifferent elements. If the maximum count becomes greater than n/2, then break the loops and return\\nthe element having maximum count. If maximum count doesn’t become more than n/2, then the\\nmajority element doesn’t exist.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-55\\u2003\\u2003Can we improve Problem-54 time complexity to O(nlogn)?\\nSolution: Using binary search we can achieve this. Node of the Binary Search Tree (used in this\\napproach) will be as follows.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 578, 'file_type': 'pdf'}, page_content='Insert elements in BST one by one and if an element is already present then increment the count of\\nthe node. At any stage, if the count of a node becomes more than n/2, then return. This method\\nworks well for the cases where n/2 +1 occurrences of the majority element are present at the start\\nof the array, for example {1,1,1,1,1,2,3, and 4}.\\nTime Complexity: If a binary search tree is used then worst time complexity will be O(n2). If a\\nbalanced-binary-search tree is used then O(nlogn). Space Complexity: O(n).\\nProblem-56\\u2003\\u2003Is there any other of achieving O(nlogn) complexity for Problem-54?\\nSolution: Sort the input array and scan the sorted array to find the majority element.\\nTime Complexity: O(nlogn). Space Complexity: O(1).\\nProblem-57\\u2003\\u2003Can we improve the complexity for Problem-54?\\nSolution: If an element occurs more than n/2 times in A then it must be the median of A. But, the\\nreverse is not true, so once the median is found, we must check to see how many times it occurs in\\nA. We can use linear selection which takes O(n) time (for algorithm, refer to Selection\\nAlgorithms chapter).\\nint CheckMajority(int A[], in n) {\\n1)\\nUse linear selection to find the median m of A.\\n2)\\nDo one more pass through A and count the number of occurrences of m.\\na.\\nIf m occurs more than n/2 times then return true;\\nb.\\nOtherwise return false.\\n}\\nProblem-58\\u2003\\u2003Is there any other way of solving Problem-54?\\nSolution: Since only one element is repeating, we can use a simple scan of the input array by\\nkeeping track of the count for the elements. If the count is 0, then we can assume that the element\\nvisited for the first time otherwise that the resultant element.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 579, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-59\\u2003\\u2003Given an array of 2n elements of which n elements are the same and the\\nremaining n elements are all different. Find the majority element.\\nSolution: The repeated elements will occupy half the array. No matter what arrangement it is,\\nonly one of the below will be true:\\n•\\nAll duplicate elements will be at a relative distance of 2 from each other. Ex:n, 1, n,\\n100, n, 54, n...\\n•\\nAt least two duplicate elements will be next to each other.\\nEx: n,n, 1,100, n, 54, n,....\\nn, 1,n,n,n,54,100...\\n1,100,54, n.n.n.n....\\nIn worst case, we will need two passes over the array:\\n•\\nFirst Pass: compare A[i] and A[i + 1]\\n•\\nSecond Pass: compare A[i] and A[i + 2]\\nSomething will match and that’s your element. This will cost O(n) in time and O(1) in space.\\nProblem-60\\u2003\\u2003Given an array with 2n + 1 integer elements, n elements appear twice in\\narbitrary places in the array and a single integer appears only once somewhere inside.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 580, 'file_type': 'pdf'}, page_content='Find the lonely integer with O(n) operations and O(1) extra memory.\\nSolution: Except for one element, all elements are repeated. We know that A XOR A = 0. Based\\non this if we XOR all the input elements then we get the remaining element.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-61\\u2003\\u2003Throwing eggs from an n-story building: Suppose we have an n story building\\nand a number of eggs. Also assume that an egg breaks if it is thrown from floor F or higher,\\nand will not break otherwise. Devise a strategy to determine floor F, while breaking\\nO(logn) eggs.\\nSolution: Refer to Divide and Conquer chapter.\\nProblem-62\\u2003\\u2003Local minimum of an array: Given an array A of n distinct integers, design an\\nO(logn) algorithm to find a local minimum: an index i such that A[i – 1] < A[i] < A[i + 1].\\nSolution: Check the middle value A[n/2], and two neighbors A[n/2 – 1] and A[n/2 + 1]. If A[n/2]\\nis local minimum, stop; otherwise search in half with smaller neighbor.\\nProblem-63\\u2003\\u2003Give an n × n array of elements such that each row is in ascending order and\\neach column is in ascending order, devise an O(n) algorithm to determine if a given\\nelement x is in the array. You may assume all elements in the n × n array are distinct.\\nSolution: Let us assume that the given matrix is A[n][n]. Start with the last row, first column [or\\nfirst row, last column]. If the element we are searching for is greater than the element at A[1][n],\\nthen the first column can be eliminated. If the search element is less than the element at A[1][n],\\nthen the last row can be completely eliminated. Once the first column or the last row is\\neliminated, start the process again with the left-bottom end of the remaining array. In this\\nalgorithm, there would be maximum n elements that the search element would be compared with.\\nTime Complexity: O(n). This is because we will traverse at most 2n points. Space Complexity:\\nO(1).\\nProblem-64\\u2003\\u2003Given an n × n array a of n2 numbers, give an O(n) algorithm to find a pair of\\nindices i and j such that A[i][j] < A[i + 1][j].A[i][j] < A[i][j + 1],A[i][j] < A[i – 1][j], and\\nA[i][j] < A[i][j – 1].\\nSolution: This problem is the same as Problem-63.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 581, 'file_type': 'pdf'}, page_content='Problem-65\\u2003\\u2003Given n × n matrix, and in each row all 1’s are followed by 0’s. Find the row\\nwith the maximum number of 0’s.\\nSolution: Start with first row, last column. If the element is 0 then move to the previous column in\\nthe same row and at the same time increase the counter to indicate the maximum number of 0’s. If\\nthe element is 1 then move to the next row in the the same column. Repeat this process until your\\nreach last row, first column.\\nTime Complexity: O(2n) ≈ O(n) (similar to Problem-63).\\nProblem-66\\u2003\\u2003Given an input array of size unknown, with all numbers in the beginning and\\nspecial symbols in the end. Find the index in the array from where the special symbols\\nstart.\\nSolution: Refer to Divide and Conquer chapter.\\nProblem-67\\u2003\\u2003Separate even and odd numbers: Given an array A[], write a function that\\nsegregates even and odd numbers. The functions should put all even numbers first, and then\\nodd numbers. Example: Input = {12,34,45,9,8,90,3} Output = {12,34,90,8,9,45,3}\\nNote: In the output, the order of numbers can be changed, i.e., in the above example 34 can\\ncome before 12, and 3 can come before 9.\\nSolution: The problem is very similar to Separate 0’s and 1’s (Problem-68) in an array, and both\\nproblems are variations of the famous Dutch national flag problem.\\nAlgorithm: The logic is similar to Quick sort.\\n1)\\nInitialize two index variables left and right: left = 0, right = n – 1\\n2)\\nKeep incrementing the left index until you see an odd number.\\n3)\\nKeep decrementing the right index until youe see an even number.\\n4)\\nIf left < right then swap A[left] and A[right]'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 582, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nProblem-68\\u2003\\u2003The following is another way of structuring Problem-67, but with a slight\\ndifference.\\nSeparate 0’s and 1’s in an array: We are given an array of 0’s and 1’s in random order.\\nSeparate 0’s on the left side and 1’s on the right side of the array. Traverse the array only\\nonce.\\nInput array = [0,1,0,1,0,0,1,1,1,0] Output array = [0,0,0,0,0,1,1,1,1,1]\\nSolution: Counting 0’s or 1’s\\n1.\\nCount the number of 0’s. Let the count be C.\\n2.\\nOnce we have the count, put C 0’s at the beginning and 1’s at the remaining n- C\\npositions in the array.\\nTime Complexity: O(n). This solution scans the array two times.\\nProblem-69\\u2003\\u2003Can we solve Problem-68 in one scan?\\nSolution: Yes. Use two indexes to traverse: Maintain two indexes. Initialize the first index left as\\n0 and the second index right as n – 1. Do the following while left < right:\\n1)\\nKeep the incrementing index left while there are Os in it\\n2)\\nKeep the decrementing index right while there are Is in it\\n3)\\nIf left < right then exchange A[left] and A[right]'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 583, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-70\\u2003\\u2003Sort an array of 0’s, 1’s and 2’s [or R’s, G’s and B’s]: Given an array A[]\\nconsisting of 0’s, 1’s and 2’s, give an algorithm for sorting A[].The algorithm should put all\\n0’s first, then all 1’s and finally all 2’s at the end. Example Input =\\n{0,1,1,0,1,2,1,2,0,0,0,1}, Output = {0,0,0,0,0,1,1,1,1,1,2,2}\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 584, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-71\\u2003\\u2003Maximum difference between two elements: Given an array A[] of integers,\\nfind out the difference between any two elements such that the larger element appears after\\nthe smaller number in A[].\\nExamples: If array is [2,3,10,6,4,8,1] then returned value should be 8 (Difference between\\n10 and 2). If array is [ 7,9,5,6,3,2 ] then the returned value should be 2 (Difference\\nbetween 7 and 9)\\nSolution: Refer to Divide and Conquer chapter.\\nProblem-72\\u2003\\u2003Given an array of 101 elements. Out of 101 elements, 25 elements are repeated\\ntwice, 12 elements are repeated 4 times, and one element is repeated 3 times. Find the\\nelement which repeated 3 times in O(1).\\nSolution: Before solving this problem, let us consider the following XOR operation property: a\\nXOR a = 0. That means, if we apply the XOR on the same elements then the result is 0.\\nAlgorithm:\\n•\\nXOR all the elements of the given array and assume the result is A.\\n•\\nAfter this operation, 2 occurrences of the number which appeared 3 times becomes 0\\nand one occurrence remains the same.\\n•\\nThe 12 elements that are appearing 4 times become 0.\\n•\\nThe 25 elements that are appearing 2 times become 0.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 585, 'file_type': 'pdf'}, page_content='•\\nSo just XOR’ing all the elements gives the result.\\nTime Complexity: O(n), because we are doing only one scan. Space Complexity: O(1).\\nProblem-73\\u2003\\u2003Given a number n, give an algorithm for finding the number of trailing zeros in\\nn!.\\nSolution:\\nTime Complexity: O(logn).\\nProblem-74\\u2003\\u2003Given an array of 2n integers in the following format a1 a2 a3 ...an b1 b2 b3\\n...bn. Shuffle the array to a1 b1 a2 b2 a3 b3 ... an bn without any extra memory.\\nSolution: A brute force solution involves two nested loops to rotate the elements in the second\\nhalf of the array to the left. The first loop runs n times to cover all elements in the second half of\\nthe array. The second loop rotates the elements to the left. Note that the start index in the second\\nloop depends on which element we are rotating and the end index depends on how many positions\\nwe need to move to the left.\\nTime Complexity: O(n2).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 586, 'file_type': 'pdf'}, page_content='Problem-75\\u2003\\u2003Can we improve Problem-74 solution?\\nSolution: Refer to the Divide and Conquer chapter. A better solution of time complexity\\nO(nlogn) can be achieved using the Divide and Concur technique. Let us look at an example\\n1.\\nStart with the array: a1 a2 a3 a4 b1 b2 b3 b4\\n2.\\nSplit the array into two halves: a1 a2 a3 a4 : b1 b2 b3 b4\\n3.\\nExchange elements around the center: exchange a3 a4 with b1 b2 and you get: a1 a.2\\nb1 b2 a3 a4 b3 b4\\n4.\\nSplit a1 a2 b1 b2 into a1 a2 : b1 b2. Then split a3 a4 b3 b4 into a3 a4 : b3 b4\\n5.\\nExchange elements around the center for each subarray you get: a1 b1 a2 b2 and a3\\nb3 a4 b4\\nNote that this solution only handles the case when n = 2i where i = 0,1,2,3, etc. In our example n\\n= 22 = 4 which makes it easy to recursively split the array into two halves. The basic idea behind\\nswapping elements around the center before calling the recursive function is to produce smaller\\nsize problems. A solution with linear time complexity may be achieved if the elements are of a\\nspecific nature. For example, if you can calculate the new position of the element using the value\\nof the element itself. This is nothing but a hashing technique.\\nProblem-76\\u2003\\u2003Given an array A[], find the maximum j – i such that A[j] > A[i]. For example,\\nInput: {34, 8, 10, 3, 2, 80, 30, 33, 1} and Output: 6 (j = 7, i = 1).\\nSolution: Brute Force Approach: Run two loops. In the outer loop, pick elements one by one\\nfrom the left. In the inner loop, compare the picked element with the elements starting from the\\nright side. Stop the inner loop when you see an element greater than the picked element and keep\\nupdating the maximum j – i so far.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-77\\u2003\\u2003Can we improve the complexity of Problem-76?\\nSolution: To solve this problem, we need to get two optimum indexes of A[]: left index i and'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 587, 'file_type': 'pdf'}, page_content='right index j. For an element A[i], we do not need to consider A[i] for the left index if there is an\\nelement smaller than A[i] on the left side of A[i]. Similarly, if there is a greater element on the\\nright side of A[j] then we do not need to consider this j for the right index.\\nSo we construct two auxiliary Arrays LeftMins[] and RightMaxs[] such that LeftMins[i] holds the\\nsmallest element on the left side of A[i] including A[i], and RightMaxs[j] holds the greatest\\nelement on the right side of A[j] including A[j]. After constructing these two auxiliary arrays, we\\ntraverse both these arrays from left to right.\\nWhile traversing LeftMins[] and RightMaxs[], if we see that LeftMins[i] is greater than\\nRightMaxs[j], then we must move ahead in LeftMins[] (or do i++) because all elements on the left\\nof LeftMins[i] are greater than or equal to LeftMins[i]. Otherwise we must move ahead in\\nRightMaxs[j] to look for a greater y – i value.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-78\\u2003\\u2003Given an array of elements, how do you check whether the list is pairwise\\nsorted or not? A list is considered pairwise sorted if each successive pair of numbers is in\\nsorted (non-decreasing) order.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 588, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-79\\u2003\\u2003Given an array of n elements, how do you print the frequencies of elements\\nwithout using extra space. Assume all elements are positive, editable and less than n.\\nSolution: Use negation technique.\\nArray should have numbers in the range [1, n] (where n is the size of the array). The if condition'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 589, 'file_type': 'pdf'}, page_content='(A[pos] > 0 && A[expectedPos] > 0) means that both the numbers at indices pos and\\nexpectedPos are actual numbers in the array but not their frequencies. So we will swap them so\\nthat the number at the index pos will go to the position where it should have been if the numbers\\n1, 2, 3, ...., n are kept in 0, 1, 2, ..., n – 1 indices. In the above example input array, initially pos =\\n0, so 10 at index 0 will go to index 9 after the swap. As this is the first occurrence of 10, make it\\nto -1. Note that we are storing the frequencies as negative numbers to differentiate between actual\\nnumbers and frequencies.\\nThe else if condition (A[pos] > 0) means A[pos] is a number and A[expectedPos] is its frequency\\nwithout including the occurrence of A[pos]. So increment the frequency by 1 (that is decrement by\\n1 in terms of negative numbers). As we count its occurrence we need to move to next pos, so pos\\n+ +, but before moving to that next position we should make the frequency of the number pos + 1\\nwhich corresponds to index pos of zero, since such a number has not yet occurred.\\nThe final else part means the current index pos already has the frequency of the number pos + 1,\\nso move to the next pos, hence pos + +.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-80\\u2003\\u2003Which is faster and by how much, a linear search of only 1000 elements on a 5-\\nGHz computer or a binary search of 1 million elements on a 1-GHz computer. Assume that\\nthe execution of each instruction on the 5-GHz computer is five times faster than on the 1-\\nGHz computer and that each iteration of the linear search algorithm is twice as fast as each\\niteration of the binary search algorithm.\\nSolution: A binary search of 1 million elements would require \\n or about 20\\niterations at most (i.e., worst case). A linear search of 1000 elements would require 500\\niretations on the average (i.e., going halfway through the array). Therefore, binary search would\\nbe \\n faster (in terms of iterations) than linear search. However, since linear search\\niterations are twice as fast, binary search would be \\n or about 12 times faster than linear search\\noverall, on the same machine. Since we run them on different machines, where an instruction on\\nthe 5-GhZ machine is 5 times faster than an instruction on a 1-GHz machine, binary search would\\nbe \\n or about 2 times faster than linear search! The key idea is that software improvements can\\nmake an algorithm run much faster without having to use more powerful software.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 590, 'file_type': 'pdf'}, page_content='12.1 What are Selection Algorithms?\\nSelection algorithm is an algorithm for finding the kth smallest/largest number in a list (also\\ncalled as kth order statistic). This includes finding the minimum, maximum, and median elements.\\nFor finding the kth order statistic, there are multiple solutions which provide different\\ncomplexities, and in this chapter we will enumerate those possibilities.\\n12.2 Selection by Sorting\\nA selection problem can be converted to a sorting problem. In this method, we first sort the input\\nelements and then get the desired element. It is efficient if we want to perform many selections.\\nFor example, let us say we want to get the minimum element. After sorting the input elements we\\ncan simply return the first element (assuming the array is sorted in ascending order). Now, if we\\nwant to find the second smallest element, we can simply return the second element from the sorted\\nlist.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 591, 'file_type': 'pdf'}, page_content='That means, for the second smallest element we are not performing the sorting again. The same is\\nalso the case with subsequent queries. Even if we want to get kth smallest element, just one scan\\nof the sorted list is enough to find the element (or we can return the kth-indexed value if the\\nelements are in the array).\\nFrom the above discussion what we can say is, with the initial sorting we can answer any query\\nin one scan, O(n). In general, this method requires O(nlogn) time (for sorting), where n is the\\nlength of the input list. Suppose we are performing n queries, then the average cost per operation\\nis just \\n. This kind of analysis is called amortized analysis.\\n12.3 Partition-based Selection Algorithm\\nFor the algorithm check Problem-6. This algorithm is similar to Quick sort.\\n12.4 Linear Selection Algorithm - Median of Medians Algorithm\\nWorst-case performance\\nO(n)\\nBest-case performance\\nO(n)\\nWorst-case space complexity\\nO(1) auxiliary\\nRefer to Problem-11.\\n12.5 Finding the K Smallest Elements in Sorted Order\\nFor the algorithm check Problem-6. This algorithm is similar to Quick sort.\\n12.6 Selection Algorithms: Problems & Solutions\\nProblem-1\\u2003\\u2003Find the largest element in an array A of size n.\\nSolution: Scan the complete array and return the largest element.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 592, 'file_type': 'pdf'}, page_content='Time Complexity - O(n). Space Complexity - O(1).\\nNote: Any deterministic algorithm that can find the largest of n keys by comparison of keys takes\\nat least n -1 comparisons.\\nProblem-2\\u2003\\u2003Find the smallest and largest elements in an array A of size n.\\nSolution:\\nTime Complexity - O(n). Space Complexity - O(1). The worst-case number of comparisons is 2(n\\n– 1).\\nProblem-3\\u2003\\u2003Can we improve the previous algorithms?\\nSolution: Yes. We can do this by comparing in pairs.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 593, 'file_type': 'pdf'}, page_content='Time Complexity - O(n). Space Complexity - O(1).\\nNumber of comparisons: \\nSummary:\\nStraightforward comparison – 2(n – 1) comparisons\\nCompare for min only if comparison for max fails\\nBest case: increasing order – n – 1 comparisons\\nWorst case: decreasing order – 2(n – 1) comparisons\\nAverage case: 3n/2 – 1 comparisons\\nNote: For divide and conquer techniques refer to Divide and Conquer chapter.\\nProblem-4\\u2003\\u2003Give an algorithm for finding the second largest element in the given input list of\\nelements.\\nSolution: Brute Force Method'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 594, 'file_type': 'pdf'}, page_content='Algorithm:\\n•\\nFind largest element: needs n – 1 comparisons\\n•\\nDelete (discard) the largest element\\n•\\nAgain find largest element: needs n – 2 comparisons\\nTotal number of comparisons: n – 1 + n – 2 = 2n – 3\\nProblem-5\\u2003\\u2003Can we reduce the number of comparisons in Problem-4 solution?\\nSolution: The Tournament method: For simplicity, assume that the numbers are distinct and that\\nn is a power of 2. We pair the keys and compare the pairs in rounds until only one round remains.\\nIf the input has eight keys, there are four comparisons in the first round, two in the second, and\\none in the last. The winner of the last round is the largest key. The figure below shows the\\nmethod.\\nThe tournament method directly applies only when n is a power of 2. When this is not the case,\\nwe can add enough items to the end of the array to make the array size a power of 2. If the tree is\\ncomplete then the maximum height of the tree is logn. If we construct the complete binary tree, we\\nneed n – 1 comparisons to find the largest. The second largest key has to be among the ones that\\nwere lost in a comparison with the largest one. That means, the second largest element should be\\none of the opponents of the largest element. The number of keys that are lost to the largest key is\\nthe height of the tree, i.e. logn [if the tree is a complete binary tree]. Then using the selection\\nalgorithm to find the largest among them, take logn – 1 comparisons. Thus the total number of\\ncomparisons to find the largest and second largest keys is n + logn – 2.\\nProblem-6\\u2003\\u2003Find the k-smallest elements in an array S of n elements using partitioning\\nmethod.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 595, 'file_type': 'pdf'}, page_content='Solution: Brute Force Approach: Scan through the numbers k times to have the desired element.\\nThis method is the one used in bubble sort (and selection sort), every time we find out the\\nsmallest element in the whole sequence by comparing every element. In this method, the sequence\\nhas to be traversed k times. So the complexity is O(n × k).\\nProblem-7\\u2003\\u2003Can we use the sorting technique for solving Problem-6?\\nSolution: Yes. Sort and take the first k elements.\\n1.\\nSort the numbers.\\n2.\\nPick the first k elements.\\nThe time complexity calculation is trivial. Sorting of n numbers is of O(nlogn) and picking k\\nelements is of O(k). The total complexity is O(nlogn + k) = O(nlogn).\\nProblem-8\\u2003\\u2003Can we use the tree sorting technique for solving Problem-6?\\nSolution: Yes.\\n1.\\nInsert all the elements in a binary search tree.\\n2.\\nDo an InOrder traversal and print k elements which will be the smallest ones. So, we\\nhave the k smallest elements.\\nThe cost of creation of a binary search tree with n elements is O(nlogn) and the traversal up to k\\nelements is O(k). Hence the complexity is O(nlogn + k) = O(nlogn).\\nDisadvantage: If the numbers are sorted in descending order, we will be getting a tree which\\nwill be skewed towards the left. In that case, the construction of the tree will be 0 + l + 2 + ... +\\n(n– 1) \\n which is O(n2). To escape from this, we can keep the tree balanced, so that the\\ncost of constructing the tree will be only nlogn.\\nProblem-9\\u2003\\u2003Can we improve the tree sorting technique for solving Problem-6?\\nSolution: Yes. Use a smaller tree to give the same result.\\n1.\\nTake the first k elements of the sequence to create a balanced tree of k nodes (this\\nwill cost klogk).\\n2.\\nTake the remaining numbers one by one, and\\na.\\nIf the number is larger than the largest element of the tree, return.\\nb.\\nIf the number is smaller than the largest element of the tree, remove the\\nlargest element of the tree and add the new element. This step is to\\nmake sure that a smaller element replaces a larger element from the\\ntree. And of course the cost of this operation is logk since the tree is a\\nbalanced tree of k elements.\\nOnce Step 2 is over, the balanced tree with k elements will have the smallest k elements. The only'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 596, 'file_type': 'pdf'}, page_content='remaining task is to print out the largest element of the tree.\\nTime Complexity:\\n1.\\nFor the first k elements, we make the tree. Hence the cost is klogk.\\n2.\\nFor the rest n – k elements, the complexity is O(logk).\\nStep 2 has a complexity of (n – k) logk. The total cost is klogk + (n – k) logk = nlogk which is\\nO(nlogk). This bound is actually better than the ones provided earlier.\\nProblem-10\\u2003\\u2003Can we use the partitioning technique for solving Problem-6?\\nSolution: Yes.\\nAlgorithm\\n1.\\nChoose a pivot from the array.\\n2.\\nPartition the array so that: A[low...pivotpoint – 1] <= pivotpoint <= A[pivotpoint +\\n1..high].\\n3.\\nif k < pivotpoint then it must be on the left of the pivot, so do the same method\\nrecursively on the left part.\\n4.\\nif k = pivotpoint then it must be the pivot and print all the elements from low to\\npivotpoint.\\n5.\\nif k > pivotpoint then it must be on the right of pivot, so do the same method\\nrecursively on the right part.\\nThe top-level call would be kthSmallest = Selection(1, n, k).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 597, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2) in worst case as similar to Quicksort. Although the worst case is the\\nsame as that of Quicksort, this performs much better on the average [O(nlogk) – Average case].\\nProblem-11\\u2003\\u2003Find the kth-smallest element in an array S of n elements in best possible way.\\nSolution: This problem is similar to Problem-6 and all the solutions discussed for Problem-6 are\\nvalid for this problem. The only difference is that instead of printing all the k elements, we print\\nonly the kth element. We can improve the solution by using the median of medians algorithm.\\nMedian is a special case of the selection algorithm. The algorithm Selection(A, k) to find the kth\\nsmallest element from set A of n elements is as follows:\\nAlgorithm: Selection(A, k)\\n1.\\nPartition A into \\n groups, with each group having five items (the\\nlast group may have fewer items).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 598, 'file_type': 'pdf'}, page_content='2.\\nSort each group separately (e.g., insertion sort).\\n3.\\nFind the median of each of the  groups and store them in some array (let us say A′).\\n4.\\nUse Selection recursively to find the median of A′ (median of medians). Let us asay\\nthe median of medians is m.\\n5.\\nLet q = # elements of A smaller than m;\\n6.\\nIf(k == q + 1)\\n7.\\nElse partition A into X and Y\\n•\\nX = {items smaller than m)\\n•\\nY = {items larger than m}\\n8.\\nIf(k < q + 1)\\n9.\\nElse\\nBefore developing recurrence, let us consider the representation of the input below. In the figure,\\neach circle is an element and each column is grouped with 5 elements. The black circles indicate\\nthe median in each group of 5 elements. As discussed, sort each column using constant time\\ninsertion sort.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 599, 'file_type': 'pdf'}, page_content='oO 0 00 0\\n\\nMedians\\n\\noO 0 00 0\\n\\neo @ @ O+-\\n\\n0oOo0 000\\n00000\\n\\nOO. 04: @<-O\\nO O<- GO O\\nO O<«-@-O O\\n——\\n\\nOO. Ot @e-O _O\\na\\n\\nO, Or @-0 0\\noO Mn WO O'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 600, 'file_type': 'pdf'}, page_content='In the figure above the gray circled item is the median of medians (let us call this m). It can be\\nseen that at least 1/2 of 5 element group medians ≤m. Also, these 1/2 of 5 element groups\\ncontribute 3 elements that are ≤ m except 2 groups [last group which may contain fewer than 5\\nelements, and other group which contains m]. Similarly, at least 1/2 of 5 element groups\\ncontribute 3 elements that are ≥ m as shown above. 1/2 of 5 element groups contribute 3 elements,\\nexcept \\n2 \\ngroups \\ngives: \\n. \\nThe \\nremaining \\nare \\n. Since \\n is greater than \\n we need to consider \\nfor worst.\\nComponents in recurrence:\\n•\\nIn our selection algorithm, we choose m, which is the median of medians, to be a pivot, and\\npartition A into two sets X and Y. We need to select the set which gives maximum size (to\\nget the worst case).\\n•\\nThe time in function Selection when called from procedure partition. The number of keys\\nin the input to this call to Selection is .\\n•\\nThe number of comparisons required to partition the array. This number is length(S), let us\\nsay n.\\nWe \\nhave \\nestablished \\nthe \\nfollowing \\nrecurrence:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 601, 'file_type': 'pdf'}, page_content='From the above discussion we have seen that, if we select median of medians m as pivot, the\\npartition sizes are: \\n and \\n. If we select the maximum of these, then we get:\\nProblem-12\\u2003\\u2003In Problem-11, we divided the input array into groups of 5 elements. The\\nconstant 5 play an important part in the analysis. Can we divide in groups of 3 which work\\nin linear time?\\nSolution: In this case the modification causes the routine to take more than linear time. In the\\nworst case, at least half of the \\n medians found in the grouping step are greater than the\\nmedian of medians m, but two of those groups contribute less than two elements larger than m. So\\nas an upper bound, the number of elements larger than the pivotpoint is at least:\\nLikewise this is a lower bound. Thus up to \\n elements are fed into the\\nrecursive call to Select. The recursive step that finds the median of medians runs on a problem of\\nsize \\n, and consequently the time recurrence is:\\nAssuming \\nthat \\nT(n) \\nis \\nmonotonically \\nincreasing, \\nwe \\nmay \\nconclude \\nthat \\n, and we can say the upper bound for this as \\n, which is O(nlogn). Therefore, we cannot select 3 as the group\\nsize.\\nProblem-13\\u2003\\u2003As in Problem-12, can we use groups of size 7?\\nSolution: Following a similar reasoning, we once more modify the routine, now using groups of 7\\ninstead of 5. In the worst case, at least half the \\n medians found in the grouping step are\\ngreater than the median of medians m, but two of those groups contribute less than four elements\\nlarger than m. So as an upper bound, the number of elements larger than the pivotpoint is at least:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 602, 'file_type': 'pdf'}, page_content='Likewise this is a lower bound. Thus up to \\n elements are fed into the\\nrecursive call to Select. The recursive step that finds the median of medians runs on a problem of\\nsize \\n, and consequently the time recurrence is\\nThis is bounded above by (a + c) n provided that \\n. Therefore, we can select 7\\nas the group size.\\nProblem-14\\u2003\\u2003Given two arrays each containing n sorted elements, give an O(logn)-time\\nalgorithm to find the median of all 2n elements.\\nSolution: The simple solution to this problem is to merge the two lists and then take the average\\nof the middle two elements (note the union always contains an even number of values). But, the\\nmerge would be Θ(n), so that doesn’t satisfy the problem statement. To get logn complexity, let\\nmedianA and medianB be the medians of the respective lists (which can be easily found since\\nboth lists are sorted). If medianA == medianB, then that is the overall median of the union and we\\nare done. Otherwise, the median of the union must be between medianA and medianB. Suppose\\nthat medianA < medianB (the opposite case is entirely similar). Then we need to find the median\\nof the union of the following two sets:\\nSo, we can do this recursively by resetting the boundaries of the two arrays. The algorithm tracks\\nboth arrays (which are sorted) using two indices. These indices are used to access and compare\\nthe median of both arrays to find where the overall median lies.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 603, 'file_type': 'pdf'}, page_content='Time Complexity: O(logn), since we are reducing the problem size by half every time.\\nProblem-15\\u2003\\u2003Let A and B be two sorted arrays of n elements each. We can easily find the kth\\nsmallest element in A in O(1) time by just outputting A[k]. Similarly, we can easily find the\\nkth smallest element in B. Give an O(logk) time algorithm to find the kth smallest element\\noverall {i.e., the kth smallest in the union of A and B.\\nSolution: It’s just another way of asking Problem-14.\\nProblem-16\\u2003\\u2003Find the k smallest elements in sorted order: Given a set of n elements from a\\ntotally-ordered domain, find the k smallest elements, and list them in sorted order. Analyze\\nthe worst-case running time of the best implementation of the approach.\\nSolution: Sort the numbers, and list the k smallest.\\nT(n) = Time complexity of sort + listing k smallest elements = Θ(nlogn) + Θ(n) = Θ(nlogn).\\nProblem-17\\u2003\\u2003For Problem-16, if we follow the approach below, then what is the complexity?\\nSolution: Using the priority queue data structure from heap sort, construct a min-heap over the\\nset, and perform extract-min k times. Refer to the Priority Queues (Heaps) chapter for more\\ndetails.\\nProblem-18\\u2003\\u2003For Problem-16, if we follow the approach below then what is the complexity?\\nFind the kth-smallest element of the set, partition around this pivot element, and sort the k smallest\\nelements.\\nSolution:\\nT (n) = Time complexity of kth – smallest + Finding pivot + Sorting prefix\\n\\u2003\\u2003= Θ(n) + Θ(n) + Θ(klogk) = Θ(n + klogk)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 604, 'file_type': 'pdf'}, page_content='Since, k ≤ n, this approach is better than Problem-16 and Problem-17.\\nProblem-19\\u2003\\u2003Find k nearest neighbors to the median of n distinct numbers in O(n) time.\\nSolution: Let us assume that the array elements are sorted. Now find the median of n numbers and\\ncall its index as X (since array is sorted, median will be at \\n location). All we need to do is\\nselect k elements with the smallest absolute differences from the median, moving from X – 1 to 0,\\nand X + 1 to n – 1 when the median is at index m.\\nTime Complexity: Each step takes Θ(n). So the total time complexity of the algorithm is Θ(n).\\nProblem-20\\u2003\\u2003Is there any other way of solving Problem-19?\\nSolution: Assume for simplicity that n is odd and k is even. If set A is in sorted order, the median\\nis in position n/2 and the k numbers in A that are closest to the median are in positions (n – k)/2\\nthrough (n + k)/2.\\nWe first use linear time selection to find the (n – k)/2, n/2, and (n + k)/2 elements and then pass\\nthrough set A to find the numbers less than the (n + k)/2 element, greater than the (n – k)/2\\nelement, and not equal to the n/ 2 element. The algorithm takes O(n) time as we use linear time\\nselection exactly three times and traverse the n numbers in A once.\\nProblem-21\\u2003\\u2003Given (x,y) coordinates of n houses, where should you build a road parallel to\\nx-axis to minimize the construction cost of building driveways?\\nSolution: The road costs nothing to build. It is the driveways that cost money. The driveway cost\\nis proportional to its distance from the road. Obviously, they will be perpendicular. The solution\\nis to put the street at the median of the y coordinates.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 605, 'file_type': 'pdf'}, page_content='Problem-22\\u2003\\u2003Given a big file containing billions of numbers, find the maximum 10 numbers\\nfrom that file.\\nSolution: Refer to the Priority Queues chapter.\\nProblem-23\\u2003\\u2003Suppose there is a milk company. The company collects milk every day from all\\nits agents. The agents are located at different places. To collect the milk, what is the best\\nplace to start so that the least amount of total distance is travelled?\\nSolution: Starting at the median reduces the total distance travelled because it is the place which\\nis at the center of all the places.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 606, 'file_type': 'pdf'}, page_content='13.1 Introduction\\nSince childhood, we all have used a dictionary, and many of us have a word processor (say,\\nMicrosoft Word) which comes with a spell checker. The spell checker is also a dictionary but\\nlimited in scope. There are many real time examples for dictionaries and a few of them are:\\n•\\nSpell checker\\n•\\nThe data dictionary found in database management applications\\n•\\nSymbol tables generated by loaders, assemblers, and compilers\\n•\\nRouting tables in networking components (DNS lookup)\\nIn computer science, we generally use the term ‘symbol table’ rather than ‘dictionary’ when\\nreferring to the abstract data type (ADT).\\n13.2 What are Symbol Tables?\\nWe can define the symbol table as a data structure that associates a value with a key. It supports\\nthe following operations:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 607, 'file_type': 'pdf'}, page_content='•\\nSearch whether a particular name is in the table\\n•\\nGet the attributes of that name\\n•\\nModify the attributes of that name\\n•\\nInsert a new name and its attributes\\n•\\nDelete a name and its attributes\\nThere are only three basic operations on symbol tables: searching, inserting, and deleting.\\nExample: DNS lookup. Let us assume that the key in this case is the URL and the value is an IP\\naddress.\\n•\\nInsert URL with specified IP address\\n•\\nGiven URL, find corresponding IP address\\nKey[Website]\\nValue [IP Address]\\nwww.CareerMonks.com\\n128.112.136.11\\nwww.AuthorsInn.com\\n128.112.128.15\\nwww.AuthInn.com\\n130.132.143.21\\nwww.klm.com\\n128.103.060.55\\nwww.CareerMonk.com\\n209.052.165.60\\n13.3 Symbol Table Implementations\\nBefore implementing symbol tables, let us enumerate the possible implementations. Symbol tables\\ncan be implemented in many ways and some of them are listed below.\\nUnordered Array Implementation\\nWith this method, just maintaining an array is enough. It needs O(n) time for searching, insertion\\nand deletion in the worst case.\\nOrdered [Sorted] Array Implementation\\nIn this we maintain a sorted array of keys and values.\\n•\\nStore in sorted order by key\\n•\\nkeys[i] = ith largest key\\n•\\nvalues[i] = value associated with ith largest key'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 608, 'file_type': 'pdf'}, page_content='Since the elements are sorted and stored in arrays, we can use a simple binary search for finding\\nan element. It takes O(logn) time for searching and O(n) time for insertion and deletion in the\\nworst case.\\nUnordered Linked List Implementation\\nJust maintaining a linked list with two data values is enough for this method. It needs O(n) time\\nfor searching, insertion and deletion in the worst case.\\nOrdered Linked List Implementation\\nIn this method, while inserting the keys, maintain the order of keys in the linked list. Even if the\\nlist is sorted, in the worst case it needs O(n) time for searching, insertion and deletion.\\nBinary Search Trees Implementation\\nRefer to Trees chapter. The advantages of this method are: it does not need much code and it has a\\nfast search [O(logn) on average].\\nBalanced Binary Search Trees Implementation\\nRefer to Trees chapter. It is an extension of binary search trees implementation and takes O(logn)\\nin worst case for search, insert and delete operations.\\nTernary Search Implementation\\nRefer to String Algorithms chapter. This is one of the important methods used for implementing\\ndictionaries.\\nHashing Implementation\\nThis method is important. For a complete discussion, refer to the Hashing chapter.\\n13.4 Comparison Table of Symbols for Implementations\\nLet us consider the following comparison table for all the implementations.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 609, 'file_type': 'pdf'}, page_content='Notes:\\n•\\nIn the above table, n is the input size.\\n•\\nTable indicates the possible implementations discussed in this book. But, there could\\nbe other implementations.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 610, 'file_type': 'pdf'}, page_content='14.1 What is Hashing?\\nHashing is a technique used for storing and retrieving information as quickly as possible. It is\\nused to perform optimal searches and is useful in implementing symbol tables.\\n14.2 Why Hashing?\\nIn the Trees chapter we saw that balanced binary search trees support operations such as insert,\\ndelete and search in O(logn) time. In applications, if we need these operations in O(1), then\\nhashing provides a way. Remember that worst case complexity of hashing is still O(n), but it\\ngives O(1) on the average.\\n14.3 HashTable ADT\\nThe common operations for hash table are:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 611, 'file_type': 'pdf'}, page_content='•\\nCreatHashTable: Creates a new hash table\\n•\\nHashSearch: Searches the key in hash table\\n•\\nHashlnsert: Inserts a new key into hash table\\n•\\nHashDelete: Deletes a key from hash table\\n•\\nDeleteHashTable: Deletes the hash table\\n14.4 Understanding Hashing\\nIn simple terms we can treat array as a hash table. For understanding the use of hash tables, let us\\nconsider the following example: Give an algorithm for printing the first repeated character if\\nthere are duplicated elements in it. Let us think about the possible solutions. The simple and brute\\nforce way of solving is: given a string, for each character check whether that character is repeated\\nor not. The time complexity of this approach is O(n2) with O(1) space complexity.\\nNow, let us find a better solution for this problem. Since our objective is to find the first repeated\\ncharacter, what if we remember the previous characters in some array?\\nWe know that the number of possible characters is 256 (for simplicity assume ASCII characters\\nonly). Create an array of size 256 and initialize it with all zeros. For each of the input characters\\ngo to the corresponding position and increment its count. Since we are using arrays, it takes\\nconstant time for reaching any location. While scanning the input, if we get a character whose\\ncounter is already 1 then we can say that the character is the one which is repeating for the first\\ntime.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 612, 'file_type': 'pdf'}, page_content='Why not Arrays?\\nIn the previous problem, we have used an array of size 256 because we know the number of\\ndifferent possible characters [256] in advance. Now, let us consider a slight variant of the same\\nproblem. Suppose the given array has numbers instead of characters, then how do we solve the\\nproblem?\\nIn this case the set of possible values is infinity (or at least very big). Creating a huge array and\\nstoring the counters is not possible. That means there are a set of universal keys and limited\\nlocations in the memory. If we want to solve this problem we need to somehow map all these\\npossible keys to the possible memory locations. From the above discussion and diagram it can be\\nseen that we need a mapping of possible keys to one of the available locations. As a result using\\nsimple arrays is not the correct choice for solving the problems where the possible keys are very\\nbig. The process of mapping the keys to locations is called hashing.\\nNote: For now, do not worry about how the keys are mapped to locations. That depends on the\\nfunction used for conversions. One such simple function is key % table size.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 613, 'file_type': 'pdf'}, page_content='14.5 Components of Hashing\\nHashing has four key components:\\n1)\\nHash Table\\n2)\\nHash Functions\\n3)\\nCollisions\\n4)\\nCollision Resolution Techniques\\n14.6 Hash Table\\nHash table is a generalization of array. With an array, we store the element whose key is k at a\\nposition k of the array. That means, given a key k, we find the element whose key is k by just\\nlooking in the kth position of the array. This is called direct addressing.\\nDirect addressing is applicable when we can afford to allocate an array with one position for\\nevery possible key. But if we do not have enough space to allocate a location for each possible\\nkey, then we need a mechanism to handle this case. Another way of defining the scenario is: if we\\nhave less locations and more possible keys, then simple array implementation is not enough.\\nIn these cases one option is to use hash tables. Hash table or hash map is a data structure that\\nstores the keys and their associated values, and hash table uses a hash function to map keys to\\ntheir associated values. The general convention is that we use a hash table when the number of\\nkeys actually stored is small relative to the number of possible keys.\\n14.7 Hash Function\\nThe hash function is used to transform the key into the index. Ideally, the hash function should map\\neach possible key to a unique slot index, but it is difficult to achieve in practice.\\nGiven a collection of elements, a hash function that maps each item into a unique slot is referred\\nto as a perfect hash function. If we know the elements and the collection will never change, then\\nit is possible to construct a perfect hash function. Unfortunately, given an arbitrary collection of\\nelements, there is no systematic way to construct a perfect hash function. Luckily, we do not need\\nthe hash function to be perfect to still gain performance efficiency.\\nOne way to always have a perfect hash function is to increase the size of the hash table so that\\neach possible value in the element range can be accommodated. This guarantees that each element\\nwill have a unique slot. Although this is practical for small numbers of elements, it is not feasible\\nwhen the number of possible elements is large. For example, if the elements were nine-digit\\nSocial Security numbers, this method would require almost one billion slots. If we only want to\\nstore data for a class of 25 students, we will be wasting an enormous amount of memory.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 614, 'file_type': 'pdf'}, page_content='Our goal is to create a hash function that minimizes the number of collisions, is easy to compute,\\nand evenly distributes the elements in the hash table. There are a number of common ways to\\nextend the simple remainder method. We will consider a few of them here.\\nThe folding method for constructing hash functions begins by dividing the elements into equal-\\nsize pieces (the last piece may not be of equal size). These pieces are then added together to give\\nthe resulting hash value. For example, if our element was the phone number 436-555-4601, we\\nwould take the digits and divide them into groups of 2 (43,65,55,46,01). After the addition,\\n43+65+55+46+01, we get 210. If we assume our hash table has 11 slots, then we need to perform\\nthe extra step of dividing by 11 and keeping the remainder. In this case 210 % 11 is 1, so the\\nphone number 436-555-4601 hashes to slot 1. Some folding methods go one step further and\\nreverse every other piece before the addition. For the above example, we get\\n43+56+55+64+01=219 which gives 219 % 11 = 10.\\nHow to Choose Hash Function?\\nThe basic problems associated with the creation of hash tables are:\\n•\\nAn efficient hash function should be designed so that it distributes the index values\\nof inserted objects uniformly across the table.\\n•\\nAn efficient collision resolution algorithm should be designed so that it computes an\\nalternative index for a key whose hash index corresponds to a location previously\\ninserted in the hash table.\\n•\\nWe must choose a hash function which can be calculated quickly, returns values\\nwithin the range of locations in our table, and minimizes collisionsns.\\nCharacteristics of Good Hash Functions\\nA good hash function should have the following characteristics:\\n•\\nMinimize collision\\n•\\nBe easy and quick to compute\\n•\\nDistribute key values evenly in the hash table\\n•\\nUse all the information provided in the key\\n•\\nHave a high load factor for a given set of keys\\n14.8 Load Factor\\nThe load factor of a non-empty hash table is the number of items stored in the table divided by the\\nsize of the table. This is the decision parameter used when we want to rehash or expand the\\nexisting hash table entries. This also helps us in determining the efficiency of the hashing function.\\nThat means, it tells whether the hash function is distributing the keys uniformly or not.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 615, 'file_type': 'pdf'}, page_content='14.9 Collisions\\nHash functions are used to map each key to a different address space, but practically it is not\\npossible to create such a hash function and the problem is called collision. Collision is the\\ncondition where two records are stored in the same location.\\n14.10 Collision Resolution Techniques\\nThe process of finding an alternate location is called collision resolution. Even though hash\\ntables have collision problems, they are more efficient in many cases compared to all other data\\nstructures, like search trees. There are a number of collision resolution techniques, and the most\\npopular are direct chaining and open addressing.\\n•\\nDirect Chaining: An array of linked list application\\n○\\nSeparate chaining\\n•\\nOpen Addressing: Array-based implementation\\n○\\nLinear probing (linear search)\\n○\\nQuadratic probing (nonlinear search)\\n○\\nDouble hashing (use two hash functions)\\n14.11 Separate Chaining\\nCollision resolution by chaining combines linked representation with hash table. When two or\\nmore records hash to the same location, these records are constituted into a singly-linked list\\ncalled a chain.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 616, 'file_type': 'pdf'}, page_content='14.12 Open Addressing\\nIn open addressing all keys are stored in the hash table itself. This approach is also known as\\nclosed hashing. This procedure is based on probing. A collision is resolved by probing.\\nLinear Probing\\nThe interval between probes is fixed at 1. In linear probing, we search the hash table sequentially,\\nstarting from the original hash location. If a location is occupied, we check the next location. We\\nwrap around from the last table location to the first table location if necessary. The function for\\nrehashing is the following:\\nrehash(key) = (n + 1)% tablesize\\nOne of the problems with linear probing is that table items tend to cluster together in the hash\\ntable. This means that the table contains groups of consecutively occupied locations that are'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 617, 'file_type': 'pdf'}, page_content='called clustering.\\nClusters can get close to one another, and merge into a larger cluster. Thus, the one part of the\\ntable might be quite dense, even though another part has relatively few items. Clustering causes\\nlong probe searches and therefore decreases the overall efficiency.\\nThe next location to be probed is determined by the step-size, where other step-sizes (more than\\none) are possible. The step-size should be relatively prime to the table size, i.e. their greatest\\ncommon divisor should be equal to 1. If we choose the table size to be a prime number, then any\\nstep-size is relatively prime to the table size. Clustering cannot be avoided by larger step-sizes.\\nQuadratic Probing\\nThe interval between probes increases proportionally to the hash value (the interval thus\\nincreasing linearly, and the indices are described by a quadratic function). The problem of\\nClustering can be eliminated if we use the quadratic probing method.\\nIn quadratic probing, we start from the original hash location i. If a location is occupied, we\\ncheck the locations i + 12 , i +22, i + 32, i + 42... We wrap around from the last table location to\\nthe first table location if necessary. The function for rehashing is the following:\\nrehash(key) = (n + k2)% tablesize\\nExample: Let us assume that the table size is 11 (0..10)\\nHash Function: h(key) = key mod 11'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 618, 'file_type': 'pdf'}, page_content='Insert keys\\n31 mod 11 = 9\\n19 mod 11 = 8\\n2 mod 11 = 2\\n13 mod 11 = 2 → 2 + 12 = 3\\n25 mod 11 = 3 → 3 + 12=4\\n24 mod 11 = 2 → 2 + 12, 2 + 22 = 6\\n21 mod 11 = 10\\n9 mod 11 = 9 → 9 + 12, 9 + 22 mod 11, 9 + 32 mod 11=7\\nEven though clustering is avoided by quadratic probing, still there are chances of clustering.\\nClustering is caused by multiple search keys mapped to the same hash key. Thus, the probing\\nsequence for such search keys is prolonged by repeated conflicts along the probing sequence.\\nBoth linear and quadratic probing use a probing sequence that is independent of the search key.\\nDouble Hashing\\nThe interval between probes is computed by another hash function. Double hashing reduces\\nclustering in a better way. The increments for the probing sequence are computed by using a\\nsecond hash function. The second hash function h2 should be:\\nh2(key) ≠ 0 and h2 ≠ h1'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 619, 'file_type': 'pdf'}, page_content='We first probe the location h1(key). If the location is occupied, we probe the location h1(key) +\\nh2(key), h1(key) + 2 * h2(key), ...\\nExample:\\nTable size is 11 (0..10)\\nHash Function: assume h1(key) = key mod 11 and h2(key) = 7- (key mod 7)\\nInsert keys:\\n58 mod 11 = 3\\n14 mod 11 = 3 → 3 + 7 = 10\\n91 mod 11 = 3 → 3+ 7,3+ 2* 7 mod 11 = 6\\n25 mod 11 = 3 → 3 + 3,3 + 2*3 = 9\\n14.13 Comparison of Collision Resolution Techniques\\nComparisons: Linear Probing vs. Double Hashing\\nThe choice between linear probing and double hashing depends on the cost of computing the hash\\nfunction and on the load factor [number of elements per slot] of the table. Both use few probes but\\ndouble hashing take more time because it hashes to compare two hash functions for long keys.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 620, 'file_type': 'pdf'}, page_content='Comparisons: Open Addressing vs. Separate Chaining\\nIt is somewhat complicated because we have to account for the memory usage. Separate chaining\\nuses extra memory for links. Open addressing needs extra memory implicitly within the table to\\nterminate the probe sequence. Open-addressed hash tables cannot be used if the data does not\\nhave unique keys. An alternative is to use separate chained hash tables.\\nComparisons: Open Addressing methods\\n14.14 How Hashing Gets O(1) Complexity\\nFrom the previous discussion, one doubts how hashing gets O(1) if multiple elements map to the\\nsame location...\\nThe answer to this problem is simple. By using the load factor we make sure that each block (for\\nexample, linked list in separate chaining approach) on the average stores the maximum number of\\nelements less than the load factor. Also, in practice this load factor is a constant (generally, 10 or\\n20). As a result, searching in 20 elements or 10 elements becomes constant.\\nIf the average number of elements in a block is greater than the load factor, we rehash the\\nelements with a bigger hash table size. One thing we should remember is that we consider\\naverage occupancy (total number of elements in the hash table divided by table size) when\\ndeciding the rehash.\\nThe access time of the table depends on the load factor which in turn depends on the hash\\nfunction. This is because hash function distributes the elements to the hash table. For this reason,\\nwe say hash table gives O(1) complexity on average. Also, we generally use hash tables in cases'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 621, 'file_type': 'pdf'}, page_content='where searches are more than insertion and deletion operations.\\n14.15 Hashing Techniques\\nThere are two types of hashing techniques: static hashing and dynamic hashing\\nStatic Hashing\\nIf the data is fixed then static hashing is useful. In static hashing, the set of keys is kept fixed and\\ngiven in advance, and the number of primary pages in the directory are kept fixed.\\nDynamic Hashing\\nIf the data is not fixed, static hashing can give bad performance, in which case dynamic hashing is\\nthe alternative, in which case the set of keys can change dynamically.\\n14.16 Problems for which Hash Tables are not suitable\\n•\\nProblems for which data ordering is required\\n•\\nProblems having multidimensional data\\n•\\nPrefix searching, especially if the keys are long and of variable-lengths\\n•\\nProblems that have dynamic data\\n•\\nProblems in which the data does not have unique keys.\\n14.17 Bloom Filters\\nA Bloom filter is a probabilistic data structure which was designed to check whether an element\\nis present in a set with memory and time efficiency. It tells us that the element either definitely is\\nnot in the set or may be in the set. The base data structure of a Bloom filter is a Bit Vector. The\\nalgorithm was invented in 1970 by Burton Bloom and it relies on the use of a number of different\\nhash functions.\\nHow it works?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 622, 'file_type': 'pdf'}, page_content='Now that the bits in the bit vector have been set for Element1 and Element2;\\nwe can query the bloom filter to tell us if something has been seen before.\\nThe element is hashed but instead of setting the bits, this time a check is done\\nand if the bits that would have been set are already set the bloom filter will\\nreturn true that the element has been seen before.\\nA Bloom filter starts off with a bit array initialized to zero. To store a data value, we simply\\napply k different hash functions and treat the resulting k values as indices in the array, and we set\\neach of the k array elements to 1. We repeat this for every element that we encounter.\\nNow suppose an element turns up and we want to know if we have seen it before. What we do is\\napply the k hash functions and look up the indicated array elements. If any of them are 0 we can be\\n100% sure that we have never encountered the element before - if we had, the bit would have\\nbeen set to 1. However, even if all of them are one, we still can’t conclude that we have seen the\\nelement before because all of the bits could have been set by the k hash functions applied to\\nmultiple other elements. All we can conclude is that it is likely that we have encountered the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 623, 'file_type': 'pdf'}, page_content='element before.\\nNote that it is not possible to remove an element from a Bloom filter. The reason is simply that\\nwe can’t unset a bit that appears to belong to an element because it might also be set by another\\nelement.\\nIf the bit array is mostly empty, i.e., set to zero, and the k hash functions are independent of one\\nanother, then the probability of a false positive (i.e., concluding that we have seen a data item\\nwhen we actually haven’t) is low. For example, if there are only k bits set, we can conclude that\\nthe probability of a false positive is very close to zero as the only possibility of error is that we\\nentered a data item that produced the same k hash values - which is unlikely as long as the ‘has’\\nfunctions are independent.\\nAs the bit array fills up, the probability of a false positive slowly increases. Of course when the\\nbit array is full, every element queried is identified as having been seen before. So clearly we can\\ntrade space for accuracy as well as for time.\\nOne-time removal of an element from a Bloom filter can be simulated by having a second Bloom\\nfilter that contains elements that have been removed. However, false positives in the second filter\\nbecome false negatives in the composite filter, which may be undesirable. In this approach, re-\\nadding a previously removed item is not possible, as one would have to remove it from the\\nremoved filter.\\nSelecting hash functions\\nThe requirement of designing k different independent hash functions can be prohibitive for large\\nk. For a good hash function with a wide output, there should be little if any correlation between\\ndifferent bit-fields of such a hash, so this type of hash can be used to generate multiple different\\nhash functions by slicing its output into multiple bit fields. Alternatively, one can pass k different\\ninitial values (such as 0, 1, ..., k - 1) to a hash function that takes an initial value – or add (or\\nappend) these values to the key. For larger m and/or k, independence among the hash functions\\ncan be relaxed with negligible increase in the false positive rate.\\nSelecting size of bit vector\\nA Bloom filter with 1% error and an optimal value of k, in contrast, requires only about 9.6 bits\\nper element – regardless of the size of the elements. This advantage comes partly from its\\ncompactness, inherited from arrays, and partly from its probabilistic nature. The 1% false-\\npositive rate can be reduced by a factor of ten by adding only about 4.8 bits per element.\\nSpace Advantages'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 624, 'file_type': 'pdf'}, page_content='While risking false positives, Bloom filters have a strong space advantage over other data\\nstructures for representing sets, such as self-balancing binary search trees, tries, hash tables, or\\nsimple arrays or linked lists of the entries. Most of these require storing at least the data items\\nthemselves, which can require anywhere from a small number of bits, for small integers, to an\\narbitrary number of bits, such as for strings (tries are an exception, since they can share storage\\nbetween elements with equal prefixes). Linked structures incur an additional linear space\\noverhead for pointers.\\nHowever, if the number of potential values is small and many of them can be in the set, the Bloom\\nfilter is easily surpassed by the deterministic bit array, which requires only one bit for each\\npotential element.\\nTime Advantages\\nBloom filters also have the unusual property that the time needed either to add items or to check\\nwhether an item is in the set is a fixed constant, O(k), completely independent of the number of\\nitems already in the set. No other constant-space set data structure has this property, but the\\naverage access time of sparse hash tables can make them faster in practice than some Bloom\\nfilters. In a hardware implementation, however, the Bloom filter shines because its k lookups are\\nindependent and can be parallelized.\\nImplementation\\nRefer to Problems Section.\\n14.18 Hashing: Problems & Solutions\\nProblem-1\\u2003\\u2003Implement a separate chaining collision resolution technique. Also, discuss time\\ncomplexities of each function.\\nSolution: To create a hashtable of given size, say n, we allocate an array of n/L (whose value is\\nusually between 5 and 20) pointers to list, initialized to NULL. To perform Search/Insert/Delete\\noperations, we first compute the index of the table from the given key by using hashfunction and\\nthen do the corresponding operation in the linear list maintained at that location. To get uniform\\ndistribution of keys over a hashtable, maintain table size as the prime number.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 625, 'file_type': 'pdf'}, page_content='f#define LOAD_PACTOR 20\\nstruct ListNode {\\n\\nint key;\\n\\nint data;\\n\\nstruct ListNode *next;\\nk\\n‘struct HashTableNode {\\n\\nint beount; // Number of elements in block\\nstruct ListNode *next;\\n\\nk\\n\\nstruct HashTable {\\nint tsize;\\nint count; J [Number of elements in table\\n\\nstruct HashTableNode *Table;\\nr\\nstruct HashTable *CreatHashTable(nt size) {\\n\\nstruct HashTable *h;\\n\\nhh (struct HashTable *)mallocjsizeofistruct HashTable);\\n\\nitt)\\nreturn NULL:\\nhh-tsize = size/ LOAD FACTOR;\\n‘h-seount = 0;\\nTable = (struct HashTableNode *) malloc{sizef[struct HashTableNode 4) * h-tsie)\\n‘iffh—sTable) |\\nprintf Memory Error”);\\nreturn NULL:\\n\\n}\\n\\nfort i0:i< hai; +4)\\n‘h—Table{i]—-next = NULL;\\nTable —beount =O:\\n\\n’\\n\\nreturn hs\\n\\n)\\nint HashSearch(Struct HashTable “h, int data) {\\nstruct ListNode “temp;\\n\\nsh(data, h--tsize)]--next; {/Assume Hash isa buil\\n\\n)\\nint Hashinsert(Struct HashTable *h, int data) {\\n\\n‘nt index;\\n\\nstruct ListNode “temp, “newNode;\\n\\niffHashSearch(h, data))\\n\\nreturn 0;\\n\\nindex = Hashidata, h—tsize); //Assume Hash isa built-in function.\\n\\ntemp = h~Table(index|--next;\\n\\nnewNode = (struct ListNode *) malloc(sizeof(struct ListNode)};\\n\\niffinewNode) {\\n\\nprint{(‘Out of Space”);\\n\\n)\\nnnewNode--key = index;\\nnewNode—data = data;\\nnewNode—next = h—Tablefindex!—snext;'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 626, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 627, 'file_type': 'pdf'}, page_content='hTablfindex}~next = newNode;\\nh-+Tablefindex}~beount++;\\nh-countts;\\niffhcount | htsize > LOAD_FACTOR)\\nRehashfh);\\nreturn 1;\\n}\\nint HashDeleteStruct HashTable *, int data |\\nint index;\\nstruct ListNode ‘temp, “prev;\\nindex = Hash(data, h—tsize);\\nfor(temp = hTablelindex|-snext, prev = NULL; temp; prev = temp, temp = temp—next){\\nifemp-data == data) {\\nifprev!= NULL)\\nprev-mnext « temp—next;\\nfree(temp);\\nh-Tablefindex}beount--\\nhecount-;\\nreturn I;\\n)\\n}\\nretum 0;\\n)\\nvid RehashjStruct HashTable “) |\\nint oldsze, i, index;\\nstruct ListNode *p, * temp, “temp2;\\nstruct HashiTableNode *oldTabl;\\noldsize = hse;\\noldTable = h=Tat\\nhhtsize = htsize*2;\\nh-+Table = (struct HashTableNode *| mallocth-+tsize * sieofstruct HashTableNode *);\\niffh-Table)\\nprint] “Allocation Filed’);\\nreturn\\n\\n4\\nfori = 0; i< oldsiz; i+)\\nfor(temp = oldTablej—next; temp; temp = temp-next {\\nindex = Hash{temp—data, h—tsize);\\ntemp2 = temp; temp = temp-next;\\ntemp2-next = h-+Tabe(index}~next;\\nh-Tablelindex!~next = temp2;'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 628, 'file_type': 'pdf'}, page_content='CreatHashTable – O(n). HashSearch - O(1) average. Hashlnsert - O(1) average. HashDelete -\\nO(1) average.\\nProblem-2\\u2003\\u2003Given an array of characters, give an algorithm for removing the duplicates.\\nSolution: Start with the first character and check whether it appears in the remaining part of the\\nstring using a simple linear search. If it repeats, bring the last character to that position and\\ndecrement the size of the string by one. Continue this process for each distinct character of the\\ngiven string.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-3\\u2003\\u2003Can we find any other idea to solve this problem in better time than O(n2)?\\nObserve that the order of characters in solutions do not matter.\\nSolution: Use sorting to bring the repeated characters together. Finally scan through the array to\\nremove duplicates in consecutive positions.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 629, 'file_type': 'pdf'}, page_content='Time Complexity: Θ(nlogn). Space Complexity: O(1).\\nProblem-4\\u2003\\u2003Can we solve this problem in a single pass over given array?\\nSolution: We can use hash table to check whether a character is repeating in the given string or\\nnot. If the current character is not available in hash table, then insert it into hash table and keep\\nthat character in the given string also. If the current character exists in the hash table then skip that\\ncharacter.\\nTime Complexity: Θ(n) on average. Space Complexity: O(n).\\nProblem-5\\u2003\\u2003Given two arrays of unordered numbers, check whether both arrays have the same\\nset of numbers?\\nSolution: Let us assume that two given arrays are A and B. A simple solution to the given'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 630, 'file_type': 'pdf'}, page_content='problem is: for each element of A, check whether that element is in B or not. A problem arises\\nwith this approach if there are duplicates. For example consider the following inputs:\\nA = {2,5,6,8,10,2,2}\\nB = {2,5,5,8,10,5,6}\\nThe above algorithm gives the wrong result because for each element of A there is an element in\\nB also. But if we look at the number of occurrences, they are not the same. This problem we can\\nsolve by moving the elements which are already compared to the end of the list. That means, if we\\nfind an element in B, then we move that element to the end of B, and in the next searching we will\\nnot find those elements. But the disadvantage of this is it needs extra swaps. Time Complexity of\\nthis approach is O(n2), since for each element of A we have to scan B.\\nProblem-6\\u2003\\u2003Can we improve the time complexity of Problem-5?\\nSolution: Yes. To improve the time complexity, let us assume that we have sorted both the lists.\\nSince the sizes of both arrays are n, we need O(n log n) time for sorting them. After sorting, we\\njust need to scan both the arrays with two pointers and see whether they point to the same element\\nevery time, and keep moving the pointers until we reach the end of the arrays.\\nTime Complexity of this approach is O(n log n). This is because we need O(n log n) for sorting\\nthe arrays. After sorting, we need O(n) time for scanning but it is less compared to O(n log n).\\nProblem-7\\u2003\\u2003Can we further improve the time complexity of Problem-5?\\nSolution: Yes, by using a hash table. For this, consider the following algorithm.\\nAlgorithm:\\n•\\nConstruct the hash table with array A elements as keys.\\n•\\nWhile inserting the elements, keep track of the number frequency for each number.\\nThat means, if there are duplicates, then increment the counter of that corresponding\\nkey.\\n•\\nAfter constructing the hash table for A’s elements, now scan the array B.\\n•\\nFor each occurrence of B’s elements reduce the corresponding counter values.\\n•\\nAt the end, check whether all counters are zero or not.\\n•\\nIf all counters are zero, then both arrays are the same otherwise the arrays are\\ndifferent.\\nTime Complexity; O(n) for scanning the arrays. Space Complexity; O(n) for hash table.\\nProblem-8\\u2003\\u2003Given a list of number pairs; if pair(i,j) exists, and pair(j,i) exists, report all such\\npairs. For example, in {{1,3},{2,6},{3,5},{7,4},{5,3},{8,7}}, we see that {3,5} and\\n{5,3} are present. Report this pair when you encounter {5,3}. We call such pairs\\n‘symmetric pairs’. So, give an efficient algorithm for finding all such pairs.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 631, 'file_type': 'pdf'}, page_content='Solution: By using hashing, we can solve this problem in just one scan. Consider the following\\nalgorithm.\\nAlgorithm:\\n•\\nRead the pairs of elements one by one and insert them into the hash table. For each\\npair, consider the first element as key and the second element as value.\\n•\\nWhile inserting the elements, check if the hashing of the second element of the\\ncurrent pair is the same as the first number of the current pair.\\n•\\nIf they are the same, then that indicates a symmetric pair exits and output that pair.\\n•\\nOtherwise, insert that element into that. That means, use the first number of the\\ncurrent pair as key and the second number as value and insert them into the hash\\ntable.\\n•\\nBy the time we complete the scanning of all pairs, we have output all the symmetric\\npairs.\\nTime Complexity; O(n) for scanning the arrays. Note that we are doing a scan only of the input.\\nSpace Complexity; O(n) for hash table.\\nProblem-9\\u2003\\u2003Given a singly linked list, check whether it has a loop in it or not.\\nSolution: Using Hash Tables\\nAlgorithm:\\n•\\nTraverse the linked list nodes one by one.\\n•\\nCheck if the node’s address is there in the hash table or not.\\n•\\nIf it is already there in the hash table, that indicates we are visiting a node which\\nwas already visited. This is possible only if the given linked list has a loop in it.\\n•\\nIf the address of the node is not there in the hash table, then insert that node’s address\\ninto the hash table.\\n•\\nContinue this process until we reach the end of the linked list or we find the loop.\\nTime Complexity; O(n) for scanning the linked list. Note that we are doing a scan only of the\\ninput. Space Complexity; O(n) for hash table.\\nNote: for an efficient solution, refer to the Linked Lists chapter.\\nProblem-10\\u2003\\u2003Given an array of 101 elements. Out of them 50 elements are distinct, 24\\nelements are repeated 2 times, and one element is repeated 3 times. Find the element that is\\nrepeated 3 times in O(1).\\nSolution: Using Hash Tables\\nAlgorithm:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 632, 'file_type': 'pdf'}, page_content='•\\nScan the input array one by one.\\n•\\nCheck if the element is already there in the hash table or not.\\n•\\nIf it is already there in the hash table, increment its counter value [this indicates the\\nnumber of occurrences of the element].\\n•\\nIf the element is not there in the hash table, insert that node into the hash table with\\ncounter value 1.\\n•\\nContinue this process until reaching the end of the array.\\nTime Complexity: O(n), because we are doing two scans. Space Complexity: O(n), for hash\\ntable.\\nNote: For an efficient solution refer to the Searching chapter.\\nProblem-11\\u2003\\u2003Given m sets of integers that have n elements in them, provide an algorithm to\\nfind an element which appeared in the maximum number of sets?\\nSolution: Using Hash Tables\\nAlgorithm:\\n•\\nScan the input sets one by one.\\n•\\nFor each element keep track of the counter. The counter indicates the frequency of\\noccurrences in all the sets.\\n•\\nAfter completing the scan of all the sets, select the one which has the maximum\\ncounter value.\\nTime Complexity: O(mn), because we need to scan all the sets. Space Complexity: O(mn), for\\nhash table. Because, in the worst case all the elements may be different.\\nProblem-12\\u2003\\u2003Given two sets A and B, and a number K, Give an algorithm for finding whether\\nthere exists a pair of elements, one from A and one from B, that add up to K.\\nSolution: For simplicity, let us assume that the size of A is m and the size of B is n.\\nAlgorithm:\\n•\\nSelect the set which has minimum elements.\\n•\\nFor the selected set create a hash table. We can use both key and value as the same.\\n•\\nNow scan the second array and check whether (K-selected element) exists in the\\nhash table or not.\\n•\\nIf it exists then return the pair of elements.\\n•\\nOtherwise continue until we reach the end of the set.\\nTime Complexity: O(Max(m,n)), because we are doing two scans. Space Complexity:\\nO(Min(m,n)), for hash table. We can select the small set for creating the hash table.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 633, 'file_type': 'pdf'}, page_content='Problem-13\\u2003\\u2003Give an algorithm to remove the specified characters from a given string which\\nare given in another string?\\nSolution: For simplicity, let us assume that the maximum number of different characters is 256.\\nFirst we create an auxiliary array initialized to 0. Scan the characters to be removed, and for each\\nof those characters we set the value to 1, which indicates that we need to remove that character.\\nAfter initialization, scan the input string, and for each of the characters, we check whether that\\ncharacter needs to be deleted or not. If the flag is set then we simply skip to the next character,\\notherwise we keep the character in the input string. Continue this process until we reach the end\\nof the input string. All these operations we can do in-place as given below.\\nTime Complexity: Time for scanning the characters to be removed + Time for scanning the input\\narray= O(n) +O(m) ≈ O(n). Where m is the length of the characters to be removed and n is the\\nlength of the input string.\\nSpace Complexity: O(m), length of the characters to be removed. But since we are assuming the\\nmaximum number of different characters is 256, we can treat this as a constant. But we should\\nkeep in mind that when we are dealing with multi-byte characters, the total number of different\\ncharacters is much more than 256.\\nProblem-14\\u2003\\u2003Give an algorithm for finding the first non-repeated character in a string. For\\nexample, the first non-repeated character in the string “abzddab” is ‘z’.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 634, 'file_type': 'pdf'}, page_content='Solution: The solution to this problem is trivial. For each character in the given string, we can\\nscan the remaining string if that character appears in it. If it does not appears then we are done\\nwith the solution and we return that character. If the character appears in the remaining string, then\\ngo to the next character.\\nTime Complexity: O(n2), for two for loops. Space Complexity: O(1).\\nProblem-15\\u2003\\u2003Can we improve the time complexity of Problem-13?\\nSolution: Yes. By using hash tables we can reduce the time complexity. Create a hash table by\\nreading all the characters in the input string and keeping count of the number of times each\\ncharacter appears. After creating the hash table, we can read the hash table entries to see which\\nelement has a count equal to 1. This approach takes O(n) space but reduces the time complexity\\nalso to O(n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 635, 'file_type': 'pdf'}, page_content='Time Complexity; We have O(n) to create the hash table and another O(n) to read the entries of\\nhash table. So the total time is O(n) + O(n) = O(2n) ≈ O(n). Space Complexity: O(n) for keeping\\nthe count values.\\nProblem-16\\u2003\\u2003Given a string, give an algorithm for finding the first repeating letter in a string?\\nSolution: The solution to this problem is somewhat similar to Problem-13 and Problem-15. The\\nonly difference is, instead of scanning the hash table twice we can give the answer in just one\\nscan. This is because while inserting into the hash table we can see whether that element already\\nexists or not. If it already exists then we just need to return that character.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 636, 'file_type': 'pdf'}, page_content='Time Complexity: We have O(n) for scanning and creating the hash table. Note that we need only\\none scan for this problem. So the total time is O(n). Space Complexity: O(n) for keeping the count\\nvalues.\\nProblem-17\\u2003\\u2003Given an array of n numbers, create an algorithm which displays all pairs\\nwhose sum is S.\\nSolution: This problem is similar to Problem-12. But instead of using two sets we use only one\\nset.\\nAlgorithm:\\n•\\nScan the elements of the input array one by one and create a hash table. Both key and\\nvalue can be the same.\\n•\\nAfter creating the hash table, again scan the input array and check whether (S –\\nselected element) exits in the hash table or not.\\n•\\nIf it exits then return the pair of elements.\\n•\\nOtherwise continue and read all the elements of the array.\\nTime Complexity; We have O(n) to create the hash table and another O(n) to read the entries of\\nthe hash table. So the total time is O(n) + O(n) = O(2n) ≈ O(n). Space Complexity: O(n) for\\nkeeping the count values.\\nProblem-18\\u2003\\u2003Is there any other way of solving Problem-17?\\nSolution: Yes. The alternative solution to this problem involves sorting. First sort the input array.\\nAfter sorting, use two pointers, one at the starting and another at the ending. Each time add the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 637, 'file_type': 'pdf'}, page_content='values of both the indexes and see if their sum is equal to S. If they are equal then print that pair.\\nOtherwise increase the left pointer if the sum is less than S and decrease the right pointer if the\\nsum is greater than S.\\nTime Complexity: Time for sorting + Time for scanning = O(nlogn) + O(n) ≈ O(nlogn).\\nSpace Complexity: O(1).\\nProblem-19\\u2003\\u2003We have a file with millions of lines of data. Only two lines are identical; the\\nrest are unique. Each line is so long that it may not even fit in the memory. What is the most\\nefficient solution for finding the identical lines?\\nSolution: Since a complete line may not fit into the main memory, read the line partially and\\ncompute the hash from that partial line. Then read the next part of the line and compute the hash.\\nThis time use the previous hash also while computing the new hash value. Continue this process\\nuntil we find the hash for the complete line. Do this for each line and store all the hash values in a\\nfile [or maintain a hash table of these hashes]. If at any point you get same hash value, read the\\ncorresponding lines part by part and compare.\\nNote: Refer to Searching chapter for related problems.\\nProblem-20\\u2003\\u2003If h is the hashing function and is used to hash n keys into a table of size s, where\\nn <= s, the expected number of collisions involving a particular key X is :\\n(A)\\nless than 1.\\n(B)\\nless than n.\\n(C)\\nless than s.\\n(D)\\nless than .\\nSolution: A.\\nProblem-21\\u2003\\u2003Implement Bloom Filters\\nSolution: A Bloom Filter is a data structure designed to tell, rapidly and memory-efficiently,\\nwhether an element is present in a set. It is based on a probabilistic mechanism where false\\npositive retrieval results are possible, but false negatives are not. At the end we will see how to\\ntune the parameters in order to minimize the number of false positive results.\\nLet’s begin with a little bit of theory. The idea behind the Bloom filter is to allocate a bit vector\\nof length m, initially all set to 0, and then choose k independent hash functions, h1, h2, ..., hk, each\\nwith range [1..m]. When an element a is added to the set then the bits at positions h1(a), h2(a), ...,\\nhk(a) in the bit vector are set to 1. Given a query element q we can test whether it is in the set\\nusing the bits at positions h1(q), h2(q), ..., hk(q) in the vector. If any of these bits is 0 we report\\nthat q is not in the set otherwise we report that q is. The thing we have to care about is that in the\\nfirst case there remains some probability that q is not in the set which could lead us to a false\\npositive response.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 638, 'file_type': 'pdf'}, page_content=\"‘typedef unsigned int *hashFunctionPointerconst char *\\nstruct Bloom\\n“int bloomArraySize\\n{unsigned char “bloomArray,\\nint alfashFunctions:\\nhhashFunctonPointerTunesArray:\\n*\\n‘fdefine SETBLOOMBITIa,n faln/CHAR_BIT] |= (1<<{n%4CHAR_BIT))\\n‘define GETBLOOMBITIa n) (aln/CHAR-BIT] & (1<(u%%CHAR BIT)\\nstruct Bloom “eeateBloomfnt sie, int nllashFunctions,\\n‘struct Bloom “bm:\\nalist\\nInes\\nifl(b-mallo(sizeofstruct Bloor)\\nreturn NULL;\\n‘nbin--bloomArray-calloc(size: CHAR_BIT-1)/CHAR_BIT, sizefichar))\\nfree)\\nreturn NULL:\\n\\n)\\n‘ift(bIm—-funcsArray=(hashFunctionPointer+}mallocinHashPunctions*sizeofthashFunctionPointer) {\\nfreefblm—-bloomAtray);\\nfreefbim);\\nreturn NULL;\\n)\\nvva_startl, nashPunctions);\\nforin-0; nenHashPunctions; +4n) {\\n‘blm—-funesArray(n|=va_argl, hashFunctionPointer};\\n)\\nvaLendi);\\nbim_-nHashFunctions=nHashFunctions;\\nbim—bloomArraySize=size;\\nreturn Bim:\\n’\\nint deleteBloom(struct Bloom *blm\\n{reefblm-bloomArray},\\nfreefblm—funesArray);\\nfre(blm);\\nreturn 0:\\n’\\nint addBlementBloom(struct Bloom *bim, const chars),\\nfeel uO, ncblan-on HashFenctione\\n‘SETBLOOMBIT(bIm—bloomArray,\\n)\\n\\nreturn 0;\\n’\\nint checkElementBloom(struct Bloom *bim, const char *s)\\nforfint n=O; n<blm—snHashFunctions; +n) {\\n‘M{(GETBLOOMBIT\\\\bim—bloomArray, blm—funceArray[n\\\\(s)%sblm—bloomArraySize)) return 0;\\n\\n)\\nreturn 15\\n\\n}\\n\\n‘unsigned int shiftAddXORHashiconst char *key){\\n‘unsigned int h-o;\\nwhile key) h*(h<<S)}(h>>2}+(unsigned char}key'+;\\nreturn hi\\n\\n’\\n\\n‘unsigned int XORHash(const char *key},\\nUnsigned int h-O;\\nhhash-t h=0:\\nwhileikey) hYotkey++;\\nreturn hi\"), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 639, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 640, 'file_type': 'pdf'}, page_content='15.1 Introduction\\nTo understand the importance of string algorithms let us consider the case of entering the URL\\n(Uniform To understand the importance of string algorithms let us consider the case of entering the\\nURL (Uniform Resource Locator) in any browser (say, Internet Explorer, Firefox, or Google\\nChrome). You will observe that after typing the prefix of the URL, a list of all possible URLs is\\ndisplayed. That means, the browsers are doing some internal processing and giving us the list of\\nmatching URLs. This technique is sometimes called auto – completion.\\nSimilarly, consider the case of entering the directory name in the command line interface (in both\\nWindows and UNIX). After typing the prefix of the directory name, if we press the tab button, we\\nget a list of all matched directory names available. This is another example of auto completion.\\nIn order to support these kinds of operations, we need a data structure which stores the string data\\nefficiently. In this chapter, we will look at the data structures that are useful for implementing\\nstring algorithms.\\nWe start our discussion with the basic problem of strings: given a string, how do we search a'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 641, 'file_type': 'pdf'}, page_content='substring (pattern)? This is called a string matching problem. After discussing various string\\nmatching algorithms, we will look at different data structures for storing strings.\\n15.2 String Matching Algorithms\\nIn this section, we concentrate on checking whether a pattern P is a substring of another string T\\n(T stands for text) or not. Since we are trying to check a fixed string P, sometimes these\\nalgorithms are called exact string matching algorithms. To simplify our discussion, let us assume\\nthat the length of given text T is n and the length of the pattern P which we are trying to match has\\nthe length m. That means, T has the characters from 0 to n – 1 (T[0 ...n – 1]) and P has the\\ncharacters from 0 to m – 1 (T[0 ...m – 1]). This algorithm is implemented in C + + as strstr().\\nIn the subsequent sections, we start with the brute force method and gradually move towards\\nbetter algorithms.\\n•\\nBrute Force Method\\n•\\nRabin-Karp String Matching Algorithm\\n•\\nString Matching with Finite Automata\\n•\\nKMP Algorithm\\n•\\nBoyer-Moore Algorithm\\n•\\nSuffix Trees\\n15.3 Brute Force Method\\nIn this method, for each possible position in the text T we check whether the pattern P matches or\\nnot. Since the length of T is n, we have n – m + 1 possible choices for comparisons. This is\\nbecause we do not need to check the last m – 1 locations of T as the pattern length is m. The\\nfollowing algorithm searches for the first occurrence of a pattern string P in a text string T.\\nAlgorithm'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 642, 'file_type': 'pdf'}, page_content='Time Complexity: O((n – m + 1) × m) ≈ O(n × m). Space Complexity: O(1).\\n15.4 Rabin-Karp String Matching Algorithm\\nIn this method, we will use the hashing technique and instead of checking for each possible\\nposition in T, we check only if the hashing of P and the hashing of m characters of T give the same\\nresult.\\nInitially, apply the hash function to the first m characters of T and check whether this result and\\nP’s hashing result is the same or not. If they are not the same, then go to the next character of T and\\nagain apply the hash function to m characters (by starting at the second character). If they are the\\nsame then we compare those m characters of T with P.\\nSelecting Hash Function\\nAt each step, since we are finding the hash of m characters of T, we need an efficient hash\\nfunction. If the hash function takes O(m) complexity in every step, then the total complexity is O(n\\n× m). This is worse than the brute force method because first we are applying the hash function\\nand also comparing.\\nOur objective is to select a hash function which takes O(1) complexity for finding the hash of m\\ncharacters of T every time. Only then can we reduce the total complexity of the algorithm. If the\\nhash function is not good (worst case), the complexity of the Rabin-Karp algorithm is O(n – m +\\n1) × m) ≈ O(n × m). If we select a good hash function, the complexity of the Rabin-Karp\\nalgorithm complexity is O(m + n). Now let us see how to select a hash function which can\\ncompute the hash of m characters of T at each step in O(1).\\nFor simplicity, let’s assume that the characters used in string T are only integers. That means, all\\ncharacters in T ∈ {0,1,2,...,9 }. Since all of them are integers, we can view a string of m\\nconsecutive characters as decimal numbers. For example, string ′61815′ corresponds to the\\nnumber 61815. With the above assumption, the pattern P is also a decimal value, and let us\\nassume that the decimal value of P is p. For the given text T[0..n – 1], let t(i) denote the decimal\\nvalue of length–m substring T[i.. i + m – 1] for i = 0,1, ...,n – m– 1. So, t(i) == p if and only if\\nT[i..i + m – 1] == P[0..m – 1].\\nWe can compute p in O(m) time using Horner’s Rule as:\\nThe code for the above assumption is:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 643, 'file_type': 'pdf'}, page_content='We can compute all t(i), for i = 0,1,..., n – m – 1 values in a total of O(n) time. The value of t(0)\\ncan be similarly computed from T[0.. m – 1] in O(m) time. To compute the remaining values t(0),\\nt(1),..., t(n – m – 1), understand that t(i + 1) can be computed from t(i) in constant time.\\nFor example, if T = ″123456″ and m = 3\\nStep by Step explanation\\nFirst : remove the first digit : 123 – 100 * 1 = 23\\nSecond: Multiply by 10 to shift it : 23 * 10 = 230\\nThird: Add last digit : 230 + 4 = 234\\nThe algorithm runs by comparing, t(i) with p. When t(i) == p, then we have found the substring P\\nin T, starting from position i.\\n15.5 String Matching with Finite Automata\\nIn this method we use the finite automata which is the concept of the Theory of Computation\\n(ToC). Before looking at the algorithm, first let us look at the definition of finite automata.\\nFinite Automata\\nA finite automaton F is a 5-tuple (Q,q0,A,∑,δ), where\\n•\\nQ is a finite set of states\\n•\\nq0 ∈ Q is the start state\\n•\\nA ⊆ Q is a set of accepting states\\n•\\n∑ is a finite input alphabet\\n•\\nδ is the transition function that gives the next state for a given current state and input'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 644, 'file_type': 'pdf'}, page_content='How does Finite Automata Work?\\n•\\nThe finite automaton F begins in state q0\\n•\\nReads characters from ∑ one at a time\\n•\\nIf F is in state q and reads input character a, F moves to state δ(q,d)\\n•\\nAt the end, if its state is in A, then we say, F accepted the input string read so far\\n•\\nIf the input string is not accepted it is called the rejected string\\nExample: Let us assume that Q = {0,1{,q0 = 0,A = {1},∑ = {a, b}. δ(q,d) as shown in the\\ntransition table/diagram. This accepts strings that end in an odd number of a’s; e.g., abbaaa is\\naccepted, aa is rejected.\\nImportant Notes for Constructing the Finite Automata\\nFor building the automata, first we start with the initial state. The FA will be in state k if k\\ncharacters of the pattern have been matched. If the next text character is equal to the pattern\\ncharacter c, we have matched k + 1 characters and the FA enters state k + 1. If the next text\\ncharacter is not equal to the pattern character, then the FA go to a state 0,1,2,....or k, depending on\\nhow many initial pattern characters match the text characters ending with c.\\nMatching Algorithm\\nNow, let us concentrate on the matching algorithm.\\n•\\nFor a given pattern P[0.. m – 1], first we need to build a finite automaton F\\n○\\nThe state set is Q = {0,1,2, ...,m}\\n○\\nThe start state is 0\\n○\\nThe only accepting state is m\\n○\\nTime to build F can be large if ∑ is large\\n•\\nScan the text string T[0.. n – 1] to find all occurrences of the pattern P[0.. m – 1]'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 645, 'file_type': 'pdf'}, page_content='•\\nString matching is efficient: Θ(n)\\n○\\nEach character is examined exactly once\\n○\\nConstant time for each character\\n○\\nBut the time to compute δ (transition function) is O(m|∑|). This is\\nbecause δ has O(m|∑|) entries. If we assume |∑| is constant then the\\ncomplexity becomes O(m).\\nAlgorithm:\\nTime Complexity: O(m).\\n15.6 KMP Algorithm\\nAs before, let us assume that T is the string to be searched and P is the pattern to be matched. This\\nalgorithm was presented by Knuth, Morris and Pratt. It takes O(n) time complexity for searching a\\npattern. To get O(n) time complexity, it avoids the comparisons with elements of T that were\\npreviously involved in comparison with some element of the pattern P.\\nThe algorithm uses a table and in general we call it prefix function or prefix table or fail\\nfunction F. First we will see how to fill this table and later how to search for a pattern using this\\ntable. The prefix function F for a pattern stores the knowledge about how the pattern matches\\nagainst shifts of itself. This information can be used to avoid useless shifts of the pattern P. It\\nmeans that this table can be used for avoiding backtracking on the string T.\\nPrefix Table'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 646, 'file_type': 'pdf'}, page_content='As an example, assume that P = a b a b a c a. For this pattern, let us follow the step-by-step\\ninstructions for filling the prefix table F. Initially: m = length[P] = 7,F[0] = 0 and F[1] = 0.\\nStep 1: i = 1,j = 0,F[1] =0\\nStep 2: i = 2,j = 0,F[2] = 1\\nStep 3: i = 3,j = 1,F[3] =2\\nStep 4: i = 4,j = 2,F[4] =3'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 647, 'file_type': 'pdf'}, page_content='Step 5: i = 5,j = 3,F[5] = 1\\nStep 6: i = 6,j = 1,F[6] =1\\nAt this step the filling of the prefix table is complete.\\nMatching Algorithm\\nThe KMP algorithm takes pattern P, string T and prefix function F as input, and finds a match of P\\nin T.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 648, 'file_type': 'pdf'}, page_content='Time Complexity: O(m + n), where m is the length of the pattern and n is the length of the text to\\nbe searched. Space Complexity: O(m).\\nNow, to understand the process let us go through an example. Assume that T = b a c b a b a b a b\\na c a c a & P = a b a b a c a. Since we have already filled the prefix table, let us use it and go to\\nthe matching algorithm. Initially: n = size of T = 15; m = size of P = 7.\\nStep 1: i = 0, j = 0, comparing P[0] with T[0]. P[0] does not match with T[0]. P will be shifted\\none position to the right.\\nStep 2 :i = 1, j = 0, comparing P[0] with T[1]. P[0] matches with T[1]. Since there is a match, P\\nis not shifted.\\nStep 3: i = 2, j = 1, comparing P[1] with T[2]. P[1] does not match with T[2]. Backtracking on P,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 649, 'file_type': 'pdf'}, page_content='comparing P[0] and T[2].\\nStep 4: i = 3, j = 0, comparing P[0] with T[3]. P[0] does not match with T[3].\\nStep 5: i = 4, j = 0, comparing P[0] with T[4]. P[0] matches with T[4].\\nStep 6: i = 5, j = 1, comparing P[1] with T[5]. P[1] matches with T[5].\\nStep 7: i = 6, j = 2, comparing P[2] with T[6]. P[2] matches with T[6].\\nStep 8: i = 7, j = 3, comparing P[3] with T[7]. P[3] matches with T[7].\\nStep 9: i = 8, j = 4, comparing P[4] with T[8]. P[4] matches with T[8].'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 650, 'file_type': 'pdf'}, page_content='Step 10: i = 9, j = 5, comparing P[5] with T[9]. P[5] does not match with T[9]. Backtracking on\\nP, comparing P[4] with T[9] because after mismatch ; = F[4] = 3.\\nComparing P[3] with T[9].\\nStep 11: i = 10, j = 4, comparing P[4] with T[10]. P[4] matches with T[10].\\nStep 12: i = 11, j = 5, comparing P[5] with T[11]. P[5] matches with T[11].\\nStep 13: i = 12, j = 6, comparing P[6] with T[12]. P[6] matches with T[12].\\nPattern P has been found to completely occur in string T. The total number of shifts that took place\\nfor the match to be found are: i – m= 13 – 7 = 6 shifts.\\nNotes:\\n•\\nKMP performs the comparisons from left to right\\n•\\nKMP algorithm needs a preprocessing (prefix function) which takes O(m) space and\\ntime complexity\\n•\\nSearching takes O(n + m) time complexity (does not depend on alphabet size)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 651, 'file_type': 'pdf'}, page_content='15.7 Boyer-Moore Algorithm\\nLike the KMP algorithm, this also does some pre-processing and we call it last function. The\\nalgorithm scans the characters of the pattern from right to left beginning with the rightmost\\ncharacter. During the testing of a possible placement of pattern P in T, a mismatch is handled as\\nfollows: Let us assume that the current character being matched is T[i] = c and the corresponding\\npattern character is P[j]. If c is not contained anywhere in P, then shift the pattern P completely\\npast T[i]. Otherwise, shift P until an occurrence of character c in P gets aligned with T[i]. This\\ntechnique avoids needless comparisons by shifting the pattern relative to the text.\\nThe last function takes O(m + |∑|) time and the actual search takes O(nm) time. Therefore the\\nworst case running time of the Boyer-Moore algorithm is O(nm + |∑|). This indicates that the\\nworst-case running time is quadratic, in the case of n == m, the same as the brute force algorithm.\\n•\\nThe Boyer-Moore algorithm is very fast on the large alphabet (relative to the length\\nof the pattern).\\n•\\nFor the small alphabet, Boyer-Moore is not preferable.\\n•\\nFor binary strings, the KMP algorithm is recommended.\\n•\\nFor the very shortest patterns, the brute force algorithm is better.\\n15.8 Data Structures for Storing Strings\\nIf we have a set of strings (for example, all the words in the dictionary) and a word which we\\nwant to search in that set, in order to perform the search operation faster, we need an efficient\\nway of storing the strings. To store sets of strings we can use any of the following data structures.\\n•\\nHashing Tables\\n•\\nBinary Search Trees\\n•\\nTries\\n•\\nTernary Search Trees\\n15.9 Hash Tables for Strings\\nAs seen in the Hashing chapter, we can use hash tables for storing the integers or strings. In this\\ncase, the keys are nothing but the strings. The problem with hash table implementation is that we\\nlose the ordering information – after applying the hash function, we do not know where it will\\nmap to. As a result, some queries take more time. For example, to find all the words starting with\\nthe letter “K”, with hash table representation we need to scan the complete hash table. This is\\nbecause the hash function takes the complete key, performs hash on it, and we do not know the\\nlocation of each word.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 652, 'file_type': 'pdf'}, page_content='15.10 Binary Search Trees for Strings\\nIn this representation, every node is used for sorting the strings alphabetically. This is possible\\nbecause the strings have a natural ordering: A comes before B, which comes before C, and so on.\\nThis is because words can be ordered and we can use a Binary Search Tree (BST) to store and\\nretrieve them. For example, let us assume that we want to store the following strings using BSTs:\\nthis is a career monk string\\nFor the given string there are many ways of representing them in BST. One such possibility is\\nshown in the tree below.\\nIssues with Binary Search Tree Representation\\nThis method is good in terms of storage efficiency. But the disadvantage of this representation is\\nthat, at every node, the search operation performs the complete match of the given key with the\\nnode data, and as a result the time complexity of the search operation increases. So, from this we\\ncan say that BST representation of strings is good in terms of storage but not in terms of time.\\n15.11 Tries\\nNow, let us see the alternative representation that reduces the time complexity of the search\\noperation. The name trie is taken from the word re”trie”.\\nWhat is a Trie?\\nA trie is a tree and each node in it contains the number of pointers equal to the number of\\ncharacters of the alphabet. For example, if we assume that all the strings are formed with English\\nalphabet characters “a” to “z” then each node of the trie contains 26 pointers. A trie data\\nstructure can be declared as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 653, 'file_type': 'pdf'}, page_content='Suppose we want to store the strings “a”,”all”,”als”, and “as”“: trie for these strings will look\\nlike:\\nWhy Tries?\\nThe tries can insert and find strings in O(L) time (where L represents the length of a single word).\\nThis is much faster than hash table and binary search tree representations.\\nTrie Declaration\\nThe structure of the TrieNode has data (char), is_End_Of_String (boolean), and has a collection\\nof child nodes (Collection of TrieNodes). It also has one more method called subNode(char).\\nThis method takes a character as argument and will return the child node of that character type if\\nthat is present. The basic element - TrieNode of a TRIE data structure looks like this:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 654, 'file_type': 'pdf'}, page_content='Now that we have defined our TrieNode, let’s go ahead and look at the other operations of TRIE.\\nFortunately, the TRIE data structure is simple to implement since it has two major methods:\\ninsert() and search(). Let’s look at the elementary implementation of both these methods.\\nInserting a String in Trie\\nTo insert a string, we just need to start at the root node and follow the corresponding path (path\\nfrom root indicates the prefix of the given string). Once we reach the NULL pointer, we just need\\nto create a skew of tail nodes for the remaining characters of the given string.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 655, 'file_type': 'pdf'}, page_content='Time Complexity: O(L), where L is the length of the string to be inserted.\\nNote: For real dictionary implementation, we may need a few more checks such as checking\\nwhether the given string is already there in the dictionary or not.\\nSearching a String in Trie\\nThe same is the case with the search operation: we just need to start at the root and follow the\\npointers. The time complexity of the search operation is equal to the length of the given string that\\nwant to search.\\nTime Complexity: O(L), where L is the length of the string to be searched.\\nIssues with Tries Representation\\nThe main disadvantage of tries is that they need lot of memory for storing the strings. As we have\\nseen above, for each node we have too many node pointers. In many cases, the occupancy of each\\nnode is less. The final conclusion regarding tries data structure is that they are faster but require\\nhuge memory for storing the strings.\\nNote: There are some improved tries representations called trie compression techniques. But,\\neven with those techniques we can reduce the memory only at the leaves and not at the internal\\nnodes.\\n15.12 Ternary Search Trees\\nThis representation was initially provided by Jon Bentley and Sedgewick. A ternary search tree\\ntakes the advantages of binary search trees and tries. That means it combines the memory'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 656, 'file_type': 'pdf'}, page_content='efficiency of BSTs and the time efficiency of tries.\\nTernary Search Trees Declaration\\nThe Ternary Search Tree (TST) uses three pointers:\\n•\\nThe left pointer points to the TST containing all the strings which are alphabetically\\nless than data.\\n•\\nThe right pointer points to the TST containing all the strings which are\\nalphabetically greater than data.\\n•\\nThe eq pointer points to the TST containing all the strings which are alphabetically\\nequal to data. That means, if we want to search for a string, and if the current\\ncharacter of the input string and the data of current node in TST are the same, then\\nwe need to proceed to the next character in the input string and search it in the\\nsubtree which is pointed by eq.\\nInserting strings in Ternary Search Tree\\nFor simplicity let us assume that we want to store the following words in TST (also assume the\\nsame order): boats, boat, bat and bats. Initially, let us start with the boats string.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 657, 'file_type': 'pdf'}, page_content='Now if we want to insert the string boat, then the TST becomes [the only change is setting the\\nis_End_Of_String flag of “t” node to 1]:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 658, 'file_type': 'pdf'}, page_content='Set the is End_Of String flag\\ntol.\\n\\nNow, let us insert the next string: bat\\n\\nNULL\\n\\n‘o i?)\\nNULL | NULL\\n0\\nNULL | NULL\\nt’ 1\\nNULL NULL\\nb\\n‘s’ pi\\nNULL yu NULL\\nNULL'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 659, 'file_type': 'pdf'}, page_content='a\\n\\nNULL\\n‘a [0 ‘ [0\\nar\\nNULL NULL NULL NULL\\nv [i ‘a [0\\n\\\\\\nNULL yypp NULL NULL NULL\\nv 1\\nNULL NULL\\na\\nsf 1\\nv\\nNULL = yypp NULL\\n\\nNow, let us insert the final word: bats.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 660, 'file_type': 'pdf'}, page_content='Based on these examples, we can write the insertion algorithm as below. We will combine the\\ninsertion operation of BST and tries.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 661, 'file_type': 'pdf'}, page_content='Time Complexity: O(L), where L is the length of the string to be inserted.\\nSearching in Ternary Search Tree\\nIf after inserting the words we want to search for them, then we have to follow the same rules as\\nthat of binary search. The only difference is, in case of match we should check for the remaining\\ncharacters (in eq subtree) instead of return. Also, like BSTs we will see both recursive and non-\\nrecursive versions of the search method.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 662, 'file_type': 'pdf'}, page_content='Time Complexity: O(L), where L is the length of the string to be searched.\\nDisplaying All Words of Ternary Search Tree\\nIf we want to print all the strings of TST we can use the following algorithm. If we want to print\\nthem in sorted order, we need to follow the inorder traversal of TST.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 663, 'file_type': 'pdf'}, page_content='Finding the Length of the Largest Word in TST\\nThis is similar to finding the height of the BST and can be found as:\\n15.13 Comparing BSTs, Tries and TSTs\\n•\\nHash table and BST implementation stores complete the string at each node. As a\\nresult they take more time for searching. But they are memory efficient.\\n•\\nTSTs can grow and shrink dynamically but hash tables resize only based on load\\nfactor.\\n•\\nTSTs allow partial search whereas BSTs and hash tables do not support it.\\n•\\nTSTs can display the words in sorted order, but in hash tables we cannot get the\\nsorted order.\\n•\\nTries perform search operations very fast but they take huge memory for storing the\\nstring.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 664, 'file_type': 'pdf'}, page_content='•\\nTSTs combine the advantages of BSTs and Tries. That means they combine the\\nmemory efficiency of BSTs and the time efficiency of tries\\n15.14 Suffix Trees\\nSuffix trees are an important data structure for strings. With suffix trees we can answer the queries\\nvery fast. But this requires some preprocessing and construction of a suffix tree. Even though the\\nconstruction of a suffix tree is complicated, it solves many other string-related problems in linear\\ntime.\\nNote: Suffix trees use a tree (suffix tree) for one string, whereas Hash tables, BSTs, Tries and\\nTSTs store a set of strings. That means, a suffix tree answers the queries related to one string.\\nLet us see the terminology we use for this representation.\\nPrefix and Suffix\\nGiven a string T = T1T2 … Tn, the prefix of T is a string T1 ...Ti where i can take values from 1 to\\nn. For example, if T = banana, then the prefixes of T are: b, ba, ban, bana, banan, banana.\\nSimilarly, given a string T = T1T2 … Tn, the suffix of T is a string Ti ...Tn where i can take values\\nfrom n to 1. For example, if T = banana, then the suffixes of T are: a, na, ana, nana, anana,\\nbanana.\\nObservation\\nFrom the above example, we can easily see that for a given text T and pattern P, the exact string\\nmatching problem can also be defined as:\\n•\\nFind a suffix of T such that P is a prefix of this suffix or\\n•\\nFind a prefix of T such that P is a suffix of this prefix.\\nExample: Let the text to be searched be T = acebkkbac and the pattern be P = kkb. For this\\nexample, P is a prefix of the suffix kkbac and also a suffix of the prefix acebkkb.\\nWhat is a Suffix Tree?\\nIn simple terms, the suffix tree for text T is a Trie-like data structure that represents the suffixes of\\nT. The definition of suffix trees can be given as: A suffix tree for a n character string T[1 ...n] is a\\nrooted tree with the following properties.\\n•\\nA suffix tree will contain n leaves which are numbered from 1 to n'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 665, 'file_type': 'pdf'}, page_content='•\\nEach internal node (except root) should have at least 2 children\\n•\\nEach edge in a tree is labeled by a nonempty substring of T\\n•\\nNo two edges of a node (children edges) begin with the same character\\n•\\nThe paths from the root to the leaves represent all the suffixes of T\\nThe Construction of Suffix Trees\\nAlgorithm\\n1.\\nLet S be the set of all suffixes of T. Append $ to each of the suffixes.\\n2.\\nSort the suffixes in S based on their first character.\\n3.\\nFor each group Sc (c ∈ ∑):\\n(i) If Sc group has only one element, then create a leaf node.\\n(ii) Otherwise, find the longest common prefix of the suffixes in Sc group,\\ncreate an internal node, and recursively continue with Step 2, S being\\nthe set of remaining suffixes from Sc after splitting off the longest\\ncommon prefix.\\nFor better understanding, let us go through an example. Let the given text be T = tatat. For this\\nstring, give a number to each of the suffixes.\\nIndex\\nSuffix\\n1\\n$\\n2\\nt$\\n3\\nat$\\n4\\ntat$\\n5\\natat$\\n6\\ntatat$\\nNow, sort the suffixes based on their initial characters.\\nIn the three groups, the first group has only one element. So, as per the algorithm, create a leaf'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 666, 'file_type': 'pdf'}, page_content='node for it, as shown below.\\nNow, for S2 and S3 (as they have more than one element), let us find the longest prefix in the\\ngroup, and the result is shown below.\\nFor S2 and S3, create internal nodes, and the edge contains the longest common prefix of those\\ngroups.\\nNow we have to remove the longest common prefix from the S2 and S3 group elements.\\nOut next step is solving S2 and S3 recursively. First let us take S2. In this group, if we sort them\\nbased on their first character, it is easy to see that the first group contains only one element $, and\\nthe second group also contains only one element, at$. Since both groups have only one element,\\nwe can directly create leaf nodes for them.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 667, 'file_type': 'pdf'}, page_content='At this step, both S1 and S2 elements are done and the only remaining group is S3. As similar to\\nearlier steps, in the S3 group, if we sort them based on their first character, it is easy to see that\\nthere is only one element in the first group and it is $. For S3 remaining elements, remove the\\nlongest common prefix.\\nIn the S3 second group, there are two elements: $ and at$. We can directly add the leaf nodes for\\nthe first group element $. Let us add S3 subtree as shown below.\\nNow, S3 contains two elements. If we sort them based on their first character, it is easy to see that\\nthere are only two elements and among them one is $ and other is at$. We can directly add the\\nleaf nodes for them. Let us add S3 subtree as shown below.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 668, 'file_type': 'pdf'}, page_content='Since there are no more elements, this is the completion of the construction of the suffix tree for\\nstring T = tatat. The time-complexity of the construction of a suffix tree using the above algorithm\\nis O(n2) where n is the length of the input string because there are n distinct suffixes. The longest\\nhas length n, the second longest has length n – 1, and so on.\\nNote:\\n•\\nThere are O(n) algorithms for constructing suffix trees.\\n•\\nTo improve the complexity, we can use indices instead of string for branches.\\nApplications of Suffix Trees\\nAll the problems below (but not limited to these) on strings can be solved with suffix trees very\\nefficiently (for algorithms refer to Problems section).\\n•\\nExact String Matching: Given a text T and a pattern P, how do we check whether P\\nappears in T or not?\\n•\\nLongest Repeated Substring: Given a text T how do we find the substring of T that\\nis the maximum repeated substring?\\n•\\nLongest Palindrome: Given a text T how do we find the substring of T that is the\\nlongest palindrome of T?\\n•\\nLongest Common Substring: Given two strings, how do we find the longest\\ncommon substring?\\n•\\nLongest Common Prefix: Given two strings X[i ...n] and Y[j ...m],how do we find\\nthe longest common prefix?\\n•\\nHow do we search for a regular expression in given text T?\\n•\\nGiven a text T and a pattern P, how do we find the first occurrence of P in T?\\n15.15 String Algorithms: Problems & Solutions'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 669, 'file_type': 'pdf'}, page_content='Problem-1\\u2003\\u2003Given a paragraph of words, give an algorithm for finding the word which\\nappears the maximum number of times. If the paragraph is scrolled down (some words\\ndisappear from the first frame, some words still appear, and some are new words), give\\nthe maximum occurring word. Thus, it should be dynamic.\\nSolution: For this problem we can use a combination of priority queues and tries. We start by\\ncreating a trie in which we insert a word as it appears, and at every leaf of trie. Its node contains\\nthat word along with a pointer that points to the node in the heap [priority queue] which we also\\ncreate. This heap contains nodes whose structure contains a counter. This is its frequency and\\nalso a pointer to that leaf of trie, which contains that word so that there is no need to store the\\nword twice.\\nWhenever a new word comes up, we find it in trie. If it is already there, we increase the\\nfrequency of that node in the heap corresponding to that word, and we call it heapify. This is done\\nso that at any point of time we can get the word of maximum frequency. While scrolling, when a\\nword goes out of scope, we decrement the counter in heap. If the new frequency is still greater\\nthan zero, heapify the heap to incorporate the modification. If the new frequency is zero, delete the\\nnode from heap and delete it from trie.\\nProblem-2\\u2003\\u2003Given two strings, how can we find the longest common substring?\\nSolution: Let us assume that the given two strings are T1 and T2. The longest common substring of\\ntwo strings, T1 and T2, can be found by building a generalized suffix tree for T1 and T2. That\\nmeans we need to build a single suffix tree for both the strings. Each node is marked to indicate if\\nit represents a suffix of T1 or T2 or both. This indicates that we need to use different marker\\nsymbols for both the strings (for example, we can use $ for the first string and # for the second\\nsymbol). After constructing the common suffix tree, the deepest node marked for both T1 and T2\\nrepresents the longest common substring.\\nAnother way of doing this is: We can build a suffix tree for the string T1$T2#. This is equivalent\\nto building a common suffix tree for both the strings.\\nTime Complexity: O(m + n), where m and n are the lengths of input strings T1 and T2.\\nProblem-3\\u2003\\u2003Longest Palindrome: Given a text T how do we find the substring of T which is\\nthe longest palindrome of T?\\nSolution: The longest palindrome of T[1..n] can be found in O(n) time. The algorithm is: first\\nbuild a suffix tree for T$reverse(T)# or build a generalized suffix tree for T and reverse(T). After\\nbuilding the suffix tree, find the deepest node marked with both $ and #. Basically it means find\\nthe longest common substring.\\nProblem-4\\u2003\\u2003Given a string (word), give an algorithm for finding the next word in the\\ndictionary.\\nSolution: Let us assume that we are using Trie for storing the dictionary words. To find the next'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 670, 'file_type': 'pdf'}, page_content='word in Tries we can follow a simple approach as shown below. Starting from the rightmost\\ncharacter, increment the characters one by one. Once we reach Z, move to the next character on\\nthe left side.\\nWhenever we increment, check if the word with the incremented character exists in the dictionary\\nor not. If it exists, then return the word, otherwise increment again. If we use TST, then we can\\nfind the inorder successor for the current word.\\nProblem-5\\u2003\\u2003Give an algorithm for reversing a string.\\nSolution:\\nTime Complexity: O(n), where n is the length of the given string. Space Complexity: O(n).\\nProblem-6\\u2003\\u2003If the string is not editable, how do we create a string that is the reverse of the\\ngiven string?\\nSolution: If the string is not editable, then we need to create an array and return the pointer of\\nthat.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 671, 'file_type': 'pdf'}, page_content='Time Complexity: \\n, where n is the length of the given string. Space Complexity:\\nO(1).\\nProblem-7\\u2003\\u2003Can we reverse the string without using any temporary variable?\\nSolution: Yes, we can use XOR logic for swapping the variables.\\nTime Complexity: \\n, where n is the length of the given string. Space Complexity:\\nO(1).\\nProblem-8\\u2003\\u2003Given a text and a pattern, give an algorithm for matching the pattern in the text.\\nAssume ? (single character matcher) and * (multi character matcher) are the wild card\\ncharacters.\\nSolution: Brute Force Method. For efficient method, refer to the theory section.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 672, 'file_type': 'pdf'}, page_content='Time Complexity: O(mn), where m is the length of the text and n is the length of the pattern.\\nSpace Complexity: O(1).\\nProblem-9\\u2003\\u2003Give an algorithm for reversing words in a sentence.\\nExample: Input: “This is a Career Monk String”, Output: “String Monk Career a is This”\\nSolution: Start from the beginning and keep on reversing the words. The below implementation\\nassumes that ‘ ‘ (space) is the delimiter for words in given sentence.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 673, 'file_type': 'pdf'}, page_content='Time Complexity: O(2n) ≈ O(n), where n is the length of the string. Space Complexity: O(1).\\nProblem-10\\u2003\\u2003Permutations of a string [anagrams]: Give an algorithm for printing all\\npossible permutations of the characters in a string. Unlike combinations, two permutations\\nare considered distinct if they contain the same characters but in a different order. For\\nsimplicity assume that each occurrence of a repeated character is a distinct character. That\\nis, if the input is “aaa”, the output should be six repetitions of “aaa”. The permutations may\\nbe output in any order.\\nSolution: The solution is reached by generating n! strings, each of length n, where n is the length\\nof the input string.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 674, 'file_type': 'pdf'}, page_content='Problem-11\\u2003\\u2003Combinations Combinations of a String: Unlike permutations, two\\ncombinations are considered to be the same if they contain the same characters, but may be\\nin a different order. Give an algorithm that prints all possible combinations of the\\ncharacters in a string. For example, “ac” and “ab” are different combinations from the\\ninput string “abc”, but “ab” is the same as “ba”.\\nSolution: The solution is achieved by generating n!/r! (n – r)! strings, each of length between 1\\nand n where n is the length of the given input string.\\nAlgorithm:\\nFor each of the input characters\\na.\\nPut the current character in output string and print it.\\nb.\\nIf there are any remaining characters, generate combinations with those\\nremaining characters.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 675, 'file_type': 'pdf'}, page_content='Problem-12\\u2003\\u2003Given a string “ABCCBCBA”, give an algorithm for recursively removing the\\nadjacent characters if they are the same. For example, ABCCBCBA nnnnnn> ABBCBA-\\n>ACBA\\nSolution: First we need to check if we have a character pair; if yes, then cancel it. Now check for\\nnext character and previous element. Keep canceling the characters until we either reach the start\\nof the array, reach the end of the array, or don’t find a pair.\\nProblem-13\\u2003\\u2003Given a set of characters CHARS and a input string INPUT, find the minimum\\nwindow in str which will contain all the characters in CHARS in complexity O(n). For\\nexample, INPUT = ABBACBAA and CHARS = AAB has the minimum window BAA.\\nSolution: This algorithm is based on the sliding window approach. In this approach, we start\\nfrom the beginning of the array and move to the right. As soon as we have a window which has all\\nthe required elements, try sliding the window as far right as possible with all the required\\nelements. If the current window length is less than the minimum length found until now, update the\\nminimum length. For example, if the input array is ABBACBAA and the minimum window should\\ncover characters AAB, then the sliding window will move like this:\\nAlgorithm: The input is the given array and chars is the array of characters that need to be found.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 676, 'file_type': 'pdf'}, page_content='1\\nMake an integer array shouldfind[] of len 256. The ith element of this array will have\\nthe count of how many times we need to find the element of ASCII value i.\\n2\\nMake another array hasfound of 256 elements, which will have the count of the\\nrequired elements found until now.\\n3\\nCount <= 0\\n4\\nWhile input[i]\\na.\\nIf input[i] element is not to be found→ continue\\nb.\\nIf input[i] element is required => increase count by 1.\\nc.\\nIf count is length of chars[] array, slide the window as much right as\\npossible.\\nd.\\nIf current window length is less than min length found until now, update\\nmin length.\\nComplexity: If we walk through the code, i and j can traverse at most n steps (where n is the input'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 677, 'file_type': 'pdf'}, page_content='size) in the worst case, adding to a total of 2n times. Therefore, time complexity is O(n).\\nProblem-14\\u2003\\u2003We are given a 2D array of characters and a character pattern. Give an algorithm\\nto find if the pattern is present in the 2D array. The pattern can be in any order (all 8\\nneighbors to be considered) but we can’t use the same character twice while matching.\\nReturn 1 if match is found, 0 if not. For example: Find “MICROSOFT” in the below\\nmatrix.\\nSolution: Manually finding the solution of this problem is relatively intuitive; we just need to\\ndescribe an algorithm for it. Ironically, describing the algorithm is not the easy part.\\nHow do we do it manually? First we match the first element, and when it is matched we match\\nthe second element in the 8 neighbors of the first match. We do this process recursively, and when\\nthe last character of the input pattern matches, return true.\\nDuring the above process, take care not to use any cell in the 2D array twice. For this purpose,\\nyou mark every visited cell with some sign. If your pattern matching fails at some point, start\\nmatching from the beginning (of the pattern) in the remaining cells. When returning, you unmark\\nthe visited cells.\\nLet’s convert the above intuitive method into an algorithm. Since we are doing similar checks for\\npattern matching every time, a recursive solution is what we need. In a recursive solution, we\\nneed to check if the substring passed is matched in the given matrix or not. The condition is not to\\nuse the already used cell, and to find the already used cell, we need to add another 2D array to the\\nfunction (or we can use an unused bit in the input array itself.) Also, we need the current position\\nof the input matrix from where we need to start. Since we need to pass a lot more information than\\nis actually given, we should be having a wrapper function to initialize the extra information to be\\npassed.\\nAlgorithm:\\nIf we are past the last character in the pattern\\nReturn true\\nIf we get a used cell again\\nReturn false if we got past the 2D matrix\\nReturn false\\nIf searching for first element and cell doesn’t match\\nFindMatch with next cell in row-first order (or column-first order)'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 678, 'file_type': 'pdf'}, page_content='Otherwise if character matches\\nmark this cell as used\\nres = FindMatch with next position of pattern in 8 neighbors\\nmark this cell as unused\\nReturn res\\nOtherwise\\nReturn false'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 679, 'file_type': 'pdf'}, page_content='Problem-15\\u2003\\u2003Given two strings str1 and str2, write a function that prints all interleavings of\\nthe given two strings. We may assume that all characters in both strings are different.\\nExample: Input: str1 = “AB”, str2 = “CD” and Output: ABCD ACBD ACDB CABD'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 680, 'file_type': 'pdf'}, page_content='CADB CDAB. An interleaved string of given two strings preserves the order of characters\\nin individual strings. For example, in all the interleavings of above first example, ‘A’\\ncomes before ‘B’ and ‘C comes before ‘D’.\\nSolution: Let the length of str1 be m and the length of str2 be n. Let us assume that all characters\\nin str1 and str2 are different. Let Count(m,n) be the count of all interleaved strings in such strings.\\nThe value of Count(m,n) can be written as following.\\nTo print all interleavings, we can first fix the first character of strl[0..m-1] in output string, and\\nrecursively call for str1[1..m-1] and str2[0..n-1]. And then we can fix the first character of\\nstr2[0..n-1] and recursively call for str1[0..m-1] and str2[1..n-1].'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 681, 'file_type': 'pdf'}, page_content='Problem-16\\u2003\\u2003Given a matrix with size n × n containing random integers. Give an algorithm\\nwhich checks whether rows match with a column(s) or not. For example, if ith row\\nmatches with jth column, and ith row contains the elements - [2,6,5,8,9]. Then;’’1 column\\nwould also contain the elements - [2,6,5,8,9].\\nSolution: We can build a trie for the data in the columns (rows would also work). Then we can\\ncompare the rows with the trie. This would allow us to exit as soon as the beginning of a row\\ndoes not match any column (backtracking). Also this would let us check a row against all columns\\nin one pass.\\nIf we do not want to waste memory for empty pointers then we can further improve the solution by\\nconstructing a suffix tree.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 682, 'file_type': 'pdf'}, page_content='Problem-17\\u2003\\u2003Write a method to replace all spaces in a string with ‘%20’. Assume string has\\nsufficient space at end of string to hold additional characters.\\nSolution: Find the number of spaces. Then, starting from end (assuming string has enough space),\\nreplace the characters. Starting from end reduces the overwrites.\\nTime Complexity: O(n). Space Complexity: O(1). Here, we do not have to worry about the space\\nneeded for extra characters.\\nProblem-18\\u2003\\u2003Running length encoding: Write an algorithm to compress the given string by\\nusing the count of repeated characters and if new corn-pressed string length is not smaller\\nthan the original string then return the original string.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 683, 'file_type': 'pdf'}, page_content='With extra space of O(2):\\nTime Complexity: O(n). Space Complexity: O(1), but it uses a temporary array of size two.\\nWithout extra space (inplace):'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 684, 'file_type': 'pdf'}, page_content='‘Time Complexity: O(n). Space Complexity: O(1)..'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 685, 'file_type': 'pdf'}, page_content='16.1 Introduction\\nIn the previous chapters, we have seen many algorithms for solving different kinds of problems.\\nBefore solving a new problem, the general tendency is to look for the similarity of the current\\nproblem to other problems for which we have solutions. This helps us in getting the solution\\neasily.\\nIn this chapter, we will see different ways of classifying the algorithms and in subsequent\\nchapters we will focus on a few of them (Greedy, Divide and Conquer, Dynamic Programming).\\n16.2 Classification\\nThere are many ways of classifying algorithms and a few of them are shown below:\\n•\\nImplementation Method\\n•\\nDesign Method\\n•\\nOther Classifications'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 686, 'file_type': 'pdf'}, page_content='16.3 Classification by Implementation Method\\nRecursion or Iteration\\nA recursive algorithm is one that calls itself repeatedly until a base condition is satisfied. It is a\\ncommon method used in functional programming languages like C,C + +, etc.\\nIterative algorithms use constructs like loops and sometimes other data structures like stacks and\\nqueues to solve the problems.\\nSome problems are suited for recursive and others are suited for iterative. For example, the\\nTowers of Hanoi problem can be easily understood in recursive implementation. Every recursive\\nversion has an iterative version, and vice versa.\\nProcedural or Declarative (non-Procedural)\\nIn declarative programming languages, we say what we want without having to say how to do it.\\nWith procedural programming, we have to specify the exact steps to get the result. For example,\\nSQL is more declarative than procedural, because the queries don’t specify the steps to produce\\nthe result. Examples of procedural languages include: C, PHP, and PERL.\\nSerial or Parallel or Distributed\\nIn general, while discussing the algorithms we assume that computers execute one instruction at a\\ntime. These are called serial algorithms.\\nParallel algorithms take advantage of computer architectures to process several instructions at a\\ntime. They divide the problem into subproblems and serve them to several processors or threads.\\nIterative algorithms are generally parallelizable.\\nIf the parallel algorithms are distributed on to different machines then we call such algorithms\\ndistributed algorithms.\\nDeterministic or Non-Deterministic\\nDeterministic algorithms solve the problem with a predefined process, whereas non –\\ndeterministic algorithms guess the best solution at each step through the use of heuristics.\\nExact or Approximate'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 687, 'file_type': 'pdf'}, page_content='As we have seen, for many problems we are not able to find the optimal solutions. That means,\\nthe algorithms for which we are able to find the optimal solutions are called exact algorithms. In\\ncomputer science, if we do not have the optimal solution, we give approximation algorithms.\\nApproximation algorithms are generally associated with NP-hard problems (refer to the\\nComplexity Classes chapter for more details).\\n16.4 Classification by Design Method\\nAnother way of classifying algorithms is by their design method.\\nGreedy Method\\nGreedy algorithms work in stages. In each stage, a decision is made that is good at that point,\\nwithout bothering about the future consequences. Generally, this means that some local best is\\nchosen. It assumes that the local best selection also makes for the global optimal solution.\\nDivide and Conquer\\nThe D & C strategy solves a problem by:\\n1)\\nDivide: Breaking the problem into sub problems that are themselves smaller\\ninstances of the same type of problem.\\n2)\\nRecursion: Recursively solving these sub problems.\\n3)\\nConquer: Appropriately combining their answers.\\nExamples: merge sort and binary search algorithms.\\nDynamic Programming\\nDynamic programming (DP) and memoization work together. The difference between DP and\\ndivide and conquer is that in the case of the latter there is no dependency among the sub problems,\\nwhereas in DP there will be an overlap of sub-problems. By using memoization [maintaining a\\ntable for already solved sub problems], DP reduces the exponential complexity to polynomial\\ncomplexity (O(n2), O(n3), etc.) for many problems.\\nThe difference between dynamic programming and recursion is in the memoization of recursive\\ncalls. When sub problems are independent and if there is no repetition, memoization does not\\nhelp, hence dynamic programming is not a solution for all problems.\\nBy using memoization [maintaining a table of sub problems already solved], dynamic'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 688, 'file_type': 'pdf'}, page_content='programming reduces the complexity from exponential to polynomial.\\nLinear Programming\\nIn linear programming, there are inequalities in terms of inputs and maximizing (or minimizing)\\nsome linear function of the inputs. Many problems (example: maximum flow for directed graphs)\\ncan be discussed using linear programming.\\nReduction [Transform and Conquer]\\nIn this method we solve a difficult problem by transforming it into a known problem for which we\\nhave asymptotically optimal algorithms. In this method, the goal is to find a reducing algorithm\\nwhose complexity is not dominated by the resulting reduced algorithms. For example, the\\nselection algorithm for finding the median in a list involves first sorting the list and then finding\\nout the middle element in the sorted list. These techniques are also called transform and conquer.\\n16.5 Other Classifications\\nClassification by Research Area\\nIn computer science each field has its own problems and needs efficient algorithms. Examples:\\nsearch algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms,\\nstring algorithms, geometric algorithms, combinatorial algorithms, machine learning,\\ncryptography, parallel algorithms, data compression algorithms, parsing techniques, and more.\\nClassification by Complexity\\nIn this classification, algorithms are classified by the time they take to find a solution based on\\ntheir input size. Some algorithms take linear time complexity (O(n)) and others take exponential\\ntime, and some never halt. Note that some problems may have multiple algorithms with different\\ncomplexities.\\nRandomized Algorithms\\nA few algorithms make choices randomly. For some problems, the fastest solutions must involve\\nrandomness. Example: Quick Sort.\\nBranch and Bound Enumeration and Backtracking'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 689, 'file_type': 'pdf'}, page_content='These were used in Artificial Intelligence and we do not need to explore these fully. For the\\nBacktracking method refer to the Recusion and Backtracking chapter.\\nNote: In the next few chapters we discuss the Greedy, Divide and Conquer, and Dynamic\\nProgramming] design methods. These methods are emphasized because they are used more often\\nthan other methods to solve problems.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 690, 'file_type': 'pdf'}, page_content='17.1 Introduction\\nLet us start our discussion with simple theory that will give us an understanding of the Greedy\\ntechnique. In the game of Chess, every time we make a decision about a move, we have to also\\nthink about the future consequences. Whereas, in the game of Tennis (or Volleyball), our action is\\nbased on the immediate situation.\\nThis means that in some cases making a decision that looks right at that moment gives the best\\nsolution (Greedy), but in other cases it doesn’t. The Greedy technique is best suited for looking at\\nthe immediate situation.\\n17.2 Greedy Strategy\\nGreedy algorithms work in stages. In each stage, a decision is made that is good at that point,\\nwithout bothering about the future. This means that some local best is chosen. It assumes that a\\nlocal good selection makes for a global optimal solution.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 691, 'file_type': 'pdf'}, page_content='17.3 Elements of Greedy Algorithms\\nThe two basic properties of optimal Greedy algorithms are:\\n1)\\nGreedy choice property\\n2)\\nOptimal substructure\\nGreedy choice property\\nThis property says that the globally optimal solution can be obtained by making a locally optimal\\nsolution (Greedy). The choice made by a Greedy algorithm may depend on earlier choices but not\\non the future. It iteratively makes one Greedy choice after another and reduces the given problem\\nto a smaller one.\\nOptimal substructure\\nA problem exhibits optimal substructure if an optimal solution to the problem contains optimal\\nsolutions to the subproblems. That means we can solve subproblems and build up the solutions to\\nsolve larger problems.\\n17.4 Does Greedy Always Work?\\nMaking locally optimal choices does not always work. Hence, Greedy algorithms will not always\\ngive the best solutions. We will see particular examples in the Problems section and in the\\nDynamic Programming chapter.\\n17.5 Advantages and Disadvantages of Greedy Method\\nThe main advantage of the Greedy method is that it is straightforward, easy to understand and\\neasy to code. In Greedy algorithms, once we make a decision, we do not have to spend time re-\\nexamining the already computed values. Its main disadvantage is that for many problems there is\\nno greedy algorithm. That means, in many cases there is no guarantee that making locally optimal\\nimprovements in a locally optimal solution gives the optimal global solution.\\n17.6 Greedy Applications\\n•\\nSorting: Selection sort, Topological sort\\n•\\nPriority Queues: Heap sort\\n•\\nHuffman coding compression algorithm'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 692, 'file_type': 'pdf'}, page_content='•\\nPrim’s and Kruskal’s algorithms\\n•\\nShortest path in Weighted Graph [Dijkstra’s]\\n•\\nCoin change problem\\n•\\nFractional Knapsack problem\\n•\\nDisjoint sets-UNION by size and UNION by height (or rank)\\n•\\nJob scheduling algorithm\\n•\\nGreedy techniques can be used as an approximation algorithm for complex problems\\n17.7 Understanding Greedy Technique\\nFor better understanding let us go through an example.\\nHuffman Coding Algorithm\\nDefinition\\nGiven a set of n characters from the alphabet A [each character c ∈ A] and their associated\\nfrequency freq(c), find a binary code for each character c ∈ A, such that ∑c ∈ A\\nfreq(c)|binarycode(c)| is minimum, where /binarycode(c)/represents the length of binary code of\\ncharacter c. That means the sum of the lengths of all character codes should be minimum [the sum\\nof each character’s frequency multiplied by the number of bits in the representation].\\nThe basic idea behind the Huffman coding algorithm is to use fewer bits for more frequently\\noccurring characters. The Huffman coding algorithm compresses the storage of data using\\nvariable length codes. We know that each character takes 8 bits for representation. But in general,\\nwe do not use all of them. Also, we use some characters more frequently than others. When\\nreading a file, the system generally reads 8 bits at a time to read a single character. But this\\ncoding scheme is inefficient. The reason for this is that some characters are more frequently used\\nthan other characters. Let’s say that the character ′e′ is used 10 times more frequently than the\\ncharacter ′q′. It would then be advantageous for us to instead use a 7 bit code for e and a 9 bit\\ncode for q because that could reduce our overall message length.\\nOn average, using Huffman coding on standard files can reduce them anywhere from 10% to 30%\\ndepending on the character frequencies. The idea behind the character coding is to give longer\\nbinary codes for less frequent characters and groups of characters. Also, the character coding is\\nconstructed in such a way that no two character codes are prefixes of each other.\\nAn Example\\nLet’s assume that after scanning a file we find the following character frequencies:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 693, 'file_type': 'pdf'}, page_content='Character\\nFrequency\\na\\n12\\nb\\n2\\nc\\n7\\nd\\n13\\ne\\n14\\nf\\n85\\nGiven this, create a binary tree for each character that also stores the frequency with which it\\noccurs (as shown below).\\nThe algorithm works as follows: In the list, find the two binary trees that store minimum\\nfrequencies at their nodes.\\nConnect these two nodes at a newly created common node that will store no character but will\\nstore the sum of the frequencies of all the nodes connected below it. So our picture looks like\\nthis:\\nRepeat this process until only one tree is left:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 694, 'file_type': 'pdf'}, page_content='21 27\\n9 a-12 4-13 e-14\\nws\\neT\\n133 |\\n\\n£85\\n\\n£85'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 695, 'file_type': 'pdf'}, page_content='Once the tree is built, each leaf node corresponds to a letter with a code. To determine the code\\nfor a particular node, traverse from the root to the leaf node. For each move to the left, append a 0\\nto the code, and for each move to the right, append a 1. As a result, for the above generated tree,\\nwe get the following codes:\\nLetter\\nCode\\na\\n001\\nb\\n0000\\nc\\n0001\\nd\\n010\\ne\\n011\\nf\\n1\\nCalculating Bits Saved\\nNow, let us see how many bits that Huffman coding algorithm is saving. All we need to do for this\\ncalculation is see how many bits are originally used to store the data and subtract from that the\\nnumber of bits that are used to store the data using the Huffman code. In the above example, since\\nwe have six characters, let’s assume each character is stored with a three bit code. Since there\\nare 133 such characters (multiply total frequencies by 3), the total number of bits used is 3 * 133\\n= 399. Using the Huffman coding frequencies we can calculate the new total number of bits used:\\nThus, we saved 399 – 238 = 161 bits, or nearly 40% of the storage space.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 696, 'file_type': 'pdf'}, page_content='Time Complexity: O(nlogn), since there will be one build_heap, 2n – 2 delete_mins, and n – 2\\ninserts, on a priority queue that never has more than n elements. Refer to the Priority Queues\\nchapter for details.\\n17.8 Greedy Algorithms: Problems & Solutions\\nProblem-1\\u2003\\u2003Given an array F with size n. Assume the array content F[i] indicates the length of\\nthe ith file and we want to merge all these files into one single file. Check whether the\\nfollowing algorithm gives the best solution for this problem or not?\\nAlgorithm: Merge the files contiguously. That means select the first two files and merge\\nthem. Then select the output of the previous merge and merge with the third file, and keep\\ngoing...\\nNote: Given two files A and B with sizes m and n, the complexity of merging is O(m + n).\\nSolution: This algorithm will not produce the optimal solution. For a counter example, let us\\nconsider the following file sizes array.\\nF = {10,5,100,50,20,15}\\nAs per the above algorithm, we need to merge the first two files (10 and 5 size files), and as a\\nresult we get the following list of files. In the list below, 15 indicates the cost of merging two\\nfiles with sizes 10 and 5.\\n{15,100,50,20,15}\\nSimilarly, merging 15 with the next file 100 produces: {115,50,20,15}. For the subsequent steps'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 697, 'file_type': 'pdf'}, page_content='the list becomes\\n{165,20,15}, {185,15}\\nFinally,\\n{200}\\nThe total cost of merging = Cost of all merging operations = 15 + 115 + 165 + 185 + 200 = 680.\\nTo see whether the above result is optimal or not, consider the order: {5,10,15,20,50,100}. For\\nthis example, following the same approach, the total cost of merging = 15 + 30 + 50 + 100 + 200\\n= 395. So, the given algorithm is not giving the best (optimal) solution.\\nProblem-2\\u2003\\u2003Similar to Problem-1, does the following algorithm give the optimal solution?\\nAlgorithm: Merge the files in pairs. That means after the first step, the algorithm produces\\nthe n/2 intermediate files. For the next step, we need to consider these intermediate files\\nand merge them in pairs and keep going.\\nNote: Sometimes this algorithm is called 2-way merging. Instead of two files at a time, if\\nwe merge K files at a time then we call it K-way merging.\\nSolution: This algorithm will not produce the optimal solution and consider the previous example\\nfor a counter example. As per the above algorithm, we need to merge the first pair of files (10 and\\n5 size files), the second pair of files (100 and 50) and the third pair of files (20 and 15). As a\\nresult we get the following list of files.\\n{15,150,35}\\nSimilarly, merge the output in pairs and this step produces [below, the third element does not have\\na pair element, so keep it the same]:\\n{165,35}\\nFinally,\\n{185}\\nThe total cost of merging = Cost of all merging operations = 15 + 150 + 35 + 165 + 185 = 550.\\nThis is much more than 395 (of the previous problem). So, the given algorithm is not giving the\\nbest (optimal) solution.\\nProblem-3\\u2003\\u2003In Problem-1, what is the best way to merge all the files into a single file?\\nSolution: Using the Greedy algorithm we can reduce the total time for merging the given files. Let\\nus consider the following algorithm.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 698, 'file_type': 'pdf'}, page_content='Algorithm:\\n1.\\nStore file sizes in a priority queue. The key of elements are file lengths.\\n2.\\nRepeat the following until there is only one file:\\na.\\nExtract two smallest elements X and Y.\\nb.\\nMerge X and Y and insert this new file in the priority queue.\\nVariant of same algorithm:\\n1.\\nSort the file sizes in ascending order.\\n2.\\nRepeat the following until there is only one file:\\na.\\nTake the first two elements (smallest) X and Y.\\nb.\\nMerge X and Y and insert this new file in the sorted list.\\nTo check the above algorithm, let us trace it with the previous example. The given array is:\\nF = {10,5,100,50,20,15}\\nAs per the above algorithm, after sorting the list it becomes: {5,10,15,20,50,100}. We need to\\nmerge the two smallest files (5 and 10 size files) and as a result we get the following list of files.\\nIn the list below, 15 indicates the cost of merging two files with sizes 10 and 5.\\n{15,15,20,50,100}\\nSimilarly, merging the two smallest elements (15 and 15) produces: {20,30,50,100}. For the\\nsubsequent steps the list becomes\\n{50,50,100} // merging 20 and 30\\n{100,100} // merging 20 and 30\\nFinally,\\n{200}\\nThe total cost of merging = Cost of all merging operations = 15 + 30 + 50 + 100 + 200 = 395. So,\\nthis algorithm is producing the optimal solution for this merging problem.\\nTime Complexity: O(nlogn) time using heaps to find best merging pattern plus the optimal cost of\\nmerging the files.\\nProblem-4\\u2003\\u2003Interval Scheduling Algorithm: Given a set of n intervals S = {(starti, endj)|1 ≤ i\\n≤ n}. Let us assume that we want to find a maximum subset S′ of S such that no pair of\\nintervals in S′ overlaps. Check whether the following algorithm works or not.\\nAlgorithm:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 699, 'file_type': 'pdf'}, page_content='Solution: This algorithm does not solve the problem of finding a maximum subset of non-\\noverlapping intervals. Consider the following intervals. The optimal solution is {M,O,N,K}.\\nHowever, the interval that overlaps with the fewest others is C, and the given algorithm will\\nselect C first.\\nProblem-5\\u2003\\u2003In Problem-4, if we select the interval that starts earliest (also not overlapping\\nwith already chosen intervals), does it give the optimal solution?\\nSolution: No. It will not give the optimal solution. Let us consider the example below. It can be\\nseen that the optimal solution is 4 whereas the given algorithm gives 1.\\nProblem-6\\u2003\\u2003In Problem-4, if we select the shortest interval (but it is not overlapping the\\nalready chosen intervals), does it give the optimal solution?\\nSolution: This also will not give the optimal solution. Let us consider the example below. It can\\nbe seen that the optimal solution is 2 whereas the algorithm gives 1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 700, 'file_type': 'pdf'}, page_content='Problem-7\\u2003\\u2003For Problem-4, what is the optimal solution?\\nSolution: Now, let us concentrate on the optimal greedy solution.\\nAlgorithm:\\nTime complexity = Time for sorting + Time for scanning = O(nlogn + n) = O(nlogn).\\nProblem-8\\u2003\\u2003Consider the following problem.\\nInput: S = {(starti,endi)|1 ≤ i ≤ n} of intervals. The interval (starti,endi) we can treat as a\\nrequest for a room for a class with time start; to time endi.\\nOutput: Find an assignment of classes to rooms that uses the fewest number of rooms.\\nConsider the following iterative algorithm. Assign as many classes as possible to the first\\nroom, then assign as many classes as possible to the second room, then assign as many\\nclasses as possible to the third room, etc. Does this algorithm give the best solution?\\nNote: In fact, this problem is similar to the interval scheduling algorithm. The only\\ndifference is the application.\\nSolution: This algorithm does not solve the interval-coloring problem. Consider the following\\nintervals:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 701, 'file_type': 'pdf'}, page_content='Maximizing the number of classes in the first room results in having {B, C, F, G} in one room, and\\nclasses A, D, and E each in their own rooms, for a total of 4. The optimal solution is to put A in\\none room, { B, C, D } in another, and {E,F, G} in another, for a total of 3 rooms.\\nProblem-9\\u2003\\u2003For Problem-8, consider the following algorithm. Process the classes in\\nincreasing order of start times. Assume that we are processing class C. If there is a room R\\nsuch that R has been assigned to an earlier class, and C can be assigned to R without\\noverlapping previously assigned classes, then assign C to R. Otherwise, put C in a new\\nroom. Does this algorithm solve the problem?\\nSolution: This algorithm solves the interval-coloring problem. Note that if the greedy algorithm\\ncreates a new room for the current class ci, then because it examines classes in order of start\\ntimes, ci start point must intersect with the last class in all of the current rooms. Thus when greedy\\ncreates the last room, n, it is because the start time of the current class intersects with n – 1 other\\nclasses. But we know that for any single point in any class it can only intersect with at most s\\nother class, so it must then be that n ≤ S. As s is a lower bound on the total number needed, and\\ngreedy is feasible, it is thus also optimal.\\nNote: For optimal solution refer to Problem-7 and for code refer to Problem-10.\\nProblem-10\\u2003\\u2003Suppose we are given two arrays Start[1 ..n] and Finish[1 ..n] listing the start\\nand finish times of each class. Our task is to choose the largest possible subset X ∈\\n{1,2,...,n} so that for any pair i,j ∈ X, either Start [i] > Finish[j] or Start [j] > Finish [i]\\nSolution: Our aim is to finish the first class as early as possible, because that leaves us with the\\nmost remaining classes. We scan through the classes in order of finish time, and whenever we\\nencounter a class that doesn’t conflict with the latest class so far, then we take that class.\\nThis algorithm clearly runs in O(nlogn) time due to sorting.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 702, 'file_type': 'pdf'}, page_content='Problem-11\\u2003\\u2003Consider the making change problem in the country of India. The input to this\\nproblem is an integer M. The output should be the minimum number of coins to make M\\nrupees of change. In India, assume the available coins are 1,5,10,20,25,50 rupees. Assume\\nthat we have an unlimited number of coins of each type.\\nFor this problem, does the following algorithm produce the optimal solution or not?\\nTake as many coins as possible from the highest denominations. So for example, to make\\nchange for 234 rupees the greedy algorithm would take four 50 rupee coins, one 25 rupee\\ncoin, one 5 rupee coin, and four 1 rupee coins.\\nSolution: The greedy algorithm is not optimal for the problem of making change with the\\nminimum number of coins when the denominations are 1,5,10,20,25, and 50. In order to make 40\\nrupees, the greedy algorithm would use three coins of 25,10, and 5 rupees. The optimal solution\\nis to use two 20-shilling coins.\\nNote: For the optimal solution, refer to the Dynamic Programming chapter.\\nProblem-12\\u2003\\u2003Let us assume that we are going for a long drive between cities A and B. In\\npreparation for our trip, we have downloaded a map that contains the distances in miles\\nbetween all the petrol stations on our route. Assume that our car’s tanks can hold petrol for\\nn miles. Assume that the value n is given. Suppose we stop at every point. Does it give the\\nbest solution?\\nSolution: Here the algorithm does not produce optimal solution. Obvious Reason: filling at each\\npetrol station does not produce optimal solution.\\nProblem-13\\u2003\\u2003For problem Problem-12, stop if and only if you don’t have enough petrol to\\nmake it to the next gas station, and if you stop, fill the tank up all the way. Prove or\\ndisprove that this algorithm correctly solves the problem.\\nSolution: The greedy approach works: We start our trip from A with a full tank. We check our\\nmap to determine the farthest petrol station on our route within n miles. We stop at that petrol\\nstation, fill up our tank and check our map again to determine the farthest petrol station on our\\nroute within n miles from this stop. Repeat the process until we get to B.\\nNote: For code, refer to Dynamic Programming chapter.\\nProblem-14\\u2003\\u2003Fractional Knapsack problem: Given items t1: t2, ...,tn (items we might want to\\ncarry in our backpack) with associated weights s1; s2, ... , sn and benefit values vx, v2, …,\\nvn, how can we maximize the total benefit considering that we are subject to an absolute\\nweight limit C?\\nSolution:\\nAlgorithm:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 703, 'file_type': 'pdf'}, page_content='1)\\nCompute value per size density for each item \\n.\\n2)\\nSort each item by its value density.\\n3)\\nTake as much as possible of the density item not already in the bag\\nTime Complexity: O(nlogn) for sorting and O(n) for greedy selections.\\nNote: The items can be entered into a priority queue and retrieved one by one until either the bag\\nis full or all items have been selected. This actually has a better runtime of O(n + clogn) where c\\nis the number of items that actually get selected in the solution. There is a savings in runtime if c =\\nO(n), but otherwise there is no change in the complexity.\\nProblem-15\\u2003\\u2003Number of railway-platforms: At a railway station, we have a time-table with\\nthe trains’ arrivals and departures. We need to find the minimum number of platforms so\\nthat all the trains can be accommodated as per their schedule.\\nExample: The timetable is as given below, the answer is 3. Otherwise, the railway station\\nwill not be able to accommodate all the trains.\\nSolution: Let’s take the same example as described above. Calculating the number of platforms is\\ndone by determining the maximum number of trains at the railway station at any time.\\nFirst, sort all the arrival(A) and departure(D) times in an array. Then, save the corresponding\\narrivals anddepartures in the array also. After sorting, our array will look like this:\\nNow modify the array by placing 1 for A and -1 for D. The new array will look like this:\\nFinally make a cumulative array out of this:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 704, 'file_type': 'pdf'}, page_content='Our solution will be the maximum value in this array. Here it is 3.\\nNote: If we have a train arriving and another departing at the same time, then put the departure\\ntime first in the sorted array.\\nProblem-16\\u2003\\u2003Consider a country with very long roads and houses along the road. Assume that\\nthe residents of all houses use cell phones. We want to place cell phone towers along the\\nroad, and each cell phone tower covers a range of 7 kilometers. Create an efficient\\nalgorithm that allow for the fewest cell phone towers.\\nSolution:\\nThe algorithm to locate the least number of cell phone towers:\\n1)\\nStart from the beginning of the road\\n2)\\nFind the first uncovered house on the road\\n3)\\nIf there is no such house, terminate this algorithm. Otherwise, go to next step\\n4)\\nLocate a cell phone tower 7 miles away after we find this house along the road\\n5)\\nGo to step 2\\nProblem-17\\u2003\\u2003Preparing Songs Cassette: Suppose we have a set of n songs and want to store\\nthese on a tape. In the future, users will want to read those songs from the tape. Reading a\\nsong from a tape is not like reading from a disk; first we have to fast-forward past all the\\nother songs, and that takes a significant amount of time. Let A[1 .. n] be an array listing the\\nlengths of each song, specifically, song i has length A[i]. If the songs are stored in order\\nfrom 1 to n, then the cost of accessing the kth song is:\\nThe cost reflects the fact that before we read song k we must first scan past all the earlier\\nsongs on the tape. If we change the order of the songs on the tape, we change the cost of\\naccessing the songs, with the result that some songs become more expensive to read, but\\nothers become cheaper. Different song orders are likely to result in different expected\\ncosts. If we assume that each song is equally likely to be accessed, which order should we\\nuse if we want the expected cost to be as small as possible?\\nSolution: The answer is simple. We should store the songs in the order from shortest to longest.\\nStoring the short songs at the beginning reduces the forwarding times for the remaining jobs.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 705, 'file_type': 'pdf'}, page_content='Problem-18\\u2003\\u2003Let us consider a set of events at HITEX (Hyderabad Convention Center).\\nAssume that there are n events where each takes one unit of time. Event i will provide a\\nprofit of P [i ] rupees (P [i ] > 0) if started at or before time T[i], where T[i] is an arbitrary\\nnumber. If an event is not started by T[i] then there is no benefit in scheduling it at all. All\\nevents can start as early as time 0. Give the efficient algorithm to find a schedule that\\nmaximizes the profit.\\nSolution:\\nAlgorithm:\\n•\\nSort the jobs according to floor(T[i]) (sorted from largest to smallest).\\n•\\nLet time t be the current time being considered (where initially t = floor(T[i])).\\n•\\nAll jobs i where floor(T[i]) = t are inserted into a priority queue with the profit g,\\nused as the key.\\n•\\nA DeleteMax is performed to select the job to run at time t.\\n•\\nThen t is decremented and the process is continued.\\nClearly the time complexity is O(nlogn). The sort takes O(nlogn) and there are at most n insert\\nand DeleteMax operations performed on the priority queue, each of which takes O(logn) time.\\nProblem-19\\u2003\\u2003Let us consider a customer-care server (say, mobile customer-care) with n\\ncustomers to be served in the queue. For simplicity assume that the service time required\\nby each customer is known in advance and it is wt minutes for customer i. So if, for\\nexample, the customers are served in order of increasing i, then the ith customer has to\\nwait: \\n minutes. The total waiting time of all customers can be given as \\n. What is the best way to serve the customers so that the total waiting\\ntime can be reduced?\\nSolution: This problem can be easily solved using greedy technique. Since our objective is to\\nreduce the total waiting time, what we can do is, select the customer whose service time is less.\\nThat means, if we process the customers in the increasing order of service time then we can\\nreduce the total waiting time.\\nTime Complexity: O(nlogn).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 706, 'file_type': 'pdf'}, page_content='18.1 Introduction\\nIn the Greedy chapter, we have seen that for many problems the Greedy strategy failed to provide\\noptimal solutions. Among those problems, there are some that can be easily solved by using the\\nDivide and Conquer (D & C) technique. Divide and Conquer is an important algorithm design\\ntechnique based on recursion.\\nThe D & C algorithm works by recursively breaking down a problem into two or more sub\\nproblems of the same type, until they become simple enough to be solved directly. The solutions\\nto the sub problems are then combined to give a solution to the original problem.\\n18.2 What is Divide and Conquer Strategy?\\nThe D & C strategy solves a problem by:\\n1)\\nDivide: Breaking the problem into sub problems that are themselves smaller\\ninstances of the same type of problem.\\n2)\\nRecursion: Recursively solving these sub problems.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 707, 'file_type': 'pdf'}, page_content='3)\\nConquer: Appropriately combining their answers.\\n18.3 Does Divide and Conquer Always Work?\\nIt’s not possible to solve all the problems with the Divide & Conquer technique. As per the\\ndefinition of D & C, the recursion solves the subproblems which are of the same type. For all\\nproblems it is not possible to find the subproblems which are the same size and D & C is not a\\nchoice for all problems.\\n18.4 Divide and Conquer Visualization\\nFor better understanding, consider the following visualization. Assume that n is the size of the\\noriginal problem. As described above, we can see that the problem is divided into sub problems\\nwith each of size n/b (for some constant b). We solve the sub problems recursively and combine\\ntheir solutions to get the solution for the original problem.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 708, 'file_type': 'pdf'}, page_content='18.5 Understanding Divide and Conquer\\nFor a clear understanding of D & C, let us consider a story. There was an old man who was a rich\\nfarmer and had seven sons. He was afraid that when he died, his land and his possessions would\\nbe divided among his seven sons, and that they would quarrel with one another.\\nSo he gathered them together and showed them seven sticks that he had tied together and told them\\nthat anyone who could break the bundle would inherit everything. They all tried, but no one could\\nbreak the bundle. Then the old man untied the bundle and broke the sticks one by one. The\\nbrothers decided that they should stay together and work together and succeed together. The moral\\nfor problem solvers is different. If we can’t solve the problem, divide it into parts, and solve one\\npart at a time.\\nIn earlier chapters we have already solved many problems based on D & C strategy: like Binary\\nSearch, Merge Sort, Quick Sort, etc.... Refer to those topics to get an idea of how D & C works.\\nBelow are a few other real-time problems which can easily be solved with D & C strategy. For\\nall these problems we can find the subproblems which are similar to the original problem.\\n•\\nLooking for a name in a phone book: We have a phone book with names in\\nalphabetical order. Given a name, how do we find whether that name is there in the\\nphone book or not?\\n•\\nBreaking a stone into dust: We want to convert a stone into dust (very small stones).\\n•\\nFinding the exit in a hotel: We are at the end of a very long hotel lobby with a long\\nseries of doors, with one door next to us. We are looking for the door that leads to\\nthe exit.\\n•\\nFinding our car in a parking lot.\\n18.6 Advantages of Divide and Conquer\\nSolving difficult problems: D & C is a powerful method for solving difficult problems. As an\\nexample, consider the Tower of Hanoi problem. This requires breaking the problem into\\nsubproblems, solving the trivial cases and combining the subproblems to solve the original\\nproblem. Dividing the problem into subproblems so that subproblems can be combined again is a\\nmajor difficulty in designing a new algorithm. For many such problems D & C provides a simple\\nsolution.\\nParallelism: Since D & C allows us to solve the subproblems independently, this allows for\\nexecution in multiprocessor machines, especially shared-memory systems where the\\ncommunication of data between processors does not need to be planned in advance, because\\ndifferent subproblems can be executed on different processors.\\nMemory access: D & C algorithms naturally tend to make efficient use of memory caches. This is\\nbecause once a subproblem is small, all its subproblems can be solved within the cache, without'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 709, 'file_type': 'pdf'}, page_content='accessing the slower main memory.\\n18.7 Disadvantages of Divide and Conquer\\nOne disadvantage of the D & C approach is that recursion is slow. This is because of the\\noverhead of the repeated subproblem calls. Also, the D & C approach needs stack for storing the\\ncalls (the state at each point in the recursion). Actually this depends upon the implementation\\nstyle. With large enough recursive base cases, the overhead of recursion can become negligible\\nfor many problems.\\nAnother problem with D & C is that, for some problems, it may be more complicated than an\\niterative approach. For example, to add n numbers, a simple loop to add them up in sequence is\\nmuch easier than a D & C approach that breaks the set of numbers into two halves, adds them\\nrecursively, and then adds the sums.\\n18.8 Master Theorem\\nAs stated above, in the D & C method, we solve the sub problems recursively. All problems are\\ngenerally defined in terms of recursive definitions. These recursive problems can easily be\\nsolved using Master theorem. For details on Master theorem, refer to the Introduction to Analysis\\nof Algorithms chapter. Just for continuity, let us reconsider the Master theorem.\\nIf the recurrence is of the form \\n, where a ≥ 1, b > 1, k ≥\\n0 and p is a real number, then the complexity can be directly given as:\\n1)\\nIf a > bk, then \\n2)\\nIf a = bk\\na.\\nIf p > –1, then \\nb.\\nIf p = –1, then \\nc.\\nIf p < –1, then \\n3)\\nIf a < bk\\na.\\nIf p > 0, then T(n) = Θ(nklogpn)\\nb.\\nIf p < 0, then T(n) = O(nk)\\n18.9 Divide and Conquer Applications\\n•\\nBinary Search\\n•\\nMerge Sort and Quick Sort'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 710, 'file_type': 'pdf'}, page_content='•\\nMedian Finding\\n•\\nMin and Max Finding\\n•\\nMatrix Multiplication\\n•\\nClosest Pair problem\\n18.10 Divide and Conquer: Problems & Solutions\\nProblem-1\\u2003\\u2003Let us consider an algorithm A which solves problems by dividing them into five\\nsubproblems of half the size, recursively solving each subproblem, and then combining the\\nsolutions in linear time. What is the complexity of this algorithm?\\nSolution: Let us assume that the input size is n and T(n) defines the solution to the given problem.\\nAs per the description, the algorithm divides the problem into 5 sub problems with each of size \\n. So we need to solve \\n subproblems. After solving these sub problems, the given array\\n(linear time) is scanned to combine these solutions. The total recurrence algorithm for this\\nproblem can be given as: \\n. Using the Master theorem (of D & C), we\\nget the complexity \\n.\\nProblem-2\\u2003\\u2003Similar to Problem-1, an algorithm B solves problems of size n by recursively\\nsolving two subproblems of size n – 1 and then combining the solutions in constant time.\\nWhat is the complexity of this algorithm?\\nSolution: Let us assume that the input size is n and T(n) defines the solution to the given problem.\\nAs per the description of algorithm we divide the problem into 2 sub problems with each of size\\nn – 1. So we have to solve 2T(n – 1) sub problems. After solving these sub problems, the\\nalgorithm takes only a constant time to combine these solutions. The total recurrence algorithm for\\nthis problem can be given as:\\nUsing Master theorem (of Subtract and Conquer), we get the complexity as \\n. (Refer to Introduction chapter for more details).\\nProblem-3\\u2003\\u2003Again similar to Problem-1, another algorithm C solves problems of size n by\\ndividing them into nine subproblems of size , recursively solving each subproblem, and\\nthen combining the solutions in O(n2) time. What is the complexity of this algorithm?\\nSolution: Let us assume that input size is n and T(n) defines the solution to the given problem. As\\nper the description of algorithm we divide the problem into 9 sub problems with each of size .\\nSo we need to solve \\n sub problems. After solving the sub problems, the algorithm takes'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 711, 'file_type': 'pdf'}, page_content='quadratic time to combine these solutions. The total recurrence algorithm for this problem can be\\ngiven as: \\n. Using D & C Master theorem, we get the complexity\\nas O(n2logn).\\nProblem-4\\u2003\\u2003Write a recurrence and solve it.\\nSolution: Let us assume that input size is n and T(n) defines the solution to the given problem. As\\nper the given code, after printing the character and dividing the problem into 2 subproblems with\\neach of size \\n and solving them. So we need to solve \\n subproblems. After solving these\\nsubproblems, the algorithm is not doing anything for combining the solutions. The total recurrence\\nalgorithm for this problem can be given as:\\nUsing Master theorem (of D & C), we get the complexity as \\n.\\nProblem-5\\u2003\\u2003Given an array, give an algorithm for finding the maximum and minimum.\\nSolution: Refer Selection Algorithms chapter.\\nProblem-6\\u2003\\u2003Discuss Binary Search and its complexity.\\nSolution: Refer Searching chapter for discussion on Binary Search.\\nAnalysis: Let us assume that input size is n and T(n) defines the solution to the given problem.\\nThe elements are in sorted order. In binary search we take the middle element and check whether\\nthe element to be searched is equal to that element or not. If it is equal then we return that element.\\nIf the element to be searched is greater than the middle element then we consider the right sub-\\narray for finding the element and discard the left sub-array. Similarly, if the element to be\\nsearched is less than the middle element then we consider the left sub-array for finding the\\nelement and discard the right sub-array.\\nWhat this means is, in both the cases we are discarding half of the sub-array and considering the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 712, 'file_type': 'pdf'}, page_content='remaining half only. Also, at every iteration we are dividing the elements into two equal halves.\\nAs per the above discussion every time we divide the problem into 2 sub problems with each of\\nsize \\n and solve one \\n sub problem. The total recurrence algorithm for this problem can be\\ngiven as:\\nUsing Master theorem (of D & C), we get the complexity as O(logn).\\nProblem-7\\u2003\\u2003Consider the modified version of binary search. Let us assume that the array is\\ndivided into 3 equal parts (ternary search) instead of 2 equal parts. Write the recurrence\\nfor this ternary search and find its complexity.\\nSolution: From the discussion on Problem-5, binary search has the recurrence relation: \\n. Similar to the Problem-5 discussion, instead of 2 in the recurrence\\nrelation we use “3”. That indicates that we are dividing the array into 3 sub-arrays with equal\\nsize and considering only one of them. So, the recurrence for the ternary search can be given as:\\nUsing Master theorem (of D & C), we get the complexity as \\n ≈ O(logn)\\n(we don’t have to worry about the base of log as they are constants).\\nProblem-8\\u2003\\u2003In Problem-5, what if we divide the array into two sets of sizes approximately\\none-third and two-thirds.\\nSolution: We now consider a slightly modified version of ternary search in which only one\\ncomparison is made, which creates two partitions, one of roughly  elements and the other of \\n.\\nHere the worst case comes when the recursive call is on the larger \\n element part. So the\\nrecurrence corresponding to this worst case is:\\nUsing Master theorem (of D & C), we get the complexity as O(logn). It is interesting to note that\\nwe will get the same results for general k-ary search (as long as k is a fixed constant which does\\nnot depend on n) as n approaches infinity.\\nProblem-9\\u2003\\u2003Discuss Merge Sort and its complexity.\\nSolution: Refer to Sorting chapter for discussion on Merge Sort. In Merge Sort, if the number of\\nelements are greater than 1, then divide them into two equal subsets, the algorithm is recursively'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 713, 'file_type': 'pdf'}, page_content='invoked on the subsets, and the returned sorted subsets are merged to provide a sorted list of the\\noriginal set. The recurrence equation of the Merge Sort algorithm is:\\nIf we solve this recurrence using D & C Master theorem it gives O(nlogn) complexity.\\nProblem-10\\u2003\\u2003Discuss Quick Sort and its complexity.\\nSolution: Refer to Sorting chapter for discussion on Quick Sort. For Quick Sort we have\\ndifferent complexities for best case and worst case.\\nBest Case: In Quick Sort, if the number of elements is greater than 1 then they are divided into\\ntwo equal subsets, and the algorithm is recursively invoked on the subsets. After solving the sub\\nproblems we don’t need to combine them. This is because in Quick Sort they are already in\\nsorted order. But, we need to scan the complete elements to partition the elements. The recurrence\\nequation of Quick Sort best case is\\nIf we solve this recurrence using Master theorem of D & C gives O(nlogn) complexity.\\nWorst Case: In the worst case, Quick Sort divides the input elements into two sets and one of\\nthem contains only one element. That means other set has n – 1 elements to be sorted. Let us\\nassume that the input size is n and T(n) defines the solution to the given problem. So we need to\\nsolve T(n – 1), T(1) subproblems. But to divide the input into two sets Quick Sort needs one scan\\nof the input elements (this takes O(n)).\\nAfter solving these sub problems the algorithm takes only a constant time to combine these\\nsolutions. The total recurrence algorithm for this problem can be given as:\\nThis is clearly a summation recurrence equation. So, \\n.\\nNote: For the average case analysis, refer to Sorting chapter.\\nProblem-11\\u2003\\u2003Given an infinite array in which the first n cells contain integers in sorted order\\nand the rest of the cells are filled with some special symbol (say, $). Assume we do not\\nknow the n value. Give an algorithm that takes an integer K as input and finds a position in\\nthe array containing K, if such a position exists, in O(logn) time.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 714, 'file_type': 'pdf'}, page_content='Solution: Since we need an O(logn) algorithm, we should not search for all the elements of the\\ngiven list (which gives O(n) complexity). To get O(logn) complexity one possibility is to use\\nbinary search. But in the given scenario we cannot use binary search as we do not know the end\\nof the list. Our first problem is to find the end of the list. To do that, we can start at the first\\nelement and keep searching with doubled index. That means we first search at index 1 then, 2,4,8\\n...\\nIt is clear that, once we have identified a possible interval A[i,...,2i] in which K might be, its\\nlength is at most n (since we have only n numbers in the array A), so searching for K using binary\\nsearch takes O(logn) time.\\nProblem-12\\u2003\\u2003Given a sorted array of non-repeated integers A[1.. n], check whether there is an\\nindex i for which A[i] = i. Give a divide-and-conquer algorithm that runs in time O(logn).\\nSolution: We can’t use binary search on the array as it is. If we want to keep the O(logn) property\\nof the solution we have to implement our own binary search. If we modify the array (in place or\\nin a copy) and subtract i from A[i], we can then use binary search. The complexity for doing so is\\nO(n).\\nProblem-13\\u2003\\u2003We are given two sorted lists of size n. Give an algorithm for finding the median\\nelement in the union of the two lists.\\nSolution: We use the Merge Sort process. Use merge procedure of merge sort (refer to Sorting\\nchapter). Keep track of the count while comparing elements of two arrays. If the count becomes n\\n(since there are 2n elements), we have reached the median. Take the average of the elements at\\nindexes n – 1 and n in the merged array.\\nTime Complexity: O(n).\\nProblem-14\\u2003\\u2003Can we give the algorithm if the size of the two lists are not the same?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 715, 'file_type': 'pdf'}, page_content='Solution: The solution is similar to the previous problem. Let us assume that the lengths of two\\nlists are m and n. In this case we need to stop when the counter reaches (m + n)/2.\\nTime Complexity: O((m + n)/2).\\nProblem-15\\u2003\\u2003Can we improve the time complexity of Problem-13 to O(logn)?\\nSolution: Yes, using the D & C approach. Let us assume that the given two lists are L1 and L2.\\nAlgorithm:\\n1.\\nFind the medians of the given sorted input arrays L1[] and L2[]. Assume that those\\nmedians are m1 and m2.\\n2.\\nIf m1 and m2 are equal then return m1 (or m2).\\n3.\\nIf m1 is greater than m2, then the final median will be below two sub arrays.\\n4.\\nFrom first element of L1 to m1.\\n5.\\nFrom m2 to last element of L2.\\n6.\\nIf m2 is greater than m1, then median is present in one of the two sub arrays below.\\n7.\\nFrom m1 to last element of L1.\\n8.\\nFrom first element of L2 to m2.\\n9.\\nRepeat the above process until the size of both the sub arrays becomes 2.\\n10. If size of the two arrays is 2, then use the formula below to get the median.\\n11. Median = (max(L1[0],L2[0]) + min(L1[1],L2[1])/2\\nTime Complexity: O(logn) since we are considering only half of the input and throwing the\\nremaining half.\\nProblem-16\\u2003\\u2003Given an input array A. Let us assume that there can be duplicates in the list.\\nNow search for an element in the list in such a way that we get the highest index if there\\nare duplicates.\\nSolution: Refer to Searching chapter.\\nProblem-17\\u2003\\u2003Discuss Strassen’s Matrix Multiplication Algorithm using Divide and Conquer.\\nThat means, given two n × n matrices, A and B, compute the n × n matrix C = A × B, where\\nthe elements of C are given by\\nSolution: Before Strassen’s algorithm, first let us see the basic divide and conquer algorithm. The\\ngeneral approach we follow for solving this problem is given below. To determine, C[i,j] we\\nneed to multiply the ith row of A with jth column of B.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 716, 'file_type': 'pdf'}, page_content='The matrix multiplication problem can be solved with the D & C technique. To implement a D &\\nC algorithm we need to break the given problem into several subproblems that are similar to the\\noriginal one. In this instance we view each of the n × n matrices as a 2 × 2 matrix, the elements of\\nwhich are \\n submatrices. So, the original matrix multiplication, C = A × B can be written as:\\nFrom the given definition o f Ci,j, we get that the result sub matrices can be computed as follows:\\nHere the symbols + and × are taken to mean addition and multiplication (respectively) of \\nmatrices.\\nIn order to compute the original n × n matrix multiplication we must compute eight \\n matrix\\nproducts (divide) followed by four \\n matrix sums (conquer). Since matrix addition is an O(n2)\\noperation, the total running time for the multiplication operation is given by the recurrence:\\nUsing master theorem, we get T(n) = O(n3).\\nFortunately, it turns out that one of the eight matrix multiplications is redundant (found by\\nStrassen). Consider the following series of seven \\n matrices:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 717, 'file_type': 'pdf'}, page_content='Each equation above has only one multiplication. Ten additions and seven multiplications are\\nrequired to compute M0 through M6. Given M0 through M6, we can compute the elements of the\\nproduct matrix C as follows:\\nThis approach requires seven \\n matrix multiplications and 18 \\n additions. Therefore, the\\nworst-case running time is given by the following recurrence:\\nUsing master theorem, we get, \\n.\\nProblem-18\\u2003\\u2003Stock Pricing Problem: Consider the stock price of CareerMonk.com in n\\nconsecutive days. That means the input consists of an array with stock prices of the\\ncompany. We know that the stock price will not be the same on all the days. In the input\\nstock prices there may be dates where the stock is high when we can sell the current\\nholdings, and there may be days when we can buy the stock. Now our problem is to find\\nthe day on which we can buy the stock and the day on which we can sell the stock so that\\nwe can make maximum profit.\\nSolution: As given in the problem, let us assume that the input is an array with stock prices\\n[integers]. Let us say the given array is A[1],...,A[n]. From this array we have to find two days\\n[one for buy and one for sel1] in such a way that we can make maximum profit. Also, another\\npoint to make is that the buy date should be before sell date. One simple approach is to look at all\\npossible buy and sell dates.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 718, 'file_type': 'pdf'}, page_content='The two nested loops take n(n + l)/2 computations, so this takes time Θ(n2).\\nProblem-19\\u2003\\u2003For Problem-18, can we improve the time complexity?\\nSolution: Yes, by opting for the Divide-and-Conquer Θ(nlogn) solution. Divide the input list into\\ntwo parts and recursively find the solution in both the parts. Here, we get three cases:\\n•\\nbuyDatelndex and sellDatelndex both are in the earlier time period.\\n•\\nbuyDatelndex and sellDatelndex both are in the later time period.\\n•\\nbuyDatelndex is in the earlier part and sellDatelndex is in the later part of the time\\nperiod.\\nThe first two cases can be solved with recursion. The third case needs care. This is because\\nbuyDatelndex is one side and sellDatelndex is on other side. In this case we need to find the\\nminimum and maximum prices in the two sub-parts and this we can solve in linear-time.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 719, 'file_type': 'pdf'}, page_content='Algorithm StockStrategy is used recursively on two problems of half the size of the input, and in\\naddition Θ(n) time is spent searching for the maximum and minimum prices. So the time\\ncomplexity is characterized by the recurrence T(n) = 2T(n/2) + Θ(n) and by the Master theorem\\nwe get O(nlogn).\\nProblem-20\\u2003\\u2003We are testing “unbreakable” laptops and our goal is to find out how\\nunbreakable they really are. In particular, we work in an n-story building and want to find\\nout the lowest floor from which we can drop the laptop without breaking it (call this “the\\nceiling”). Suppose we are given two laptops and want to find the highest ceiling possible.\\nGive an algorithm that minimizes the number of tries we need to make f(n) (hopefully, f(n)\\nis sub-linear, as a linear f(n) yields a trivial solution).\\nSolution: For the given problem, we cannot use binary search as we cannot divide the problem\\nand solve it recursively. Let us take an example for understanding the scenario. Let us say 14 is\\nthe answer. That means we need 14 drops to find the answer. First we drop from height 14, and if\\nit breaks we try all floors from 1 to 13. If it doesn’t break then we are left 13 drops, so we will\\ndrop it from 14 + 13 + 1 = 28th floor. The reason being if it breaks at the 28th floor we can try all\\nthe floors from 15 to 27 in 12 drops (total of 14 drops). If it did not break, then we are left with\\n11 drops and we can try to figure out the floor in 14 drops.\\nFrom the above example, it can be seen that we first tried with a gap of 14 floors, and then\\nfollowed by 13 floors, then 12 and so on. So if the answer is k then we are trying the intervals at\\nk, k – 1, k – 2 ....1. Given that the number of floors is n, we have to relate these two. Since the\\nmaximum floor from which we can try is n, the total skips should be less than n. This gives:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 720, 'file_type': 'pdf'}, page_content='Complexity of this process is \\n.\\nProblem-21\\u2003\\u2003Given n numbers, check if any two are equal.\\nSolution: Refer to Searching chapter.\\nProblem-22\\u2003\\u2003Give an algorithm to find out if an integer is a square? E.g. 16 is, 15 isn’t.\\nSolution: Initially let us say i = 2. Compute the value i × i and see if it is equal to the given\\nnumber. If it is equal then we are done; otherwise increment the i vlaue. Continue this process\\nuntil we reach i × i greater than or equal to the given number.\\nTime Complexity: \\n. Space Complexity: O(1).\\nProblem-23\\u2003\\u2003Given an array of 2n integers in the following format a1 a2 a3 ...an b1 b2 b3\\n...bn. Shuffle the array to a1 b1 a2 b2 a3 b3 ... an bn without any extra memory [MA].\\nSolution: Let us take an example (for brute force solution refer to Searching chapter)\\n1.\\nStart with the array: a1 a2 a3 a4 b1 b2 b3 b4\\n2.\\nSplit the array into two halves: a1 a2 a3 a4 : b1 b2 b3 b4\\n3.\\nExchange elements around the center: exchange a3 a4 with b1 b2 you get: a1 a2 b1\\nb2 a3 a4 b3 b4\\n4.\\nSplit a1 a2 b1 b2 into a1 a2 : b1 b2 then split a3 a4 b3 b4 into a3 a4 : b3 b4\\n5.\\nExchange elements around the center for each subarray you get: a1 b1 a2 b2 and a3\\nb3 a4 b4\\nPlease note that this solution only handles the case when n = 2i where i = 0,1,2,3, etc. In our\\nexample n = 22 = 4 which makes it easy to recursively split the array into two halves. The basic\\nidea behind swapping elements around the center before calling the recursive function is to\\nproduce smaller size problems. A solution with linear time complexity may be achieved if the\\nelements are of a specific nature. For example you can calculate the new position of the element\\nusing the value of the element itself. This is a hashing technique.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 721, 'file_type': 'pdf'}, page_content='Time Complexity: O(nlogn).\\nProblem-24\\u2003\\u2003Nuts and Bolts Problem: Given a set of n nuts of different sizes and n bolts\\nsuch that there is a one-to-one correspondence between the nuts and the bolts, find for each\\nnut its corresponding bolt. Assume that we can only compare nuts to bolts (cannot compare\\nnuts to nuts and bolts to bolts).\\nSolution: Refer to Sorting chapter.\\nProblem-25\\u2003\\u2003Maximum Value Contiguous Subsequence: Given a sequence of n numbers\\nA(1) ...A(n), give an algorithm for finding a contiguous subsequence A(i) ...A(j) for which\\nthe sum of elements in the subsequence is maximum. Example : {-2, 11, -4, 13, -5, 2} →\\n20 and {1, -3, 4, -2, -1, 6 } → 7.\\nSolution: Divide this input into two halves. The maximum contiguous subsequence sum can occur\\nin one of 3 ways:\\n•\\nCase 1: It can be completely in the first half\\n•\\nCase 2: It can be completely in the second half\\n•\\nCase 3: It begins in the first half and ends in the second half\\nWe begin by looking at case 3. To avoid the nested loop that results from considering all n/2\\nstarting points and n/2 ending points independently, replace two nested loops with two\\nconsecutive loops. The consecutive loops, each of size n/2, combine to require only linear work.\\nAny contiguous subsequence that begins in the first half and ends in the second half must include\\nboth the last element of the first half and the first element of the second half. What we can do in\\ncases 1 and 2 is apply the same strategy of dividing into more halves. In summary, we do the\\nfollowing:\\n1.\\nRecursively compute the maximum contiguous subsequence that resides entirely in the\\nfirst half.\\n2.\\nRecursively compute the maximum contiguous subsequence that resides entirely in the'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 722, 'file_type': 'pdf'}, page_content='second half.\\n3.\\nCompute, via two consecutive loops, the maximum contiguous subsequence sum that\\nbegins in the first half but ends in the second half.\\n4.\\nChoose the largest of the three sums.\\nThe base case cost is 1. The program performs two recursive calls plus the linear work involved\\nin computing the maximum sum for case 3. The recurrence relation is:\\nUsing D & C Master theorem, we get the time complexity as T(n) = O(nlogn).\\nNote: For an efficient solution refer to the Dynamic Programming chapter.\\nProblem-26\\u2003\\u2003Closest-Pair of Points: Given a set of n points, S = {p1,p2,p3,…,pn}, where pi =\\n(xi,yi). Find the pair of points having the smallest distance among all pairs (assume that all\\npoints are in one dimension).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 723, 'file_type': 'pdf'}, page_content='Solution: Let us assume that we have sorted the points. Since the points are in one dimension, all\\nthe points are in a line after we sort them (either on X-axis or Y-axis). The complexity of sorting\\nis O(nlogn). After sorting we can go through them to find the consecutive points with the least\\ndifference. So the problem in one dimension is solved in O(nlogn) time which is mainly\\ndominated by sorting time.\\nTime Complexity: O(nlogn).\\nProblem-27\\u2003\\u2003For Problem-26, how do we solve it if the points are in two-dimensional space?\\nSolution: Before going to the algorithm, let us consider the following mathematical equation:\\nThe above equation calculates the distance between two points p1 = (x1,y1) and p2 = (x2,y2).\\nBrute Force Solution:\\n•\\nCalculate the distances between all the pairs of points. From n points there are \\nways of selecting 2 points. \\n.\\n•\\nAfter finding distances for all n2 possibilities, we select the one which is giving the\\nminimum distance and this takes O(n2).\\nThe overall time complexity is O(n2).\\nProblem-28\\u2003\\u2003Give O(nlogn) solution for closest pair problem (Problem-27)?\\nSolution: To find O(nlogn) solution, we can use the D & C technique. Before starting the divide-\\nand-conquer process let us assume that the points are sorted by increasing x-coordinate. Divide\\nthe points into two equal halves based on median of x-coordinates. That means the problem is\\ndivided into that of finding the closest pair in each of the two halves. For simplicity let us\\nconsider the following algorithm to understand the process.\\nAlgorithm:\\n1)\\nSort the given points in S (given set of points) based on their x –coordinates.\\nPartition S into two subsets, S1 and S2, about the line l through median of S. This\\nstep is the Divide part of the D & C technique.\\n2)\\nFind the closest-pairs in S1 andS2 and call them L and R recursively.\\n3)\\nNow, steps 4 to 8 form the Combining component of the D & C technique.\\n4)\\nLet us assume that δ = min (L,R).\\n5)\\nEliminate points that are farther than δ apart from l.\\n6)\\nConsider the remaining points and sort based on their y-coordinates.\\n7)\\nScan the remaining points in the y order and compute the distances of each point to\\nall its neighbors that are distanced no more than 2 × δ (that’s the reason for sorting'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 724, 'file_type': 'pdf'}, page_content='according to y).\\n8)\\nIf any of these distances is less than δ then update δ.\\nCombining the results in linear time'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 725, 'file_type': 'pdf'}, page_content='Let δ = min(L,R), where L is the solution to first sub problem and R is the solution to second sub\\nproblem. The possible candidates for closest-pair, which are across the dividing line, are those\\nwhich are less than δ distance from the line. So we need only the points which are inside the 2 × δ\\narea across the dividing line as shown in the figure. Now, to check all points within distance δ\\nfrom the line, consider the following figure.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 726, 'file_type': 'pdf'}, page_content='From the above diagram we can see that a maximum of 12 points can be placed inside the square\\nwith a distance not less than δ. That means, we need to check only the distances which are within\\n11 positions in the sorted list. This is similar to the one above, but with the difference that in the\\nabove combining of subproblems, there are no vertical bounds. So we can apply the 12-point box\\ntactic over all the possible boxes in the 2 × δ area with the dividing line as the middle line. As\\nthere can be a maximum of n such boxes in the area, the total time for finding the closest pair in\\nthe corridor is O(n).\\nAnalysis:\\n1)\\nStep-1 and Step-2 take O(nlogn) for sorting and recursively finding the minimum.\\n2)\\nStep-4 takes O(1).\\n3)\\nStep-5 takes O(n) for scanning and eliminating.\\n4)\\nStep-6 takes O(nlogn) for sorting.\\n5)\\nStep-7 takes O(n) for scanning.\\nThe total complexity: T(n) = O(nlogn) + O(1) + O(n) + O(n) + O(n) ≈ O(nlogn).\\nProblem-29\\u2003\\u2003To calculate kn, give algorithm and discuss its complexity.\\nSolution: The naive algorithm to compute kn is: start with 1 and multiply by k until reaching kn.\\nFor this approach; there are n – 1 multiplications and each takes constant time giving a Θ(n)\\nalgorithm.\\nBut there is a faster way to compute kn. For example,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 727, 'file_type': 'pdf'}, page_content='Note that taking the square of a number needs only one multiplication; this way, to compute 924 we\\nneed only 5 multiplications instead of 23.\\nLet T(n) be the number of multiplications required to compute kn. For simplicity, assume k = 2i\\nfor some i ≥ 1.\\nUsing master theorem we get T(n) = O(logn).\\nProblem-30\\u2003\\u2003The Skyline Problem: Given the exact locations and shapes of n rectangular\\nbuildings in a 2-dimensional city. There is no particular order for these rectangular\\nbuildings. Assume that the bottom of all buildings lie on a fixed horizontal line (bottom\\nedges are collinear). The input is a list of triples; one per building. A building Bi is\\nrepresented by the triple (li, hi, ri) where li denote the x-position of the left edge and ri\\ndenote the x-position of the right edge, and hi denotes the building’s height. Give an\\nalgorithm that computes the skyline (in 2 dimensions) of these buildings, eliminating\\nhidden lines. In the diagram below there are 8 buildings, represented from left to right by\\nthe triplets (1, 14, 7), (3, 9, 10), (5, 17, 12), (14, 11, 18), (15, 6, 27), (20, 19, 22), (23, 15,\\n30) and (26, 14, 29).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 728, 'file_type': 'pdf'}, page_content='The output is a collection of points which describe the path of the skyline. In some versions of the\\nproblem this collection of points is represented by a sequence of numbers p1; p2, ..., pn, such that\\nthe point pi represents a horizontal line drawn at height pi if i is even, and it represents a vertical\\nline drawn at position pi if i is odd. In our case the collection of points will be a sequence of p1,\\np2, ..., pn pairs of (xi, hi) where pi(xi, hi) represents the hi height of the skyline at position xi. In the\\ndiagram above the skyline is drawn with a thick line around the buildings and it is represented by\\nthe sequence of position-height pairs (1, 14), (5, 17), (12, 0), (14, 11), (18, 6), (20, 19), (22, 6),\\n(23, 15) and (30, 0). Also, assume that Ri of the right most building can be maximum of 1000.\\nThat means, the Li co-ordinate of left building can be minimum of 1 and Ri of the right most\\nbuilding can be maximum of 1000.\\nSolution: The most important piece of information is that we know that the left and right\\ncoordinates of each and every building are non-negative integers less than 1000. Now why is this\\nimportant? Because we can assign a height-value to every distinct xi coordinate where i is\\nbetween 0 and 9,999.\\nAlgorithm:\\n•\\nAllocate an array for 1000 elements and initialize all of the elements to 0. Let’s call\\nthis array auxHeights.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 729, 'file_type': 'pdf'}, page_content='•\\nIterate over all of the buildings and for every Bi building iterate on the range of [li..\\nri) where li is the left, ri is the right coordinate of the building Bi.\\n•\\nFor every xj element of this range check if hi>auxHeights[xj], that is if building Bi is\\ntaller than the current height-value at position xj. If so, replace auxHeights[xj] with\\nhi.\\nOnce we checked all the buildings, the auxHeights array stores the heights of the tallest buildings\\nat every position. There is one more thing to do: convert the auxHeights array to the expected\\noutput format, that is to a sequence of position-height pairs. It’s also easy: just map each and\\nevery i index to an (i, auxHeights[i]) pair.\\nLet’s have a look at the time complexity of this algorithm. Assume that, n indicates the number of\\nbuildings in the input sequence and m indicates the maximum coordinate (right most building ri).\\nFrom the above code, it is clear that for every new input building, we are traversing from left (li)\\nto right (ri) to update the heights. In the worst case, with n equal-size buildings, each having l = 0\\nleft and r = m – 1 right coordinates, that is every building spans over the whole [0.. m) interval.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 730, 'file_type': 'pdf'}, page_content='Thus the running time of setting the height of every position is O(n × m). The overall time-\\ncomplexity is O(n × m), which is a lot larger than O(n2) if m > n.\\nProblem-31\\u2003\\u2003Can we improve the solution of the Problem-30?\\nSolution: It would be a huge speed-up if somehow we could determine the skyline by calculating\\nthe height for those coordinates only where it matters, wouldn’t it? Intuition tells us that if we can\\ninsert a building into an existing skyline then instead of all the coordinates the building spans\\nover we only need to check the height at the left and right coordinates of the building plus those\\ncoordinates of the skyline the building overlaps with and may modify.\\nIs merging two skylines substantially different from merging a building with a skyline? The\\nanswer is, of course, No. This suggests that we use divide-and-conquer. Divide the input of n\\nbuildings into two equal sets. Compute (recursively) the skyline for each set then merge the two\\nskylines. Inserting the buildings one after the other is not the fastest way to solve this problem as\\nwe’ve seen it above. If, however, we first merge pairs of buildings into skylines, then we merge\\npairs of these skylines into bigger skylines (and not two sets of buildings), and then merge pairs\\nof these bigger skylines into even bigger ones, then - since the problem size is halved in every\\nstep -after logn steps we can compute the final skyline.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 731, 'file_type': 'pdf'}, page_content=''), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 732, 'file_type': 'pdf'}, page_content=\"vector<paircint,int>>rightSkyline = getSkylinelmid + 1, end, buildings);\\ni nt>> result = mergeSkylinesleftSkyline, rghtSkyline},\\n\\nvector<paircint,int>> mergeSkylinesvector<paircint, int>>& le'Skyline, vector<paircint,int>>& rightSkyline){\\nvector<paircint,int>> result;\\nint i= 0, j= 0, currentHeight! = 0, currentHeight2 = 0;\\n\\nwhile (i != leftSkyline.size() &é j != rightSkyline. size()) {\\nif fleftSkyline|i].first < rightSkyline[j).first) {\\n‘currentHeght! = lefikglinei second:\\nif maxH != maxjcurrentHeight1, currenth\\nresults, backiparsin, inte\\nmaxi = maxjcurentHeightl,curentHeight2);\\n\\night)\\nfirst, max(currentHeight1, currentHeight2}};\\n\\n}\\nelseif eftSkyline| frst > right Skyline. first) {\\n‘currentHeight2 = rightSkyline|j] second;\\nif maxH I* maxjcurrentHeightl,currentHeight2)\\nresult push backipaircint,int(ightSkyline] rst, maxjcurrentHeghtl,currentHeight2);\\nmax ~ max(currentHeightl, currentHeight2}\\ni\\nJelse\\nifle'Syin} second >= rghtSkyline|j second) {\\ncurrentHeight! = letSkvtnei second;\\ncurrentHeight2 = rightSkinej second\\niffmast != maxicurrentHeightl, currentHeight2)\\nresult push backipaircint,int>(eftSkylinel.frst, let Skyline}. second)};\\nmati = maxlcurrentHeight!, currentHeight2};\\n\\nre\\nJase |\\ncurrentHeight = lfeSkylinei second\\ncurrentHeight2 = rightSkylinej second;\\n\\nimaxH * maxfcurentHeight1, currentHeight2)\\n\\nresult push _backipairsint,int>(ightSkylinejfirt, rightSkylinelj}. second);\\nsaxH = max(curentHeight], eurrenteight2)\\ned\\n)\\nt\\nwhile < rghtSkyline sie) {\\nresult push_backirightSkylinei);\\n\\nits\\n\\n’\\n\\nwhile = let Skyline size) {\\nreault push backileSkylinli);\\n\\n”\\n\\n‘return result;\"), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 733, 'file_type': 'pdf'}, page_content='For example, given two skylines A=(a1, ha1, a2, ha2, ..., an, 0) and B=(b1, hb1, b2, hb2, ..., bm, 0),\\nwe merge these lists as the new list: (c1, hc1, c2, hc2, ..., cn+m, 0). Clearly, we merge the list of a’s\\nand b’s just like in the standard Merge algorithm. But, in addition to that, we have to decide on the\\ncorrect height in between these boundary values. We use two variables currentHeight1 and\\ncurrentHeight2 (note that these are the heights prior to encountering the heads of the lists) to store\\nthe current height of the first and the second skyline, respectively. When comparing the head\\nentries (currentHeight1, currentHeight2) of the two skylines, we introduce a new strip (and\\nappend to the output skyline) whose x-coordinate is the minimum of the entries’ x-coordinates and\\nwhose height is the maximum of currentHeight1 and currentHeight2. This algorithm has a\\nstructure similar to Mergesort. So the overall running time of the divide and conquer approach\\nwill be O(nlogn).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 734, 'file_type': 'pdf'}, page_content='19.1 Introduction\\nIn this chapter we will try to solve the problems for which we failed to get the optimal solutions\\nusing other techniques (say, Divide & Conquer and Greedy methods). Dynamic Programming\\n(DP) is a simple technique but it can be difficult to master. One easy way to identify and solve DP\\nproblems is by solving as many problems as possible. The term Programming is not related to\\ncoding but it is from literature, and means filling tables (similar to Linear Programming).\\n19.2 What is Dynamic Programming Strategy?\\nDynamic programming and memoization work together. The main difference between dynamic\\nprogramming and divide and conquer is that in the case of the latter, sub problems are\\nindependent, whereas in DP there can be an overlap of sub problems. By using memoization\\n[maintaining a table of sub problems already solved], dynamic programming reduces the\\nexponential complexity to polynomial complexity (O(n2), O(n3), etc.) for many problems. The\\nmajor components of DP are:\\n•\\nRecursion: Solves sub problems recursively.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 735, 'file_type': 'pdf'}, page_content='•\\nMemoization: Stores already computed values in table (Memoization means\\ncaching).\\nDynamic Programming = Recursion + Memoization\\n19.3 Properties of Dynamic Programming Strategy\\nThe two dynamic programming properties which can tell whether it can solve the given problem\\nor not are:\\n•\\nOptimal substructure: an optimal solution to a problem contains optimal solutions\\nto sub problems.\\n•\\nOverlapping sub problems: a recursive solution contains a small number of distinct\\nsub problems repeated many times.\\n19.4 Can Dynamic Programming Solve All Problems?\\nLike Greedy and Divide and Conquer techniques, DP cannot solve every problem. There are\\nproblems which cannot be solved by any algorithmic technique [Greedy, Divide and Conquer and\\nDynamic Programming].\\nThe difference between Dynamic Programming and straightforward recursion is in memoization\\nof recursive calls. If the sub problems are independent and there is no repetition then memoization\\ndoes not help, so dynamic programming is not a solution for all problems.\\n19.5 Dynamic Programming Approaches\\nBasically there are two approaches for solving DP problems:\\n•\\nBottom-up dynamic programming\\n•\\nTop-down dynamic programming\\nBottom-up Dynamic Programming\\nIn this method, we evaluate the function starting with the smallest possible input argument value\\nand then we step through possible values, slowly increasing the input argument value. While\\ncomputing the values we store all computed values in a table (memory). As larger arguments are\\nevaluated, pre-computed values for smaller arguments can be used.\\nTop-down Dynamic Programming'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 736, 'file_type': 'pdf'}, page_content='In this method, the problem is broken into sub problems; each of these sub problems is solved;\\nand the solutions remembered, in case they need to be solved. Also, we save each computed\\nvalue as the final action of the recursive function, and as the first action we check if pre-computed\\nvalue exists.\\nBottom-up versus Top-down Programming\\nIn bottom-up programming, the programmer has to select values to calculate and decide the order\\nof calculation. In this case, all sub problems that might be needed are solved in advance and then\\nused to build up solutions to larger problems. In top-down programming, the recursive structure\\nof the original code is preserved, but unnecessary recalculation is avoided. The problem is\\nbroken into sub problems, these sub problems are solved and the solutions remembered, in case\\nthey need to be solved again.\\nNote: Some problems can be solved with both the techniques and we will see examples in the\\nnext section.\\n19.6 Examples of Dynamic Programming Algorithms\\n•\\nMany string algorithms including longest common subsequence, longest increasing\\nsubsequence, longest common substring, edit distance.\\n•\\nAlgorithms on graphs can be solved efficiently: Bellman-Ford algorithm for finding\\nthe shortest distance in a graph, Floyd’s All-Pairs shortest path algorithm, etc.\\n•\\nChain matrix multiplication\\n•\\nSubset Sum\\n•\\n0/1 Knapsack\\n•\\nTravelling salesman problem, and many more\\n19.7 Understanding Dynamic Programming\\nBefore going to problems, let us understand how DP works through examples.\\nFibonacci Series\\nIn Fibonacci series, the current number is the sum of previous two numbers. The Fibonacci series\\nis defined as follows:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 737, 'file_type': 'pdf'}, page_content='The recursive implementation can be given as:\\nSolving the above recurrence gives:\\nNote: For proof, refer to Introduction chapter.\\nHow does Memoization help?\\nCalling fib(5) produces a call tree that calls the function on the same value many times:\\nfib(5)\\nfib(4) + fib(3)\\n(fib(3) + fib(2)) + (fib(2) + fib(1))\\n((fib(2) + fib(1)) + (fib(1) + fib(0))) + ((fib(1) + fib(0)) + fib(1))\\n(((fib(1) + fib(0)) + fib(1)) + (fib(1) + fib(0))) + ((fib(1) + fib(0)) + fib(1))\\nIn the above example, fib(2) was calculated three times (overlapping of subproblems). If n is big,\\nthen many more values of fib (sub problems) are recalculated, which leads to an exponential time\\nalgorithm. Instead of solving the same sub problems again and again we can store the previous\\ncalculated values and reduce the complexity.\\nMemoization works like this: Start with a recursive function and add a table that maps the\\nfunction’s parameter values to the results computed by the function. Then if this function is called\\ntwice with the same parameters, we simply look up the answer in the table.\\nImproving: Now, we see how DP reduces this problem complexity from exponential to\\npolynomial. As discussed earlier, there are two ways of doing this. One approach is bottom-up:\\nthese methods start with lower values of input and keep building the solutions for higher values.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 738, 'file_type': 'pdf'}, page_content='The other approach is top-down. In this method, we preserve the recursive calls and use the\\nvalues if they are already computed. The implementation for this is given as:\\nNote: For all problems, it may not be possible to find both top-down and bottom-up programming\\nsolutions.\\nBoth versions of the Fibonacci series implementations clearly reduce the problem complexity to\\nO(n). This is because if a value is already computed then we are not calling the subproblems\\nagain. Instead, we are directly taking its value from the table.\\nTime Complexity: O(n). Space Complexity: O(n), for table.\\nFurther Improving: One more observation from the Fibonacci series is: The current value is the\\nsum of the previous two calculations only. This indicates that we don’t have to store all the\\nprevious values. Instead, if we store just the last two values, we can calculate the current value.\\nThe implementation for this is given below:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 739, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nNote: This method may not be applicable (available) for all problems.\\nObservations\\nWhile solving the problems using DP, try to figure out the following:\\n•\\nSee how the problems are defined in terms of subproblems recursively.\\n•\\nSee if we can use some table [memoization] to avoid the repeated calculations.\\nFactorial of a Number\\nAs another example, consider the factorial problem: n! is the product of all integers between n\\nand 1. The definition of recursive factorial can be given as:\\nThis definition can easily be converted to implementation. Here the problem is finding the value\\nof n!, and the sub-problem is finding the value of (n – l)!. In the recursive case, when n is greater\\nthan 1, the function calls itself to find the value of (n – l)! and multiplies that with n. In the base\\ncase, when n is 0 or 1, the function simply returns 1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 740, 'file_type': 'pdf'}, page_content='The recurrence for the above implementation can be given as: T(n) = n × T(n – 1) ≈ O(n)\\nTime Complexity: O(n). Space Complexity: O(n), recursive calls need a stack of size n.\\nIn the above recurrence relation and implementation, for any n value, there are no repetitive\\ncalculations (no overlapping of sub problems) and the factorial function is not getting any benefits\\nwith dynamic programming. Now, let us say we want to compute a series of m! for some arbitrary\\nvalue m. Using the above algorithm, for each such call we can compute it in O(m). For example,\\nto find both n! and m! we can use the above approach, wherein the total complexity for finding n!\\nand m! is O(m + n).\\nTime Complexity: O(n + m).\\nSpace Complexity: O(max(m,n)), recursive calls need a stack of size equal to the maximum of m\\nand n.\\nImproving: Now let us see how DP reduces the complexity. From the above recursive definition\\nit can be seen that fact(n) is calculated from fact(n -1) and n and nothing else. Instead of calling\\nfact(n) every time, we can store the previous calculated values in a table and use these values to\\ncalculate a new value. This implementation can be given as:\\nFor simplicity, let us assume that we have already calculated n! and want to find m!. For finding\\nm!, we just need to see the table and use the existing entries if they are already computed. If m < n\\nthen we do not have to recalculate m!. If m > n then we can use n! and call the factorial on the\\nremaining numbers only.\\nThe above implementation clearly reduces the complexity to O(max(m,n)). This is because if the\\nfact(n) is already there, then we are not recalculating the value again. If we fill these newly\\ncomputed values, then the subsequent calls further reduce the complexity.\\nTime Complexity: O(max(m,n)). Space Complexity: O(max(m,n)) for table.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 741, 'file_type': 'pdf'}, page_content='19.8 Longest Common Subsequence\\nGiven two strings: string X of length m [X(1..m)], and string Y of length n [Y(1..n)], find the\\nlongest common subsequence: the longest sequence of characters that appear left-to-right (but not\\nnecessarily in a contiguous block) in both strings. For example, if X = “ABCBDAB” and Y =\\n“BDCABA”, the LCS(X, Y) = {“BCBA”, “BDAB”, “BCAB”}. We can see there are several\\noptimal solutions.\\nBrute Force Approach: One simple idea is to check every subsequence of X[1.. m] (m is the\\nlength of sequence X) to see if it is also a subsequence of Y[1..n] (n is the length of sequence Y).\\nChecking takes O(n) time, and there are 2m subsequences of X. The running time thus is\\nexponential O(n. 2m) and is not good for large sequences.\\nRecursive Solution: Before going to DP solution, let us form the recursive solution for this and\\nlater we can add memoization to reduce the complexity. Let’s start with some simple observations\\nabout the LCS problem. If we have two strings, say “ABCBDAB” and “BDCABA”, and if we\\ndraw lines from the letters in the first string to the corresponding letters in the second, no two\\nlines cross:\\nFrom the above observation, we can see that the current characters of X and Y may or may not\\nmatch. That means, suppose that the two first characters differ. Then it is not possible for both of\\nthem to be part of a common subsequence - one or the other (or maybe both) will have to be\\nremoved. Finally, observe that once we have decided what to do with the first characters of the\\nstrings, the remaining sub problem is again a LCS problem, on two shorter strings. Therefore we\\ncan solve it recursively.\\nThe solution to LCS should find two sequences in X and Y and let us say the starting index of\\nsequence in X is i and the starting index of sequence in Y is j. Also, assume that X[i ...m] is a\\nsubstring of X starting at character i and going until the end of X, and that Y[j ...n] is a substring of\\nY starting at character j and going until the end of Y.\\nBased on the above discussion, here we get the possibilities as described below:\\n1)\\nIf X[i] == Y[j] : 1 + LCS(i + 1,j + 1)\\n2)\\nIf X[i] ≠ Y[j]. LCS(i,j + 1) // skipping jth character of Y\\n3)\\nIf X[i] ≠ Y[j]. LCS(i + 1,j) // skipping ith character of X\\nIn the first case, if X[i] is equal to Y[j], we get a matching pair and can count it towards the total\\nlength of the LCS. Otherwise, we need to skip either ith character of X or jth character of Y and\\nfind the longest common subsequence. Now, LCS(i,j) can be defined as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 742, 'file_type': 'pdf'}, page_content='LCS has many applications. In web searching, if we find the smallest number of changes that are\\nneeded to change one word into another. A change here is an insertion, deletion or replacement of\\na single character.\\nThis is a correct solution but it is very time consuming. For example, if the two strings have no\\nmatching characters, the last line always gets executed which gives (if m == n) close to O(2n).\\nDP Solution: Adding Memoization: The problem with the recursive solution is that the same\\nsubproblems get called many different times. A subproblem consists of a call to LCS_length, with\\nthe arguments being two suffixes of X and Y, so there are exactly (i + 1)(j + 1) possible\\nsubproblems (a relatively small number). If there are nearly 2n recursive calls, some of these\\nsubproblems must be being solved over and over.\\nThe DP solution is to check, whenever we want to solve a sub problem, whether we’ve already\\ndone it before. So we look up the solution instead of solving it again. Implemented in the most\\ndirect way, we just add some code to our recursive solution. To do this, look up the code. This\\ncan be given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 743, 'file_type': 'pdf'}, page_content='First, take care of the base cases. We have created an LCS table with one row and one column\\nlarger than the lengths of the two strings. Then run the iterative DP loops to fill each cell in the\\ntable. This is like doing recursion backwards, or bottom up.\\nThe value of LCS[i][j] depends on 3 other values (LCS[i + 1][j + 1], LCS[i][j + 1] and LCS[i +\\n1][j]), all of which have larger values of i or j. They go through the table in the order of\\ndecreasing i and j values. This will guarantee that when we need to fill in the value of LCS[i][j],\\nwe already know the values of all the cells on which it depends.\\nTime Complexity: O(mn), since i takes values from 1 to m and and j takes values from 1 to n.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 744, 'file_type': 'pdf'}, page_content='Space Complexity: O(mn).\\nNote: In the above discussion, we have assumed LCS(i,j) is the length of the LCS with X[i ...m]\\nand Y[j ...n]. We can solve the problem by changing the definition as LCS(i,j) is the length of the\\nLCS with X[1 ...i] and Y[1...j].\\nPrinting the subsequence: The above algorithm can find the length of the longest common\\nsubsequence but cannot give the actual longest subsequence. To get the sequence, we trace it\\nthrough the table. Start at cell (0,0). We know that the value of LC5[0][0] was the maximum of 3\\nvalues of the neighboring cells. So we simply recompute LC5[0][0] and note which cell gave the\\nmaximum value. Then we move to that cell (it will be one of (1,1), (0,1) or (1,0)) and repeat this\\nuntil we hit the boundary of the table. Every time we pass through a cell (i,j’) where X[i] == Y[j],\\nwe have a matching pair and print X[i]. At the end, we will have printed the longest common\\nsubsequence in O(mn) time.\\nAn alternative way of getting path is to keep a separate table for each cell. This will tell us which\\ndirection we came from when computing the value of that cell. At the end, we again start at cell\\n(0,0) and follow these directions until the opposite corner of the table.\\nFrom the above examples, I hope you understood the idea behind DP. Now let us see more\\nproblems which can be easily solved using the DP technique.\\nNote: As we have seen above, in DP the main component is recursion. If we know the recurrence\\nthen converting that to code is a minimal task. For the problems below, we concentrate on getting\\nthe recurrence.\\n19.9 Dynamic Programming: Problems & Solutions\\nProblem-1\\u2003\\u2003Convert the following recurrence to code.\\nSolution: The code for the given recursive formula can be given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 745, 'file_type': 'pdf'}, page_content='Problem-2\\u2003\\u2003Can we improve the solution to Problem-1 using memoization of DP?\\nSolution: Yes. Before finding a solution, let us see how the values are calculated.\\nT(0) = T(1) = 2\\nT(2) = 2 * T(1) * T(0)\\nT(3) = 2 * T(1) * T(0) + 2 * T(2) * T(1)\\nT(4) = 2 * T(1) * T(0) + 2 * T(2) * T(1) + 2 * T(3) * T(2)\\nFrom the above calculations it is clear that there are lots of repeated calculations with the same\\ninput values. Let us use a table for avoiding these repeated calculations, and the implementation\\ncan be given as:\\nTime Complexity: O(n2), two for loops. Space Complexity: O(n), for table.\\nProblem-3\\u2003\\u2003Can we further improve the complexity of Problem-2?\\nSolution: Yes, since all sub problem calculations are dependent only on previous calculations,\\ncode can be modified as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 746, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), since only one for loop. Space Complexity: O(n).\\nProblem-4\\u2003\\u2003Maximum Value Contiguous Subsequence: Given an array of n numbers, give\\nan algorithm for finding a contiguous subsequence A(i)... A(j) for which the sum of\\nelements is maximum. Example: {-2, 11, -4, 13, -5, 2} → 20 and {1, -3, 4, -2, -1, 6} → 7\\nSolution:\\nInput: Array. A(1) ... A(n) of n numbers.\\nGoal: If there are no negative numbers, then the solution is just the sum of all elements in the\\ngiven array. If negative numbers are there, then our aim is to maximize the sum [there can be a\\nnegative number in the contiguous sum].\\nOne simple and brute force approach is to see all possible sums and select the one which has\\nmaximum value.\\nTime Complexity: O(n3). Space Complexity: O(1).\\nProblem-5\\u2003\\u2003Can we improve the complexity of Problem-4?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 747, 'file_type': 'pdf'}, page_content='Solution: Yes. One important observation is that, if we have already calculated the sum for the\\nsubsequence i,...,j – 1, then we need only one more addition to get the sum for the subsequence\\ni,...,j. But, the Problem-4 algorithm ignores this information. If we use this fact, we can get an\\nimproved algorithm with the running time O(n2).\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-6\\u2003\\u2003Can we solve Problem-4 using Dynamic Programming?\\nSolution: Yes. For simplicity, let us say, M(i) indicates maximum sum over all windows ending at\\ni.\\nTo find maximum sum we have to do one of the following and select maximum among them.\\n•\\nEither extend the old sum by adding A[i]\\n•\\nor start new window starting with one element A[i]\\nWhere, M(i – 1) + A[i] indicates the case of extending the previous sum by adding A[i] and 0\\nindicates the new window starting at A[i].'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 748, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for table.\\nProblem-7\\u2003\\u2003Is there any other way of solving Problem-4?\\nSolution: Yes. We can solve this problem without DP too (without memory). The algorithm is a\\nlittle tricky. One simple way is to look for all positive contiguous segments of the array\\n(sumEndingHere) and keep track of the maximum sum contiguous segment among all positive\\nsegments (sumSoFar). Each time we get a positive sum compare it (sumEndingHere) with\\nsumSoFar and update sumSoFar if it is greater than sumSoFar. Let us consider the following\\ncode for the above observation.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 749, 'file_type': 'pdf'}, page_content='Note: The algorithm doesn’t work if the input contains all negative numbers. It returns 0 if all\\nnumbers are negative. To overcome this, we can add an extra check before the actual\\nimplementation. The phase will look if all numbers are negative, and if they are it will return\\nmaximum of them (or smallest in terms of absolute value).\\nTime Complexity: O(n), because we are doing only one scan. Space Complexity: O(1), for table.\\nProblem-8\\u2003\\u2003In Problem-7 solution, we have assumed that M(i) indicates maximum sum over\\nall windows ending at i. Can we assume M(i) indicates maximum sum over all windows\\nstarting at i and ending at n?\\nSolution: Yes. For simplicity, let us say, M(i) indicates maximum sum over all windows starting\\nat i.\\nTo find maximum window we have to do one of the following and select maximum among them.\\n•\\nEither extend the old sum by adding A[i]\\n•\\nOr start new window starting with one element A[i]\\nWhere, M(i + 1) + A[t] indicates the case of extending the previous sum by adding A[i], and 0\\nindicates the new window starting at A[i].\\nTime Complexity: O(n). Space Complexity: O(n), for table.\\nNote: For O(nlogn) solution, refer to the Divide and Conquer chapter.\\nProblem-9\\u2003\\u2003Given a sequence of n numbers A(1) ...A(n), give an algorithm for finding a\\ncontiguous subsequence A(i) ...A(j) for which the sum of elements in the subsequence is\\nmaximum. Here the condition is we should not select two contiguous numbers.\\nSolution: Let us see how DP solves this problem. Assume that M(i) represents the maximum sum\\nfrom 1 to i numbers without selecting two contiguous numbers. While computing M(i), the\\ndecision we have to make is, whether to select the ith element or not. This gives us two\\npossibilities and based on this we can write the recursive formula as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 750, 'file_type': 'pdf'}, page_content='•\\nThe first case indicates whether we are selecting the ith element or not. If we don’t\\nselect the ith element then we have to maximize the sum using the elements 1 to i –\\n1. If ith element is selected then we should not select i – 1th element and need to\\nmaximize the sum using 1 to i – 2 elements.\\n•\\nIn the above representation, the last two cases indicate the base cases.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-10\\u2003\\u2003In Problem-9, we assumed that M(i) represents the maximum sum from 1 to i\\nnumbers without selecting two contiguous numbers. Can we solve the same problem by\\nchanging the definition as: M(i) represents the maximum sum from i to n numbers without\\nselecting two contiguous numbers?\\nSolution: Yes. Let us assume that M(i) represents the maximum sum from i to n numbers without\\nselecting two contiguous numbers:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 751, 'file_type': 'pdf'}, page_content='As similar to Problem-9 solution, we can write the recursive formula as:\\n•\\nThe first case indicates whether we are selecting the ith element or not. If we don’t\\nselect the ith element then we have to maximize the sum using the elements i + 1 to\\nn. If ith element is selected then we should not select i + 1th element need to\\nmaximize the sum using i + 2 to n elements.\\n•\\nIn the above representation, the last two cases indicate the base cases.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-11\\u2003\\u2003Given a sequence of n numbers A(1) ...A(n), give an algorithm for finding a\\ncontiguous subsequence A(i) ...A(j) for which the sum of elements in the subsequence is\\nmaximum. Here the condition is we should not select three continuous numbers.\\nSolution: Input: Array A(1) ...A(n) of n numbers.\\nAssume that M(i) represents the maximum sum from 1 to i numbers without selecting three\\ncontiguous numbers. While computing M(i), the decision we have to make is, whether to select ith\\nelement or not. This gives us the following possibilities:\\n•\\nIn the given problem the restriction is not to select three continuous numbers, but we\\ncan select two elements continuously and skip the third one. That is what the first\\ncase says in the above recursive formula. That means we are skipping A[i – 2].\\n•\\nThe other possibility is, selecting ith element and skipping second i – 1th element.\\nThis is the second case (skipping A[i – 1]).\\n•\\nThe third term defines the case of not selecting ith element and as a result we should\\nsolve the problem with i – 1 elements.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 752, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-12\\u2003\\u2003In Problem-11, we assumed that M(i) represents the maximum sum from 1 to i\\nnumbers without selecting three contiguous numbers. Can we solve the same problem by\\nchanging the definition as: M(i) represents the maximum sum from i to n numbers without\\nselecting three contiguous numbers?\\nSolution: Yes. The reasoning is very much similar. Let us see how DP solves this problem.\\nAssume that M(i) represents the maximum sum from i to n numbers without selecting three\\ncontiguous numbers.\\nWhile computing M(i), the decision we have to make is, whether to select ith element or not. This\\ngives us the following possibilities:\\n•\\nIn the given problem the restriction is to not select three continuous numbers, but we\\ncan select two elements continuously and skip the third one. That is what the first\\ncase says in the above recursive formula. That means we are skipping A[i + 2].\\n•\\nThe other possibility is, selecting ith element and skipping second i – 1th element.\\nThis is the second case (skipping A[i + 1]).\\n•\\nAnd the third case is not selecting ith element and as a result we should solve the\\nproblem with i + 1 elements.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-13\\u2003\\u2003Catalan Numbers: How many binary search trees are there with n vertices?\\nSolution: Binary Search Tree (BST) is a tree where the left subtree elements are less than the\\nroot element, and the right subtree elements are greater than the root element. This property\\nshould be satisfied at every node in the tree. The number of BSTs with n nodes is called Catalan\\nNumber and is denoted by Cn. For example, there are 2 BSTs with 2 nodes (2 choices for the\\nroot) and 5 BSTs with 3 nodes.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 753, 'file_type': 'pdf'}, page_content='Let us assume that the nodes of the tree are numbered from 1 to n. Among the nodes, we have to\\nselect some node as root, and then divide the nodes which are less than root node into left sub\\ntree, and elements greater than root node into right sub tree. Since we have already numbered the\\nvertices, let us assume that the root element we selected is ith element.\\nIf we select ith element as root then we get i – 1 elements on left sub-tree and n – i elements on\\nright sub tree. Since Cn is the Catalan number for n elements, Ci–1 represents the Catalan number\\nfor left sub tree elements (i – 1 elements) and Cn–i represents the Catalan number for right sub\\ntree elements. The two sub trees are independent of each other, so we simply multiply the two\\nnumbers. That means, the Catalan number for a fixed i value is Ci–1 × Cn–i.\\nSince there are n nodes, for i we will get n choices. The total Catalan number with n nodes can\\nbe given as:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 754, 'file_type': 'pdf'}, page_content='Time Complexity: O(4n). For proof, refer Introduction chapter.\\nProblem-14\\u2003\\u2003Can we improve the time complexity of Problem-13 using DP?\\nSolution: The recursive call Cn depends only on the numbers C0 to Cn–1 and for any value of i,\\nthere are a lot of recalculations. We will keep a table of previously computed values of Ci. If the\\nfunction CatalanNumber() is called with parameter i, and if it has already been computed before,\\nthen we can simply avoid recalculating the same subproblem.\\nThe time complexity of this implementation O(n2), because to compute CatalanNumber(n), we\\nneed to compute all of the CatalanNumber(i) values between 0 and n – 1, and each one will be\\ncomputed exactly once, in linear time.\\nIn mathematics, Catalan Number can be represented by direct equation as: \\nProblem-15\\u2003\\u2003Matrix Product Parenthesizations: Given a series of matrices: A1 × A2 × A3 ×\\n. . . × An with their dimensions, what is the best way to parenthesize them so that it\\nproduces the minimum number of total multiplications. Assume that we are using standard\\nmatrix and not Strassen’s matrix multiplication algorithm.\\nSolution: Input: Sequence of matrices A1 × A2 × A3 × . . . × An, where Ai is a Pi–1 × Pi. The\\ndimensions are given in an array P.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 755, 'file_type': 'pdf'}, page_content='Goal: Parenthesize the given matrices in such a way that it produces the optimal number of\\nmultiplications needed to compute A1 × A2 × A3 × . . . × An.\\nFor the matrix multiplication problem, there are many possibilities. This is because matrix\\nmultiplication is associative. It does not matter how we parenthesize the product, the result will\\nbe the same. As an example, for four matrices A, B, C, and D, the possibilities could be:\\n(ABC)D = (AB)(CD) = A(BCD) = A(BC)D =..\\nMultiplying (p × q) matrix with (q × r) matrix requires pqr multiplications. Each of the above\\npossibilities produces a different number of products during multiplication. To select the best\\none, we can go through each possible parenthesization (brute force), but this requires O(2n) time\\nand is very slow. Now let us use DP to improve this time complexity. Assume that, M[i,j]\\nrepresents the least number of multiplications needed to multiply Ai … Aj.\\nThe above recursive formula says that we have to find point k such that it produces the minimum\\nnumber of multiplications. After computing all possible values for k, we have to select the k value\\nwhich gives minimum value. We can use one more table (say, S[i,j]) to reconstruct the optimal\\nparenthesizations. Compute the M[i,j] and S[i,j] in a bottom-up fashion.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 756, 'file_type': 'pdf'}, page_content='How many sub problems are there? In the above formula, i can range from 1 to n and j can\\nrange from 1 to n. So there are a total of n2 subproblems, and also we are doing n – 1 such\\noperations [since the total number of operations we need for A1 × A2 ×A3 ×. . . × An ise n – 1]. So\\nthe time complexity is O(n3).\\nSpace Complexity: O(n2).\\nProblem-16\\u2003\\u2003For the Problem-15, can we use greedy method?\\nSolution: Greedy method is not an optimal way of solving this problem. Let us go through some\\ncounter example for this. As we have seen already, greedy method makes the decision that is good\\nlocally and it does not consider the future optimal solutions. In this case, if we use Greedy, then\\nwe always do the cheapest multiplication first. Sometimes it returns a parenthesization that is not\\noptimal.\\nExample: Consider A1 × A2 × A3 with dimentions 3 × 100, 100 × 2 and 2 × 2. Based on greedy\\nwe parenthesize them as: A1 × (A2 ×A3) with 100 · 2 · 2 + 3 · 100 · 2 = 1000 multiplications. But\\nthe optimal solution to this problem is: (A1 × A2) × A3 with 3 · 100 · 2 + 3 · 2 · 2 = 612'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 757, 'file_type': 'pdf'}, page_content='multiplications. ∴ we cannot use greedy for solving this problem.\\nProblem-17\\u2003\\u2003Integer Knapsack Problem [Duplicate Items Permitted]: Given n types of\\nitems, where the ith item type has an integer size si and a value vi. We need to fill a\\nknapsack of total capacity C with items of maximum value. We can add multiple items of\\nthe same type to the knapsack.\\nNote: For Fractional Knapsack problem refer to Greedy Algorithms chapter.\\nSolution: Input: n types of items where ith type item has the size si and value vi. Also, assume\\ninfinite number of items for each item type.\\nGoal: Fill the knapsack with capacity C by using n types of items and with maximum value.\\nOne important note is that it’s not compulsory to fill the knapsack completely. That means, filling\\nthe knapsack completely [of size C] if we get a value V and without filling the knapsack\\ncompletely [1et us say C – 1] with value U and if V < U then we consider the second one. In this\\ncase, we are basically filling the knapsack of size C – 1. If we get the same situation for C – 1\\nalso, then we try to fill the knapsack with C – 2 size and get the maximum value.\\nLet us say M(j) denotes the maximum value we can pack into a j size knapsack. We can express\\nM(j) recursively in terms of solutions to sub problems as follows:\\nFor this problem the decision depends on whether we select a particular ith item or not for a\\nknapsack of size j.\\n•\\nIf we select ith item, then we add its value vi to the optimal solution and decrease the\\nsize of the knapsack to be solved to j – si.\\n•\\nIf we do not select the item then check whether we can get a better solution for the\\nknapsack of size j – 1.\\nThe value of M(C) will contain the value of the optimal solution. We can find the list of items in\\nthe optimal solution by maintaining and following “back pointers”.\\nTime Complexity: Finding each M(j) value will require Θ(n) time, and we need to sequentially\\ncompute C such values. Therefore, total running time is Θ(nC).\\nSpace Complexity: Θ(C).\\nProblem-18\\u2003\\u20030-1 Knapsack Problem: For Problem-17, how do we solve it if the items are\\nnot duplicated (not having an infinite number of items for each type, and each item is\\nallowed to be used for 0 or 1 time)?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 758, 'file_type': 'pdf'}, page_content='Real-time example: Suppose we are going by flight, and we know that there is a\\nlimitation on the luggage weight. Also, the items which we are carrying can be of different\\ntypes (like laptops, etc.). In this case, our objective is to select the items with maximum\\nvalue. That means, we need to tell the customs officer to select the items which have more\\nweight and less value (profit).\\nSolution: Input is a set of n items with sizes si and values vi and a Knapsack of size C which we\\nneed to fill with a subset of items from the given set. Let us try to find the recursive formula for\\nthis problem using DP. Let M(i,j) represent the optimal value we can get for filling up a knapsack\\nof size j with items 1... i. The recursive formula can be given as:\\nTime Complexity: O(nC), since there are nC subproblems to be solved and each of them takes\\nO(1) to compute. Space Complexity: O(nC), where as Integer Knapsack takes only O(C).\\nNow let us consider the following diagram which helps us in reconstructing the optimal solution\\nand also gives further understanding. Size of below matrix is M.\\nSince i takes values from 1 ...n and j takes values from 1... C, there are a total of nC subproblems.\\nNow let us see what the above formula says:\\n•\\nM(i – 1,j): Indicates the case of not selecting the ith item. In this case, since we are\\nnot adding any size to the knapsack we have to use the same knapsack size for\\nsubproblems but excluding the ith item. The remaining items are i – 1.\\n•\\nM(i – 1,j – si) + vi indicates the case where we have selected the ith item. If we add'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 759, 'file_type': 'pdf'}, page_content='the ith item then we have to reduce the subproblem knapsack size to j – si and at the\\nsame time we need to add the value vi to the optimal solution. The remaining items\\nare i – 1.\\nNow, after finding all M(i,j) values, the optimal objective value can be obtained as:\\nMaxj{M(n,j)}\\nThis is because we do not know what amount of capacity gives the best solution.\\nIn order to compute some value M(i,j), we take the maximum of M(i – 1,j) and M(i – 1,j – si) + vi.\\nThese two values (M(i,j) and M(i – 1,j – si)) appear in the previous row and also in some\\nprevious columns. So, M(i,j) can be computed just by looking at two values in the previous row\\nin the table.\\nProblem-19 Making Change: Given n types of coin denominations of values v1 < v2 <...< vn\\n(integers). Assume v1 = 1, so that we can always make change for any amount of money C.\\nGive an algorithm which makes change for an amount of money C with as few coins as\\npossible.\\nSolution:\\nThis problem is identical to the Integer Knapsack problem. In our problem, we have coin\\ndenominations, each of value vi. We can construct an instance of a Knapsack problem for each\\nitem that has a sizes si, which is equal to the value of vi coin denomination. In the Knapsack we\\ncan give the value of every item as –1.\\nNow it is easy to understand an optimal way to make money C with the fewest coins is\\ncompletely equivalent to the optimal way to fill the Knapsack of size C. This is because since\\nevery value has a value of –1, and the Knapsack algorithm uses as few items as possible which\\ncorrespond to as few coins as possible.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 760, 'file_type': 'pdf'}, page_content='Let us try formulating the recurrence. Let M(j) indicate the minimum number of coins required to\\nmake change for the amount of money equal to j.\\nM(j) = Mini{M(j – vj)} + 1\\nWhat this says is, if coin denomination i was the last denomination coin added to the solution,\\nthen the optimal way to finish the solution with that one is to optimally make change for the\\namount of money j – vi and then add one extra coin of value vi.\\nTime Complexity: O(nC). Since we are solving C sub-problems and each of them requires\\nminimization of n terms. Space Complexity: O(nC).\\nProblem-20\\u2003\\u2003Longest Increasing Subsequence: Given a sequence of n numbers A1 . . . An,\\ndetermine a subsequence (not necessarily contiguous) of maximum length in which the\\nvalues in the subsequence form a strictly increasing sequence.\\nSolution:\\nInput: Sequence of n numbers A1 . . . An.\\nGoal: To find a subsequence that is just a subset of elements and does not happen to be\\ncontiguous. But the elements in the subsequence should form a strictly increasing sequence and at\\nthe same time the subsequence should contain as many elements as possible.\\nFor example, if the sequence is (5,6,2,3,4,1.9,9,8,9,5), then (5,6), (3,5), (1,8,9) are all increasing\\nsub-sequences. The longest one of them is (2,3,4,8,9), and we want an algorithm for finding it.\\nFirst, let us concentrate on the algorithm for finding the longest subsequence. Later, we can try\\nprinting the sequence itself by tracing the table. Our first step is finding the recursive formula.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 761, 'file_type': 'pdf'}, page_content='First, let us create the base conditions. If there is only one element in the input sequence then we\\ndon’t have to solve the problem and we just need to return that element. For any sequence we can\\nstart with the first element (A[1]). Since we know the first number in the LIS, let’s find the second\\nnumber (A[2]). If A[2] is larger than A[1] then include A[2] also. Otherwise, we are done - the LIS\\nis the one element sequence(A[1]).\\nNow, let us generalize the discussion and decide about ith element. Let L(i) represent the optimal\\nsubsequence which is starting at position A[1] and ending at A[i]. The optimal way to obtain a\\nstrictly increasing subsequence ending at position i is to extend some subsequence starting at\\nsome earlier position j. For this the recursive formula can be written as:\\nL(i) = Maxj < i and A [j] < A [i]{L(j)} + 1\\nThe above recurrence says that we have to select some earlier position j which gives the\\nmaximum sequence. The 1 in the recursive formula indicates the addition of ith element.\\nNow after finding the maximum sequence for all positions we have to select the one among all\\npositions which gives the maximum sequence and it is defined as:\\nMaxi{L(i)}'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 762, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2), since two for loops. Space Complexity: O(n), for table.\\nProblem-21\\u2003\\u2003Longest Increasing Subsequence: In Problem-20, we assumed that L(i)\\nrepresents the optimal subsequence which is starting at position A[1] and ending at A[i].\\nNow, let us change the definition of L(i) as: L(i) represents the optimal subsequence which\\nis starting at position A[i] and ending at A[n]. With this approach can we solve the\\nproblem?\\nSolution: Yes.\\nLet L(i) represent the optimal subsequence which is starting at position A[i] and ending at A[n].\\nThe optimal way to obtain a strictly increasing subsequence starting at position i is going to be to\\nextend some subsequence starting at some later position j. For this the recursive formula can be\\nwritten as:\\nL(i) = Maxj < i and A [j] < A [i]{L(j)} + 1\\nWe have to select some later position j which gives the maximum sequence. The 1 in the recursive\\nformula is the addition of ith element. After finding the maximum sequence for all positions select'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 763, 'file_type': 'pdf'}, page_content='the one among all positions which gives the maximum sequence and it is defined as:\\nMaxi{L(i)}\\nTime Complexity: O(n2) since two nested for loops. Space Complexity: O(n), for table.\\nProblem-22\\u2003\\u2003Is there an alternative way of solving Problem-21?\\nSolution: Yes. The other method is to sort the given sequence and save it into another array and\\nthen take out the “Longest Common Subsequence” (LCS) of the two arrays. This method has a\\ncomplexity of O(n2). For LCS problem refer theory section of this chapter.\\nProblem-23\\u2003\\u2003Box Stacking: Assume that we are given a set of n rectangular 3 – D boxes. The\\ndimensions of ith box are height hi, width wi and depth di. Now we want to create a stack\\nof boxes which is as tall as possible, but we can only stack a box on top of another box if\\nthe dimensions of the 2 –D base of the lower box are each strictly larger than those of the 2\\n–D base of the higher box. We can rotate a box so that any side functions as its base. It is\\npossible to use multiple instances of the same type of box.\\nSolution: Box stacking problem can be reduced to LIS [Problem-21.\\nInput: n boxes where ith with height hi, width wi and depth di. For all n boxes we have to\\nconsider all the orientations with respect to rotation. That is, if we have, in the original set, a box\\nwith dimensions 1 × 2 × 3, then we consider 3 boxes,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 764, 'file_type': 'pdf'}, page_content='This simplification allows us to forget about the rotations of the boxes and we just focus on the\\nstacking of n boxes with each height as hi and a base area of (wi x di). Also assume that wi ≤ di.\\nNow what we do is, make a stack of boxes that is as tall as possible and has maximum height. We\\nallow a box i on top of box j only if box i is smaller than box j in both the dimensions. That\\nmeans, if wi < wj && di < dj. Now let us solve this using DP. First select the boxes in the order\\nof decreasing base area.\\nNow, let us say H(j) represents the tallest stack of boxes with box j on top. This is very similar to\\nthe LIS problem because the stack of n boxes with ending box j is equal to finding a subsequence\\nwith the first j boxes due to the sorting by decreasing base area. The order of the boxes on the\\nstack is going to be equal to the order of the sequence.\\nNow we can write H(j) recursively. In order to form a stack which ends on box j, we need to\\nextend a previous stack ending at i. That means, we need to put j box at the top of the stack [i box\\nis the current top of the stack]. To put j box at the top of the stack we should satisfy the condition\\nwi > wj and di > dj [this ensures that the low level box has more base than the boxes above it].\\nBased on this logic, we can write the recursive formula as:\\nSimilar to the LIS problem, at the end we have to select the best j over all potential values. This\\nis because we are not sure which box might end up on top.\\nMaxj{H(j)}\\nTime Complexity: O(n2).\\nProblem-24\\u2003\\u2003Building Bridges in India: Consider a very long, straight river which moves\\nfrom north to south. Assume there are n cities on both sides of the river: n cities on the left\\nof the river and n cities on the right side of the river. Also, assume that these cities are\\nnumbered from 1 to n but the order is not known. Now we want to connect as many left-'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 765, 'file_type': 'pdf'}, page_content='right pairs of cities as possible with bridges such that no two bridges cross. When\\nconnecting cities, we can only connect city i on the left side to city i on the right side.\\nSolution:\\nInput: Two pairs of sets with each numbered from 1 to n.\\nGoal: Construct as many bridges as possible without any crosses between left side cities to right\\nside cities of the river.\\nTo understand better let us consider the diagram below. In the diagram it can be seen that there\\nare n cities on the left side of river and n cities on the right side of river. Also, note that we are\\nconnecting the cities which have the same number [a requirement in the problem]. Our goal is to\\nconnect the maximum cities on the left side of river to cities on the right side of the river, without\\nany cross edges. Just to make it simple, let us sort the cities on one side of the river.\\nIf we observe carefully, since the cities on the left side are already sorted, the problem can be\\nsimplified to finding the maximum increasing sequence. That means we have to use the LIS\\nsolution for finding the maximum increasing sequence on the right side cities of the river.\\nTime Complexity: O(n2), (same as LIS).\\nProblem-25\\u2003\\u2003Subset Sum: Given a sequence of n positive numbers A1 . . . An, give an\\nalgorithm which checks whether there exists a subset of A whose sum of all numbers is T?\\nSolution: This is a variation of the Knapsack problem. As an example, consider the following\\narray:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 766, 'file_type': 'pdf'}, page_content='A = [3,2,4,19,3,7,13,10,6,11]\\nSuppose we want to check whether there is any subset whose sum is 17. The answer is yes,\\nbecause the sum of 4 + 13 = 17 and therefore {4,13} is such a subset.\\nLet us try solving this problem using DP. We will define n × T matrix, where n is the number of\\nelements in our input array and T is the sum we want to check.\\nLet, M[i,j] = 1 if it is possible to find a subset of the numbers 1 through i that produce sum/ and\\nM[i,j] = 0 otherwise.\\nM[i, j] = Max(M[i – 1,j], M[i – 1, j – Ai])\\nAccording to the above recursive formula similar to the Knapsack problem, we check if we can\\nget the sum j by not including the element i in our subset, and we check if we can get the sum j by\\nincluding i and checking if the sum j – Ai exists without the ith element. This is identical to\\nKnapsack, except that we are storing 0/1’s instead of values. In the below implementation we can\\nuse binary OR operation to get the maximum among M[i – 1,j] and M[i – 1,j – Ai].\\nHow many subproblems are there? In the above formula, i can range from 1 to n and j can range\\nfrom l to T. There are a total of nT subproblems and each one takes O(1). So the time complexity\\nis O(nT) and this is not polynomial as the running time depends on two variables [n and T], and\\nwe can see that they are anexponential function of the other.\\nSpace Complexity: O(nT).\\nProblem-26\\u2003\\u2003Given a set of n integers and the sum of all numbers is at most if. Find the subset\\nof these n elements whose sum is exactly half of the total sum of n numbers.\\nSolution: Assume that the numbers are A1 . . . An. Let us use DP to solve this problem. We will'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 767, 'file_type': 'pdf'}, page_content='create a boolean array T with size equal to K + 1. Assume that T[x] is 1 if there exists a subset of\\ngiven n elements whose sum is x. That means, after the algorithm finishes, T[K] will be 1, if and\\nonly if there is a subset of the numbers that has sum K. Once we have that value then we just need\\nto return T[K/2]. If it is 1, then there is a subset that adds up to half the total sum.\\nInitially we set all values of T to 0. Then we set T[0] to 1. This is because we can always build 0\\nby taking an empty set. If we have no numbers in A, then we are done! Otherwise, we pick the first\\nnumber, A[0]. We can either throw it away or take it into our subset. This means that the new T[]\\nshould have T[0] and T[A[0]] set to 1. This creates the base case. We continue by taking the next\\nelement of A.\\nSuppose that we have already taken care of the first i – 1 elements of A. Now we take A[i] and\\nlook at our table T[]. After processing i – 1 elements, the array T has a 1 in every location that\\ncorresponds to a sum that we can make from the numbers we have already processed. Now we\\nadd the new number, A[i]. What should the table look like? First of all, we can simply ignore\\nA[i]. That means, no one should disappear from T[] - we can still make all those sums. Now\\nconsider some location of T[j] that has a 1 in it. It corresponds to some subset of the previous\\nnumbers that add up to j. If we add A[i] to that subset, we will get a new subset with total sum j +\\nA[i]. So we should set T[j + A[i]] to 1 as well. That’s all. Based on the above discussion, we can\\nwrite the algorithm as:\\nIn the above code, j loop moves from right to left. This reduces the double counting problem. That\\nmeans, if we move from left to right, then we may do the repeated calculations.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 768, 'file_type': 'pdf'}, page_content='Time Complexity: O(nK), for the two for loops. Space Complexity: O(K), for the boolean table T.\\nProblem-27\\u2003\\u2003Can we improve the performance of Problem-26?\\nSolution: Yes. In the above code what we are doing is, the inner j loop is starting from K and\\nmoving left. That means, it is unnecessarily scanning the whole table every time.\\nWhat we actually want is to find all the 1 entries. At the beginning, only the 0th entry is 1. If we\\nkeep the location of the rightmost 1 entry in a variable, we can always start at that spot and go left\\ninstead of starting at the right end of the table.\\nTo take full advantage of this, we can sort A[] first. That way, the rightmost 1 entry will move to\\nthe right as slowly as possible. Finally, we don’t really care about what happens in the right half\\nof the table (after T[K/2]) because if T[x] is 1, then T[Kx] must also be 1 eventually – it\\ncorresponds to the complement of the subset that gave us x. The code based on above discussion\\nis given below.\\nAfter the improvements, the time complexity is still O(nK), but we have removed some useless\\nsteps.\\nProblem-28\\u2003\\u2003Partition partition problem is to determine whether a given set can be\\npartitioned into two subsets such that the sum of elements in both subsets is the same [the\\nsame as the previous problem but a different way of asking]. For example, if A[] = {1, 5,'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 769, 'file_type': 'pdf'}, page_content='11, 5}, the array can be partitioned as {1, 5, 5} and {11}. Similarly, if A[] = {1, 5, 3}, the\\narray cannot be partitioned into equal sum sets.\\nSolution: Let us try solving this problem another way. Following are the two main steps to solve\\nthis problem:\\n1.\\nCalculate the sum of the array. If the sum is odd, there cannot be two subsets with an\\nequal sum, so return false.\\n2.\\nIf the sum of the array elements is even, calculate sum/2 and find a subset of the array\\nwith a sum equal to sum/2.\\nThe first step is simple. The second step is crucial, and it can be solved either using recursion or\\nDynamic Programming.\\nRecursive Solution: Following is the recursive property of the second step mentioned above. Let\\nsubsetSum(A, n, sum/2) be the function that returns true if there is a subset of A[0..n-1] with sum\\nequal to sum/2. The isSubsetSum problem can be divided into two sub problems:\\na)\\nisSubsetSum() without considering last element (reducing n to n – 1)\\nb)\\nisSubsetSum considering the last element (reducing sum/2 by A[n-1] and n to n – 1)\\nIf any of the above sub problems return true, then return true.\\nsubsetSum (A,n,sum/2) = isSubsetSum (A,n – 1,sum/2) \\\\\\\\ subsetSum (A,n – 1,sum/2 – A[n – 1])'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 770, 'file_type': 'pdf'}, page_content='Time Complexity: O(2n) In worst case, this solution tries two possibilities (whether to include or\\nexclude) for every element.\\nDynamic Programming Solution: The problem can be solved using dynamic programming when\\nthe sum of the elements is not too big. We can create a 2D array part[][] of size (sum/2)*(n + 1).\\nAnd we can construct the solution in a bottom-up manner such that every filled entry has a\\nfollowing property\\npart [i][j] = true if a subset of {A[0],A[1],..A[j – 1]} has sum equal to sum/2, otherwise false'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 771, 'file_type': 'pdf'}, page_content='Time Complexity: O(sum × n). Space Complexity: O(sum × n). Please note that this solution will\\nnot be feasible for arrays with a big sum.\\nProblem-29\\u2003\\u2003Counting Boolean Parenthesizations: Let us assume that we are given a\\nboolean expression consisting of symbols ‘true’, ‘false’, ‘and’, ‘or’, and ‘xor’. Find the\\nnumber of ways to parenthesize the expression such that it will evaluate to true. For\\nexample, there is only 1 way to parenthesize ‘true and false xor true’ such that it\\nevaluates to true.\\nSolution: Let the number of symbols be n and between symbols there are boolean operators like\\nand, or, xor, etc. For example, if n = 4, T or F and T xor F. Our goal is to count the numbers of\\nways to parenthesize the expression with boolean operators so that it evaluates to true. In the\\nabove case, if we use T or ( (F and T) xor F) then it evaluates to true.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 772, 'file_type': 'pdf'}, page_content='T or{ (F and T)xor F) = True\\nNow let us see how DP solves this problem. Let T(i,j) represent the number of ways to\\nparenthesize the sub expression with symbols i ...j [symbols means only T and F and not the\\noperators] with boolean operators so that it evaluates to true. Also, i and j take the values from 1\\nto n. For example, in the above case, T(2,4) = 0 because there is no way to parenthesize the\\nexpression F and T xor F to make it true.\\nJust for simplicity and similarity, let F(i,j) represent the number of ways to parenthesize the sub\\nexpression with symbols i ...j with boolean operators so that it evaluates to false. The base cases\\nare T(i,i) and F(i,i).\\nNow we are going to compute T(i, i + 1) and F(i, i + 1) for all values of i. Similarly, T(i, i + 2)\\nand F(i, i + 2) for all values of i and so on. Now let’s generalize the solution.\\nWhat this above recursive formula says is, T(i,j) indicates the number of ways to parenthesize the\\nexpression. Let us assume that we have some sub problems which are ending at k. Then the total\\nnumber of ways to parenthesize from i to j is the sum of counts of parenthesizing from i to k and\\nfrom k + 1 to j. To parenthesize between k and k + 1 there are three ways: “and”, “or” and\\n“xor”.\\n•\\nIf we use “and” between k and k + 1, then the final expression becomes true only\\nwhen both are true. If both are true then we can include them to get the final count.\\n•\\nIf we use “or”, then if at least one of them is true, the result becomes true. Instead\\nof including all three possibilities for “or”, we are giving one alternative where we\\nare subtracting the “false” cases from total possibilities.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 773, 'file_type': 'pdf'}, page_content='•\\nThe same is the case with “xor”. The conversation is as in the above two cases.\\nAfter finding all the values we have to select the value of k, which produces the maximum count,\\nand for k there are i to j – 1 possibilities.\\nHow many subproblems are there? In the above formula, i can range from 1 to n, and j can\\nrange from 1 to n. So there are a total of n2 subproblems, and also we are doing summation for all\\nsuch values. So the time complexity is O(n3).\\nProblem-30\\u2003\\u2003Optimal Binary Search Trees: Given a set of n (sorted) keys A[1..n], build the\\nbest binary search tree for the elements of A. Also assume that each element is associated\\nwith frequency which indicates the number of times that a particular item is searched in the\\nbinary search trees. That means we need to construct a binary search tree so that the total\\nsearch time will be reduced.\\nSolution: Before solving the problem let us understand the problem with an example. Let us\\nassume that the given array is A = [3,12,21,32,35]. There are many ways to represent these\\nelements, two of which are listed below.\\nOf the two, which representation is better? The search time for an element depends on the\\ndepth of the node. The average number of comparisons for the first tree is: \\n and\\nfor the second tree, the average number of comparisons is: \\n. Of the two, the first\\ntree gives better results.\\nIf frequencies are not given and if we want to search all elements, then the above simple'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 774, 'file_type': 'pdf'}, page_content='calculation is enough for deciding the best tree. If the frequencies are given, then the selection\\ndepends on the frequencies of the elements and also the depth of the elements. For simplicity let\\nus assume that the given array is A and the corresponding frequencies are in array F. F[i] indicates\\nthe frequency of ith element A[i]. With this, the total search time S(root) of the tree with root can\\nbe defined as:\\nIn the above expression, depth(root, i) + 1 indicates the number of comparisons for searching the\\nith element. Since we are trying to create a binary search tree, the left subtree elements are less\\nthan root element and the right subtree elements are greater than root element. If we separate the\\nleft subtree time and right subtree time, then the above expression can be written as:\\nIf we replace the left subtree and right subtree times with their corresponding recursive calls, then\\nthe expression becomes:\\nBinary Search Tree node declaration\\nRefer to Trees chapter.\\nImplementation:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 775, 'file_type': 'pdf'}, page_content='Problem-31\\u2003\\u2003Edit Distance: Given two strings A of length m and B of length n, transform A\\ninto B with a minimum number of operations of the following types: delete a character\\nfrom A, insert a character into A, or change some character in A into a new character. The\\nminimal number of such operations required to transform A into B is called the edit\\ndistance between A and B.\\nSolution:\\nInput: Two text strings A of length m and B of length n.\\nGoal: Convert string A into B with minimal conversions.\\nBefore going to a solution, let us consider the possible operations for converting string A into B.\\n•\\nIf m > n, we need to remove some characters of A\\n•\\nIf m == n, we may need to convert some characters of A\\n•\\nIf m < n, we need to remove some characters from A\\nSo the operations we need are the insertion of a character, the replacement of a character and the\\ndeletion of a character, and their corresponding cost codes are defined below.\\nCosts of operations:\\nInsertion of a character\\nci\\nReplacement of a character\\ncr'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 776, 'file_type': 'pdf'}, page_content='Deletion of a character\\ncd\\nNow let us concentrate on the recursive formulation of the problem. Let, T(i,j) represents the\\nminimum cost required to transform first i characters of A to first; characters of B. That means,\\nA[1... i] to B[1...j].\\nBased on the above discussion we have the following cases.\\n•\\nIf we delete ith character from A, then we have to convert remaining i – 1 characters\\nof A to j characters of B\\n•\\nIf we insert ith character in A, then convert these i characters of A to j – 1 characters\\nof B\\n•\\nIf A[i] == B[j], then we have to convert the remaining i – 1 characters of A to j – 1\\ncharacters of B\\n•\\nIf A[i] ≠ B[j], then we have to replace ith character of A to jth character of B and\\nconvert remaining i – 1 characters of A to j – 1 characters of B\\nAfter calculating all the possibilities we have to select the one which gives the lowest cost.\\nHow many subproblems are there? In the above formula, i can range from l to m and j can range\\nfrom 1 to n. This gives mn subproblems and each one takes O(1) and the time complexity is\\nO(mn). Space Complexity: O(mn) where m is number of rows and n is number of columns in the\\ngiven matrix.\\nProblem-32\\u2003\\u2003All Pairs Shortest Path Problem: Floyd’s Algorithm: Given a weighted\\ndirected graph G = (V,E), where V = {1,2,...,n}. Find the shortest path between any pair of\\nnodes in the graph. Assume the weights are represented in the matrix C[V][V], where C[i]\\n[j] indicates the weight (or cost) between the nodes i and j. Also, C[i][j] = ∞ or -1 if there\\nis no path from node i to node j.\\nSolution: Let us try to find the DP solution (Floyd’s algorithm) for this problem. The Floyd’s\\nalgorithm for all pairs shortest path problem uses matrix A[1. .n][1..n] to compute the lengths of\\nthe shortest paths. Initially,\\nFrom the definition, C[i,j] = ∞ if there is no path from i to j. The algorithm makes n passes over\\nA. Let A0,A1, ...,An be the values of A on the n passes, with A0 being the initial value.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 777, 'file_type': 'pdf'}, page_content='Just after the k– 1th iteration, Ak–1[i,j] = smallest length of any path from vertex i to vertex j that\\ndoes not pass through the vertices {k + 1, k + 2,.... n}. That means, it passes through the vertices\\npossibly through {1,2,3,..., k – 1}.\\nIn each iteration, the value A[i][j] is updated with minimum of Ak–1[i,j] and Ak–1[i, k] + Ak–1[k,j].\\nThe kth pass explores whether the vertex k lies on an optimal path from i to j, for all i,j. The same\\nis shown in the diagram below.\\nTime Complexity: O(n3).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 778, 'file_type': 'pdf'}, page_content='Problem-33\\u2003\\u2003Optimal Strategy for a Game: Consider a row of n coins of values v1 ... vn,\\nwhere n is even [since it’s a two player game]. We play this game with the opponent. In\\neach turn, a player selects either the first or last coin from the row, removes it from the\\nrow permanently, and receives the value of the coin. Determine the maximum possible\\namount of money we can definitely win if we move first.\\nAlternative way of framing the question: Given n pots, each with some number of gold\\ncoins, are arranged in a line. You are playing a game against another player. You take turns\\npicking a pot of gold. You may pick a pot from either end of the line, remove the pot, and\\nkeep the gold pieces. The player with the most gold at the end wins. Develop a strategy for\\nplaying this game.\\nSolution: Let us solve the problem using our DP technique. For each turn either we or our\\nopponent selects the coin only from the ends of the row. Let us define the subproblems as:\\nV(i,j): denotes the maximum possible value we can definitely win if it is our turn and the only\\ncoins remaining are vi ... vj.\\nBase Cases: V(i,i),V(i, i + 1) for all values of i.\\nFrom these values, we can compute V(i, i + 2),V(i,i + 3) and so on. Now let us define V(i,j) for\\neach sub problem as:\\nIn the recursive call we have to focus on ith coin to jth coin (vi... vj). Since it is our turn to pick the\\ncoin, we have two possibilities: either we can pick vi or vj. The first term indicates the case if we\\nselect ith coin (vi) and the second term indicates the case if we select jth coin (vj). The outer Max\\nindicates that we have to select the coin which gives maximum value. Now let us focus on the\\nterms:\\n•\\nSelecting ith coin: If we select the ith coin then the remaining range is from i + 1 to j.\\nSince we selected the ith coin we get the value vi for that. From the remaining range'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 779, 'file_type': 'pdf'}, page_content='i + 1 to j, the opponents can select either i + 1th coin or jth coin. But the opoonents\\nselection should be minimized as much as possible [the Min term]. The same is\\ndescribed in the below figure.\\n•\\nSelecting the jth coin: Here also the argument is the same as above. If we select the\\njth coin, then the remaining range is fromitoj-1. Since we selected the jth coin we get\\nthe value vj for that. From the remaining range i to j - 1, the opponent can select\\neither the ith coin or the j – 1th coin. But the opponent’s selection should be\\nminimized as much as possible [the Min term].\\nHow many subproblems are there? In the above formula, i can range from 1 to n and j can range\\nfrom 1 to n. There are a total of n2 subproblems and each takes O(1) and the total time complexity\\nis O(n2).\\nProblem-34\\u2003\\u2003Tiling: Assume that we use dominoes measuring 2 × 1 to tile an infinite strip of\\nheight 2. How many ways can one tile a 2 × n strip of square cells with 1x2 dominoes?\\nSolution: Notice that we can place tiles either vertically or horizontally. For placing vertical\\ntiles, we need a gap of at least 2 × 2. For placing horizontal tiles, we need a gap of 2 × 1. In this\\nmanner, the problem is reduced to finding the number of ways to partition n using the numbers 1\\nand 2 with order considered relevant [1]. For example: 11 = 1 + 2 + 2+1+2 + 2 + 1.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 780, 'file_type': 'pdf'}, page_content='If we have to find such arrangements for 12, we can either place a 1 at the end or we can add 2 in\\nthe arrangements possible with 10. Similarly, let us say we have Fn possible arrangements for n.\\nThen for (n + 1), we can either place just 1 at the end or we can find possible arrangements for (n\\n– 1) and put a 2 at the end. Going by the above theory:\\nLet’s verify the above theory for our original problem:\\n•\\nIn how many ways can we fill a 2 × 1 strip: 1 → Only one vertical tile.\\n•\\nIn how many ways can we fill a 2 × 2 strip: 2 → Either 2 horizontal or 2 vertical\\ntiles.\\n•\\nIn how many ways can we fill a 2 × 3 strip: 3 → Either put a vertical tile in the 2\\nsolutions possible for a 2 × 2 strip, or put 2 horizontal tiles in the only solution\\npossible for a 2 × 1 strip. (2 + 1 = 3).\\n•\\nSimilarly, in how many ways can we fill a 2 × n strip: Either put a vertical tile in the\\nsolutions possible for 2 X (n – 1) strip or put 2 horizontal tiles in the solution\\npossible for a 2 × (n – 2) strip. (Fn–1 + Fn–2).\\n•\\nThat’s how we verified that our final solution is: Fn = Fn–1 + Fn–2 with F1 = 1 and\\nF2 = 2.\\nProblem-35\\u2003\\u2003Longest Palindrome Subsequence: A sequence is a palindrome if it reads the\\nsame whether we read it left to right or right to left. For example A, C, G, G, G, G,C,A.\\nGiven a sequence of length n, devise an algorithm to output the length of the longest\\npalindrome subsequence. For example, the string A,G,C,T,C,B,M,A,A,C,T,G,G,A,M has\\nmany palindromes as subsequences, for instance: A,G,T,C,M,C,T,G,A has length 9.\\nSolution: Let us use DP to solve this problem. If we look at the sub-string A[i,..,j] of the string A,\\nthen we can find a palindrome sequence of length at least 2 if A[i] == A[j]. If they are not the\\nsame, then we have to find the maximum length palindrome in subsequences A[i + 1,..., j] and\\nA[i,..., j – 1].\\nAlso, every character A[i] is a palindrome of length 1. Therefore the base cases are given by A[i,\\ni] = 1. Let us define the maximum length palindrome for the substring A[i,...,j] as L(i,j).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 781, 'file_type': 'pdf'}, page_content='Time Complexity: First ‘for’ loop takes O(n) time while the second ‘for’ loop takes O(n – k)\\nwhich is also O(n). Therefore, the total running time of the algorithm is given by O(n2).\\nProblem-36\\u2003\\u2003Longest Palindrome Substring: Given a string A, we need to find the longest\\nsub-string of A such that the reverse of it is exactly the same.\\nSolution: The basic difference between the longest palindrome substring and the longest\\npalindrome subsequence is that, in the case of the longest palindrome substring, the output string\\nshould be the contiguous characters, which gives the maximum palindrome; and in the case of the\\nlongest palindrome subsequence, the output is the sequence of characters where the characters\\nmight not be contiguous but they should be in an increasing sequence with respect to their\\npositions in the given string.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 782, 'file_type': 'pdf'}, page_content='Brute-force solution exhaustively checks all n (n + 1) / 2 possible substrings of the given n-length\\nstring, tests each one if it’s a palindrome, and keeps track of the longest one seen so far. This has\\nworst-case complexity O(n3), but we can easily do better by realizing that a palindrome is\\ncentered on either a letter (for odd-length palindromes) or a space between letters (for even-\\nlength palindromes). Therefore we can examine all n + 1 possible centers and find the longest\\npalindrome for that center, keeping track of the overall longest palindrome. This has worst-case\\ncomplexity O(n2).\\nLet us use DP to solve this problem. It is worth noting that there are no more than O(n2) substrings\\nin a string of length n (while there are exactly 2n subsequences). Therefore, we could scan each\\nsubstring, check for a palindrome, and update the length of the longest palindrome substring\\ndiscovered so far. Since the palindrome test takes time linear in the length of the substring, this\\nidea takes O(n3) algorithm. We can use DP to improve this. For 1 ≤ i ≤ j ≤ n, define\\nAlso, for string of length at least 3,\\nNote that in order to obtain a well-defined recurrence, we need to explicitly initialize two distinct\\ndiagonals of the boolean array L[i,j], since the recurrence for entry [i,j] uses the value [i – 1,j –\\n1], which is two diagonals away from [i,j] (that means, for a substring of length k, we need to\\nknow the status of a substring of length k – 2).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 783, 'file_type': 'pdf'}, page_content='Time Complexity: First for loop takes O(n) time while the second for loop takes O(n – k) which\\nis also O(n). Therefore the total running time of the algorithm is given by O(n2).\\nProblem-37\\u2003\\u2003Given two strings S and T, give an algorithm to find the number of times S\\nappears in T. It’s not compulsory that all characters of S should appear contiguous to T.\\nFor example, if S = ab and T = abadcb then the solution is 4, because ab is appearing 4\\ntimes in abadcb.\\nSolution:\\nInput: Given two strings S[1.. m] and T[1 ...m].\\nGoal: Count the number of times that S appears in T.\\nAssume L(i,j) represents the count of how many times i characters of S are appearing in j\\ncharacters of T.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 784, 'file_type': 'pdf'}, page_content='If we concentrate on the components of the above recursive formula,\\n•\\nIf j = 0, then since T is empty the count becomes 0.\\n•\\nIf i = 0, then we can treat empty string S also appearing in T and we can give the\\ncount as 1.\\n•\\nIf S[i] == T[i], it means ith character of S and jth character of T are the same. In this\\ncase we have to check the subproblems with i – 1 characters of S and j – 1\\ncharacters of T and also we have to count the result of i characters of S withy – 1\\ncharacters of T. This is because even all i characters of S might be appearing in j –\\n1 characters of T.\\n•\\nIf S[i] ≠ T[i], then we have to get the result of subproblem with i – 1 characters of S\\nand j characters of T.\\nAfter computing all the values, we have to select the one which gives the maximum count.\\nHow many subproblems are there? In the above formula, i can range from 1 to m and j can\\nrange from 1 to n. There are a total of ran subproblems and each one takes O(1). Time\\nComplexity is O(mn).\\nSpace Complexity: O(mn) where m is number of rows and n is number of columns in the given\\nmatrix.\\nProblem-38\\u2003\\u2003Given a matrix with n rows and m columns (n × m). In each cell there are a\\nnumber of apples. We start from the upper-left corner of the matrix. We can go down or\\nright one cell. Finally, we need to arrive at the bottom-right corner. Find the maximum\\nnumber of apples that we can collect. When we pass through a cell, we collect all the\\napples left there.\\nSolution: Let us assume that the given matrix is A[n][m]. The first thing that must be observed is\\nthat there are at most 2 ways we can come to a cell - from the left (if it’s not situated on the first\\ncolumn) and from the top (if it’s not situated on the most upper row).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 785, 'file_type': 'pdf'}, page_content='To find the best solution for that cell, we have to have already found the best solutions for all of\\nthe cells from which we can arrive to the current cell. From above, a recurrent relation can be\\neasily obtained as:\\nS(i,j) must be calculated by going first from left to right in each row and process the rows from\\ntop to bottom, or by going first from top to bottom in each column and process the columns from\\nleft to right.\\nHow many such subproblems are there? In the above formula, i can range from 1 to n and j can\\nrange from 1 to m. There are a total of run subproblems and each one takes O(1). Time\\nComplexity is O(nm). Space Complexity: O(nm), where m is number of rows and n is number of\\ncolumns in the given matrix.\\nProblem-39\\u2003\\u2003Similar to Problem-38, assume that we can go down, right one cell, or even in a\\ndiagonal direction. We need to arrive at the bottom-right corner. Give DP solution to find\\nthe maximum number of apples we can collect.\\nSolution: Yes. The discussion is very similar to Problem-38. Let us assume that the given matrix\\nis A[n][m]. The first thing that must be observed is that there are at most 3 ways we can come to a\\ncell - from the left, from the top (if it’s not situated on the uppermost row) or from the top\\ndiagonal. To find the best solution for that cell, we have to have already found the best solutions\\nfor all of the cells from which we can arrive to the current cell. From above, a recurrent relation\\ncan be easily obtained:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 786, 'file_type': 'pdf'}, page_content='S(i,j) must be calculated by going first from left to right in each row and process the rows from\\ntop to bottom, or by going first from top to bottom in each column and process the columns from\\nleft to right.\\nHow many such subproblems are there? In the above formula, i can range from 1 to n and j can\\nrange from 1 to m. There are a total of mn subproblems and and each one takes O(1). Time\\nComplexity is O(nm).\\nSpace Complexity: O(nm) where m is number of rows and n is number of columns in the given\\nmatrix.\\nProblem-40\\u2003\\u2003Maximum size square sub-matrix with all 1’s: Given a matrix with 0’s and\\n1’s, give an algorithm for finding the maximum size square sub-matrix with all Is. For\\nexample, consider the binary matrix below.\\nThe maximum square sub-matrix with all set bits is\\nSolution: Let us try solving this problem using DP. Let the given binary matrix be B[m][m]. The\\nidea of the algorithm is to construct a temporary matrix L[][] in which each entry L[i][j]\\nrepresents size of the square sub-matrix with all 1’s including B[i][j] and B[i][j] is the rightmost'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 787, 'file_type': 'pdf'}, page_content='and bottom-most entry in the sub-matrix.\\nAlgorithm:\\n1)\\nConstruct a sum matrix L[m][n] for the given matrix B[m][n].\\na.\\nCopy first row and first columns as is from B[ ][ ] to L[ ][ ].\\nb.\\nFor other entries, use the following expressions to construct L[ ][ ]\\n2)\\nFind the maximum entry in L[m][n].\\n3)\\nUsing the value and coordinates of maximum entry in L[i], print sub-matrix of B[][].'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 788, 'file_type': 'pdf'}, page_content='How many subproblems are there? In the above formula, i can range from 1 to n and j can range\\nfrom 1 to m. There are a total of nm subproblems and each one takes O(1). Time Complexity is\\nO(nm). Space Complexity is O(nm), where n is number of rows and m is number of columns in\\nthe given matrix.\\nProblem-41\\u2003\\u2003Maximum size sub-matrix with all 1’s: Given a matrix with 0’s and 1’s, give\\nan algorithm for finding the maximum size sub-matrix with all Is. For example, consider'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 789, 'file_type': 'pdf'}, page_content='the binary matrix below.\\nThe maximum sub-matrix with all set bits is\\nSolution: If we draw a histogram of all 1’s cells in the above rows for a particular row, then\\nmaximum all 1’s sub-matrix ending in that row will be equal to maximum area rectangle in that\\nhistogram. Below is an example for 3rdrow in the above discussed matrix [1]:\\nIf we calculate this area for all the rows, maximum area will be our answer. We can extend our\\nsolution very easily to find start and end co-ordinates. For this, we need to generate an auxiliary\\nmatrix S[][] where each element represents the number of Is above and including it, up until the\\nfirst 0. S[][] for the above matrix will be as shown below:\\nNow we can simply call our maximum rectangle in histogram on every row in S[][] and update\\nthe maximum area every time. Also we don’t need any extra space for saving S. We can update\\noriginal matrix (A) to S and after calculation, we can convert S back to A.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 790, 'file_type': 'pdf'}, page_content='Problem-42\\u2003\\u2003Maximum sum sub-matrix: Given an n × n matrix M of positive and negative\\nintegers, give an algorithm to find the sub-matrix with the largest possible sum.\\nSolution: Let Aux[r, c] represent the sum of rectangular subarray of M with one corner at entry\\n[1,1] and the other at [r,c]. Since there are n2 such possibilities, we can compute them in O(n2)\\ntime. After computing all possible sums, the sum of any rectangular subarray of M can be\\ncomputed in constant time. This gives an O(n4) algorithm: we simply guess the lower-left and the\\nupper-right corner of the rectangular subarray and use the Aux table to compute its sum.\\nProblem-43\\u2003\\u2003Can we improve the complexity of Problem-42?\\nSolution: We can use the Problem-4 solution with little variation, as we have seen that the\\nmaximum sum array of a 1 – D array algorithm scans the array one entry at a time and keeps a\\nrunning total of the entries. At any point, if this total becomes negative, then set it to 0. This\\nalgorithm is called Kadane’s algorithm. We use this as an auxiliary function to solve a two-\\ndimensional problem in the following way.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 791, 'file_type': 'pdf'}, page_content='Time Complexity: O(n3).\\nProblem-44\\u2003\\u2003Given a number n, find the minimum number of squares required to sum a given\\nnumber n.\\nExamples: min[1] = 1 = 12, min[2] = 2 = 12 + 12, min[4] = 1 = 22, min[13] = 2 = 32 + 22.\\nSolution: This problem can be reduced to a coin change problem. The denominations are 1 to \\n. Now, we just need to make change for n with a minimum number of denominations.\\nProblem-45\\u2003\\u2003Finding Optimal Number of Jumps To Reach Last Element: Given an array,\\nstart from the first element and reach the last by jumping. The jump length can be at most\\nthe value at the current position in the array. The optimum result is when you reach the goal'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 792, 'file_type': 'pdf'}, page_content='in the minimum number of jumps. Example: Given array A = {2,3,1,1,4}. Possible ways\\nto reach the end (index list) are:\\n•\\n0,2,3,4 (jump 2 to index 2, and then jump 1 to index 3, and then jump 1 to\\nindex 4)\\n•\\n0,1,4 (jump 1 to index 1, and then jump 3 to index 4)\\nSince second solution has only 2 jumps it is the optimum result.\\nSolution: This problem is a classic example of Dynamic Programming. Though we can solve this\\nby brute-force, it would be complex. We can use the LIS problem approach for solving this. As\\nsoon as we traverse the array, we should find the minimum number of jumps for reaching that\\nposition (index) and update our result array. Once we reach the end, we have the optimum\\nsolution at last index in result array.\\nHow can we find the optimum number of jumps for every position (index)? For first index, the\\noptimum number of jumps will be zero. Please note that if value at first index is zero, we can’t\\njump to any element and return infinite. For n + 1th element, initialize result[n + 1] as infinite.\\nThen we should go through a loop from 0 ... n, and at every index i, we should see if we are able\\nto jump to n + 1 from i or not. If possible, then see if total number of jumps (result[i] + 1) is less\\nthan result[n + 1], then update result[n + 1], else just continue to next index.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 793, 'file_type': 'pdf'}, page_content='The above code will return optimum number of jumps. To find the jump indexes as well, we can\\nvery easily modify the code as per requirement.\\nTime Complexity: Since we are running 2 loops here and iterating from 0 to i in every loop then\\ntotal time takes will be 1 + 2 + 3 + 4 + ... + n – 1. So time efficiency O(n) = O(n * (n – 1)/2) =\\nO(n2).\\nSpace Complexity: O(n) space for result array.\\nProblem-46\\u2003\\u2003Explain what would happen if a dynamic programming algorithm is designed to\\nsolve a problem that does not have overlapping sub-problems.\\nSolution: It will be just a waste of memory, because the answers of sub-problems will never be\\nused again. And the running time will be the same as using the Divide & Conquer algorithm.\\nProblem-47\\u2003\\u2003Christmas is approaching. You’re helping Santa Claus to distribute gifts to\\nchildren. For ease of delivery, you are asked to divide n gifts into two groups such that the\\nweight difference of these two groups is minimized. The weight of each gift is a positive\\ninteger. Please design an algorithm to find an optimal division minimizing the value'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 794, 'file_type': 'pdf'}, page_content='difference. The algorithm should find the minimal weight difference as well as the\\ngroupings in O(nS) time, where S is the total weight of these n gifts. Briefly justify the\\ncorrectness of your algorithm.\\nSolution: This problem can be converted into making one set as close to  as possible. We\\nconsider an equivalent problem of making one set as close to \\n as possible. Define\\nFD(i,w) to be the minimal gap between the weight of the bag and W when using the first i gifts\\nonly. WLOG, we can assume the weight of the bag is always less than or equal to W. Then fill the\\nDP table for 0≤i≤ n and 0≤ w ≤W in which F(0, w) = W for all w, and\\nThis takes O(nS) time. FD(n,W) is the minimum gap. Finally, to reconstruct the answer, we\\nbacktrack from (n,W). During backtracking, if FD(i,j) = FD(i – 1,j) then i is not selected in the\\nbag and we move to F(i – 1,j). Otherwise, i is selected and we move to F(i – 1,j – wi).\\nProblem-48\\u2003\\u2003A circus is designing a tower routine consisting of people standing atop one\\nanother’s shoulders. For practical and aesthetic reasons, each person must be both shorter\\nand lighter than the person below him or her. Given the heights and weights of each person\\nin the circus, write a method to compute the largest possible number of people in such a\\ntower.\\nSolution: It is same as Box stacking and Longest increasing subsequence (LIS) problem.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 795, 'file_type': 'pdf'}, page_content='20.1 Introduction\\nIn the previous chapters we have solved problems of different complexities. Some algorithms\\nhave lower rates of growth while others have higher rates of growth. The problems with lower\\nrates of growth are called easy problems (or easy solved problems) and the problems with higher\\nrates of growth are called hard problems (or hard solved problems). This classification is done\\nbased on the running time (or memory) that an algorithm takes for solving the problem.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 796, 'file_type': 'pdf'}, page_content='There are lots of problems for which we do not know the solutions. All the problems we have\\nseen so far are the ones which can be solved by computer in deterministic time. Before starting\\nour discussion let us look at the basic terminology we use in this chapter.\\n20.2 Polynomial/Exponential Time\\nExponential time means, in essence, trying every possibility (for example, backtracking\\nalgorithms) and they are very slow in nature. Polynomial time means having some clever\\nalgorithm to solve a problem, and we don’t try every possibility. Mathematically, we can\\nrepresent these as:\\n•\\nPolynomial time is O(nk), for some k.\\n•\\nExponential time is O(kn), for some k.\\n20.3 What is a Decision Problem?\\nA decision problem is a question with a yes/no answer and the answer depends on the values of\\ninput. For example, the problem “Given an array of n numbers, check whether there are any\\nduplicates or not?” is a decision problem. The answer for this problem can be either yes or no\\ndepending on the values of the input array.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 797, 'file_type': 'pdf'}, page_content='20.4 Decision Procedure\\nFor a given decision problem let us assume we have given some algorithm for solving it. The\\nprocess of solving a given decision problem in the form of an algorithm is called a decision\\nprocedure for that problem.\\n20.5 What is a Complexity Class?\\nIn computer science, in order to understand the problems for which solutions are not there, the\\nproblems are divided into classes and we call them as complexity classes. In complexity theory, a\\ncomplexity class is a set of problems with related complexity. It is the branch of theory of\\ncomputation that studies the resources required during computation to solve a given problem.\\nThe most common resources are time (how much time the algorithm takes to solve a problem) and\\nspace (how much memory it takes).\\n20.6 Types of Complexity Classes\\nP Class\\nThe complexity class P is the set of decision problems that can be solved by a deterministic\\nmachine in polynomial time (P stands for polynomial time). P problems are a set of problems\\nwhose solutions are easy to find.\\nNP Class\\nThe complexity class NP (NP stands for non-deterministic polynomial time) is the set of decision\\nproblems that can be solved by a non-deterministic machine in polynomial time. NP class\\nproblems refer to a set of problems whose solutions are hard to find, but easy to verify.\\nFor better understanding let us consider a college which has 500 students on its roll. Also,\\nassume that there are 100 rooms available for students. A selection of 100 students must be paired\\ntogether in rooms, but the dean of students has a list of pairings of certain students who cannot\\nroom together for some reason.\\nThe total possible number of pairings is too large. But the solutions (the list of pairings) provided\\nto the dean, are easy to check for errors. If one of the prohibited pairs is on the list, that’s an error.\\nIn this problem, we can see that checking every possibility is very difficult, but the result is easy\\nto validate.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 798, 'file_type': 'pdf'}, page_content='That means, if someone gives us a solution to the problem, we can tell them whether it is right or\\nnot in polynomial time. Based on the above discussion, for NP class problems if the answer is\\nyes, then there is a proof of this fact, which can be verified in polynomial time.\\nCo-NP Class\\nCo – NP is the opposite of NP (complement of NP). If the answer to a problem in Co – NP is no,\\nthen there is a proof of this fact that can be checked in polynomial time.\\nP\\nSolvable in polynomial time\\nNP\\nYes answers can be checked in polynomial time\\nCo-NP\\nNo answers can be checked in polynomial time\\nRelationship between P, NP and Co-NP\\nEvery decision problem in P is also in NP. If a problem is in P, we can verify YES answers in\\npolynomial time. Similarly, any problem in P is also in Co – NP.\\nOne of the important open questions in theoretical computer science is whether or not P = NP.\\nNobody knows. Intuitively, it should be obvious that P ≠ NP, but nobody knows how to prove it.\\nAnother open question is whether NP and Co – NP are different. Even if we can verify every\\nYES answer quickly, there’s no reason to think that we can also verify NO answers quickly.\\nIt is generally believed that NP ≠ Co – NP, but again nobody knows how to prove it.\\nNP-hard Class\\nIt is a class of problems such that every problem in NP reduces to it. All NP-hard problems are\\nnot in NP, so it takes a long time to even check them. That means, if someone gives us a solution\\nfor NP-hard problem, it takes a long time for us to check whether it is right or not.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 799, 'file_type': 'pdf'}, page_content='A problem K is NP-hard indicates that if a polynomial-time algorithm (solution) exists for K then\\na polynomial-time algorithm for every problem is NP. Thus:\\nNP-complete Class\\nFinally, a problem is NP-complete if it is part of both NP-hard and NP. NP-complete problems\\nare the hardest problems in NP. If anyone finds a polynomial-time algorithm for one NP-complete\\nproblem, then we can find polynomial-time algorithm for every NP-complete problem. This\\nmeans that we can check an answer fast and every problem in NP reduces to it.\\nRelationship between P, NP Co-NP, NP-Hard and NP-Complete\\nFrom the above discussion, we can write the relationships between different components as\\nshown below (remember, this is just an assumption).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 800, 'file_type': 'pdf'}, page_content='The set of problems that are NP-hard is a strict superset of the problems that are NP-complete.\\nSome problems (like the halting problem) are NP-hard, but not in NP. NP-hard problems might be\\nimpossible to solve in general. We can tell the difference in difficulty between NP-hard and NP-\\ncomplete problems because the class NP includes everything easier than its “toughest” problems -\\nif a problem is not in NP, it is harder than all the problems in NP.\\nDoes P==NP?\\nIf P = NP, it means that every problem that can be checked quickly can be solved quickly\\n(remember the difference between checking if an answer is right and actually solving a problem).\\nThis is a big question (and nobody knows the answer), because right now there are lots of NP-\\ncomplete problems that can’t be solved quickly. If P = NP, that means there is a way to solve\\nthem fast. Remember that “quickly” means not trial-and-error. It could take a billion years, but as\\nlong as we didn’t use trial and error, it was quick. In future, a computer will be able to change\\nthat billion years into a few minutes.\\n20.7 Reductions\\nBefore discussing reductions, let us consider the following scenario. Assume that we want to\\nsolve problem X but feel it’s very complicated. In this case what do we do?\\nThe first thing that comes to mind is, if we have a similar problem to that of X (let us say Y), then\\nwe try to map X to Y and use Y’s solution to solve X also. This process is called reduction.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 801, 'file_type': 'pdf'}, page_content='In order to map problem X to problem Y, we need some algorithm and that may take linear time or\\nmore. Based on this discussion the cost of solving problem X can be given as:\\nCost of solving X = Cost of solving Y + Reduction time\\nNow, let us consider the other scenario. For solving problem X, sometimes we may need to use\\nY’s algorithm (solution) multiple times. In that case,\\nCost of solving X = Number of Times * Cost of solving X + Reduction time\\nThe main thing in NP-Complete is reducibility. That means, we reduce (or transform) given NP-\\nComplete problems to other known NP-Complete problem. Since the NP-Complete problems are\\nhard to solve and in order to prove that given NP-Complete problem is hard, we take one existing\\nhard problem (which we can prove is hard) and try to map given problem to that and finally we\\nprove that the given problem is hard.\\nNote: It’s not compulsory to reduce the given problem to known hard problem to prove its\\nhardness. Sometimes, we reduce the known hard problem to given problem.\\nImportant NP-Complete Problems (Reductions)\\nSatisfiability Problem: A boolean formula is in conjunctive normal form (CNF) if it is a\\nconjunction (AND) of several clauses, each of which is the disjunction (OR) of several literals,\\neach of which is either a variable or its negation. For example: (a ∨ b ∨ c ∨ d ∨ e)∧(b ∨ ~c ∨\\n~d) ∧ (~a ∨ c ∨ d) ∨ (a ∨ ~b)\\nA 3-CNF formula is a CNF formula with exactly three literals per clause. The previous example\\nis not a 3-CNF formula, since its first clause has five literals and its last clause has only two.\\n2-SAT Problem: 3-SAT is just SAT restricted to 3-CNF formulas: Given a 3-CNF formula, is\\nthere an assignment to the variables so that the formula evaluates to TRUE?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 802, 'file_type': 'pdf'}, page_content='2-SAT Problem: 2-SAT is just SAT restricted to 2-CNF formulas: Given a 2-CNF formula, is\\nthere an assignment to the variables so that the formula evaluates to TRUE?\\nCircuit-Satisfiability Problem: Given a boolean combinational circuit composed of AND, OR\\nand NOT gates, is it satisfiable?. That means, given a boolean circuit consisting of AND, OR and\\nNOT gates properly connected by wires, the Circuit-SAT problem is to decide whether there\\nexists an input assignment for which the output is TRUE.\\nHamiltonian Path Problem (Ham-Path): Given an undirected graph, is there a path that visits'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 803, 'file_type': 'pdf'}, page_content='every vertex exactly once?\\nHamiltonian Cycle Problem (Ham-Cycle): Given an undirected graph, is there a cycle (where\\nstart and end vertices are same) that visits every vertex exactly once?\\nDirected Hamiltonian Cycle Problem (Dir-Ham-Cycle): Given a directed graph, is there a\\ncycle (where start and end vertices are same) that visits every vertex exactly once?\\nTravelling Salesman Problem (TSP): Given a list of cities and their pair-wise distances, the\\nproblem is to find the shortest possible tour that visits each city exactly once.\\nShortest Path Problem (Shortest-Path): Given a directed graph and two vertices s and t, check\\nwhether there is a shortest simple path from s to t.\\nGraph Coloring: A k-coloring of a graph is to map one of k ‘colors’ to each vertex, so that every\\nedge has two different colors at its endpoints. The graph coloring problem is to find the smallest\\npossible number of colors in a legal coloring.\\n3-Color problem: Given a graph, is it possible to color the graph with 3 colors in such a way that\\nevery edge has two different colors?\\nClique (also called complete graph): Given a graph, the CLIQUE problem is to compute the\\nnumber of nodes in its largest complete subgraph. That means, we need to find the maximum\\nsubgraph which is also a complete graph.\\nIndependent Set Problem (Ind_Set): Let G be an arbitrary graph. An independent set in G is a\\nsubset of the vertices of G with no edges between them. The maximum independent set problem is\\nthe size of the largest independent set in a given graph.\\nVertex Cover Problem (Vertex-Cover): A vertex cover of a graph is a set of vertices that\\ntouches every edge in the graph. The vertex cover problem is to find the smallest vertex cover in\\na given graph.\\nSubset Sum Problem (Subset-Sum): Given a set S of integers and an integer T, determine\\nwhether 5 has a subset whose elements sum to T.\\nInteger Programming: Given integers bi, aij find 0/1 variables xi that satisfy a linear system of\\nequations.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 804, 'file_type': 'pdf'}, page_content='In the figure, arrows indicate the reductions. For example, Ham-Cycle (Hamiltonian Cycle\\nProblem) can be reduced to CNF-SAT. Same is the case with any pair of problems. For our\\ndiscussion, we can ignore the reduction process for each of the problems. There is a theorem\\ncalled Cook’s Theorem which proves that Circuit satisfiability problem is NP-hard. That means,\\nCircuit satisfiability is a known NP-hard problem.\\nNote: Since the problems below are NP-Complete, they are NP and NP-hard too. For simplicity\\nwe can ignore the proofs for these reductions.\\n20.8 Complexity Classes: Problems & Solutions\\nProblem-1\\u2003\\u2003What is a quick algorithm?\\nSolution: A quick algorithm (solution) means not trial-and-error solution. It could take a billion\\nyears, but as long as we do not use trial and error, it is efficient. Future computers will change\\nthose billion years to a few minutes.\\nProblem-2\\u2003\\u2003What is an efficient algorithm?\\nSolution: An algorithm is said to be efficient if it satisfies the following properties:\\n•\\nScale with input size.\\n•\\nDon’t care about constants.\\n•\\nAsymptotic running time: polynomial time.\\nProblem-3\\u2003\\u2003Can we solve all problems in polynomial time?\\nSolution: No. The answer is trivial because we have seen lots of problems which take more than\\npolynomial time.\\nProblem-4\\u2003\\u2003Are there any problems which are NP-hard?\\nSolution: By definition, NP-hard implies that it is very hard. That means it is very hard to prove\\nand to verify that it is hard. Cook’s Theorem proves that Circuit satisfiability problem is NP-hard.\\nProblem-5\\u2003\\u2003For 2-SAT problem, which of the following are applicable?\\n(a)\\nP\\n(b)\\nNP\\n(c)\\nCoNP\\n(d)\\nNP-Hard\\n(e)\\nCoNP-Hard\\n(f)\\nNP-Complete\\n(g)\\nCoNP-Complete\\nSolution: 2-SAT is solvable in poly-time. So it is P, NP, and CoNP.\\nProblem-6\\u2003\\u2003For 3-SAT problem, which of the following are applicable?'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 805, 'file_type': 'pdf'}, page_content='(a)\\nP\\n(b)\\nNP\\n(c)\\nCoNP\\n(d)\\nNP-Hard\\n(e)\\nCoNP-Hard\\n(f)\\nNP-Complete\\n(g)\\nCoNP-Complete\\nSolution: 3-SAT is NP-complete. So it is NP, NP-Hard, and NP-complete.\\nProblem-7\\u2003\\u2003For 2-Clique problem, which of the following are applicable?\\n(a)\\nP\\n(b)\\nNP\\n(c)\\nCoNP\\n(d)\\nNP-Hard\\n(e)\\nCoNP-Hard\\n(f)\\nNP-Complete\\n(g)\\nCoNP-Complete\\nSolution: 2-Clique is solvable in poly-time (check for an edge between all vertex-pairs in O(n2)\\ntime). So it is P.NP, and CoNP.\\nProblem-8\\u2003\\u2003For 3-Clique problem, which of the following are applicable?\\n(a)\\nP\\n(b)\\nNP\\n(c)\\nCoNP\\n(d)\\nNP-Hard\\n(e)\\nCoNP-Hard\\n(f)\\nNP-Complete\\n(g)\\nCoNP-Complete\\nSolution: 3-Clique is solvable in poly-time (check for a triangle between all vertex-triplets in\\nO(n3) time). So it is P, NP, and CoNP.\\nProblem-9\\u2003\\u2003Consider the problem of determining. For a given boolean formula, check\\nwhether every assignment to the variables satisfies it. Which of the following is\\napplicable?\\n(a)\\nP\\n(b)\\nNP\\n(c)\\nCoNP\\n(d)\\nNP-Hard\\n(e)\\nCoNP-Hard\\n(f)\\nNP-Complete\\n(g)\\nCoNP-Complete\\nSolution: Tautology is the complimentary problem to Satisfiability, which is NP-complete, so\\nTautology is CoNP-complete. So it is CoNP, CoNP-hard, and CoNP-complete.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 806, 'file_type': 'pdf'}, page_content='Problem-10\\u2003\\u2003Let S be an NP-complete problem and Q and R be two other problems not\\nknown to be in NP. Q is polynomial time reducible to S and S is polynomial-time reducible\\nto R. Which one of the following statements is true?\\n(a)\\nR is NP-complete\\n(b)\\nR is NP-hard\\n(c)\\nQ is NP-complete\\n(d)\\nQ is NP -hard.\\nSolution: R is NP-hard (b).\\nProblem-11\\u2003\\u2003Let A be the problem of finding a Hamiltonian cycle in a graph G = (V ,E), with\\n|V| divisible by 3 and B the problem of determining if Hamiltonian cycle exists in such\\ngraphs. Which one of the following is true?\\n(a)\\nBoth A and B are NP-hard\\n(b)\\nA is NP-hard, but B is not\\n(c)\\nA is NP-hard, but B is not\\n(d)\\nNeither A nor B is NP-hard\\nSolution: Both A and B are NP-hard (a).\\nProblem-12\\u2003\\u2003Let A be a problem that belongs to the class NP. State which of the following is\\ntrue?\\n(a)\\nThere is no polynomial time algorithm for A.\\n(b)\\nIf A can be solved deterministically in polynomial time, then P = NP.\\n(c)\\nIf A is NP-hard, then it is NP-complete.\\n(d)\\nA may be undecidable.\\nSolution: If A is NP-hard, then it is NP-complete (c).\\nProblem-13\\u2003\\u2003Suppose we assume Vertex – Cover is known to be NP-complete. Based on our\\nreduction, can we say Independent – Set is NP-complete?\\nSolution: Yes. This follows from the two conditions necessary to be NP-complete:\\n•\\nIndependent Set is in NP, as stated in the problem.\\n•\\nA reduction from a known NP-complete problem.\\nProblem-14\\u2003\\u2003Suppose Independent Set is known to be NP-complete. Based on our reduction,\\nis Vertex Cover NP-complete?\\nSolution: No. By reduction from Vertex-Cover to Independent-Set, we do not know the difficulty\\nof solving Independent-Set. This is because Independent-Set could still be a much harder problem\\nthan Vertex-Cover. We have not proved that.\\nProblem-15\\u2003\\u2003The class of NP is the class of languages that cannot be accepted in polynomial\\ntime. Is it true? Explain.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 807, 'file_type': 'pdf'}, page_content='•\\nThe class of NP is the class of languages that can be verified in polynomial time.\\n•\\nThe class of P is the class of languages that can be decided in polynomial time.\\n•\\nThe class of P is the class of languages that can be accepted in polynomial time.\\nP ⊆ NP and “languages in P can be accepted in polynomial time”, the description “languages in\\nNP cannot be accepted in polynomial time” is wrong.\\nThe term NP comes from nondeterministic polynomial time and is derived from an alternative\\ncharacterization by using nondeterministic polynomial time Turing machines. It has nothing to do\\nwith “cannot be accepted in polynomial time”.\\nProblem-16\\u2003\\u2003Different encodings would cause different time complexity for the same\\nalgorithm. Is it true?\\nSolution: True. The time complexity of the same algorithm is different between unary encoding\\nand binary encoding. But if the two encodings are polynomially related (e.g. base 2 & base 3\\nencodings), then changing between them will not cause the time complexity to change.\\nProblem-17\\u2003\\u2003If P = NP, then NPC (NP Complete) ⊆ P. Is it true?\\nSolution: True. If P = NP, then for any language L ∈ NP C (1) L ∈ NPC (2) L is NP-hard. By the\\nfirst condition, L ∈ NPC ⊆ NP = P ⇒ NPC ⊆ P.\\nProblem-18\\u2003\\u2003If NPC ⊆ P, then P = NP. Is it true?\\nSolution: True. All the NP problem can be reduced to arbitrary NPC problem in polynomial time,\\nand NPC problems can be solved in polynomial time because NPC ⊆ P. ⇒ NP problem solvable\\nin polynomial time ⇒ NP ⊆ P and trivially P ⊆ NP implies NP = P.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 808, 'file_type': 'pdf'}, page_content='21.1 Introduction\\nIn this chapter we will cover the topics which are useful for interviews and exams.\\n21.2 Hacks on Bitwise Programming\\nIn C and C + + we can work with bits effectively. First let us see the definitions of each bit\\noperation and then move onto different techniques for solving the problems. Basically, there are\\nsix operators that C and C + + support for bit manipulation:\\nSymbol\\nOperation\\n&\\nBitwise AND\\n1\\nBitwise OR\\nA\\nBitwise Exclusive-OR\\n≪\\nBitwise left shift'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 809, 'file_type': 'pdf'}, page_content='≫\\nBitwise right shift\\n~\\nBitwise complement\\n21.2.1 Bitwise AND\\nThe bitwise AND tests two binary numbers and returns bit values of 1 for positions where both\\nnumbers had a one, and bit values of 0 where both numbers did not have one:\\n21.2.2 Bitwise OR\\nThe bitwise OR tests two binary numbers and returns bit values of 1 for positions where either bit\\nor both bits are one, the result of 0 only happens when both bits are 0:\\n21.2.3 Bitwise Exclusive-OR\\nThe bitwise Exclusive-OR tests two binary numbers and returns bit values of 1 for positions\\nwhere both bits are different; if they are the same then the result is 0:\\n21.2.4 Bitwise Left Shift\\nThe bitwise left shift moves all bits in the number to the left and fills vacated bit positions with 0.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 810, 'file_type': 'pdf'}, page_content='21.2.5 Bitwise Right Shift\\nThe bitwise right shift moves all bits in the number to the right.\\nNote the use of ? for the fill bits. Where the left shift filled the vacated positions with 0, a right\\nshift will do the same only when the value is unsigned. If the value is signed then a right shift will\\nfill the vacated bit positions with the sign bit or 0, whichever one is implementation-defined. So\\nthe best option is to never right shift signed values.\\n21.2.6 Bitwise Complement\\nThe bitwise complement inverts the bits in a single binary number.\\n21.2.7 Checking Whether K-th Bit is Set or Not\\nLet us assume that the given number is n. Then for checking the Kth bit we can use the expression:\\nn & (1 ≪ K 1). If the expression is true then we can say the Kth bit is set (that means, set to 1).\\nExample:\\n21.2.8 Setting K-th Bit'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 811, 'file_type': 'pdf'}, page_content='For a given number n, to set the Kth bit we can use the expression: n | 1 ≪ (K – 1)\\nExample:\\n21.2.9 Clearing K-th Bit\\nTo clear Kth bit of a given number n, we can use the expression: n & ~(1 ≪ K – 1)\\nExample:\\n21.2.10 Toggling K-th Bit\\nFor a given number n, for toggling the Kth bit we can use the expression: n ^(1 ≪ K – 1)\\nExample:\\n21.2.11 Toggling Rightmost One Bit\\nFor a given number n, for toggling rightmost one bit we can use the expression: n & n – 1\\nExample:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 812, 'file_type': 'pdf'}, page_content='21.2.12 Isolating Rightmost One Bit\\nFor a given number n, for isolating rightmost one bit we can use the expression: n & – n\\nExample:\\nNote: For computing –n, use two’s complement representation. That means, toggle all bits and\\nadd 1.\\n21.2.13 Isolating Rightmost Zero Bit\\nFor a given number n, for isolating rightmost zero bit we can use the expression: ~n & n + 1\\nExample:\\n21.2.14 Checking Whether Number is Power of 2 or Not\\nGiven number n, to check whether the number is in 2n form for not, we can use the expression:\\nif(n & n – 1 == 0)\\nExample:\\n21.2.15 Multiplying Number by Power of 2\\nFor a given number n, to multiply the number with 2K we can use the expression: n ≪ K'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 813, 'file_type': 'pdf'}, page_content='Example:\\n21.2.16 Dividing Number by Power of 2\\nFor a given number n, to divide the number with 2K we can use the expression: n ≫ K\\nExample:\\n21.2.17 Finding Modulo of a Given Number\\nFor a given number n, to find the %8 we can use the expression: n & 0x7. Similarly, to find %32,\\nuse the expression: n & 0x1F\\nNote: Similarly, we can find modulo value of any number.\\n21.2.18 Reversing the Binary Number\\nFor a given number n, to reverse the bits (reverse (mirror) of binary number) we can use the\\nfollowing code snippet:\\nTime Complexity: This requires one iteration per bit and the number of iterations depends on the\\nsize of the number.\\n21.2.19 Counting Number of One’s in Number'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 814, 'file_type': 'pdf'}, page_content='For a given number n, to count the number of 1’s in its binary representation we can use any of the\\nfollowing methods.\\nMethod 1: Process bit by bit with bitwise and operator\\nTime Complexity: This approach requires one iteration per bit and the number of iterations\\ndepends on system.\\nMethod 2: Using modulo approach\\nTime Complexity: This requires one iteration per bit and the number of iterations depends on\\nsystem.\\nMethod 3: Using toggling approach: n & n – 1\\nTime Complexity: The number of iterations depends on the number of 1 bits in the number.\\nMethod 4: Using preprocessing idea. In this method, we process the bits in groups. For example\\nif we process them in groups of 4 bits at a time, we create a table which indicates the number of\\none’s for each of those possibilities (as shown below).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 815, 'file_type': 'pdf'}, page_content='The following code to count the number of Is in the number with this approach:\\nTime Complexity: This approach requires one iteration per 4 bits and the number of iterations\\ndepends on system.\\n21.2.20 Creating Mask for Trailing Zero’s\\nFor a given number n, to create a mask for trailing zeros, we can use the expression: (n & – n) – 1\\nExample:\\nNote: In the above case we are getting the mask as all zeros because there are no trailing zeros.\\n27.2.21 Swap all odd and even bits\\nExample:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 816, 'file_type': 'pdf'}, page_content='21.2.22 Performing Average without Division\\nIs there a bit-twiddling algorithm to replace mid = (low + high) / 2 (used in Binary Search and\\nMerge Sort) with something much faster?\\nWe can use mid = (low + high) >> 1. Note that using (low + high) / 2 for midpoint calculations\\nwon’t work correctly when integer overflow becomes an issue. We can use bit shifting and also\\novercome a possible overflow issue: low + ((high – low)/ 2) and the bit shifting operation for\\nthis is low + ((high – low) >> 1).\\n21.3 Other Programming Questions with Solutions\\nProblem-1\\u2003\\u2003Give an algorithm for printing the matrix elements in spiral order.\\nSolution: Non-recursive solution involves directions right, left, up, down, and dealing their\\ncorresponding indices. Once the first row is printed, direction changes (from right) to down, the\\nrow is discarded by incrementing the upper limit. Once the last column is printed, direction\\nchanges to left, the column is discarded by decrementing the right hand limit.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-2\\u2003\\u2003Give an algorithm for shuffling the desk of cards.\\nSolution: Assume that we want to shuffle an array of 52 cards, from 0 to 51 with no repeats, such\\nas we might want for a deck of cards. First fill the array with the values in order, then go through'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 817, 'file_type': 'pdf'}, page_content='the array and exchange each element with a randomly chosen element in the range from itself to\\nthe end. It’s possible that an element will swap with itself, but there is no problem with that.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-3\\u2003\\u2003Reversal algorithm for array rotation: Write a function rotate(A[], d, n) that\\nrotates A[] of size n by d elements. For example, the array 1,2,3,4,5,6,7 becomes\\n3,4,5,6,7,1,2 after 2 rotations.\\nSolution: Consider the following algorithm.\\nAlgorithm:\\nrotate(Array[], d, n)\\nreverse(Array[], 1, d) ; reverse(Array[], d + 1, n);\\nreverse(Array[], 1, n);\\nLet AB be the two parts of the input Arrays where A = Array[0..d-1] and B = Array[d..n-1]. The\\nidea of the algorithm is:\\nReverse A to get ArB. /* Ar is reverse of A */\\nReverse B to get ArBr. /* Br is reverse of B */\\nReverse all to get (ArBr) r = BA.\\nFor example, if Array[] = [1, 2, 3, 4, 5, 6, 7], d =2 and n = 7 then, A = [1, 2] and B = [3,\\n4, 5, 6, 7]\\nReverse A, we get ArB = [2, 1, 3, 4, 5, 6, 7], Reverse B, we get ArBr = [2, 1, 7, 6, 5, 4,\\n3]\\nReverse all, we get (ArBr)r = [3, 4, 5, 6, 7, 1, 2]\\nImplementation :'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 818, 'file_type': 'pdf'}, page_content='Problem-4\\u2003\\u2003Suppose you are given an array s[1...n] and a procedure reverse (s,i,j) which\\nreverses the order of elements in between positions i and j (both inclusive). What does the\\nfollowing sequence\\na)\\nRotates s left by k positions\\nb)\\nLeaves s unchanged\\nc)\\nReverses all elements of s\\nd)\\nNone of the above\\nSolution: (b). Effect of the above 3 reversals for any k is equivalent to left rotation of the array of\\nsize n by k [refer Problem-3].'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 819, 'file_type': 'pdf'}, page_content='Problem-5\\u2003\\u2003Finding Anagrams in Dictionary: you are given these 2 files: dictionary.txt and\\njumbles.txt\\nThejumbles.txt file contains a bunch of scrambled words. Your job is to print out those jumbles\\nwords, 1 word to a line. After each jumbled word, print a list of real dictionary words that could\\nbe formed by unscrambling the jumbled word. The dictionary words that you have to choose from\\nare in the dictionary.txt file. Sample content of jumbles.txt:\\nSolution: Step-By-Step\\nStep 1: Initialization\\n•\\nOpen the dictionary.txt file and read the words into an array (before going further\\nverify by echoing out the words back from the array out to the screen).\\n•\\nDeclare a hash table variable.\\nStep 2: Process the Dictionary for each dictionary word in the array. Do the following:\\nWe now have a hash table where each key is the sorted form of a dictionary word and the value\\nassociated to it is a string or array of dictionary words that sort to that same key.\\n•\\nRemove the newline off the end of each word via chomp($word);\\n•\\nMake a sorted copy of the word - i.e. rearrange the individual chars in the string to\\nbe sorted alphabetically\\n•\\nThink of the sorted word as the key value and think of the set of all dictionary words\\nthat sort to the exact same key word as being the value of the key\\n•\\nQuery the hashtable to see if the sortedWord is already one of the keys\\n•\\nIf it is not already present then insert the sorted word as key and the unsorted\\noriginal of the word as the value\\n•\\nElse concat the unsorted word onto the value string already out there (put a space in\\nbetween)\\nStep 3: Process the jumbled word file\\n•\\nRead through the jumbled word file one word at a time. As you read each jumbled\\nword chomp it and make a sorted copy (the sorted copy is your key)\\n•\\nPrint the unsorted jumble word\\n•\\nQuery the hashtable for the sorted copy. If found, print the associated value on same\\nline as key and then a new line.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 820, 'file_type': 'pdf'}, page_content='Step 4: Celebrate, we are all done\\nSample code in Perl:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 821, 'file_type': 'pdf'}, page_content='Problem-6 Pathways: Given a matrix as shown below, calculate the number of ways for'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 822, 'file_type': 'pdf'}, page_content='reaching destination B from A.\\nSolution: Before finding the solution, we try to understand the problem with a simpler version.\\nThe smallest problem that we can consider is the number of possible routes in a 1 × 1 grid.\\nFrom the above figure, it can be seen that:\\n•\\nFrom both the bottom-left and the top-right corners there’s only one possible route to\\nthe destination.\\n•\\nFrom the top-left corner there are trivially two possible routes.\\nSimilarly, for 2x2 and 3x3 grids, we can fill the matrix as:\\nFrom the above discussion, it is clear that to reach the bottom right corner from left top corner, the\\npaths are overlapping. As unique paths could overlap at certain points (grid cells), we could try\\nto alter the previous algorithm, as a way to avoid following the same path again. If we start filling\\n4x4 and 5x5, we can easily figure out the solution based on our childhood mathematics concepts.\\nAre you able to figure out the pattern? It is the same as Pascals triangle. So, to find the number of\\nways, we can simply scan through the table and keep counting them while we move from left to\\nright and top to bottom (starting with left-top). We can even solve this problem with mathematical\\nequation of Pascals triangle.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 823, 'file_type': 'pdf'}, page_content='Problem-7\\u2003\\u2003Given a string that has a set of words and spaces, write a program to move the\\nspaces to front of string. You need to traverse the array only once and you need to adjust\\nthe string in place.\\nInput = “move these spaces to beginning” Output =“ movethesepacestobeginning”\\nSolution: Maintain two indices i and j; traverse from end to beginning. If the current index\\ncontains char, swap chars in index i with index j. This will move all the spaces to beginning of\\nthe array.\\nTime Complexity: O(n) where n is the number of characters in the input array. Space Complexity:\\nO(1).\\nProblem-8\\u2003\\u2003For the Problem-7, can we improve the complexity?\\nSolution: We can avoid a swap operation with a simple counter. But, it does not reduce the\\noverall complexity.\\nTime Complexity: O(n) where n is the number of characters in input array. Space Complexity:\\nO(1).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 824, 'file_type': 'pdf'}, page_content='Problem-9\\u2003\\u2003Given a string that has a set of words and spaces, write a program to move the\\nspaces to end of string. You need to traverse the array only once and you need to adjust the\\nstring in place.\\nInput = “move these spaces to end” Output = “movethesepacestoend “\\nSolution: Traverse the array from left to right. While traversing, maintain a counter for non-space\\nelements in array. For every non-space character A[i], put the element at A[count] and increment\\ncount. After complete traversal, all non-space elements have already been shifted to front end and\\ncount is set as index of first 0. Now, all we need to do is run a loop which fills all elements with\\nspaces from count till end of the array.\\nTime Complexity: O(n) where n is number of characters in input array. Space Complexity: O(1).\\nProblem-10\\u2003\\u2003Moving Zeros to end: Given an array of n integers, move all the zeros of a given\\narray to the end of the array. For example, if the given array is {1, 9, 8, 4, 0, 0, 2, 7, 0, 6,\\n0}, it should be changed to {1, 9, 8, 4, 2, 7, 6, 0, 0, 0, 0}. The order of all other elements\\nshould be same.\\nSolution: Maintain two variables i and j; and initialize with 0. For each of the array element A[i],\\nif A[i] non-zero element, then replace the element A[j] with element A[i]. Variable i will always\\nbe incremented till n - 1 but we will increment j only when the element pointed by i is non-zero.'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 825, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-11\\u2003\\u2003For Problem-10, can we improve the complexity?\\nSolution: Using simple swap technique we can avoid the unnecessary second while loop from the\\nabove code.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-12\\u2003\\u2003Variant of Problem-10 and Problem-11: Given an array containing negative and\\npositive numbers; give an algorithm for separating positive and negative numbers in it.\\nAlso, maintain the relative order of positive and negative numbers. Input: -5, 3, 2, -1, 4, -8\\nOutput: -5-1 -8342\\nSolution: In the moveZerosToEnd function, just replace the condition A[i] !=0 with A[i] < 0.\\nProblem-13\\u2003\\u2003Given a number, swap odd and even bits.\\nSolution:'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 826, 'file_type': 'pdf'}, page_content='Problem-14\\u2003\\u2003Count the number of set bits in all numbers from 1 to n\\nSolution: We can use the technique of section 21.2.19 and iterate through all the numbers from 1\\nto n.\\nProblem-15\\u2003\\u2003Count the number of set bits in all numbers from 1 to n\\nSolution: We can use the technique of section 21.2.19 and iterate through all the numbers from 1\\nto n.\\nTime complexity: O(number of set bits in all numbers from 1 to n).'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 827, 'file_type': 'pdf'}, page_content='REFERENCES\\n[1]\\nAkash. Programming Interviews, tech-queries.blogspot.com.\\n[2]\\nAlfred V.Aho,J. E. (1983). Data Structures and Algorithms. Addison-Wesley.\\n[3]\\nAlgorithms.Retrieved from cs.princeton.edu/algs4/home\\n[4]\\nAnderson., S. E. Bit Twiddling Hacks. Retrieved 2010, from Bit Twiddling Hacks:\\ngraphics. Stanford. edu\\n[5]\\nBentley, J. AT&T Bell Laboratories. Retrieved from AT&T Bell Laboratories.\\n[6]\\nBondalapati, K. Interview Question Bank. Retrieved 2010, from Interview Question Bank:\\nhalcyon.usc.edu/~kiran/msqs.html\\n[7]\\nChen. Algorithms hawaii.edu/~chenx.\\n[8]\\nDatabase, \\nP.Problem \\nDatabase. \\nRetrieved \\n2010, \\nfrom \\nProblem \\nDatabase:\\ndatastructures.net\\n[9]\\nDrozdek, A. (1996). Data Structures and Algorithms in C++.\\n[10] Ellis Horowitz, S. S. Fundamentals of Data Structures.\\n[11] Gilles Brassard, P. B. (1996). Fundamentals of Algorithmics.\\n[12] Hunter., J. Introduction to Data Structures and Algorithms. Retrieved 2010, from\\nIntroduction to Data Structures and Algorithms.\\n[13] James F. Korsh, L. J. Data Structures, Algorithms and Program Style Using C.\\n[14] John Mongan, N. S. (2002). Programming Interviews Exposed. Wiley-India. .\\n[15] Judges. \\nComments \\non \\nProblems \\nand \\nSolutions. \\nhttp://www.informatik.uni-\\nulm.de/acm/Locals/2003/html/judge, html.\\n[16] Kalid. P, NP, and NP-Complete. Retrieved from P, NP, and NP-Complete.:\\ncs.princeton.edu/~kazad\\n[17] Knuth., D. E. (1973). Fundamental Algorithms, volume 1 of The Art of Computer\\nProgramming. Addison-Wesley.\\n[18] Leon, J. S. Computer Algorithms. Retrieved 2010, from Computer Algorithms :\\nmath.uic.edu/~leon\\n[19] Leon., J. S. Computer Algorithms, math.uic.edu/~leon/cs-mcs401-s08.\\n[20] OCF. Algorithms. Retrieved 2010, from Algorithms: ocf.berkeley.edu\\n[21] Parlante., \\nN. \\nBinary \\nTrees. \\nRetrieved \\n2010, \\nfrom \\ncslibrary.stanford.edu:\\ncslibrary.stanford.edu\\n[22] Patil., V. Fundamentals of data structures. Nirali Prakashan.\\n[23] Poundstone., W. HOW WOULD YOU MOVE MOUNT FUJI? New York Boston.: Little,\\nBrown and Company.\\n[24] Pryor, M. Tech Interview. Retrieved 2010, from Tech Interview: techinterview.org\\n[25] Questions, A. C. A Collection of Technical Interview Questions. Retrieved 2010, from A\\nCollection of Technical Interview Questions'), Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 828, 'file_type': 'pdf'}, page_content='[26] S. Dasgupta, C. P. Algorithms cs.berkeley.edu/~vazirani.\\n[27] Sedgewick., R. (1988). Algorithms. Addison-Wesley.\\n[28] Sells, C. (2010). Interviewing at Microsoft. Retrieved 2010, from Interviewing at\\nMicrosoft\\n[29] Shene, C.-K. Linked Lists Merge Sort Implementation.\\n[30] Sinha, P. Linux Journal. Retrieved 2010, from: linuxjournal.com/article/6828.\\n[31] Structures., d. D. www.math-cs.gordon.edu. Retrieved 2010, from www.math-\\ncs.gordon.edu\\n[32] T. H. Cormen, C. E. (1997). Introduction to Algorithms. Cambridge: The MIT press.\\n[33] Tsiombikas, J. Pointers Explained, nuclear.sdf-eu.org.\\n[34] Warren., H. S. (2003). Hackers Delight. Addison-Wesley.\\n[35] Weiss., M. A. (1992). Data Structures and Algorithm Analysis in C.\\n[36] SANDRASI http://sandrasi-sw.blogspot.in/'), Document(metadata={'source_file': 'scansmpl.pdf', 'page_number': 1, 'file_type': 'pdf'}, page_content='THE SLEREXE COMPANY LIMITED\\n\\nSAPORS LANE - BOOLE - DORSET . BH2S SER\\n‘eunrone nour (945 18) S617 = rau 125486\\n\\nOur Ref. 350/PIC/EAC 18eh January, 1972.\\n\\nDr. P.N, Cundall,\\nMining Surveys Led\\nHolroyd Road,\\nReading,\\n\\nBerks\\n\\nDear Pete,\\n\\nPermit ne to introduce you to the facility of facsinile\\n\\nIn facsimile a photocell is caused to perform a raster scan over\\nthe subject copy, The variations of print density on the document\\ncause the photocell to generate an analogous electrical video signal.\\nThis signal is used to modulate a carrier, which is transmitted to a\\nrenote destination over a radio or cable communications link.\\n\\nAt the renote cerminal, demodulation reconstructs the video\\nsignal, which is used to modulate the density of print produced by a\\nPrinting device. This device is scanning in a raster scan synchronised\\nWith thar at the transmitting terminal. Asa result, a facsimile\\n\\ncopy of the subject docunent is produced.\\n\\nProbably you have uses for this facility in your organisation.\\n\\nYours sincerely,\\n\\nPJ. CROSS\\nGroup Leader - Facsimile Research\\n\\nRegt Ottee Wars Linn Tet. oes,')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333ac96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_page_text(text: str) -> str:\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "\n",
    "    for line in lines:\n",
    "        s = line.strip()\n",
    "\n",
    "        # keep empty lines but compress later\n",
    "        if not s:\n",
    "            cleaned.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # plain page numbers: \"3\"\n",
    "        if re.fullmatch(r\"\\d{1,4}\", s):\n",
    "            continue\n",
    "\n",
    "        # \"Page 12\", \"Page 12 of 123\", \"p. 3/10\"\n",
    "        if re.fullmatch(r\"(page|p\\.)\\s*\\d+(\\s*(/|of)\\s*\\d+)?\",\n",
    "                        s, flags=re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # tiny non-text junk like \"---\", \"•\", \"1/3\"\n",
    "        if len(s) <= 4 and not re.search(r\"[A-Za-z]\", s):\n",
    "            continue\n",
    "\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # collapse many blank lines\n",
    "    text = \"\\n\".join(cleaned)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1dc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 1, 'file_type': 'pdf'}, page_content=\"O'REILLY*\\n\\nApplied Machine\\nLearning and\\nAl for Engineers\\n\\nSolve Business Problems\\nThat Can't Be Solved\\nAlgorithmically\\n\\nJeff Prosise\\nForewor: rd by Adam Prosise\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 2, 'file_type': 'pdf'}, page_content='Praise for Applied Machine Learning and AI for Engineers\\nThis book is a fantastic guide to machine learning and AI\\nalgorithms. It’s succinct while being comprehensive, and\\nthe concrete examples with working code show how to take\\nthe theory into practice.\\nMark Russinovich, Azure CTO and Technical Fellow,\\nMicrosoft\\nWhen Jeff Prosise is passionate about something (whether\\nit be technology, his pet yellow-nape Amazon “Hawkeye,”\\nor his hobby of building and flying radio-controlled\\njets), you definitely want to listen in. He combines that\\npassion with a clarity of explanation that enables him to\\ncommunicate and teach complex topics better than anyone\\nI’ve ever known. He brings you along on a personal\\njourney of learning and understanding. Now Jeff brings\\nthese skills to the current and ongoing “technological\\ntsunami” (as he puts it) of machine learning and AI. In\\nthis new book, he builds your understanding from the\\nfoundations up, always emphasizing an intuitive approach\\nand connecting concepts and solutions to the real world.\\nIf you want to understand how AI and ML really work under\\nthe hood, and how these technologies have evolved and\\ncome to be, READ THIS BOOK.\\nTodd Fine, Chief Strategy Officer, Atmosera\\nJeff distills years of working AI/ML knowledge into a\\npractical and understandable guide for practitioners of\\nall levels.\\nKen Muse, 4x Azure MVP and Senior DevOps Architect,\\nGitHub\\nApplied Machine Learning and AI for Engineers is the book\\nI wish I had when first introduced to machine learning.\\nIt’s a fantastic introduction for novice ML engineers\\nand a great reference for those more experienced. It is\\nnow my go-to source when refreshing and enhancing my'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 3, 'file_type': 'pdf'}, page_content='understanding of various ML techniques and their\\napplicability. I love that Jeff uses real-world examples\\nand datasets to demonstrate each tool set and approach.\\nJust like a neural net, I have no idea how Jeff came up\\nwith this material, but it’s tremendously useful all the\\nsame.\\nBrent Rector, Principal Technical Program Manager,\\nAmazon; Founder, Wise Owl Consulting, LLC, Wise Owl\\nAviation Services, LLC, and Rector Aviation Law PC\\nI’ve known Jeff for decades and he has always possessed\\nthe ability to plainly explain complicated concepts. He\\nhas done it again with Applied Machine Learning and AI\\nfor Engineers. The examples, analogies, and color figures\\nmake the material truly understandable for the beginner\\nto the more advanced reader.\\nJeffrey Richter, Software Architect, Microsoft\\nThis book fills an extremely important gap for engineers\\nand scientists that want to bring the power of AI to bear\\non their most challenging data analysis problems. It\\nstrikes just the right balance between technical depth\\nand practical application, with tools and many hands-on\\nexamples, to empower the reader to become an effective AI\\npractitioner in their own application domain (including\\nbusiness, science, and other data-rich fields).\\nShaun S. Gleason, PhD, Director, Cyber Resilience and\\nIntelligence, Oak Ridge National Laboratory\\nApplied Machine Learning and AI for Engineers has the\\npotential to become the go-to for ML and AI enthusiasts.\\nWhat sets this book apart is how relevant the problem\\nstatements are in today’s fast-paced adoption of machine\\nlearning in the tech world. A must-read for newbies and\\nprofessionals alike!\\nLipi Deepaakshi Patnaik, Software Development\\nEngineer, Zeta Suite'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 4, 'file_type': 'pdf'}, page_content='Jeff has always been able to wrap great stories around\\ndeep technical concepts that make learning markedly\\neasier. This might be his best book yet, and perhaps his\\nmost relevant subject matter as well. Reading this book\\nwill make you want to go build something.\\nDoug Turnure, Azure Specialist, Microsoft\\nApplied Machine Learning and AI for Engineers is a\\npractical handbook for building useful machine learning\\nsystems. It provides accessible guidance on how to apply\\ncutting-edge AI algorithms to solve contemporary business\\nproblems.\\nBrian Spiering, Data Science Instructor, Metis\\nThis book is a perfect start for engineers and software\\ndevelopers who want to get into machine learning. It\\ncovers good ground in ML and quickly gets you started for\\na wide variety of problems that are driven by data rather\\nthan algorithms.\\nDr. Manjeet Dahiya, VP and Head, AI and Machine\\nLearning, Ecom Express\\nWhether you’re new to applied ML or a seasoned developer\\nlooking for a reference, this book is a must-have, up-to-\\ndate, comprehensive guide to all major classes of machine\\nlearning algorithms (with clean code implementations to\\nreally solidify your understanding).\\nGoku Mohandas, Founder, Made With ML\\nApplied Machine Learning and AI for Engineers is a\\nbrilliant primer for beginner to intermediate readers in\\nAI/ML. The book builds a comfortable flow from\\nconventional ML to deep learning with the desirable bonus\\nof AI implementation on the cloud, making it a complete\\nend-to-end guide for any enthusiastic reader or\\nprofessional practitioner.\\nSatyarth Praveen, Computing Science Engineer,\\nLawrence Berkeley National Lab'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 5, 'file_type': 'pdf'}, page_content='Jeff Prosise is one of the best teachers I have ever\\nencountered, and regardless of the platform (classroom,\\nblog, magazine article, webinar, book, etc.), he has a\\nspecial talent of taking complex topics and making them\\naccessible for the rest of us. In this book, Jeff goes\\nbeyond simply providing a very clear understanding of the\\nbasic concepts that undergird machine learning to provide\\neasy-to-follow examples that demonstrate those concepts\\nwithin the current environment. He delivers a great\\nintroduction/overview of a wide variety of topics within\\nmachine learning and gives clear guidance on how each can\\n(and should) be used. Jeff is one of the very few that\\ncould have taken this complex topic and put it into a\\nform that could be easily absorbed and applied. This book\\nis a MUST READ for engineers and other problem solvers\\nwho are looking to use machine learning to augment their\\nskill set.\\nLarry Clement, Assistant Professor, Computing,\\nSoftware, and Data Sciences, and former Department\\nChair, California Baptist University. Formerly a\\nsenior engineer-scientist with the Boeing Corporation\\non the C-17 program.\\nInfused with author Jeff Prosise’s iconic teaching style\\nthat has helped thousands of developers over the years,\\nApplied Machine Learning and AI for Engineers layers\\ncontext around complex topics and provides easy-to-\\nunderstand examples and tutorials. Highly recommended for\\nengineers who want to incorporate machine learning\\nconcepts and skills in their repertoire.\\nVani Mandava, Head of Engineering, Scientific\\nSoftware Engineering Center at University of\\nWashington eScience Institute'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 6, 'file_type': 'pdf'}, page_content='Applied Machine Learning and AI\\nfor Engineers\\nSolve Business Problems That Can’t Be Solved\\nAlgorithmically\\nJeff Prosise\\nForeword by Adam Prosise'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 7, 'file_type': 'pdf'}, page_content='Applied Machine Learning and AI for Engineers\\nby Jeff Prosise\\nCopyright © 2023 Jeff Prosise. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway\\nNorth, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business,\\nor sales promotional use. Online editions are also available\\nfor most titles (http://oreilly.com). For more information,\\ncontact our corporate/institutional sales department: 800-\\n998-9938 or corporate@oreilly.com.\\n\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editor: Jill Leonard\\nProduction Editor: Gregory Hyman\\nCopyeditor: Audrey Doyle\\nProofreader: Piper Editorial Consulting, LLC\\nIndexer: Potomac Indexing, LLC\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 8, 'file_type': 'pdf'}, page_content='November 2022: First Edition\\nRevision History for the First Edition\\n\\n2022-11-10: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492098058\\nfor release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly\\nMedia, Inc. Applied Machine Learning and AI for Engineers,\\nthe cover image, and related trade dress are trademarks of\\nO’Reilly Media, Inc.\\nThe views expressed in this work are those of the author and\\ndo not represent the publisher’s views. While the publisher\\nand the author have used good faith efforts to ensure that\\nthe information and instructions contained in this work are\\naccurate, the publisher and the author disclaim all\\nresponsibility for errors or omissions, including without\\nlimitation responsibility for damages resulting from the use\\nof or reliance on this work. Use of the information and\\ninstructions contained in this work is at your own risk. If\\nany code samples or other technology this work contains or\\ndescribes is subject to open source licenses or the\\nintellectual property rights of others, it is your\\nresponsibility to ensure that your use thereof complies with\\nsuch licenses and/or rights.\\n978-1-492-09805-8\\n[LSI]'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 9, 'file_type': 'pdf'}, page_content='Dedication\\nFor my Wintellect family past and present'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 10, 'file_type': 'pdf'}, page_content='Foreword\\nWhen your dad is insatiably curious about what you are\\nstudying, home is not a safe harbor.\\nI received my master’s in analytics in 2018. Through the\\ncourse of my graduate school program, my cohort learned how\\nto leverage machine learning, AI, and analytics to add value\\nto businesses and develop solutions for real-world\\nchallenges. I have a passion for these things—a passion that\\nI share with my dad, Jeff Prosise. In fact, I can’t tell you\\nthe number of times he asked if he could join me in class\\n(I’m not kidding) or launched a salvo of questions about\\nwhat we were studying in the kitchen when I escaped the grind\\nat my parents’ house.\\nIf you have ever been on an airplane experiencing turbulence\\nand the person next to you coped by striking up a\\nconversation about what a marvel of engineering modern\\njetliners are because they do not have a single point of\\nfailure, you know exactly how I felt.\\nOur love of data and analytics grew into a shared love of the\\nvalue it provides. Using the tools and techniques outlined in\\nthis book, one can draw certitude in the face of uncertainty.\\nData science allows you to find underlying truth—to discover\\nwhat is really happening and how it drives behaviors and\\noutcomes. The ability to peek behind the curtain using\\nanalytics, rather than intuition, is an appreciating skill\\nset in our information economy and is vital for people and\\ninstitutions navigating modern uncertainty.\\nEqually important is effectively communicating these findings\\nto a nontechnical audience while having a deep technical\\nunderstanding of just what is going on under the hood. This\\nlevel of communication can’t be faked (I’ve tried a time or\\ntwo during those kitchen discussions with my dad).\\nAnalytics, AI, and machine learning do not have\\ninsurmountable technical or deployment issues. Rather, the\\nchallenge and accessibility of understanding just what is'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 11, 'file_type': 'pdf'}, page_content='happening and how they work is the impediment, because this\\nunderstanding is often shrouded in technical jargon and\\nindustry shibboleths. These barriers to entry act as a\\nlimiting principle and stunt the utilization of data science\\nto address problems and questions.\\nThat is what this book seeks to change: it removes the shroud\\nand jargon, making these tools and resources accessible.\\nAnd, to be completely honest, my dad is very good at writing\\nand teaching intimidating technical topics in a manner that\\nmakes learning them almost effortless, and he has been for\\nall my life. He has made subjects all the way back to DOS\\nunderstandable for the movers and shakers in today’s\\nbusiness world. He has made subjects that I frankly can’t\\nwrap my head around accessible for generations of programmers\\nover the last few decades. To put it simply, he’s the best.\\nThe secret is this: his ethos of how he approaches teaching a\\ntopic centers around “how would I want this explained to me\\nif I had never heard of it but was interested?” Given the\\nunique challenges data science poses and the myriad\\nperspectives professionals in this space come from, his\\napproach provides a level of accessibility not found in many\\nother places.\\nTake it from me: you couldn’t have a better guide through\\nthe complexities of ML and AI. If you’re already familiar,\\nthen this book will hone your understanding, as it did for\\nme. (Looking at you, Chapter\\xa013.) If you’re interested in\\nmachine learning, AI, analytics, and the value they add to\\nhumanity, you’re in the right place.\\nRegardless of which camp you fall in, you’ll come away with\\ndeeper knowledge of the subjects he outlines, empowering you\\nto use these tools and then tell the story of what you found.\\nUsually I would end something like this by saying, “I hope\\nyou enjoy the book and learn something,” but in this case,\\nthat would not be the truth. I don’t hope—I know. I know\\nyou will learn from and alongside my dad, as I have.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 12, 'file_type': 'pdf'}, page_content='There’s no person I’ve tried to be more like in my life\\nthan Jeff Prosise, and I couldn’t be more excited to share\\nthis aspect of him with you. And maybe—just maybe—it will\\nignite a passion for this stuff, as it did with me.\\nI’ll leave you with this. I’ll say to you what he\\napocryphally told me as he held me on the day I was born:\\n“Welcome to the show, kid.”\\nAdam Prosise\\nProcess and Innovation Specialist, Delta Air Lines'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 13, 'file_type': 'pdf'}, page_content='Preface\\nI have witnessed three great technical revolutions in my\\nlifetime: first the personal computer, then the internet, and\\nlastly the smartphone. Machine learning (ML) and AI are just\\nas fundamentally important as all three and will have an\\nequally profound impact on our lives.\\nI first became interested in machine learning the day my\\ncredit card company called to confirm that I was trying to\\npurchase a $700 necklace. I was not, but I was curious: how\\ndid they know it wasn’t me? I use my card all over the\\nworld, and for the record, I do buy my wife nice things from\\ntime to time. Not once had the credit card company declined a\\nlegitimate purchase, but several times they had correctly\\nflagged fraudulent purchases, the one prior to this being an\\nattempt by someone in Brazil to use my card to buy an airline\\nticket. This time was different: the jewelry store was 2\\nmiles from my house. I tried to imagine an algorithm that\\ncould so reliably detect credit card fraud at the point of\\nsale. It didn’t take long to realize that something more\\npowerful than a mere algorithm was at work.\\nIt turned out that the credit card company runs every\\ntransaction through a sophisticated machine learning model\\nthat is incredibly adept at detecting fraud. That moment\\nchanged my life. It’s a splendid example of how ML and AI\\nare making the world a better place. Moreover, understanding\\nhow ML could analyze credit card transactions in real time\\nand pick out the bad ones while allowing legitimate charges\\nto go through became a mountain that I had to climb.\\nWho Should Read This Book'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 14, 'file_type': 'pdf'}, page_content='Recently, I received a call from the head of engineering at a\\nmanufacturing company. He started the conversation like this:\\n“Until last week, I didn’t know what ML and AI stood for.\\nNow my CEO has tasked me with figuring out how they can\\nimprove our business, and to do it before our competitors get\\nahead of us. I am starting at square one. Can you help?”\\nThe next call came from a government contracting firm\\ninterested in using machine learning to detect tax fraud and\\nmoney laundering. The team there was reasonably well versed\\nin machine learning theory but wondered how best to go about\\nbuilding the models they needed.\\nProfessionals everywhere are realizing that ML and AI\\nrepresent a technological tsunami, and they’re trying to get\\non top of the wave before it crashes over them. This book is\\nfor them: engineers, software developers, IT managers, and\\nothers whose goal is to build a practical understanding of ML\\nand AI and put that knowledge to work solving problems that\\nwere difficult or even intractable before. It seeks to impart\\nan intuitive understanding and resorts to equations only when\\nnecessary. Despite what you may have heard, you don’t have\\nto be an expert in calculus or linear algebra to build\\nsystems that recognize objects in photos, translate English\\nto French, or expose drug traffickers and tax cheats.\\nWhy I Wrote This Book\\nInside every author is a tiny gremlin that says they can tell\\nthe story in a way that no one else has. I wrote my first\\ncomputer book more than 30 years ago and my last one more\\nthan 20 years ago, and I didn’t intend to write another one.\\nBut now I have a story to tell. It’s an important story—one\\nthat every engineer and software developer should hear. I’m\\nnot entirely satisfied with the way others have told it, so I\\nwrote the book that I wish I had had when I was learning the\\ncraft. It starts with the basics and leads you on a journey\\nto the heights of ML and AI. By the end, you’ll understand'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 15, 'file_type': 'pdf'}, page_content='how credit card companies detect fraud, how aircraft\\ncompanies use machine learning to perform predictive\\nmaintenance on jet engines, how self-driving cars see the\\nworld around them, how Google Translate translates text\\nbetween languages, and how facial recognition systems work.\\nMoreover, you’ll be able to build systems like them\\nyourself, or use existing systems to infuse AI into the apps\\nthat you write.\\nToday’s most advanced machine learning models are trained on\\ncomputers equipped with graphics processing units (GPUs) or\\ntensor processing units (TPUs), often at great time and\\nexpense. A point of emphasis in this book is presenting\\nexamples that can be built on a typical PC or laptop without\\na GPU. When we tackle computer-vision models that recognize\\nobjects in photos, I’ll describe how such models work and\\nhow they’re trained with millions of images on GPU clusters.\\nBut then I’ll show you how to use a technique called\\ntransfer learning to repurpose existing models to solve\\ndomain-specific problems and train them on an ordinary\\nlaptop.\\nThis book draws heavily from the classes and workshops that I\\nteach at companies and research institutions around the\\nworld. I love teaching because I love seeing the light bulbs\\ncome on. I often kick off classes on ML and AI by saying I’m\\nnot here to teach; I’m here to change your life. Here’s\\nhoping that your life will be a little bit different, and a\\nlittle bit better, than it was before you read this book.\\nRunning the Book’s Code Samples\\nEngineers learn best by doing, not merely by reading. This\\nbook contains numerous code samples that you can run to\\nreinforce the concepts presented in each chapter. Most are\\nwritten in Python and use popular open source libraries such\\nas Scikit-Learn, Keras, and TensorFlow. All are available in'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 16, 'file_type': 'pdf'}, page_content='a public GitHub repo. It’s the single source of truth for\\nthe code samples because I can update it at any time.\\nThere are machine learning platforms that allow you to build\\nand train models with no code. But the best way to understand\\nwhat these platforms do and how they do it is to write code.\\nPython is a simple programming language. It’s easy to learn.\\nEngineers today have to be comfortable writing code. You can\\nlearn Python as you go by working the examples in this book,\\nand if you’re already comfortable with Python (and with\\nprogramming in general), then you’re ahead of the game.\\nTo run my samples on your PC or laptop, you need a 64-bit\\nversion of Python 3.7 or higher. You can download a Python\\nruntime from Python.org, or you can install a Python\\ndistribution such as Anaconda. You also need to make sure the\\nfollowing packages and their dependencies are installed:\\n\\nScikit-Learn and TensorFlow for building machine\\nlearning models\\nPandas, Matplotlib, and Seaborn for data\\nwrangling and visualization\\nOpenCV and Pillow for handling images\\nFlask and Requests for calling REST APIs and\\nbuilding web services\\nSklearn-onnx and Onnxruntime for Open Neural\\nNetwork Exchange (ONNX) models\\nLibrosa for generating spectrogram images from\\naudio files\\nMTCNN and Keras-vggface for building facial\\nrecognition systems\\nKerasNLP, Transformers, Datasets, and PyTorch for\\nbuilding natural language processing (NLP) models'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 17, 'file_type': 'pdf'}, page_content='Azure-cognitiveservices-vision-computervision,\\nAzure-ai-textanalytics, and Azure-\\ncognitiveservices-speech for calling Azure\\nCognitive Services\\nYou can install most of these packages with pip install\\ncommands. If you installed Anaconda, many of these packages\\nare already there, and you can install the rest using conda\\ninstall commands or an equivalent.\\nSpeaking of environments, it’s never a bad idea to use\\nvirtual Python environments to prevent package installs from\\nconflicting with other package installs. If you’re not\\nfamiliar with virtual environments, you can read about them\\nat Python.org. If you use Anaconda, virtual environments are\\nbaked right in.\\nMost of my code samples were built for Jupyter notebooks,\\nwhich provide an interactive platform for writing and\\nexecuting Python code. Notebooks are incredibly popular in\\nthe data science community for exploring data and training\\nmachine learning models. You can run Jupyter notebooks\\nlocally by installing packages such as Notebook or\\nJupyterLab, or you can use cloud-hosted environments like\\nGoogle Colab. One of the advantages of Colab is that you\\ndon’t have to install anything on your computer—not even\\nPython. And in the rare cases in which my samples require a\\nGPU, Colab provides it for you.\\nPython development environments are notoriously finicky to\\nset up and maintain, especially on Windows. If you’d prefer\\nnot to have to create such an environment, or if you tried\\nbut failed to get it working, help is only a download away. I\\npackaged a complete development environment suitable for\\nrunning every sample in this book in a Docker container\\nimage. Assuming you have the Docker Engine installed on your\\ncomputer, you can launch the container with the following\\ncommand:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 18, 'file_type': 'pdf'}, page_content='docker run -it -p 8888:8888 jeffpro/applied-machine-learning:latest\\nNavigate in your browser to the URL that appears in the\\noutput. You’ll land in a full Jupyter environment with all\\nmy code samples and everything needed to run them. They’re\\nin a folder named Applied-Machine-Learning cloned from the\\nGitHub repo of the same name. The downside to using a\\ncontainer is that changes you make aren’t persisted by\\ndefault. One way to remedy that is to use a -v switch in the\\ndocker command to bind to a local directory. For more\\ninformation, refer to “Use Bind Mounts” in the Docker\\ndocumentation.\\nNavigating This Book\\nThis book is organized into two parts:\\n\\nPart\\xa0I (Chapters 1 through 7) teaches the ABCs\\nof machine learning and introduce popular\\nlearning algorithms such as logistic regression\\nand gradient boosting.\\nPart\\xa0II (Chapters 8 through 14) covers deep\\nlearning, which is synonymous with AI today and\\nuses deep neural networks to fit mathematical\\nmodels to data.\\nI highly encourage you to work the exercises as you read the\\nbook. You’ll come away with a deeper understanding of the\\nmaterial, and you’ll no doubt think of ways to modify my\\nexamples to play “what if?” with the code.\\nConventions Used in This Book\\nThe following typographical conventions are used in this\\nbook:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 19, 'file_type': 'pdf'}, page_content='Italic\\nIndicates new terms, URLs, email addresses,\\nfilenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within\\nparagraphs to refer to program elements such as\\nvariable or function names, databases, data types,\\nenvironment variables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed\\nliterally by the user.\\nTIP\\nThis element signifies a tip or suggestion.\\nNOTE\\nThis element signifies a general note.\\nWARNING\\nThis element indicates a warning or caution.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 20, 'file_type': 'pdf'}, page_content='Using Code Examples\\nAs mentioned, supplemental material (code examples,\\nexercises, etc.) is available for download at\\nhttps://oreil.ly/applied-machine-learning-code.\\nIf you have a technical question or a problem using the code\\nexamples, please send email to bookquestions@oreilly.com.\\nThis book is here to help you get your job done. In general,\\nif example code is offered with this book, you may use it in\\nyour programs and documentation. You do not need to contact\\nus for permission unless you’re reproducing a significant\\nportion of the code. For example, writing a program that uses\\nseveral chunks of code from this book does not require\\npermission. Selling or distributing examples from O’Reilly\\nbooks does require permission. Answering a question by citing\\nthis book and quoting example code does not require\\npermission. Incorporating a significant amount of example\\ncode from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but generally do not require, attribution. An\\nattribution usually includes the title, author, publisher,\\nand ISBN. For example: “Applied Machine Learning and AI for\\nEngineers by Jeff Prosise (O’Reilly). Copyright 2023 Jeff\\nProsise, 978-1-492-09805-8.”\\nIf you feel your use of code examples falls outside fair use\\nor the permission given above, feel free to contact us at\\npermissions@oreilly.com.\\nO’Reilly Online Learning\\nNOTE\\nFor more than 40 years, O’Reilly Media has provided technology\\nand business training, knowledge, and insight to help companies'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 21, 'file_type': 'pdf'}, page_content='succeed.\\nOur unique network of experts and innovators share their\\nknowledge and expertise through books, articles, and our\\nonline learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses,\\nin-depth learning paths, interactive coding environments, and\\na vast collection of text and video from O’Reilly and 200+\\nother publishers. For more information, visit\\nhttps://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to\\nthe publisher:\\n\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 22, 'file_type': 'pdf'}, page_content='We have a web page for this book, where we list errata,\\nexamples, and any additional information. You can access this\\npage at https://oreil.ly/applied-machine-learning.\\nEmail bookquestions@oreilly.com to comment or ask technical\\nquestions about this book.\\nFor news and information about our books and courses, visit\\nhttps://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-\\nmedia\\nFollow us on Twitter: https://twitter.com/oreillymedia\\nWatch us on YouTube: https://youtube.com/oreillymedia\\nAcknowledgments\\nWriting and publishing a book is a team effort. It starts\\nwith the author, but before it lands on shelves, it goes\\nthrough reviewers, developmental editors, copy editors,\\nartists, and production personnel.\\nI’d like to thank several friends and family members for\\nreviewing chapters as I wrote them and providing constructive\\nfeedback. They are engineers, mathematicians, data analysts,\\nand professors, and they happen to be among the smartest\\npeople I know: Larry Clement, Manjeet Dahiya, Tom Marshall,\\nDon Meyer, Goku Mohandas, Ken Muse, Lipi Deepaakshi Patnaik,\\nCharles Petzold, Abby Prosise, Adam Prosise, Jeffrey Richter,\\nBruce Schecter, Vishwesh Ravi Shrimali, Brian Spiering, and\\nRon Sumida.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 24, 'file_type': 'pdf'}, page_content='A big thank-you to the team at O’Reilly too, for turning my\\nwords into prose and my sketches into art. That includes Jill\\nLeonard, Audrey Doyle, Gregory Hyman, David Futato, Karen\\nMontgomery, and Nicole Butterfield. A special shout-out to\\nJon Hassell, who heard my vision for this book and said,\\n“Let’s do it.” I have had the privilege of working with\\nsome great publishing teams over the years. None were better\\nthan this one.\\nFinally, to Lori—my wife, travel companion, and partner in\\ncrime for the last 40 years. I couldn’t have done it without\\nyou. And I promise this will be the last book I write!'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 25, 'file_type': 'pdf'}, page_content='Part I. Machine Learning with\\nScikit-Learn'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 26, 'file_type': 'pdf'}, page_content='Chapter 1. Machine Learning\\nMachine learning expands the boundaries of what’s possible\\nby allowing computers to solve problems that were intractable\\njust a few short years ago. From fraud detection and medical\\ndiagnoses to product recommendations and cars that “see”\\nwhat’s in front of them, machine learning impacts our lives\\nevery day. As you read this, scientists are using machine\\nlearning to unlock the secrets of the human genome. When we\\none day cure cancer, we will thank machine learning for\\nmaking it possible.\\nMachine learning is revolutionary because it provides an\\nalternative to algorithmic problem-solving. Given a recipe,\\nor algorithm, it’s not difficult to write an app that hashes\\na password or computes a monthly mortgage payment. You code\\nup the algorithm, feed it input, and receive output in\\nreturn. It’s another proposition altogether to write code\\nthat determines whether a photo contains a cat or a dog. You\\ncan try to do it algorithmically, but the minute you get it\\nworking, you’ll come across a cat or dog picture that breaks\\nthe algorithm.\\nMachine learning takes a different approach to turning input\\ninto output. Rather than relying on you to implement an\\nalgorithm, it examines a dataset of inputs and outputs and\\nlearns how to generate output of its own in a process known\\nas training. Under the hood, special algorithms called\\nlearning algorithms fit mathematical models to the data and\\ncodify the relationship between data going in and data coming\\nout. Once trained, a model can accept new inputs and generate\\noutputs consistent with the ones in the training data.\\nTo use machine learning to distinguish between cats and dogs,\\nyou don’t code a cat-versus-dog algorithm. Instead, you\\ntrain a machine learning model with cat and dog photos.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 27, 'file_type': 'pdf'}, page_content='Success depends on the learning algorithm used and the\\nquality and volume of the training data.\\nPart of becoming a machine learning engineer is familiarizing\\nyourself with the various learning algorithms and developing\\nan intuition for when to use one versus another. That\\nintuition comes from experience and from an understanding of\\nhow machine learning fits mathematical models to data. This\\nchapter represents the first step on that journey. It begins\\nwith an overview of machine learning and the most common\\ntypes of machine learning models, and it concludes by\\nintroducing two popular learning algorithms and using them to\\nbuild simple yet fully functional models.\\nWhat Is Machine Learning?\\nAt an existential level, machine learning (ML) is a means for\\nfinding patterns in numbers and exploiting those patterns to\\nmake predictions. ML makes it possible to train a model with\\nrows or sequences of 1s and 0s, and to learn from the data so\\nthat, given a new sequence, the model can predict what the\\nresult will be. Learning is the process by which ML finds\\npatterns that can be used to predict future outputs, and\\nit’s where the “learning” in “machine learning” comes\\nfrom.\\nAs an example, consider the table of 1s and 0s depicted in\\nFigure\\xa01-1. Each number in the fourth column is somehow\\nbased on the three numbers preceding it in the same row.\\nWhat’s the missing number?'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 28, 'file_type': 'pdf'}, page_content='Figure 1-1. Simple dataset consisting of 0s and 1s\\nOne possible solution is that for a given row, if the first\\nthree columns contain more 0s than 1s, then the fourth\\ncontains a 0. If the first three columns contain more 1s than\\n0s, then the answer is 1. By this logic, the empty box should\\ncontain a 1. Data scientists refer to the column containing\\nanswers (the red column in the figure) as the label column.\\nThe remaining columns are feature columns. The goal of a\\npredictive model is to find patterns in the rows in the\\nfeature columns that allow it to predict what the label will\\nbe.\\nIf all datasets were this simple, you wouldn’t need machine\\nlearning. But real-world datasets are larger and more\\ncomplex. What if the dataset contained millions of rows and\\nthousands of columns, which, as it happens, is common in\\nmachine learning? For that matter, what if the dataset\\nresembled the one in Figure\\xa01-2?'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 29, 'file_type': 'pdf'}, page_content='Figure 1-2. A more complex dataset\\nIt’s difficult for any human to examine this dataset and\\ncome up with a set of rules for predicting whether the red\\nbox should contain a 0 or a 1. (And no, it’s not as simple\\nas counting 1s and 0s.) Just imagine how much more difficult\\nit would be if the dataset really did have millions of rows\\nand thousands of columns.\\nThat’s what machine learning is all about: finding patterns\\nin massive datasets of numbers. It doesn’t matter whether\\nthere are 100 rows or 1,000,000 rows. In many cases, more is\\nbetter, because 100 rows might not provide enough samples for\\npatterns to be discerned.\\nIt isn’t an oversimplification to say that machine learning\\nsolves problems by mathematically modeling patterns in sets\\nof numbers. Most any problem can be reduced to a set of\\nnumbers. For example, one of the common applications for ML\\ntoday is sentiment analysis: looking at a text sample such as\\na movie review or a comment left on a website and assigning\\nit a 0 for negative sentiment (for example, “The food was\\nbland and the service was terrible.”) or a 1 for positive\\nsentiment (“Excellent food and service. Can’t wait to visit\\nagain!”). Some reviews might be mixed—for example, “The\\nburger was great but the fries were soggy”—so we use the\\nprobability that the label is a 1 as a sentiment score. A\\nvery negative comment might score a 0.1, while a very'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 30, 'file_type': 'pdf'}, page_content='positive comment might score a 0.9, as in there’s a 90%\\nchance that it expresses positive sentiment.\\nSentiment analyzers and other models that work with text are\\nfrequently trained on datasets like the one in Figure\\xa01-3,\\nwhich contains one row for every text sample and one column\\nfor every word in the corpus of text (all the words in the\\ndataset). A typical dataset like this one might contain\\nmillions of rows and 20,000 or more columns. Each row\\ncontains a 0 for negative sentiment in the label column, or a\\n1 for positive sentiment. Within each row are word counts—\\nthe number of times a given word appears in an individual\\nsample. The dataset is sparse, meaning it is mostly 0s with\\nan occasional nonzero number sprinkled in. But machine\\nlearning doesn’t care about the makeup of the numbers. If\\nthere are patterns that can be exploited to determine whether\\nthe next sample expresses positive or negative sentiment, it\\nwill find them. Spam filters use datasets such as these with\\n1s and 0s in the label column denoting spam and nonspam\\nmessages. This allows modern spam filters to achieve an\\nastonishing degree of accuracy. Moreover, these models grow\\nsmarter over time as they are trained with more and more\\nemails.\\nFigure 1-3. Dataset for sentiment analysis'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 31, 'file_type': 'pdf'}, page_content='Sentiment analysis is an example of a text classification\\ntask: analyzing a text sample and classifying it as positive\\nor negative. Machine learning has proven adept at image\\nclassification as well. A simple example of image\\nclassification is looking at photos of cats and dogs and\\nclassifying each one as a cat picture (0) or a dog picture\\n(1). Real-world uses for image classification include\\nflagging defective parts coming off an assembly line,\\nidentifying objects in view of a self-driving car, and\\nrecognizing faces in photos.\\nImage classification models are trained with datasets like\\nthe one in Figure\\xa01-4, in which each row represents an image\\nand each column holds a pixel value. A dataset with 1,000,000\\nimages that are 200 pixels wide and 200 pixels high contains\\n1,000,000 rows and 40,000 columns. That’s 40 billion numbers\\nin all, or 120,000,000,000 if the images are color rather\\nthan grayscale. (In color images, pixel values comprise three\\nnumbers rather than one.) The label column contains a number\\nrepresenting the class or category to which the corresponding\\nimage belongs—in this case, the person whose face appears in\\nthe picture: 0 for Gerhard Schroeder, 1 for George W. Bush,\\nand so on.\\nFigure 1-4. Dataset for image classification'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 32, 'file_type': 'pdf'}, page_content='These facial images come from a famous public dataset called\\nLabeled Faces in the Wild, or LFW for short. It is one of\\ncountless labeled datasets that are published in various\\nplaces for public consumption. Machine learning isn’t hard\\nwhen you have labeled datasets to work with—datasets that\\nothers (often grad students) have laboriously spent hours\\nlabeling with 1s and 0s. In the real world, engineers\\nsometimes spend the bulk of their time generating these\\ndatasets. One of the more popular repositories for public\\ndatasets is Kaggle.com, which makes lots of useful datasets\\navailable and holds competitions allowing budding ML\\npractitioners to test their skills.\\nMachine Learning Versus Artificial\\nIntelligence\\nThe terms machine learning and artificial intelligence (AI)\\nare used almost interchangeably today, but in fact, each term\\nhas a specific meaning, as shown in Figure\\xa01-5.\\nTechnically speaking, machine learning is a subset of AI,\\nwhich encompasses not only machine learning models but also\\nother types of models such as expert systems (systems that\\nmake decisions based on rules that you define) and\\nreinforcement learning systems, which learn behaviors by\\nrewarding positive outcomes while penalizing negative ones.\\nAn example of a reinforcement learning system is AlphaGo,\\nwhich was the first computer program to beat a professional\\nhuman Go player. It trains on games that have already been\\nplayed and learns strategies for winning on its own.\\nAs a practical matter, what most people refer to as AI today\\nis in fact deep learning, which is a subset of machine\\nlearning. Deep learning is machine learning performed with\\nneural networks. (There are forms of deep learning that\\ndon’t involve neural networks—deep Boltzmann machines are\\none example—but the vast majority of deep learning today\\ninvolves neural networks.) Thus, ML models can be divided'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 33, 'file_type': 'pdf'}, page_content='into conventional models that use learning algorithms to\\nmodel patterns in data, and deep-learning models that use\\nneural networks to do the same.\\nFigure 1-5. Relationship between machine learning, deep learning, and AI\\nA BRIEF HISTORY OF AI\\nML and AI have surged in popularity in recent years. AI\\nwas a big deal in the 1980s, when it was widely believed\\nthat computers would soon be able to mimic the human'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 34, 'file_type': 'pdf'}, page_content='mind. But excitement waned, and for decades—up until\\n2010 or so—AI rarely made the news. Then a strange thing\\nhappened.\\nThanks to the availability of graphics processing units\\n(GPUs) from companies such as NVIDIA, researchers finally\\nhad the horsepower they needed to train advanced neural\\nnetworks. This led to advancements in the state of the\\nart, which led to renewed enthusiasm, which led to\\nadditional funding, which precipitated further\\nadvancements, and suddenly AI was a thing again. Neural\\nnetworks have been around (at least in theory) since the\\n1950s, but researchers lacked the computational power to\\ntrain them on large datasets. Today anyone can buy a GPU\\nor spin up a GPU cluster in the cloud. AI is advancing\\nmore rapidly now than ever before, and with that progress\\ncomes the ability to do things in software that engineers\\ncould only have dreamed about as recently as a decade\\nago.\\nOver time, data scientists have devised special types of\\nneural networks that excel at certain tasks, including tasks\\ninvolving computer vision—for example, distilling\\ninformation from images—and tasks that involve human\\nlanguages such as translating English to French. We’ll take\\na deep dive into neural networks beginning in Chapter\\xa08, and\\nyou’ll learn specifically how deep learning has elevated\\nmachine learning to new heights.\\nSupervised Versus Unsupervised Learning\\nMost ML models fall into one of two broad categories:\\nsupervised learning models and unsupervised learning models.\\nThe purpose of supervised learning models is to make\\npredictions. You train them with labeled data so that they\\ncan take future inputs and predict what the labels will be.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 35, 'file_type': 'pdf'}, page_content='Most of the ML models in use today are supervised learning\\nmodels. A great example is the model that the US Postal\\nService uses to turn handwritten zip codes into digits that a\\ncomputer can recognize to sort the mail. Another example is\\nthe model that your credit card company uses to authorize\\npurchases.\\nUnsupervised learning models, by contrast, don’t require\\nlabeled data. Their purpose is to provide insights into\\nexisting data, or to group data into categories and\\ncategorize future inputs accordingly. A classic example of\\nunsupervised learning is inspecting records regarding\\nproducts purchased from your company and the customers who\\npurchased them to determine which customers might be most\\ninterested in a new product you are launching and then\\nbuilding a marketing campaign that targets those customers.\\nA spam filter is a supervised learning model. It requires\\nlabeled data. A model that segments customers based on\\nincomes, credit scores, and purchasing history is an\\nunsupervised learning model, and the data that it consumes\\ndoesn’t have to be labeled. To help drive home the\\ndifference, the remainder of this chapter explores supervised\\nand unsupervised learning in greater detail.\\nUnsupervised Learning with k-Means Clustering\\nUnsupervised learning frequently employs a technique called\\nclustering. The purpose of clustering is to group data by\\nsimilarity. The most popular clustering algorithm is k-means\\nclustering, which takes n data samples and groups them into m\\nclusters, where m is a number you specify.\\nGrouping is performed using an iterative process that\\ncomputes a centroid for each cluster and assigns samples to\\nclusters based on their proximity to the cluster centroids.\\nIf the distance from a particular sample to the centroid of\\ncluster 1 is 2.0 and the distance from the same sample to the'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 36, 'file_type': 'pdf'}, page_content='center of cluster 2 is 3.0, then the sample is assigned to\\ncluster 1. In Figure\\xa01-6, 200 samples are loosely arranged\\nin three clusters. The diagram on the left shows the raw,\\nungrouped samples. The diagram on the right shows the cluster\\ncentroids (the red dots) with the samples colored by cluster.\\nFigure 1-6. Data points grouped using k-means clustering\\nHow do you code up an unsupervised learning model that\\nimplements k-means clustering? The easiest way to do it is to\\nuse the world’s most popular machine learning library:\\nScikit-Learn. It’s free, it’s open source, and it’s\\nwritten in Python. The documentation is great, and if you\\nhave a question, chances are you’ll find an answer by\\nGoogling it. I’ll use Scikit for most of the examples in the\\nfirst half of this book. The book’s Preface describes how to\\ninstall Scikit and configure your computer to run my examples\\n(or use a Docker container to do the same), so if you\\nhaven’t done so already, now’s a great time to set up your\\nenvironment.\\nTo get your feet wet with k-means clustering, start by\\ncreating a new Jupyter notebook and pasting the following\\nstatements into the first cell:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 37, 'file_type': 'pdf'}, page_content='import\\nseaborn\\nas\\nsns\\nsns.set()\\nRun that cell, and then run the following code in the next\\ncell to generate a semirandom assortment of x and y\\ncoordinate pairs. This code uses Scikit’s make_blobs\\nfunction to generate the coordinate pairs, and Matplotlib’s\\nscatter function to plot them:\\n\\nfrom\\nsklearn.datasets\\nimport\\nmake_blobs\\n\\npoints,\\ncluster_indexes\\nmake_blobs(n_samples=300,\\ncenters=4,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cluster_std=0.8,\\nrandom_state=0)\\n\\nx\\npoints[:,\\ny\\npoints[:,\\n\\nplt.scatter(x,\\ny,\\ns=50,\\nalpha=0.7)\\nYour output should be identical to mine, thanks to the\\nrandom_state parameter that seeds the random-number generator\\nused internally by make_blobs:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 38, 'file_type': 'pdf'}, page_content=\"Next, use k-means clustering to divide the coordinate pairs\\ninto four groups. Then render the cluster centroids in red\\nand color-code the data points by cluster. Scikit’s KMeans\\nclass does the heavy lifting, and once it’s fit to the\\ncoordinate pairs, you can get the locations of the centroids\\nfrom KMeans’ cluster_centers_ attribute:\\n\\nfrom\\nsklearn.cluster\\nimport\\nKMeans\\n\\nkmeans\\nKMeans(n_clusters=4,\\nrandom_state=0)\\nkmeans.fit(points)\\npredicted_cluster_indexes\\nkmeans.predict(points)\\n\\nplt.scatter(x,\\ny,\\nc=predicted_cluster_indexes,\\ns=50,\\nalpha=0.7,\\ncmap='viridis')\\n\\ncenters\\nkmeans.cluster_centers_\\nplt.scatter(centers[:,\\ncenters[:,\\nc='red',\\ns=100)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 39, 'file_type': 'pdf'}, page_content='Here is the result:\\nTry setting n_clusters to other values, such as 3 and 5, to\\nsee how the points are grouped with different cluster counts.\\nWhich begs the question: how do you know what the right\\nnumber of clusters is? The answer isn’t always obvious from\\nlooking at a plot, and if the data has more than three\\ndimensions, you can’t plot it anyway.\\nOne way to pick the right number is with the elbow method,\\nwhich plots inertias (the sum of the squared distances of the\\ndata points to the closest cluster center) obtained from\\nKMeans.inertia_ as a function of cluster counts. Plot\\ninertias this way and look for the sharpest elbow in the\\ncurve:\\ninertias\\n\\nfor\\ni\\nin\\nrange(1,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 40, 'file_type': 'pdf'}, page_content=\"kmeans\\nKMeans(n_clusters=i,\\nrandom_state=0)\\n\\xa0\\xa0\\xa0 kmeans.fit(points)\\n\\xa0\\xa0\\xa0 inertias.append(kmeans.inertia_)\\n\\nplt.plot(range(1,\\ninertias)\\nplt.xlabel('Number of clusters')\\nplt.ylabel('Inertia')\\nIn this example, it appears that 4 is the right number of\\nclusters:\\nIn real life, the elbow might not be so distinct. That’s OK,\\nbecause by clustering the data in different ways, you\\nsometimes obtain insights that you wouldn’t obtain\\notherwise.\\nApplying k-Means Clustering to Customer Data\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 41, 'file_type': 'pdf'}, page_content=\"Let’s use k-means clustering to tackle a real problem:\\nsegmenting customers to identify ones to target with a\\npromotion to increase their purchasing activity. The dataset\\nthat you’ll use is a sample customer segmentation dataset\\nnamed customers.csv. Start by creating a subdirectory named\\nData in the folder where your notebooks reside, downloading\\ncustomers.csv, and copying it into the Data subdirectory.\\nThen use the following code to load the dataset into a Pandas\\nDataFrame and display the first five rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ncustomers\\npd.read_csv('Data/customers.csv')\\ncustomers.head()\\nFrom the output, you learn that the dataset contains five\\ncolumns, two of which describe the customer’s annual income\\nand spending score. The latter is a value from 0 to 100. The\\nhigher the number, the more this customer has spent with your\\ncompany in the past:\\nNow use the following code to plot the annual incomes and\\nspending scores:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 42, 'file_type': 'pdf'}, page_content=\"%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\npoints\\ncustomers.iloc[:,\\n3:5].values\\nx\\npoints[:,\\ny\\npoints[:,\\n\\nplt.scatter(x,\\ny,\\ns=50,\\nalpha=0.7)\\nplt.xlabel('Annual Income (k$)')\\nplt.ylabel('Spending Score')\\nFrom the results, it appears that the data points fall into\\nroughly five clusters:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 43, 'file_type': 'pdf'}, page_content=\"Use the following code to segment the customers into five\\nclusters and highlight the clusters:\\n\\nfrom\\nsklearn.cluster\\nimport\\nKMeans\\n\\nkmeans\\nKMeans(n_clusters=5,\\nrandom_state=0)\\nkmeans.fit(points)\\npredicted_cluster_indexes\\nkmeans.predict(points)\\n\\nplt.scatter(x,\\ny,\\nc=predicted_cluster_indexes,\\ns=50,\\nalpha=0.7,\\ncmap='viridis')\\nplt.xlabel('Annual Income (k$)')\\nplt.ylabel('Spending Score')\\n\\ncenters\\nkmeans.cluster_centers_\\nplt.scatter(centers[:,\\ncenters[:,\\nc='red',\\ns=100)\\nHere is the result:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 44, 'file_type': 'pdf'}, page_content=\"The customers in the lower-right quadrant of the chart might\\nbe good ones to target with a promotion to increase their\\nspending. Why? Because they have high incomes but low\\nspending scores. Use the following statements to create a\\ncopy of the DataFrame and add a column named Cluster\\ncontaining cluster indexes:\\n\\ndf\\ncustomers.copy()\\ndf['Cluster']\\nkmeans.predict(points)\\ndf.head()\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 45, 'file_type': 'pdf'}, page_content=\"Now use the following code to output the IDs of customers who\\nhave high incomes but low spending scores:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\n# Get the cluster index for a customer with a high income and low spending\\nscore\\ncluster\\nkmeans.predict(np.array([[120,\\n20]]))[0]\\n\\n# Filter the DataFrame to include only customers in that cluster\\nclustered_df\\ndf[df['Cluster']\\ncluster]\\n\\n# Show the customer IDs\\nclustered_df['CustomerID'].values\\nYou could easily use the resulting customer IDs to extract\\nnames and email addresses from a customer database:\\n\\narray([125,\\n199],\\ndtype=int64)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 46, 'file_type': 'pdf'}, page_content='The key here is that you used clustering to group customers\\nby annual income and spending score. Once customers are\\ngrouped in this manner, it’s a simple matter to enumerate\\nthe customers in each cluster.\\nSegmenting Customers Using More Than Two\\nDimensions\\nThe previous\\xa0 example was an easy one because you used just\\ntwo variables: annual incomes and spending scores. You could\\nhave done the same without help from machine learning. But\\nnow let’s segment the customers again, this time using\\neverything except the customer IDs. Start by replacing the\\nstrings \"Male\" and \"Female\" in the Gender column with 1s and\\n0s, a process known as label encoding. This is necessary\\nbecause machine learning can only deal with numerical data:\\n\\nfrom\\nsklearn.preprocessing\\nimport\\nLabelEncoder\\n\\ndf\\ncustomers.copy()\\nencoder\\nLabelEncoder()\\ndf[\\'Gender\\']\\nencoder.fit_transform(df[\\'Gender\\'])\\ndf.head()\\nThe Gender column now contains 1s and 0s:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 47, 'file_type': 'pdf'}, page_content=\"Extract the gender, age, annual income, and spending score\\ncolumns. Then use the elbow method to determine the optimum\\nnumber of clusters based on these features:\\n\\npoints\\ndf.iloc[:,\\n1:5].values\\ninertias\\n\\nfor\\ni\\nin\\nrange(1,\\n\\xa0\\xa0\\xa0 kmeans\\nKMeans(n_clusters=i,\\nrandom_state=0)\\n\\xa0\\xa0\\xa0 kmeans.fit(points)\\n\\xa0\\xa0\\xa0 inertias.append(kmeans.inertia_)\\n\\nplt.plot(range(1,\\ninertias)\\nplt.xlabel('Number of Clusters')\\nplt.ylabel('Inertia')\\nThe elbow is less distinct this time, but 5 appears to be a\\nreasonable number:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 48, 'file_type': 'pdf'}, page_content=\"Segment the customers into five clusters and add a column\\nnamed Cluster containing the index of the cluster (0-4) to\\nwhich the customer was assigned:\\n\\nkmeans\\nKMeans(n_clusters=5,\\nrandom_state=0)\\nkmeans.fit(points)\\n\\ndf['Cluster']\\nkmeans.predict(points)\\ndf.head()\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 49, 'file_type': 'pdf'}, page_content=\"You have a cluster number for each customer, but what does it\\nmean? You can’t plot gender, age, annual income, and\\nspending score in a two-dimensional chart the way you plotted\\nannual income and spending score in the previous example. But\\nyou can get the mean (average) of these values for each\\ncluster from the cluster centroids. Create a new DataFrame\\nwith columns for average age, average income, and so on, and\\nthen show the results in a table:\\n\\nresults\\npd.DataFrame(columns\\n['Cluster',\\n'Average Age',\\n'Average Income',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Average Spending Index',\\n'Number of\\nFemales',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Number of Males'])\\n\\nfor\\ni,\\ncenter\\nin\\nenumerate(kmeans.cluster_centers_):\\n\\xa0\\xa0\\xa0 age\\ncenter[1]\\xa0\\xa0\\xa0 # Average age for current cluster\\n\\xa0\\xa0\\xa0 income\\ncenter[2]\\n# Average income for current cluster\\n\\xa0\\xa0\\xa0 spend\\ncenter[3]\\xa0 # Average spending score for current cluster\\n\\n\\xa0\\xa0\\xa0 gdf\\ndf[df['Cluster']\\ni]\\n\\xa0\\xa0\\xa0 females\\ngdf[gdf['Gender']\\n0].shape[0]\\n\\xa0\\xa0\\xa0 males\\ngdf[gdf['Gender']\\n1].shape[0]\\n\\n\\xa0\\xa0\\xa0 results.loc[i]\\n([i,\\nage,\\nincome,\\nspend,\\nfemales,\\nmales])\\n\\nresults.head()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 50, 'file_type': 'pdf'}, page_content='The output is as follows:\\nBased on this, if you were going to target customers with\\nhigh incomes but low spending scores for a promotion, which\\ngroup of customers (which cluster) would you choose? Would it\\nmatter whether you targeted males or females? For that\\nmatter, what if your goal was to create a loyalty program\\nrewarding customers with high spending scores, but you wanted\\nto give preference to younger customers who might be loyal\\ncustomers for a long time? Which cluster would you target\\nthen?\\nAmong the more interesting insights that clustering reveals\\nis that some of the biggest spenders are young people\\n(average age = 25.5) with modest incomes. Those customers are\\nmore likely to be female than male. All of this is useful\\ninformation to have if you’re growing a company and want to\\nbetter understand the demographics that you serve.\\nNOTE\\nk-means might be the most commonly used clustering algorithm, but\\nit’s not the only one. Others include agglomerative clustering,\\nwhich clusters data points in a hierarchical manner, and DBSCAN,\\nwhich stands for density-based spatial clustering of applications\\nwith noise. DBSCAN doesn’t require the cluster count to be\\nspecified ahead of time. It can also identify points that fall\\noutside the clusters it identifies, which is useful for detecting\\noutliers—anomalous data points that don’t fit in with the rest.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 51, 'file_type': 'pdf'}, page_content='Scikit-Learn provides implementations of both algorithms in its\\nAgglomerative\\u200bClus\\u2060ter\\u2060ing and DBSCAN classes.\\nDo real companies use clustering to extract insights from\\ncustomer data? Indeed they do. During grad school, my son,\\nnow a data analyst for Delta Air Lines, interned at a pet\\nsupplies company. He used k-means clustering to determine\\nthat the number one reason that leads coming in through the\\ncompany’s website weren’t converted to sales was the length\\nof time between when the lead came in and Sales first\\ncontacted the customer. As a result, his employer introduced\\nadditional automation to the sales workflow to ensure that\\nleads were acted on quickly. That’s unsupervised learning at\\nwork. And it’s a splendid example of a company using machine\\nlearning to improve its business processes.\\nSupervised Learning\\nUnsupervised learning is an important branch of machine\\nlearning, but when most people hear the term machine learning\\nthey think about supervised learning. Recall that supervised\\nlearning models make predictions. For example, they predict\\nwhether a credit card transaction is fraudulent or a flight\\nwill arrive on time. They’re also trained with labeled data.\\nSupervised learning models come in two varieties: regression\\nmodels and classification models. The purpose of a regression\\nmodel is to predict a numeric outcome such as the price that\\na home will sell for or the age of a person in a photo.\\nClassification models, by contrast, predict a class or\\ncategory from a finite set of classes defined in the training\\ndata. Examples include whether a credit card transaction is\\nlegitimate or fraudulent and what number a handwritten digit\\nrepresents. The former is a binary classification model\\nbecause there are just two possible outcomes: the transaction'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 52, 'file_type': 'pdf'}, page_content='is legitimate or it’s not. The latter is an example of\\nmulticlass classification. Because there are 10 digits (0–9)\\nin the Western Arabic numeral system, there are 10 possible\\nclasses that a handwritten digit could represent.\\nThe two types of supervised learning models are pictured in\\nFigure\\xa01-7. On the left, the goal is to input an x and\\npredict what y will be. On the right, the goal is to input an\\nx and a y and predict what class the point corresponds to: a\\ntriangle or an ellipse. In both cases, the purpose of\\napplying machine learning to the problem is to build a model\\nfor making predictions. Rather than build that model\\nyourself, you train a machine learning model with labeled\\ndata and allow it to devise a mathematical model for you.\\nFigure 1-7. Regression versus classification\\nFor these datasets, you could easily build mathematical\\nmodels without resorting to machine learning. For a\\nregression model, you could draw a line through the data\\npoints and use the equation of that line to predict a y given\\nan x (Figure\\xa01-8). For a classification model, you could\\ndraw a line that cleanly separates triangles from ellipses—\\nwhat data scientists call a classification boundary—and\\npredict which class a new point represents by determining'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 53, 'file_type': 'pdf'}, page_content='whether the point falls above or below the line. A point just\\nabove the line would be a triangle, while a point just below\\nit would classify as an ellipse.\\nFigure 1-8. Regression line and linear separation boundary\\nIn the real world, datasets are rarely this orderly. They\\ntypically look more like the ones in Figure\\xa01-9, in which\\nthere is no single line you can draw to correlate the x and y\\nvalues on the left or cleanly separate the classes on the\\nright. The goal, therefore, is to build the best model you\\ncan. That means picking the learning algorithm that produces\\nthe most accurate model.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 54, 'file_type': 'pdf'}, page_content='Figure 1-9. Real-world datasets\\nThere are many supervised learning algorithms. They go by\\nnames such as linear regression, random forests, gradient-\\nboosting machines (GBMs), and support vector machines (SVMs).\\nMany, but not all, can be used for regression and\\nclassification. Even seasoned data scientists frequently\\nexperiment to determine which learning algorithm produces the\\nmost accurate model. These and other learning algorithms will\\nbe covered in subsequent chapters.\\nk-Nearest Neighbors\\nOne of the simplest supervised learning algorithms is k-\\nnearest neighbors. The premise behind it is that given a set\\nof data points, you can predict a label for a new point by\\nexamining the points nearest it. For a simple regression\\nproblem in which each data point is characterized by x and y\\ncoordinates, this means that given an x, you can predict a y\\nby finding the n points with the nearest xs and averaging\\ntheir ys. For a classification problem, you find the n points\\nclosest to the point whose class you want to predict and\\nchoose the class with the highest occurrence count. If n = 5\\nand the five nearest neighbors include three triangles and'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 55, 'file_type': 'pdf'}, page_content='two ellipses, then the answer is a triangle, as pictured in\\nFigure\\xa01-10.\\nFigure 1-10. Classification with k-nearest neighbors\\nHere’s an example involving regression. Suppose you have 20\\ndata points describing how much programmers earn per year\\nbased on years of experience. Figure\\xa01-11 plots years of\\nexperience on the x-axis and annual income on the y-axis.\\nYour goal is to predict what someone with 10 years of\\nexperience should earn. In this example, x = 10, and you want\\nto predict what y should be.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 56, 'file_type': 'pdf'}, page_content='Figure 1-11. Programmers’ salaries in dollars versus years of experience\\nApplying k-nearest neighbors with n = 10 identifies the\\npoints highlighted in orange in Figure\\xa01-12 as the nearest\\nneighbors—the 10 whose x coordinates are closest to x = 10.\\nThe average of these points’ y coordinates is 94,838.\\nTherefore, k-nearest neighbors with n = 10 predicts that a\\nprogrammer with 10 years of experience will earn $94,838, as\\nindicated by the red dot.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 57, 'file_type': 'pdf'}, page_content='Figure 1-12. Regression with k-nearest neighbors and n = 10\\nThe value of n that you use with k-nearest neighbors\\nfrequently influences the outcome. Figure\\xa01-13 shows the\\nsame solution with n = 5. The answer is slightly different\\nthis time because the average y for the five nearest\\nneighbors is 98,713.\\nIn real life, it’s a little more nuanced because while the\\ndataset has just one label column, it probably has several\\nfeature columns—not just x, but x1, x2, x3, and so on. You\\ncan compute distances in n-dimensional space easily enough,\\nbut there are several ways to measure distances to identify a\\npoint’s nearest neighbors, including Euclidean distance,\\nManhattan distance, and Minkowski distance. You can even use\\nweights so that nearby points contribute more to the outcome\\nthan faraway points. And rather than find the n nearest\\nneighbors, you can select all the neighbors within a given\\nradius, a technique known as radius neighbors. Still, the\\nprinciple is the same regardless of the number of dimensions\\nin the dataset, the method used to measure distance, or'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 58, 'file_type': 'pdf'}, page_content='whether you choose n nearest neighbors or all the neighbors\\nwithin a specified radius: find data points that are similar\\nto the target point and use them to regress or classify the\\ntarget.\\nFigure 1-13. Regression with k-nearest neighbors and n = 5\\nUsing k-Nearest Neighbors to Classify Flowers\\nScikit-Learn includes classes named KNeighborsRegressor and\\nKNeighbors\\u200bClassi\\u2060fier to help you train regression and\\nclassification models using the k-nearest neighbors learning\\nalgorithm. It also includes classes named\\nRadiusNeigh\\u2060borsRe\\u2060gres\\u2060sor and RadiusNeighborsClassifier that\\naccept a radius rather than a number of neighbors. Let’s\\nlook at an example that uses KNeighbor\\u2060s\\u200bClassifier to classify\\nflowers using the famous Iris dataset. That dataset includes\\n150 samples, each representing one of three species of iris.\\nEach row contains four measurements—sepal length, sepal\\nwidth, petal length, and petal width, all in centimeters—'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 59, 'file_type': 'pdf'}, page_content=\"plus a label: 0 for a setosa iris, 1 for versicolor, and 2\\nfor virginica. Figure\\xa01-14 shows an example of each species\\nand illustrates the difference between petals and sepals.\\nFigure 1-14. Iris dataset (Middle panel: “Blue Flag Flower Close-Up [Iris\\nVersicolor]” by Danielle Langlois is licensed under CC BY-SA 2.5,\\nhttps://creativecommons.org/licenses/by-sa/2.5/deed.en; rightmost panel:\\n“Image of Iris Virginica Shrevei BLUE FLAG” by Frank Mayfield is licensed\\nunder CC BY-SA 2.0, https://creativecommons.org/licenses/by-sa/2.0/deed.en)\\nTo train a machine learning model to differentiate between\\nspecies of iris based on sepal and petal measurements, begin\\nby running the following code in a Jupyter notebook to load\\nthe dataset, add a column containing the class name, and show\\nthe first five rows:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nload_iris\\n\\niris\\nload_iris()\\ndf\\npd.DataFrame(iris.data,\\ncolumns=iris.feature_names)\\ndf['class']\\niris.target\\ndf['class name']\\niris.target_names[iris['target']]\\ndf.head()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 60, 'file_type': 'pdf'}, page_content='The Iris dataset is one of several sample datasets included\\nwith Scikit. That’s why you can load it by calling Scikit’s\\nload_iris function rather than reading it from an external\\nfile. Here’s the output from the code:\\nBefore you train a machine learning model from the data, you\\nneed to split the dataset into two datasets: one for training\\nand one for testing. That’s important, because if you don’t\\ntest a model with data it hasn’t seen before—that is, data\\nit wasn’t trained with—you have no idea how accurate it is\\nat making predictions.\\nFortunately, Scikit’s train_test_split function makes it\\neasy to split a dataset using a fractional split that you\\nspecify. Use the following statements to perform an 80/20\\nsplit with 80% of the rows set aside for training and 20%\\nreserved for testing:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 iris.data,\\niris.target,\\ntest_size=0.2,\\nrandom_state=0)\\nNow, x_train and y_train hold 120 rows of randomly selected\\nmeasurements and labels, while x_test and y_test hold the\\nremaining 30. Although 80/20 splits are customary for small\\ndatasets like this one, there’s no rule saying you have to\\nsplit 80/20. The more data you train with, the more accurate'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 61, 'file_type': 'pdf'}, page_content='the model is. (That’s not strictly true, but generally\\nspeaking, you always want as much training data as you can\\nget.) The more data you test with, the more confidence you\\nhave in measurements of the model’s accuracy. For a small\\ndataset, 80/20 is a reasonable place to start.\\nThe next step is to train a machine learning model. Thanks to\\nScikit, that requires just a few lines of code:\\n\\nfrom\\nsklearn.neighbors\\nimport\\nKNeighborsClassifier\\n\\nmodel\\nKNeighborsClassifier()\\nmodel.fit(x_train,\\ny_train)\\nIn Scikit, you create a machine learning model by\\ninstantiating the class encapsulating the learning algorithm\\nyou selected—in this case, KNeighborsClassifier. Then you\\ncall fit on the model to train it by fitting it to the\\ntraining data. With just 120 rows of training data, training\\nhappens very quickly.\\nThe final step is to use the 30 rows of test data split off\\nfrom the original dataset to measure the model’s accuracy.\\nIn Scikit, that’s accomplished by calling the model’s score\\nmethod:\\n\\nmodel.score(x_test,\\ny_test)\\nIn this example, score returns 0.966667, which means the\\nmodel got it right about 97% of the time when making\\npredictions with the features in x_test and comparing the\\npredicted labels to the actual labels in y_test.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 62, 'file_type': 'pdf'}, page_content='Of course, the whole purpose of training a predictive model\\nis to make predictions with it. In Scikit, you make a\\nprediction by calling the model’s predict method. Use the\\nfollowing statements to predict the class—0 for setosa, 1\\nfor versicolor, and 2 for virginica—identifying the species\\nof an iris whose sepal length is 5.6 cm, sepal width is 4.4\\ncm, petal length is 1.2 cm, and petal width is 0.4 cm:\\n\\nmodel.predict([[5.6,\\n0.4]])\\nThe predict method can make multiple predictions in a single\\ncall. That’s why you pass it a list of lists rather than\\njust a list. It returns a list whose length equals the number\\nof lists you passed in. Since you passed just one list to\\npredict, the return value is a list with one value. In this\\nexample, the predicted class is 0, meaning the model\\npredicted that an iris whose sepal length is 5.6 cm, sepal\\nwidth is 4.4 cm, petal length is 1.2 cm, and petal width is\\n0.4 cm is mostly likely a setosa iris.\\nWhen you create a KNeighborsClassifier without specifying the\\nnumber of neighbors, it defaults to 5. You can specify the\\nnumber of neighbors this way:\\n\\nmodel\\nKNeighborsClassifier(n_neighbors=10)\\nTry fitting (training) and scoring the model again using\\nn_neighbors=10. Does the model score the same? Does predict\\nstill predict class 0? Feel free to experiment with other\\nn_neighbors values to get a feel for their effect on the\\noutcome.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 63, 'file_type': 'pdf'}, page_content='KNEIGHBORSCLASSIFIER INTERNALS\\nk-nearest neighbors is sometimes referred to as a lazy\\nlearning algorithm because most of the work is done when\\nyou call predict rather than when you call fit. In fact,\\ntraining technically doesn’t have to do anything except\\nmake a copy of the training data for when predict is\\ncalled. So what happens inside KNeighborsClassifier’s\\nfit method?\\nIn most cases, fit constructs a binary tree in memory\\nthat makes predict faster by preventing it from having to\\nperform a brute-force search for neighboring samples. If\\nit determines that a binary tree won’t help,\\nKNeighborsClassifier resorts to brute force when making\\npredictions. This typically happens when the training\\ndata is sparse—that is, mostly zeros with a few nonzero\\nvalues sprinkled in.\\nOne of the wonderful things about Scikit-Learn is that it\\nis open source. If you care to know more about how a\\nparticular class or method works, you can go straight to\\nthe source code on GitHub. You’ll find the source code\\nfor KNeighborsClassifier and RadiusNeighborsClassifier on\\nGitHub.\\nThe process employed here—load the data, split the data,\\ncreate a classifier or regressor, call fit to fit it to the\\ntraining data, call score to assess the model’s accuracy\\nusing test data, and finally, call predict to make\\npredictions—is one that you will use over and over with\\nScikit. In the real world, data frequently requires cleaning\\nbefore it’s used for training and testing. For example, you\\nmight have to remove rows with missing values or dedupe the\\ndata to eliminate redundant rows. You’ll see plenty of\\nexamples of this later, but in this example, the data was\\ncomplete and well structured right out of the box, and\\ntherefore required no further preparation.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 64, 'file_type': 'pdf'}, page_content='Summary\\nMachine learning offers engineers and software developers an\\nalternative approach to problem-solving. Rather than use\\ntraditional computer algorithms to transform input into\\noutput, machine learning relies on learning algorithms to\\nbuild mathematical models from training data. Then it uses\\nthose models to turn future inputs into outputs.\\nMost machine learning models fall into either of two\\ncategories. Unsupervised learning models are widely used to\\nanalyze datasets by highlighting similarities and\\ndifferences. They don’t require labeled data. Supervised\\nlearning models learn from labeled data in order to make\\npredictions—for example, to predict whether a credit card\\ntransaction is legitimate. Supervised learning can be used to\\nsolve regression problems or classification problems.\\nRegression models predict numeric outcomes, while\\nclassification models predict classes (categories).\\nk-means clustering is a popular unsupervised learning\\nalgorithm, while k-nearest neighbors is a simple yet\\neffective supervised learning algorithm. Many, but not all,\\nsupervised learning algorithms can be used for regression and\\nfor classification. Scikit-Learn’s KNeighborsRegressor\\nclass, for example, applies k-nearest neighbors to regression\\nproblems, while KNeighborsClassifier applies the same\\nalgorithm to classification problems.\\nEducators often use k-nearest neighbors to introduce\\nsupervised learning because it’s easily understood and it\\nperforms reasonably well in a variety of problem domains.\\nWith k-nearest neighbors under your belt, the next step on\\nthe road to machine learning proficiency is getting to know\\nother supervised learning algorithms. That’s the focus of\\nChapter\\xa02, which introduces several popular learning\\nalgorithms in the context of regression modeling.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 65, 'file_type': 'pdf'}, page_content='Chapter 2. Regression Models\\nYou learned in Chapter\\xa01 that supervised learning models\\ncome in two varieties: regression models and classification\\nmodels. You also learned that regression models predict\\nnumeric outcomes, such as the price that a home will sell for\\nor the number of visitors a website will attract. Regression\\nmodeling is a vital and sometimes underappreciated aspect of\\nmachine learning. Retailers use it to forecast demand. Banks\\nuse it to screen loan applications, factoring in variables\\nsuch as credit scores, debt-to-income ratios, and loan-to-\\nvalue ratios. Insurance companies use it to set premiums.\\nWhenever you need numerical predictions, regression modeling\\nis the right tool for the job.\\nWhen building a regression model, the first and most\\nimportant decision you make is what learning algorithm to\\nuse. Chapter\\xa01 presented a simple three-class classification\\nmodel that used the k-nearest neighbors learning algorithm to\\nidentify a species of iris given the flower’s sepal and\\npetal measurements. k-nearest neighbors can be used for\\nregression too, but it’s one of many you can choose from for\\nmaking numerical predictions. Other learning algorithms\\nfrequently produce more accurate models.\\nThis chapter introduces common regression algorithms, many of\\nwhich can be used for classification also, and guides you\\nthrough the process of building a regression model that\\npredicts taxi fares using data published by the New York City\\nTaxi and Limousine Commission. It also describes various\\nmeans for assessing a regression model’s accuracy and\\nintroduces an important technique for measuring accuracy\\ncalled cross-validation.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 66, 'file_type': 'pdf'}, page_content='Linear Regression\\nNext to k-nearest neighbors, linear regression is perhaps the\\nsimplest learning algorithm of all. It works best with data\\nthat is relatively linear—that is, data points that fall\\nroughly along a line. Thinking back to high school math\\nclass, you’ll recall that the equation for a line in two\\ndimensions is:\\nwhere m is the slope of the line and b is where the line\\nintersects the y-axis. The income-versus-years-of-experience\\ndataset in Figure\\xa01-11 lends itself well to linear\\nregression. Figure\\xa02-1 shows a regression line fit to the\\ndata points. Predicting the income for a programmer with 10\\nyears of experience is as simple as finding the point on the\\nline where x = 10. The equation of the line is y = 3,984x +\\n60,040. Plugging 10 into that equation for x, the predicted\\nincome is $99,880.\\nFigure 2-1. Linear regression'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 67, 'file_type': 'pdf'}, page_content='The goal when training a linear regression model is to find\\nvalues for m and b that produce the most accurate\\npredictions. This is typically done using an iterative\\nprocess that starts with assumed values for m and b and\\nrepeats until it converges on suitable values.\\nThe most common technique for fitting a line to a set of\\npoints is ordinary least squares regression, or OLS for\\nshort. It works by squaring the distance in the y direction\\nbetween each point and the regression line, summing the\\nsquares, and dividing by the number of points to compute the\\nmean squared error, or MSE. (Squaring each distance prevents\\nnegative distances from offsetting positive distances.) Then\\nit adjusts m and b to reduce the MSE the next time around and\\nrepeats until the MSE is sufficiently low. I won’t go into\\nthe details of how it determines in which direction to adjust\\nm and b (it’s not hard, but it involves a smidgeon of\\ncalculus—specifically, using partial derivatives of the MSE\\nfunction to determine whether to increase or decrease m and b\\nin the next iteration), but OLS can often fit a line to a set\\nof points with a dozen or fewer iterations.\\nScikit-Learn\\xa0 has a number of classes to help you build\\nlinear regression models, including the LinearRegression\\nclass, which embodies OLS, and the Polynomial\\u200bFea\\u2060tures class,\\nwhich fits a polynomial curve rather than a straight line to\\nthe training data. Training a linear regression model can be\\nas simple as this:\\n\\nmodel\\nLinearRegression()\\nmodel.fit(x,\\ny)\\nScikit has other linear regression classes with names such as\\nRidge and Lasso. One scenario in which they’re useful is\\nwhen the training data contains outliers. Recall from\\nChapter\\xa01 that outliers are data points that don’t conform'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 68, 'file_type': 'pdf'}, page_content='with the rest. Outliers can bias a model or make it less\\naccurate. Ridge and Lasso add regularization, which mitigates\\nthe effect of outliers by lessening their influence on the\\noutcome as coefficients are adjusted during training. An\\nalternate approach to dealing with outliers is to remove them\\naltogether, which is what you’ll do in the taxi-fare example\\nat the end of this chapter.\\nNOTE\\nLasso regression has a secondary benefit too. If the training data\\nsuffers from multicollinearity, a condition in which two or more\\ninput variables are linearly correlated so that one can be\\npredicted from another with a reasonable degree of accuracy, Lasso\\neffectively ignores the redundant data.\\nA classic example of multicollinearity occurs when a dataset\\nincludes one column specifying the number of rooms in a house and\\nanother column specifying the square footage. More rooms generally\\nmeans more area, so the two variables are correlated to some\\ndegree.\\nLinear regression isn’t limited to two dimensions (x and y\\nvalues); it works with any number of dimensions. Linear\\nregression with one independent variable (x) is known as\\nsimple linear regression, while linear regression with two or\\nmore independent variables—for example, x1, x2, x3, and so\\non—is called multiple linear regression. If a dataset is two\\ndimensional, it’s simple enough to plot the data to\\ndetermine its shape. You can plot three-dimensional data too,\\nbut plotting datasets with four or five dimensions is more\\nchallenging, and datasets with hundreds or thousands of\\ndimensions are impossible to visualize.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 69, 'file_type': 'pdf'}, page_content='How do you determine whether a high-dimensional dataset might\\nlend itself to linear regression? One way to do it is to\\nreduce n dimensions to two or three using techni\\u2060ques such as\\nprincipal component analysis (PCA) and t-distributed\\nstochastic neighbor embedding (t-SNE) so that you can plot\\nthem. These techniques are covered in Chapter\\xa06. Both reduce\\nthe dimensionality of a dataset without incurring a\\ncommensurate loss of information. With PCA, for example, it\\nisn’t uncommon to reduce the number of dimensions by 90%\\nwhile retaining 90% of the information in the original\\ndataset. It might sound like magic, but it’s not. It’s\\nmath.\\nIf the number of dimensions is relatively small, a simpler\\ntechnique for visualizing high-dimensional datasets is pair\\nplots, which plot pairs of dimensions in conventional 2D\\ncharts. Figure\\xa02-2 shows a pair plot charting sepal length\\nversus petal length, sepal width versus petal width, and\\nother parameter pairs for the Iris dataset introduced in\\nChapter\\xa01.\\nSeaborn’s pairplot function makes it easy to create pair\\nplots. The plot in Figure\\xa02-2 was generated with one line of\\ncode:\\n\\nsns.pairplot(df)\\nThe pair plot not only helps you visualize relationships in\\nthe dataset, but in this example, the histogram in the lower-\\nright corner reveals that the dataset is balanced too. There\\nis an equal number of samples of all three classes, and for\\nreasons you’ll learn in Chapter\\xa03, you always prefer to\\ntrain classification models with balanced datasets.\\nLinear regression is a parametric learning algorithm, which\\nmeans that its purpose is to examine a dataset and find the'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 70, 'file_type': 'pdf'}, page_content='optimum values for parameters in an equation—for example, m\\nand b. k-nearest neighbors, by contrast, is a nonparametric\\nlearning algorithm because it doesn’t fit data to an\\nequation. Why does it matter whether a learning algorithm is\\nparametric or nonparametric? Because datasets used to train\\nparametric models frequently need to be normalized. At its\\nsimplest, normalizing data means making sure all the values\\nin all the columns have consistent ranges. I’ll cover\\nnormalization in Chapter\\xa05, but for now, realize that\\ntraining parametric models with unnormalized data—for\\nexample, a dataset that contains values from 0 to 1 in one\\ncolumn and 0 to 1,000,000 in another—can make those models\\nless accurate or prevent them from converging on a solution\\naltogether. This is particularly true with support vector\\nmachines and neural networks, but it applies to other\\nparametric models as well. Even k-nearest neighbors models\\nwork best with normalized data because while the learning\\nalgorithm isn’t parametric, it uses distance-based\\ncalculations internally.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 71, 'file_type': 'pdf'}, page_content='Figure 2-2. Pair plot revealing relationships between variable pairs\\nDecision Trees\\nEven if you’ve never taken a computer science course, you\\nprobably know what a binary tree is. In machine learning, a\\ndecision tree is a tree structure that predicts an outcome by\\nanswering a series of questions. Most decision trees are\\nbinary trees, in which case the questions require simple yes-\\nor-no answers.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 72, 'file_type': 'pdf'}, page_content='Figure\\xa02-3 shows a decision tree built by Scikit from the\\nincome-versus-experience dataset introduced in Chapter\\xa01.\\nThe tree is simple because the dataset contains just one\\nfeature column (years of experience) and I limited the\\ntree’s depth to 3, but the technique extends to trees of\\nunlimited size and complexity. In this example, predicting a\\nsalary for a programmer with 10 years of experience requires\\njust three yes/no decisions, as indicated by the red arrows.\\nThe answer is about $100K, which is pretty close to what k-\\nnearest neighbors and linear regression predicted when\\napplied to the same dataset.\\nFigure 2-3. Decision tree\\nDecision trees can be used for regression and classification.\\nFor a regressor, the leaf nodes (the nodes that lack\\nchildren) represent regression values. For a classifier, they\\nrepresent classes. The output from a decision tree regressor\\nisn’t continuous. The output will always be one of the\\nvalues assigned to a leaf node, and the number of leaf nodes\\nis finite. The output from a linear regression model, by\\ncontrast, is continuous. It can assume any value along the\\nline fit to the training data. In the previous example, you\\nget the same answer if you ask the tree to predict a salary\\nfor someone with 10 years of experience and someone with 13\\nyears of experience. Bump years of experience up to 14,\\nhowever, and the predicted salary jumps to $125K (Figure\\xa02-\\n4). If you allow the tree to grow deeper, the answers become'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 73, 'file_type': 'pdf'}, page_content='more refined. But allowing it to grow too deep can lead to\\nbig problems for reasons we’ll cover momentarily.\\nOnce a decision tree model is trained—that is, once the tree\\nis built—predictions are made quickly. But how do you decide\\nwhat decisions to make at each node? For example, why is the\\nnumber of years represented by the root node in Figure\\xa02-3\\nequal to 13.634? Why not 10.000 or 8.742 or some other\\nnumber? For that matter, if the dataset has multiple feature\\ncolumns, how do you decide which column to break on at each\\ndecision node?\\nFigure 2-4. Mathematical model created from a decision tree\\nDecision trees are built by recursively splitting the\\ntraining data. The fundamental decisions that the splitting\\nalgorithm makes when it adds a node to the tree are 1) which\\ncolumn will this node split, and 2) what is the value that\\nthe split is based upon. In each iteration, the goal is to\\nselect a column and split value that does the most to reduce\\nthe “impurity” of the remaining data for classification'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 74, 'file_type': 'pdf'}, page_content='problems or the variance of the remaining data for regression\\nproblems. A common impurity measure for classifiers is Gini,\\nwhich roughly quantifies the percentage of samples that a\\nsplit value would misclassify. For regressors, the sum of the\\nsquared error or absolute error, where “error” is the\\ndifference between the split value and the values on either\\nside of the split, is typically used instead. The tree-\\nbuilding process starts at the root node and works its way\\nrecursively downward until the tree is fully leafed out or\\nexternal constraints (such as a limit on maximum depth)\\nprevent further growth.\\nScikit’s DecisionTreeRegressor class and DecisionTree\\u200b\\nClas\\u2060sifier class make building decision trees easy. Each\\nimplements the well-known CART algorithm for building binary\\ntrees, and each lets you choose from a handful of criteria\\nfor measuring impurity or variance. Each also supports\\nparameters such as max_depth, min\\u200b_sam\\u2060ples_split, and\\nmin_samples_leaf that let you constrain a decision tree’s\\ngrowth. If you accept the default values, building a decision\\ntree can be as simple as this:\\nmodel\\nDecisionTreeRegressor()\\nmodel.fit(x,\\ny)\\nDecision trees are nonparametric. Training a decision tree\\nmodel involves building a binary tree, not fitting an\\nequation to a dataset. This means data used to build a\\ndecision tree doesn’t have to be normalized.\\nDecision trees have a big upside: they work as well with\\nnonlinear data as they do with linear data. In fact, they\\nlargely don’t care how the data is shaped. But there’s a\\ndownside too. It’s a big one, and it’s one of the reasons\\nstandalone decision trees are rarely used in machine\\nlearning. That reason is overfitting.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 75, 'file_type': 'pdf'}, page_content='Decision trees are highly prone to overfitting. If allowed to\\ngrow large enough, a decision tree can essentially memorize\\nthe training data. It might appear to be accurate, but if\\nit’s fit too tightly to the training data, it might not\\ngeneralize well. That means it won’t be as accurate when\\nit’s asked to make predictions with data it hasn’t seen\\nbefore. Figure\\xa02-5 shows a decision tree fit to the income-\\nversus-experience dataset with no constraints on depth. The\\njagged path followed by the red line as it passes through all\\nthe points is a clear sign of overfitting. Overfitting is the\\nbane of data scientists. The only thing worse than a model\\nthat’s inaccurate is one that appears to be accurate but in\\nreality is not.\\nFigure 2-5. Decision tree overfit to the training data\\nOne way to prevent overfitting when using decision trees is\\nto constrain their growth so that they can’t memorize the\\ntraining data. Another way is to use groups of decision trees\\ncalled random forests.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 76, 'file_type': 'pdf'}, page_content='Random Forests\\nA random forest is a collection of decision trees (often\\nhundreds of them), each trained differently on the same data,\\nas depicted in Figure\\xa02-6. Typically, each tree is trained\\non randomly selected rows in the dataset, and branching is\\nbased on columns that are randomly selected at every split.\\nThe model can’t fit too tightly to the training data because\\nevery tree trains on a different subset of the data. The\\ntrees are built independently, and when the model makes a\\nprediction, it runs the input through all the decision trees\\nand averages the result. Because the trees are constructed\\nindependently, training can be parallelized on hardware that\\nsupports it.\\nFigure 2-6. Random forests\\nIt’s a simple concept, and one that works well in practice.\\nRandom forests can be used for both regression and\\nclassification, and Scikit provides classes such as\\nRandomFores\\u2060t\\u200bRegressor and RandomForestClassifier to help out.\\nThey feature a number of tunable parameters, including\\nn_estimators, which specifies the number of trees in the\\nrandom forest (default = 100); max_depth, which limits the\\ndepth of each tree; and max_samples, which specifies the\\nfraction of the rows in the training data used to build\\nindividual trees. Figure\\xa02-7 shows how RandomForestRegressor\\nfits to the income-versus-experience dataset with max_depth=3'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 77, 'file_type': 'pdf'}, page_content='and max_samples=0.5, meaning no tree sees more than 50% of\\nthe rows in the dataset.\\nFigure 2-7. Mathematical model created from a random forest\\nBecause decision trees are nonparametric, random forests are\\nnonparametric also. And even though Figure\\xa02-7 shows how a\\nrandom forest fits a linear dataset, random forests are\\nperfectly capable of modeling nonlinear datasets too.\\nGradient-Boosting Machines\\nRandom forests are proof of the supposition that you can take\\nmany weak learners—models that by themselves are not strong\\npredictors—and combine them to form accurate models. No\\nindividual tree in a random forest can predict an outcome\\nwith a great deal of accuracy. But put all the trees together\\nand average the results and they often outperform other\\nmodels. Data scientists refer to this as ensemble modeling or\\nensemble learning.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 78, 'file_type': 'pdf'}, page_content='Another way to exploit ensemble modeling is gradient\\nboosting. Models that use it are called gradient-boosting\\nmachines, or GBMs. Most GBMs use decision trees and are\\nsometimes referred to as gradient-boosted decision trees\\n(GBDTs). Like random forests, GBDTs comprise collections of\\ndecision trees. But rather than build independent decision\\ntrees from random subsets of the data, GBDTs build dependent\\ndecision trees, one after another, training each using output\\nfrom the last. The first decision tree models the dataset.\\nThe second decision tree models the error in the output from\\nthe first, the third models the error in the output from the\\nsecond, and so on. To make a prediction, a GBDT runs the\\ninput through each decision tree and sums all the outputs to\\narrive at a result. With each addition, the result becomes\\nslightly more accurate, giving rise to the term additive\\nmodeling. It’s like driving a golf ball down the fairway and\\nhitting successively shorter shots until you finally reach\\nthe hole.\\nEach decision tree in a GBDT model is a weak learner. In\\nfact, GBDTs typically use decision tree stumps, which are\\ndecision trees with depth 1 (a root node and two child\\nnodes), as shown in Figure\\xa02-8. During training, you start\\nby taking the mean of all the target values in the training\\ndata to create a baseline for predictions. Then you subtract\\nthe mean from the target values to generate a new set of\\ntarget values or residuals for the first tree to predict.\\nAfter training the first tree, you run the input through it\\nto generate a set of predictions. Then you add the\\npredictions to the previous set of predictions, generate a\\nnew set of residuals by subtracting the sum from the original\\n(actual) target values, and train a second tree to predict\\nthose residuals. Repeating this process for n trees, where n\\nis typically 100 or more, produces an ensemble model'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 78, 'file_type': 'pdf'}, page_content='. Repeating this process for n trees, where n\\nis typically 100 or more, produces an ensemble model. To help\\nensure that each decision tree is a weak learner, GBDT models\\nmultiply the output from each decision tree by a learning\\nrate to reduce their influence on the outcome. The learning'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 79, 'file_type': 'pdf'}, page_content='rate is usually a small number such as 0.1 and is a parameter\\nthat you can specify when using classes that implement GBMs.\\nFigure 2-8. Gradient-boosting machines\\nScikit includes classes named GradientBoostingRegressor and\\nGradientBoostingClassifier to help you build GBDTs. But if you\\nreally want to understand how GBDTs work, you can build one\\nyourself with Scikit’s DecisionTreeRegressor class. The code\\nin Example\\xa02-1 implements a GBDT with 100 decision tree\\nstumps and predicts the annual income of a programmer with 10\\nyears of experience.\\nExample 2-1. Gradient-boosted decision tree implementation\\n\\nlearning_rate\\n# Learning rate\\nn_trees\\n# Number of decision trees\\ntrees\\n# Trees that comprise the model\\n\\n# Compute the mean of all the target values\\ny_pred\\nnp.array([y.mean()]\\nlen(y))\\nbaseline\\ny_pred\\n\\n# Create n_trees and train each with the error\\n# in the output from the previous tree'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 80, 'file_type': 'pdf'}, page_content='for\\ni\\nin\\nrange(n_trees):\\n\\n\\xa0\\xa0\\xa0 error\\ny\\ny_pred\\n\\xa0\\xa0\\xa0 tree\\nDecisionTreeRegressor(max_depth=1,\\nrandom_state=0)\\n\\n\\xa0\\xa0\\xa0 tree.fit(x,\\nerror)\\n\\n\\xa0\\xa0\\xa0 predictions\\ntree.predict(x)\\n\\xa0\\xa0\\xa0 y_pred\\ny_pred\\n(learning_rate\\npredictions)\\n\\xa0\\xa0\\xa0 trees.append(tree)\\n\\n# Predict a y for x=10\\ny_pred\\nnp.array([baseline[0]]\\nlen(x))\\n\\nfor\\ntree\\nin\\ntrees:\\n\\n\\xa0\\xa0\\xa0 y_pred\\ny_pred\\n(learning_rate\\ntree.predict([[10.0]]))\\n\\ny_pred[0]\\nThe diagram on the left in Figure\\xa02-9 shows the output from\\na single decision tree stump applied to the income-versus-\\nexperience dataset. That model is such a weak learner that it\\ncan predict only two different income levels. The diagram on\\nthe right shows the output from the model in Example\\xa02-1.\\nThe additive effect of the weak learners produces a strong\\nlearner that predicts a programmer with 10 years of\\nexperience should earn $99,082 per year, which is consistent\\nwith the predictions made by other models.\\nFigure 2-9. Single decision tree versus gradient-boosted decision trees'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 81, 'file_type': 'pdf'}, page_content='GBDTs can be used for regression and classification, and they\\nare nonparametric. Aside from neural networks and support\\nvector machines, GBDTs are frequently the ones that data\\nscientists find most capable of modeling complex datasets.\\nUnlike linear regression models and random forests, GBDTs are\\nsusceptible to overfitting. One way to mitigate overfitting\\nwhen using GradientBoostingRegressor and\\nGradientBoostingClassifier is to use the subsample parameter\\nto prevent individual trees from seeing the entire dataset,\\nanalogous to what max_samples does for random forests.\\nAnother way is to use the learning_rate parameter to lower\\nthe learning rate, which defaults to 0.1.\\nSupport Vector Machines\\nI will save a full treatment of support vector machines\\n(SVMs) for Chapter\\xa05, but along with GBMs, they represent\\nthe cutting edge of statistical machine learning. They can\\noften fit models to highly nonlinear datasets that other\\nlearning algorithms cannot. They’re so important that they\\nmerit separate treatment from all other algorithms. They work\\nby employing a mathematical device called kernel tricks to\\nsimulate the effect of adding dimensions to data. The idea is\\nthat data that isn’t separable in m dimensions might be\\nseparable in n dimensions. Here’s a quick example.\\nThe classes in the two-dimensional dataset on the left in\\nFigure\\xa02-10 can’t be separated with a line. But if you add\\na third dimension so that points closer to the center have\\nhigher z values and points farther from the center have lower\\nz values, as shown on the right, you can slide a plane\\nbetween the red points and the purple points and achieve 100%\\nseparation of the classes. That is the principle by which\\nSVMs work. It is mathematically complex when generalized to\\nwork with arbitrary datasets, but it is an extremely powerful\\ntechnique that is vastly simplified by Scikit.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 82, 'file_type': 'pdf'}, page_content='Figure 2-10. Support vector machines\\nSVMs are primarily used for classification, but they can be\\nused for regression as well. Scikit includes classes for\\ndoing both, including SVC for classification problems and SVR\\nfor regression problems. You will learn all about these\\nclasses in Chapter\\xa05. For now, drop the term support vector\\nmachine at the next machine learning gathering you attend and\\nyou will instantly become the life of the party.\\nAccuracy Measures for Regression Models\\nAs you learned in Chapter\\xa01, you need one set of data for\\ntraining a model and another set for testing it, and you can\\nscore a model for accuracy by passing test data to the\\nmodel’s score method. Testing quantifies how accurate the\\nmodel is at making predictions. It is incredibly important to\\ntest a model with a dataset other than the one it was trained\\nwith because it will probably learn the training data\\nreasonably well, but that doesn’t mean it will generalize\\nwell—that is, make accurate predictions. And if you don’t\\ntest a model, you don’t know how accurate it is.\\nEngineers frequently use Scikit’s train_test_split function\\nto split a dataset into a training dataset and a test\\ndataset. But when you split a small dataset this way, you'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 83, 'file_type': 'pdf'}, page_content='can’t necessarily trust the score returned by the model’s\\nscore method. And what does the score mean, anyway? The\\nanswer is different for regression models and classification\\nmodels, and while there are a handful of ways to score\\nregression models, there are many ways to score\\nclassification models. Let’s take a moment to understand the\\nnumber that score returns for a regression model, and what\\nyou can do to have more faith in that number.\\nTo demonstrate why you need to be somewhat skeptical of the\\nvalue returned by score when dealing with small datasets, try\\na simple experiment. Use the following code to load Scikit’s\\nCalifornia housing dataset, shuffle the rows, and extract the\\nfirst 1,000 rows:\\n\\nfrom\\nsklearn.utils\\nimport\\nshuffle\\nfrom\\nsklearn.datasets\\nimport\\nfetch_california_housing\\n\\ndf\\nfetch_california_housing(as_frame=True).frame\\ndf\\nshuffle(df,\\nrandom_state=0)\\ndf\\ndf.head(1000)\\ndf.head()\\nThe dataset contains columns with names such as MedInc\\n(median income) and MedHouseVal (median home value), as shown\\nin the following figure. The details aren’t important for\\nnow. What is important is that you are going to build a model\\nthat uses the values in all the other columns to predict\\nMedHouseVal values:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 84, 'file_type': 'pdf'}, page_content=\"Use the following code to split the data 80/20, train a\\nlinear regression model with 80% of the data to predict the\\nprice of a house, and score the model with 20% of the data\\nsplit off for testing:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLinearRegression\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\ndf.drop(['MedHouseVal'],\\naxis=1)\\ny\\ndf['MedHouseVal']\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\n\\nmodel\\nLinearRegression()\\nmodel.fit(x_train,\\ny_train)\\nmodel.score(x_test,\\ny_test)\\nIn this example, score returns 0.5863, which ostensibly\\nindicates that the model was about 59% accurate using the\\nfeatures in the test data to make predictions. So far, so\\ngood.\\nNow change the random_state value passed to train_test_split\\nfrom 0 to 1 and run the code again:\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=1)\\n\\nmodel\\nLinearRegression()\\nmodel.fit(x_train,\\ny_train)\\nmodel.score(x_test,\\ny_test)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 85, 'file_type': 'pdf'}, page_content='This time, score returns 0.6255. So which is it? Is the model\\n59% accurate at making predictions or 63% accurate? Why did\\nscore return two different values?\\nWhen train_test_split splits a dataset, it randomly selects\\nrows for the training dataset and the test dataset. The\\nrandom_state parameter seeds the random-number generator used\\nto make selections. By specifying two different seed values,\\nyou train the model with two different datasets and test it\\nwith two different datasets too. Sure, there is overlap\\nbetween them. But the fact remains that the number you seed\\ntrain_test_split’s random-number generator with affects the\\noutcome. The smaller the dataset, the greater that effect is\\nlikely to be.\\nThe solution is cross-validation. To cross-validate a model,\\nyou partition the dataset into folds, as pictured in\\nFigure\\xa02-11. Using five folds is common, but you can use any\\nnumber of folds you like. Then you train the model five times\\n—once for each fold—using a different 80% of the dataset\\nfor training and a different 20% for testing each time, and\\naverage the scores to generate a cross-validated score. That\\nscore is more reliable than the score returned by score\\nbecause it is less sensitive to how the data is split. This\\nprocess is known as k-fold cross-validation.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 86, 'file_type': 'pdf'}, page_content='Figure 2-11. 5-fold cross-validation\\nThe downside to cross-validation is that it takes longer.\\nWith 5-fold cross-validation, you’re training the model five\\ntimes. The good news is that cross-validation is generally\\napplied only to small datasets, and if the dataset is small,\\nthe model probably trains quickly.\\nYou could write the code to do cross-validation yourself, but\\nyou don’t have to because Scikit does it for you. Cross-\\nvalidating a model requires just one line of code:\\n\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nThe cross-validated score in this example should come out to\\nabout 0.61, which lies between the two values generated by\\nthe model’s score method. It is a more accurate measure of\\nthe model’s accuracy than either of the other scores.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 87, 'file_type': 'pdf'}, page_content='You\\xa0 don’t have to train a model before cross-validating it\\nsince cross_val_score trains it for you. However,\\ncross_val_score trains a copy of the model, not the model\\nitself, so once you’ve used cross-validation to gauge the\\nmodel’s accuracy, you still need to call fit before making\\npredictions:\\n\\nmodel.fit(x,\\ny)\\nNote that you pass the entire dataset to fit rather than a\\nsubset split off for training. That’s another benefit of\\ncross-validation: you train your model with all of the data,\\nwhich is a big deal if the dataset is small to begin with.\\nThere’s no longer a hard requirement to hold some of it out\\nfor testing. However, even with cross-validation, it’s still\\nuseful to score the model with data reserved exclusively for\\ntesting if such data is available. Remember: you don’t truly\\nknow how accurate a model is until you know how it responds\\nto data it wasn’t trained with.\\nTIP\\nAs a rule of thumb, you should reserve cross-validation for small\\ndatasets and use train/test splits for large datasets. The larger\\nthe dataset, the less sensitive it is to how the data is split.\\nWhich begs the question: precisely what does the value\\nreturned when you score or cross-validate a model represent?\\nFor a regression model, it’s the coefficient of\\ndetermination, also known as the R-squared score or simply\\nR2. The coefficient of determination is usually a value from'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 88, 'file_type': 'pdf'}, page_content='0 to 1 (“usually” because it can, in certain circumstances,\\ngo negative) which quantifies the variance in the output that\\ncan be explained by the variables in the input. A simple way\\nto think of it is that an R2 score of 0.8 means that the\\nmodel should, on average, be about 80% accurate in making\\npredictions, or get the answer right within 20%. The higher\\nthe R2 score, the more accurate the model. There are other\\nways to measure the accuracy of a regression model, including\\nmean squared error (MSE) and mean absolute error (MAE). Those\\nnumbers are meaningful only in the context of the range of\\noutput values, whereas R2 gives you one simple number that is\\nindependent of range. You can read more about regression\\nmetrics and methods for retrieving them in the Scikit\\ndocumentation.\\nThe value returned by a model’s score method is completely\\ndifferent for a classification model. I will address the\\nvarious ways to quantify the accuracy of classification\\nmodels in Chapter\\xa03.\\nUsing Regression to Predict Taxi Fares\\nImagine that you work for a taxi company, and one of your\\ncustomers’ biggest complaints is that they don’t know how\\nmuch a ride will cost until it’s over. That’s because\\ndistance isn’t the only variable that determines a fare\\namount. You decide to do something about it by building a\\nmobile app that customers can use when they climb into a taxi\\nto estimate what the fare will be. To provide the\\nintelligence for the app, you intend to use the massive\\namounts of fare data the company has collected over the years\\nto build a machine learning model.\\nLet’s train a regression model to predict a fare amount\\ngiven the time of day, the day of the week, and the pickup\\nand drop-off locations. Start by downloading the CSV file\\ncontaining the dataset and copying it into the Data\\nsubdirectory where your Jupyter notebooks are hosted. Then'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 89, 'file_type': 'pdf'}, page_content=\"use the following code to load the dataset into a notebook.\\nThe dataset contains about 55,000 rows and is a subset of a\\nmuch larger dataset used in Kaggle’s New York City Taxi Fare\\nPrediction competition. The data requires a fair amount of\\nprep work before it’s of any use—something that’s not\\nuncommon in machine learning. Data scientists often find that\\ncollecting and preparing data accounts for 90% or more of\\ntheir time:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/taxi-fares.csv',\\nparse_dates=['pickup_datetime'])\\ndf.head()\\nNote the use of the read_csv function’s parse_dates\\nparameter to parse the strings in the pickup_datetime column\\ninto Python datetime objects. Here’s the output from the\\ncode:\\nEach row represents a taxi ride and contains information such\\nas the fare amount, the pickup and drop-off locations\\n(expressed as latitudes and longitudes), and the passenger\\ncount. It’s the fare amount that we want to predict. Use the\\nfollowing code to draw a histogram showing how many rows\\ncontain a passenger count of 1, how many contain a passenger\\ncount of 2, and so on:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 90, 'file_type': 'pdf'}, page_content=\"%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nsns.countplot(x=df['passenger_count'])\\nHere is the output:\\nMost of the rows in the dataset have a passenger count of 1.\\nSince we’re interested in predicting the fare amount only\\nfor single passengers, use the following code to remove all\\nrows with multiple passengers and remove the key column from\\nthe dataset since that column isn’t needed—in other words,\\nit’s not one of the features that we will try to predict on:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 91, 'file_type': 'pdf'}, page_content=\"df\\ndf[df['passenger_count']\\ndf\\ndf.drop(['key',\\n'passenger_count'],\\naxis=1)\\ndf.head()\\nThat leaves 38,233 rows in the dataset, which you can see for\\nyourself with the following statement:\\n\\ndf.shape\\nNow use Pandas’ corr method to find out how much influence\\ninput variables such as latitude and longitude have on the\\nvalues in the fare_amount column:\\n\\ncorr_matrix\\ndf.corr()\\ncorr_matrix['fare_amount'].sort_values(ascending=False)\\nThe output looks like this:\\n\\nfare_amount\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 1.000000\\ndropoff_longitude\\xa0\\xa0\\xa0 0.020438\\npickup_longitude\\xa0\\xa0\\xa0\\xa0 0.015742\\npickup_latitude\\xa0\\xa0\\xa0\\xa0 -0.015915\\ndropoff_latitude\\xa0\\xa0\\xa0 -0.021711\\nName: fare_amount, dtype: float64\\nThe numbers don’t look very encouraging. Latitudes and\\nlongitudes have little to do with fare amounts, at least in\\ntheir present form. And yet, intuitively, they should have a\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 92, 'file_type': 'pdf'}, page_content=\"lot to do with fare amounts since they specify starting and\\nending points, and longer rides incur higher fares.\\nNow comes the fun part: creating whole new columns of data\\nthat have more impact on the outcome—columns whose values\\nare computed from values in other columns. Add columns\\nspecifying the day of the week (0=Monday, 1=Sunday, and so\\non), the hour of the day that the passenger was picked up (0\\n–23), and the distance (by air, not on the street) in miles\\nthat the ride covered. To compute distances, this code\\nassumes that most rides are short and that it is therefore\\nsafe to ignore the curvature of the Earth:\\n\\nfrom\\nmath\\nimport\\nsqrt\\n\\nfor\\ni,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 dt\\nrow['pickup_datetime']\\n\\xa0\\xa0\\xa0 df.at[i,\\n'day_of_week']\\ndt.weekday()\\n\\xa0\\xa0\\xa0 df.at[i,\\n'pickup_time']\\ndt.hour\\n\\xa0\\xa0\\xa0 x\\n(row['dropoff_longitude']\\nrow['pickup_longitude'])\\n\\xa0\\xa0\\xa0 y\\n(row['dropoff_latitude']\\nrow['pickup_latitude'])\\n\\xa0\\xa0\\xa0 distance\\nsqrt(x**2\\ny**2)\\n\\xa0\\xa0\\xa0 df.at[i,\\n'distance']\\ndistance\\n\\ndf.head()\\nYou no longer need all the columns, so use these statements\\nto remove the ones that won’t be used:\\n\\ndf.drop(columns=['pickup_datetime',\\n'pickup_longitude',\\n'pickup_latitude',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'dropoff_longitude',\\n'dropoff_latitude'],\\ninplace=True)\\ndf.head()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 93, 'file_type': 'pdf'}, page_content=\"Let’s check the correlation again:\\n\\ncorr_matrix\\ndf.corr()\\ncorr_matrix['fare_amount'].sort_values(ascending=False)\\nThere still isn’t a strong correlation between distance\\ntraveled and fare amount. Perhaps this will explain why:\\n\\ndf.describe()\\nHere is the output:\\nThe dataset contains outliers, and outliers frequently skew\\nthe results of machine learning models. Filter the dataset by\\neliminating negative fare amounts and placing reasonable\\nlimits on fares and distance, and then run a correlation\\nagain:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 94, 'file_type': 'pdf'}, page_content=\"df\\ndf[(df['distance']\\n(df['distance']\\n10.0)]\\ndf\\ndf[(df['fare_amount']\\n(df['fare_amount']\\n50.0)]\\n\\ncorr_matrix\\ndf.corr()\\ncorr_matrix['fare_amount'].sort_values(ascending=False)\\nOnce more, here is the output:\\n\\nfare_amount\\xa0\\xa0\\xa0 1.000000\\ndistance\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.851913\\nday_of_week\\xa0\\xa0 -0.003570\\npickup_time\\xa0\\xa0 -0.023085\\nName: fare_amount, dtype: float64\\nThat looks better! Most (85%) of the variance in fare amounts\\nis explained by the distance traveled. The correlation\\nbetween the day of the week, the hour of the day, and the\\nfare amount is still weak, but that’s not surprising since\\ndistance traveled is the main factor that drives taxi fares.\\nLet’s leave those columns in since it makes sense that it\\nmight take longer to get from point A to point B during rush\\nhour, or that traffic at 5:00 p.m. on Friday might be\\ndifferent than traffic at 5:00 p.m. on Saturday.\\nNow it’s time to train a regression model. Let’s try three\\ndifferent learning algorithms to determine which one yields\\nthe most accurate fit, and use cross-validation to gauge\\naccuracy. Start with a linear regression model:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLinearRegression\\nfrom\\nsklearn.model_selection\\nimport\\ncross_val_score\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 95, 'file_type': 'pdf'}, page_content=\"x\\ndf.drop(['fare_amount'],\\naxis=1)\\ny\\ndf['fare_amount']\\n\\nmodel\\nLinearRegression()\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nTry a RandomForestRegressor with the same dataset and see how\\nits accuracy compares. Recall that random-forest models train\\nmultiple decision trees on the data and average the results\\nof all the trees to make a prediction:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nRandomForestRegressor\\n\\nmodel\\nRandomForestRegressor(random_state=0)\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nFinally, try GradientBoostingRegressor. Gradient-boosting\\nmachines use multiple decision trees, each of which is\\ntrained to compensate for the error in the output from the\\nprevious one:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nGradientBoostingRegressor\\n\\nmodel\\nGradientBoostingRegressor(random_state=0)\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nAssuming the GradientBoostingRegressor produced the highest\\ncross-validated coefficient of determination, train it using\\nthe entire dataset:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 96, 'file_type': 'pdf'}, page_content=\"model.fit(x,\\ny)\\nFinish up by using the trained model to make a pair of\\npredictions. First, estimate what it will cost to hire a taxi\\nfor a 2-mile trip at 5:00 p.m. on Friday afternoon. Because\\nyou passed DataFrames containing column names to the fit\\nmethod, recent versions of Scikit will display a warning if\\nyou pass lists or NumPy arrays to predict. Therefore, input a\\nDataFrame instead:\\n\\nmodel.predict(pd.DataFrame({\\n'day_of_week':\\n'pickup_time':\\n[17],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'distance':\\n[2.0]\\nNow predict the fare amount for a 2-mile trip taken at 5:00\\np.m. one day later (on Saturday):\\n\\nmodel.predict(pd.DataFrame({\\n'day_of_week':\\n'pickup_time':\\n[17],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'distance':\\n[2.0]\\nDoes the model predict a higher or lower fare amount for the\\nsame trip on Saturday afternoon? Do the answers make sense\\ngiven that the data comes from New York City cabs? Consider\\nthat rush-hour traffic is likely to be heavier on Friday\\nafternoon than on Saturday afternoon.\\nSummary\\nRegression models are supervised learning models that predict\\nnumeric outcomes such as the cost of a taxi ride. Prominent\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 97, 'file_type': 'pdf'}, page_content='learning algorithms used for regression include the\\nfollowing:\\nLinear regression\\nModels training data by fitting it to the equation of\\na line\\nDecision trees\\nUse binary trees to predict an outcome by answering a\\nseries of yes-and-no questions\\nRandom forests\\nUse multiple independent decision trees to model the\\ndata and are resistant to overfitting\\nGradient-boosting machines\\nUse multiple dependent decision trees, each modeling\\nthe error in the output from the last\\nSupport vector machines\\nTake an entirely different approach to modeling data\\nby adding dimensionality under the supposition that\\ndata that isn’t linearly separable in the original\\nproblem space might be linearly separable in higher-\\ndimensional space\\nScikit provides convenient implementations of these and other\\nlearning algorithms in classes such as LinearRegression,\\nRandomForestRegressor, and GradientBoostingRegressor.\\nA common metric for quantifying the accuracy of regression\\nmodels is the R2 score, also known as the coefficient of\\ndetermination. It’s typically a value from 0 to 1, with\\nhigher numbers indicating higher accuracy. Technically, it’s\\na measure of the variance in the output that can be explained\\nby the values in the input. For small datasets, k-fold cross-\\nvalidation gives you more confidence in the R2 score than'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 98, 'file_type': 'pdf'}, page_content='simply splitting the data once for training and testing. k-\\nfold trains the model k times, each time with the dataset\\nsplit differently.\\nReal-world datasets tend to be messy and often require\\nfurther preparation to be useful for machine learning. As the\\ntaxi-fare example demonstrated, outliers in training data can\\naffect a model’s accuracy or prevent the model from being\\nuseful at all. One solution is to identify the outliers and\\nremove them before training the model. Another is to employ a\\nlearning algorithm such as ridge regression or lasso\\nregression that supports regularization.\\nRegression models are common in machine learning, but\\nclassification models are more common still. Chapter\\xa03\\ntackles classification models head-on, introduces another\\nleading learning algorithm called logistic regression, and\\nbuilds on what you learned in this chapter.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 99, 'file_type': 'pdf'}, page_content='Chapter 3. Classification Models\\nThe machine learning model featured in the previous chapter\\nused various forms of regression to predict taxi fares based\\non distance to travel, the day of the week, and the time of\\nday. Regression models predict numerical outcomes and are\\nwidely used in industry to forecast sales, prices, demand,\\nand other numbers that drive business decisions. Equally\\nimportant are classification models, which predict\\ncategorical outcomes such as whether a credit card\\ntransaction is fraudulent or which letter of the alphabet a\\nhandwritten character represents.\\nMost classification models fall into two categories: binary\\nclassification models, in which there are just two possible\\noutcomes, and multiclass classification models, in which\\nthere are more than two possible outcomes. In both instances,\\nthe model assigns a single class, or class label, to an\\ninput. Less common are multilabel classification models,\\nwhich can classify a single input as belonging to several\\nclasses—for example, predicting that a document is both a\\npaper on machine learning and a paper on genomics. Some can\\npredict that an input belongs to none of the possible classes\\ntoo.\\nMuch of what you know about regression models also applies to\\nclassification models. For example, many of the learning\\nalgorithms that power regression models work equally well\\nwith classification models. One substantive difference\\nbetween regression and classification is how you measure a\\nmodel’s accuracy. There’s no such thing as an R2 score for\\na classification model. In its place are an abundance of\\nmeasures, such as precision, recall, specificity,\\nsensitivity, and F1 score, to name but a few. One of the keys\\nto becoming proficient with classification models is getting'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 100, 'file_type': 'pdf'}, page_content='comfortable with the various accuracy metrics and, more\\nimportantly, understanding which one (or ones) to use based\\non the model’s intended application.\\nYou’ve seen one example of multiclass classification in the\\niris tutorial in Chapter\\xa01. It’s time to delve deeper into\\nmachine learning classifiers, starting with one of the most\\ntried-and-true learning algorithms of all, one that works\\nonly for classification models: logistic regression.\\nLogistic Regression\\nMany learning algorithms exist for classification problems.\\nIn Chapter\\xa02, you learned how decision trees, random\\nforests, and gradient-boosting machines (GBMs) fit regression\\nmodels to training data. These algorithms can be used for\\nclassification as well, and Scikit helps out by offering\\nclasses such as DecisionTreeClassifier,\\nRandomForestClassifier, and GradientBoostingClassifier. In\\nChapter\\xa01, you used Scikit’s KNeighborsClassifier class to\\nbuild a three-class classification model with k-nearest\\nneighbors as the learning algorithm.\\nThese are important learning algorithms, and they see use in\\nmany contemporary machine learning models. But one of the\\nmost popular classification algorithms is logistic\\nregression, which analyzes a distribution of data and fits an\\nequation to it that defines the probability that a given\\nsample belongs to each of two possible classes. It might\\ndetermine, for example, that there’s a 10% chance the values\\nin a sample correspond to class 0 and a 90% chance they\\ncorrespond to class 1. In this case, logistic regression will\\npredict that the sample corresponds to class 1. Despite the\\nname, logistic regression is a classification algorithm, not\\na regression algorithm. Its purpose is not to create\\nregression models but to quantify probabilities for the\\npurpose of classifying input samples.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 101, 'file_type': 'pdf'}, page_content='As an example, consider the data points in Figure\\xa03-1, which\\nbelong to two classes: 0 (blue) and 1 (red). Let’s assume\\nthat the x-axis specifies the number of hours a person\\nstudied for an exam and the y-axis indicates whether they\\npassed (1) or failed (0). The blues fall in the range x = 0\\nto x = 10, while the reds fall in the range x = 5 to x = 15.\\nYou can’t pick a value for x that separates the classes\\nsince both have values between x = 5 and x = 10. (Try drawing\\na vertical line that has only reds on one side and only blues\\non the other.) But you can draw a curve that, given an x,\\nshows the probability that a point with that x belongs to\\nclass 1. As x increases, so too does the likelihood that the\\npoint represents class 1 (pass) rather than class 0 (fail).\\nFrom the curve, you can see that if x = 2, there is less than\\na 5% chance that the point corresponds to class 1. But if x =\\n10, there is about a 76% chance that it’s class 1. If asked\\nto classify that point as a red or a blue, you would conclude\\nthat it’s a red because it’s much more likely to be red\\nthan blue.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 102, 'file_type': 'pdf'}, page_content='Figure 3-1. Logistic regression\\nThe curve\\xa0in Figure\\xa03-1 is a sigmoid curve. It charts a\\nfunction known as the logistic function (also known as the\\nlogit function) that has been used in statistics for decades.\\nFor logistic regression, the function is defined this way,\\nwhere x is the input value and m and b are parameters learned\\nduring training:\\nThis equation reveals why logistic regression is called\\nlogistic regression, despite the fact that it’s a\\nclassification algorithm. The exponent of e happens to be the\\nequation for linear regression.\\nThe logistic regression learning algorithm fits the logistic\\nfunction to a distribution of data and uses the resulting y\\nvalues as probabilities to classify data points. It works\\nwith any number of features (not just x, but x1, x2, x3, and\\nso on), and it is a parametric learning algorithm since it\\nuses training data to find optimum values for m and b. How it\\nfinds the optimum values is an implementational detail that\\nlibraries such as Scikit-Learn handle for you. Scikit\\ndefaults to a numerical optimization algorithm known as\\nLimited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)\\nbut supports other optimization methods as well. This,\\nincidentally, is one reason why Scikit is so popular in the\\nmachine learning community. It’s not difficult to calculate\\nm and b from the training data for a linear regression model,\\nbut it’s harder to do it for a logistic regression model,\\nnot to mention more sophisticated parametric models such as\\nsupport vector machines.\\nScikit’s LogisticRegression class is logistic regression in\\na box. With it, training a logistic regression model can be\\nas simple as this:\\n\\nmodel\\nLogisticRegression()'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 103, 'file_type': 'pdf'}, page_content=\"model.fit(x,\\ny)\\nOnce the model is trained, you can call its predict method to\\npredict which class the input belongs to, or its\\npredict_proba method to get the computed probabilities for\\neach class. If you fit a LogisticRegression model to the\\ndataset in Figure\\xa03-1, the following statement predicts\\nwhether x = 10 corresponds to class 0 or class 1:\\n\\npredicted_class\\nmodel.predict([[10.0]])[0]\\nprint(predicted_class)\\n# Outputs 1\\nAnd these statements show the probabilities computed for each\\nclass:\\n\\npredicted_probabilities\\nmodel.predict_proba([[10.0]])[0]\\nprint(f'Class 0: {predicted_probabilities[0]}')\\n# 0.23508543966167028\\nprint(f'Class 1: {predicted_probabilities[1]}')\\n# 0.7649145603383297\\nScikit also includes the LogisticRegressionCV class for\\ntraining logistic regression models with built-in cross-\\nvalidation. (If you need a refresher, cross-validation was\\nintroduced in Chapter\\xa02.) At the expense of additional\\ntraining time, the following statements train a logistic\\nregression model using five folds:\\n\\nmodel\\nLogisticRegressionCV(cv=5)\\nmodel.fit(x,\\ny)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 104, 'file_type': 'pdf'}, page_content='Logistic regression is technically a binary classification\\nalgorithm, but it can be used for multiclass classification\\ntoo. I’ll say more about this toward the end of the chapter.\\nFor now, think of logistic regression as a machine learning\\nalgorithm that uses the well-known logistic function to\\nquantify the probability that an input corresponds to each of\\ntwo classes, and you’ll have an accurate conceptual\\nunderstanding of what logistic regression is.\\nAccuracy Measures for Classification Models\\nYou can quantify the accuracy of a classification model the\\nsame way you do for a regression model: by calling the\\nmodel’s score method. For a classifier, score returns the\\nsum of the true positives and the true negatives divided by\\nthe total number of samples. If the test data includes 10\\npositives (samples of class 1) and 10 negatives (samples of\\nclass 0) and the model correctly identifies 8 of the\\npositives and 7 of the negatives, then the score is (8 + 7) /\\n20, or 0.75. This is sometimes referred to as the model’s\\naccuracy score.\\nThere are many other ways to score a classification model,\\nand which one is “right” often depends on how the model\\nwill be used. Rather than compute an accuracy score, data\\nscientists sometimes measure a classification model’s\\nprecision and recall instead:\\nPrecision\\nComputed by dividing the number of true positives by\\nthe sum of the true positives and false positives\\nRecall\\nComputed by dividing the number of true positives by\\nthe sum of the true positives and false negatives'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 105, 'file_type': 'pdf'}, page_content='In effect, precision imposes a penalty on false positives\\n(instances in which the model incorrectly predicts a 1),\\nwhile recall penalizes false negatives by lowering the score\\nwhen a model incorrectly predicts a 0.\\nFigure\\xa03-2 illustrates the difference. Suppose you train a\\nmodel to differentiate between polar bear images and walrus\\nimages, and to test it you submit three polar bear images and\\nthree walrus images. Furthermore, assume that the model\\ncorrectly classifies two of the polar bear images, but\\nincorrectly classifies two walrus images as polar bear\\nimages, as indicated by the red boxes. In this case, the\\nmodel’s precision in identifying polar bears is 50% because\\nonly two of the four images the model classified as polar\\nbears were in fact polar bears. But recall is 67% since the\\nmodel correctly identified two of the three polar bear\\nimages. That’s precision and recall in a nutshell. The\\nformer quantifies how confident you can be that a positive\\nprediction is accurate, while the latter quantifies the\\nmodel’s ability to accurately identify positive samples. The\\ntwo can be combined into one score called the F1 score (also\\nknown as the F-score) using a simple formula.\\nFigure 3-2. Using precision and recall to measure the accuracy of a\\nclassifier\\nScikit provides helpful functions such as precision_score,\\nrecall_score, and f1_score for retrieving classification\\nmetrics. Whether you prefer precision or recall depends on\\nwhich is higher: the cost of false positives or the cost of'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 106, 'file_type': 'pdf'}, page_content='false negatives. Use precision when the cost of false\\npositives is high—for example, when “positive” means an\\nemail is spam. You would rather a spam filter send a few spam\\nmails to your inbox than route a legitimate (and potentially\\nimportant) email to your junk folder. By contrast, use recall\\nif the cost of false negatives is high. A great example is\\nwhen using machine learning to spot tumors in X-rays and MRI\\nscans. You would much rather mistakenly send a patient to a\\ndoctor due to a false positive than tell that patient there\\nare no tumors when there really are.\\nSPOTTING POLAR BEARS IN THE WILD\\nThe polar bear versus walrus example was taken from a\\ntutorial I wrote for Microsoft that began with the\\nfollowing introduction:\\nYou’re the leader of a group of climate scientists\\nwho are concerned about the dwindling polar bear\\npopulation in the Arctic. To address the problem,\\nyour team has placed hundreds of motion-activated\\ncameras at strategic locations throughout the region.\\nInstead of manually examining each photo that’s\\ntaken to determine whether it contains a polar bear,\\nyour challenge is to devise an automated system that\\nprocesses data from these cameras in real time and\\ndisplays an alert on a map when one of your cameras\\nphotographs a polar bear. You need a solution that\\nuses artificial intelligence (AI) to determine with a\\nhigh degree of accuracy whether a photo contains a\\npolar bear. And you need it fast, because climate\\nchange won’t wait.\\nThe tutorial combines several Azure services to form an\\nend-to-end solution, and it uses Microsoft’s Power BI\\nfor visualizations. If you’re interested, you can check\\nit out online.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 107, 'file_type': 'pdf'}, page_content='Accuracy score, precision, recall, and F1 score apply to\\nbinary classification and mul\\u2060ticlass classification models.\\nAn additional metric—one that applies to binary\\nclassification only—is the receiver operating characteristic\\n(ROC) curve, which plots the true-positive rate (TPR) against\\nthe false-positive rate (FPR) at various probability\\nthresholds. A sample ROC curve is shown in Figure\\xa03-3. A\\nstraight line stretching from the lower left to the upper\\nright would indicate that the model gets it right just 50% of\\nthe time, which is no better than guessing for a binary\\nclassifier. The more the curve arches toward the upper-left\\ncorner, the more accurate the model. Data scientists often\\nuse the area under the curve (AUC, or ROC AUC) as an overall\\nmeasure of accuracy. Scikit provides a class named\\nRocCurveDisplay for plotting ROC curves, and a function named\\nroc_auc_score for retrieving ROC AUC scores. Scores returned\\nby this function are values from 0.0 to 1.0. The higher the\\nscore, the more accurate the model.\\nYet another way to assess the accuracy of a classification\\nmodel is to plot a confusion matrix like the one in\\nFigure\\xa03-4. It works for binary and multiclass\\nclassification, and it shows for each class how the model\\nperformed during testing. In this example, the model was\\nasked to differentiate between images containing masked faces\\nand images containing unmasked faces. It got it right 78 out\\nof 85 times when presented with pictures of people wearing\\nmasks, and 41 out of 58 times when presented with pictures of\\npeople not wearing masks. Scikit offers a confusion_matrix\\nfunction for computing a confusion matrix, and a\\nConfusionMatrixDisplay class with methods named from_estimator\\nand from_predictions for plotting confusion matrices.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 108, 'file_type': 'pdf'}, page_content='Figure 3-3. Receiver operating characteristic curve'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 109, 'file_type': 'pdf'}, page_content='Figure 3-4. Confusion matrix\\nNOTE\\nCountless code samples online use Scikit’s plot_confusion_matrix\\nfunction to display confusion matrices. ConfusionMatrixDisplay was\\nintroduced in Scikit 1.0 and is the proper way to generate\\nconfusion matrices. plot_confusion_matrix is slated to be removed\\nfrom the library in version 1.2.\\nOther terms you might come across when discussing the\\naccuracy of classification models include sensitivity and'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 110, 'file_type': 'pdf'}, page_content='specificity. Sensitivity is identical to recall, so Scikit\\ndoesn’t include a separate method for computing it.\\nSpecificity is recall for the negative class rather than the\\npositive class and is calculated by dividing the number of\\ntrue negatives by the sum of the true negatives and false\\npositives. Scikit doesn’t provide a dedicated function for\\ncalculating specificity, either, but you can do it easily\\nenough by calling recall_score with a pos_label parameter\\nindicating that 0 (rather than 1) is the positive label:\\n\\nrecall_score(y_test,\\ny_predicted,\\npos_label=0)\\nSensitivity and specificity are frequently used in drug\\ntesting and cancer screening. Suppose you’re traveling\\nabroad and require a negative COVID test before returning\\nhome. If you don’t have COVID, what are the chances that a\\ntest will incorrectly say you do? (I have asked myself that\\nquestion many times recently while traveling overseas.) The\\nanswer is the test’s specificity—a measure of how accurate\\nthe test is at identifying negative samples. Sensitivity, on\\nthe other hand, reveals how likely the test is to be correct\\nif it says you do have COVID. It’s a subtle distinction, but\\nan important one if you’re concerned that a faulty test\\nmight keep you from going home (specificity) or if you want\\nto be certain you don’t have COVID before visiting an\\nelderly relative (sensitivity).\\nCategorical Data\\nMachine learning finds patterns in numbers. It works only\\nwith numbers. Yet many datasets have columns containing\\nstring values such as \"male\" and \"female\" or \"red\", \"green\",\\nand \"blue\". Data scientists refer to these as categorical\\nvalues and the columns that contain them as categorical'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 111, 'file_type': 'pdf'}, page_content=\"columns. Machine learning can’t handle categorical values\\ndirectly. To use them in a model, you must convert them into\\nnumbers.\\nTwo popular techniques exist for converting categorical\\nvalues into numerical values. One is label encoding, which\\nyou briefly saw in the k-means clustering example in\\nChapter\\xa01. Label encoding replaces categorical values with\\nintegers. If there are three unique values in a column, label\\nencoding replaces them with 0s, 1s, and 2s. To demonstrate,\\nrun the following code in a Jupyter notebook:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.preprocessing\\nimport\\nLabelEncoder\\n\\ndata\\n[[10,\\n'red'],\\n'blue'],\\n'red'],\\n'green'],\\n'blue']]\\ndf\\npd.DataFrame(data,\\ncolumns=['Length',\\n'Color'])\\n\\nencoder\\nLabelEncoder()\\ndf['Color']\\nencoder.fit_transform(df['Color'])\\ndf.head()\\nThis code creates a DataFrame containing a numeric column\\nnamed Length and a categorical column named Color, which\\ncontains three different categorical values. Here’s what the\\ndataset looks like before encoding:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 112, 'file_type': 'pdf'}, page_content='And here’s how it looks after the values in the Color column\\nare label-encoded using Scikit’s LabelEncoder class:\\nThe encoded dataset can be used to train a machine learning\\nmodel. The unencoded dataset cannot. You can get an ordered\\nlist of the classes that were encoded from the encoder’s\\nclasses_ attribute.\\nThe other, more common means for converting categorical\\nvalues into numeric values is one-hot encoding, which adds'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 113, 'file_type': 'pdf'}, page_content=\"one column to the dataset for each unique value in a\\ncategorical column and fills the encoded columns with 1s and\\n0s. One-hot encoding can be performed with Scikit’s\\nOneHotEncoder class or by calling get\\u200b_dum\\u2060mies on a Pandas\\nDataFrame. Here is how the latter is used to encode the\\ndataset:\\n\\ndata\\n[[10,\\n'red'],\\n'blue'],\\n'red'],\\n'green'],\\n'blue']]\\ndf\\npd.DataFrame(data,\\ncolumns=['Length',\\n'Color'])\\n\\ndf\\npd.get_dummies(df,\\ncolumns=['Color'])\\ndf.head()\\nAnd here are the results:\\nLabel encoding and one-hot encoding are used with regression\\nproblems and classification problems. The obvious question\\nis, which one should you use? Generally speaking, data\\nscientists prefer one-hot encoding to label encoding. The\\nformer gives every unique value an equal weight, whereas the\\nlatter implies that some values may be more important than\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 114, 'file_type': 'pdf'}, page_content='others—for example, that \"red\" (2) is more important than\\n\"blue\" (0). Label encoding, on the other hand, is more memory\\nefficient. The number of columns is the same before and after\\nencoding, whereas one-hot encoding adds one column per unique\\nvalue. For very large datasets with thousands of unique\\nvalues in a categorical column, label encoding requires\\nsubstantially less memory.\\nThe accuracy of a machine learning model is rarely impacted\\nby the encoding method you choose. If you’re in doubt,\\nyou’ll rarely go wrong with one-hot encoding. If you want to\\nbe certain, you can encode the data both ways and compare the\\nresults after training a machine learning model.\\nBinary Classification\\nBinary classifiers are supervised learning models trained\\nwith labeled data: 0s for the negative class and 1s for the\\npositive. The predictions they make are 0s and 1s too. They\\nalso divulge a probability for each class that you can factor\\ninto your conclusions. For example, a credit card company\\nmight decide that a transaction will be declined only if the\\nmodel predicts with at least 99% certainty that the\\ntransaction is fraudulent. In that case, the probability that\\nthe model computed is more important than the raw prediction\\nthat it made.\\nTo help bring home everything presented thus far regarding\\nbinary classification, let’s use Scikit to build a couple of\\nmodels: first a simple one that demonstrates core principles,\\nfollowed by a second one that solves a genuine business\\nproblem.\\nClassifying Passengers Who Sailed on the\\nTitanic'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 115, 'file_type': 'pdf'}, page_content=\"One of the more famous public datasets in machine learning is\\nthe Titanic dataset, which contains information regarding\\nhundreds of passengers who sailed on the ill-fated voyage of\\nthe RMS Titanic, including which ones survived and which ones\\ndid not. Let’s use logistic regression to build a binary\\nclassification model from the dataset and see if we can\\npredict the odds that a passenger will survive given that\\nperson’s gender, age, and fare class (whether they traveled\\nin first, second, or third class).\\nThe first step is to download the dataset and copy it to the\\nData subdirectory of the directory that hosts your Jupyter\\nnotebooks. Then run the following code in a notebook to load\\nthe dataset and get a feel for its contents:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/titanic.csv')\\ndf.head()\\nHere is the output:\\nThe dataset contains 891 rows and 12 columns. Some of the\\ncolumns, such as PassengerId and Name, aren’t relevant to a\\nmachine learning model. Others are very relevant. The ones\\nwe’ll focus on are:\\nSurvived\\nIndicates whether the passenger survived the voyage\\n(1) or did not (0)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 116, 'file_type': 'pdf'}, page_content=\"Pclass\\nIndicates whether the passenger was traveling in\\nfirst class (1), second class (2), or third class (3)\\nSex\\nIndicates the passenger’s gender\\nAge\\nIndicates the passenger’s age\\nThe Survived column is the label column—the one we’ll try\\nto predict. The other columns are relevant because first-\\nclass passengers were more likely to survive the sinking\\nbecause their cabins were closer to the top deck of the ship\\nand nearer the lifeboats. Plus, women and children were more\\nlikely to be given space in lifeboats.\\nNow use the following statement to see if the dataset is\\nmissing any values:\\n\\ndf.info()\\nHere’s the output:\\n\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 891 entries, 0 to 890\\nData columns (total 12 columns):\\n#\\xa0\\xa0 Column\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Non-Null Count\\xa0 Dtype\\xa0 \\n---\\xa0 ------\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 --------------\\xa0 -----\\xa0 \\n0\\xa0\\xa0 PassengerId\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n1\\xa0\\xa0 Survived\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n2\\xa0\\xa0 Pclass\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 117, 'file_type': 'pdf'}, page_content=\"3\\xa0\\xa0 Name\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 object \\n4\\xa0\\xa0 Sex\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 object \\n5\\xa0\\xa0 Age\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 714 non-null\\xa0\\xa0\\xa0 float64\\n6\\xa0\\xa0 SibSp\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n7\\xa0\\xa0 Parch\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 int64\\xa0 \\n8\\xa0\\xa0 Ticket\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 object \\n9\\xa0\\xa0 Fare\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 891 non-null\\xa0\\xa0\\xa0 float64\\n10\\xa0 Cabin\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 204 non-null\\xa0\\xa0\\xa0 object \\n11\\xa0 Embarked\\xa0\\xa0\\xa0\\xa0 889 non-null\\xa0\\xa0\\xa0 object \\ndtypes: float64(2), int64(5), object(5)\\nmemory usage: 83.7+ KB\\nThe Cabin column is missing a lot of values, but we don’t\\ncare since we’re not using that column. We will use the Age\\ncolumn, and that column is missing some values as well. We\\ncould replace the missing values with the mean of all the\\nother ages—an approach that data scientists refer to as\\nimputing missing values—but we’ll take the simpler approach\\nof removing rows with missing values. Use the following\\nstatements to remove the columns that aren’t needed, drop\\nrows with missing values, and one-hot-encode the values in\\nthe Sex and Pclass columns:\\n\\ndf\\ndf[['Survived',\\n'Age',\\n'Sex',\\n'Pclass']]\\ndf\\npd.get_dummies(df,\\ncolumns=['Sex',\\n'Pclass'])\\ndf.dropna(inplace=True)\\ndf.head()\\nHere is the resulting dataset:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 118, 'file_type': 'pdf'}, page_content=\"The next task is to split the dataset for training and\\ntesting:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\ndf.drop('Survived',\\naxis=1)\\ny\\ndf['Survived']\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y,\\nrandom_state=0)\\nNote the stratify=y parameter passed to train_test_split.\\nThat’s important, because of the 714 samples remaining after\\nrows with missing values are removed, 290 represent\\npassengers who survived and 424 represent passengers who did\\nnot. We want the training dataset and the test dataset to\\ncontain similar proportions of both classes, and stratify=y\\naccomplishes that. Without stratification, the model might\\nappear to be more or less accurate than it really is.\\nNow create a logistic regression model, train it with the\\ndata split off for training, and score it with the test data:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 119, 'file_type': 'pdf'}, page_content=\"model\\nLogisticRegression(random_state=0)\\nmodel.fit(x_train,\\ny_train)\\nmodel.score(x_test,\\ny_test)\\nScore the model again using cross-validation in order to have\\nmore confidence in the score. Remember that this is the\\naccuracy score computed by summing the true positives and\\ntrue negatives and dividing by the total number of samples:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ncross_val_score\\n\\ncross_val_score(model,\\nx,\\ny,\\ncv=5).mean()\\nUse the following statements to display a confusion matrix\\nshowing precisely how the model performed during testing:\\n\\n%matplotlib\\ninline\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=['Perished',\\n'Survived'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nObserve that the model is more accurate when predicting that\\npassengers won’t survive than when predicting that they\\nwill. That’s because the dataset used to train the model\\ncontained more examples of passengers who perished than of\\npassengers who survived. You always prefer to train a binary\\nclassification model with a perfectly balanced dataset\\ncontaining an equal number of positive and negative samples,\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 120, 'file_type': 'pdf'}, page_content=\"but it’s acceptable to train with an imbalanced dataset if\\nyou take the imbalance into account when assessing the\\nmodel’s accuracy.\\nNow use Scikit’s precision_score and recall_score functions\\nto compute the model’s precision, recall, sensitivity, and\\nspecificity:\\n\\nfrom\\nsklearn.metrics\\nimport\\nprecision_score,\\nrecall_score\\n\\ny_pred\\nmodel.predict(x_test)\\nprecision\\nprecision_score(y_test,\\ny_pred)\\nrecall\\nrecall_score(y_test,\\ny_pred)\\nsensitivity\\nrecall\\nspecificity\\nrecall_score(y_test,\\ny_pred,\\npos_label=0)\\n\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'Sensitivity: {sensitivity}')\\nprint(f'Specificity: {specificity}')\\nIs the high specificity score consistent with the observation\\nthat the model is more adept at identifying passengers who\\nwon’t survive than those who will? How would you explain the\\nrelatively low recall and sensitivity scores?\\nNow let’s use the trained model to make some predictions.\\nFirst, find out whether a 30-year-old female traveling in\\nfirst class is likely to survive the voyage. Since the model\\nwas trained with a DataFrame containing column names, we’ll\\nuse the same column names to formulate an input:\\n\\nfemale\\npd.DataFrame({\\n'Age':\\n[30],\\n'Sex_female':\\n'Sex_male':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Pclass_1':\\n'Pclass_2':\\n'Pclass_3':\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 121, 'file_type': 'pdf'}, page_content=\"model.predict(female)[0]\\nThe model predicts she will survive, but what are the odds\\nthat she we will survive?\\n\\nprobability\\nmodel.predict_proba(female)[0][1]\\nprint(f'Probability of survival: {probability:.1%}')\\nA 30-year-old female traveling in first class is more than\\n90% likely to survive the voyage, but what about a 60-year-\\nold male traveling in third class?\\n\\nmale\\npd.DataFrame({\\n'Age':\\n[60],\\n'Sex_female':\\n'Sex_male':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Pclass_1':\\n'Pclass_2':\\n'Pclass_3':\\n\\nprobability\\nmodel.predict_proba(male)[0][1]\\nprint(f'Probability of survival: {probability:.1%}')\\nFeel free to experiment with other inputs to see what the\\nmodel says. How likely, for example, is a 12-year-old boy\\ntraveling in second class to survive the sinking of the\\nTitanic?\\nDetecting Credit Card Fraud\\nOne of the most compelling uses for machine learning today is\\nspotting fraudulent financial transactions. Credit card\\ncompanies apply machine learning at the point of sale to\\ndecide whether to accept or decline individual charges. While\\nthese companies are understandably reluctant to publish the\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 122, 'file_type': 'pdf'}, page_content='details of how they do it or the data they use to train their\\nmodels, at least one such dataset has been published for\\npublic consumption. The data in it was anonymized using a\\ntechnique called \\xa0principal component analysis (PCA), which\\nI’ll introduce in Chapter\\xa06.\\nThe dataset is pictured in Figure\\xa03-5. The data comes from\\nreal transactions made by European credit card holders in\\nSeptember 2013. Most of the columns have uninformative names,\\nsuch as V1 and V2, and contain similarly opaque values. Three\\ncolumns—Time, Amount, and Class—have real names and\\nunaltered values revealing when the transaction took place,\\nthe amount of the transaction, and whether the transaction\\nwas legitimate (Class=0) or fraudulent (Class=1).\\nFigure 3-5. The fraud-detection dataset\\nEach row represents one transaction. Of the 284,807\\ntransactions in the dataset, only 492 are fraudulent. The\\ndataset is highly imbalanced, so you would expect a machine\\nlearning model trained on it to be much better at classifying\\nlegitimate transactions than fraudulent ones. That’s not\\nnecessarily a problem, because credit card companies would\\nrather misclassify fraudulent transactions and allow 100 of'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 123, 'file_type': 'pdf'}, page_content=\"them to slide through than misclassify one legitimate\\ntransaction and anger a customer.\\nBegin by downloading a ZIP file containing the dataset. Copy\\ncreditcard.csv from the ZIP file into your notebooks’ Data\\nsubdirectory, and then run the following code in a Jupyter\\nnotebook to load the dataset and show the first several rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/creditcard.csv')\\ndf.head()\\nFind out how many rows the dataset contains and whether any\\nof those rows have missing values:\\n\\ndf.info()\\nThe dataset contains 284,807 rows, and none of them are\\nmissing values. Split the data for training and testing, and\\nuse train_test_split’s stratify parameter to ensure that the\\nratio of legitimate and fraudulent transactions is consistent\\nin the training dataset and the testing dataset:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\ndf.drop(['Time',\\n'Class'],\\naxis=1)\\ny\\ndf['Class']\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y,\\nrandom_state=0)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 124, 'file_type': 'pdf'}, page_content=\"Train a logistic regression model to separate the classes:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\n\\nlr_model\\nLogisticRegression(random_state=0,\\nmax_iter=5000)\\nlr_model.fit(x_train,\\ny_train)\\nNote the max_iter=5000 parameter passed to the\\nLogisticRegression function. max_iter specifies the maximum\\nnumber of iterations allowed to converge on a solution when\\nfitting the logistic function to a dataset. The default is\\n100, which isn’t enough in this example. Raising the limit\\nto 5,000 gives the internal solver the headroom it needs to\\nfind a solution.\\nA typical accuracy score computed by dividing the sum of the\\ntrue positives and true negatives by the number of test\\nsamples isn’t very helpful because the dataset is so\\nimbalanced. Fraudulent transactions represent less than 0.2%\\nof all the samples, which means that the model could simply\\nguess that every transaction is legitimate and get it right\\nabout 99.8% of the time. Use a confusion matrix to visualize\\nhow the model performs during testing:\\n\\n%matplotlib\\ninline\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nlabels\\n['Legitimate',\\n'Fraudulent']\\ncmd.from_estimator(lr_model,\\nx_test,\\ny_test,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 125, 'file_type': 'pdf'}, page_content=\"A logistic regression model correctly identified 56,853\\ntransactions as legitimate while misclassifying legitimate\\ntransactions as fraudulent just 11 times. We want to minimize\\nthe latter number because we don’t want to annoy customers\\nby declining legitimate transactions. Let’s see if a random-\\nforest classifier can do better:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nRandomForestClassifier\\n\\nrf_model\\nRandomForestClassifier(random_state=0)\\nrf_model.fit(x_train,\\ny_train)\\n\\ncmd.from_estimator(rf_model,\\nx_test,\\ny_test,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 126, 'file_type': 'pdf'}, page_content=\"A random forest mistook just four legitimate transactions as\\nfraudulent. That’s an improvement over logistic regression.\\nLet’s see if a gradient-boosting classifier can do better\\nstill:\\n\\nfrom\\nsklearn.ensemble\\nimport\\nGradientBoostingClassifier\\n\\ngbm_model\\nGradientBoostingClassifier(random_state=0)\\ngbm_model.fit(x_train,\\ny_train)\\n\\ncmd.from_estimator(gbm_model,\\nx_test,\\ny_test,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 127, 'file_type': 'pdf'}, page_content='The GBM misclassified more legitimate transactions than the\\nrandom forest, so we’ll stick with the random forest. Out of\\n56,864 legitimate transactions, the random forest correctly\\nclassified 56,860 of them. This means that legitimate\\ntransactions are classified correctly more than 99.99% of the\\ntime. Meanwhile, the model caught about 75% of the fraudulent\\ntransactions.\\nUse the following statements to measure the random-forest\\nclassifier’s precision, recall, sensitivity, and\\nspecificity:\\n\\nfrom\\nsklearn.metrics\\nimport\\nprecision_score,\\nrecall_score'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 128, 'file_type': 'pdf'}, page_content=\"y_pred\\nrf_model.predict(x_test)\\nprecision\\nprecision_score(y_test,\\ny_pred)\\nrecall\\nrecall_score(y_test,\\ny_pred)\\nsensitivity\\nrecall\\nspecificity\\nrecall_score(y_test,\\ny_pred,\\npos_label=0)\\n\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'Sensitivity: {sensitivity}')\\nprint(f'Specificity: {specificity}')\\nHere is the output:\\n\\nPrecision: 0.9466666666666667\\nRecall: 0.7244897959183674\\nSensitivity: 0.7244897959183674\\nSpecificity: 0.9999296567248172\\nGiven credit card companies’ desire to keep customers happy\\nand spending money, which of these metrics do you think\\nthey’re most interested in? If you answered specificity, you\\nanswered correctly. Specificity is a measure of how reliable\\nthe test is at not falsely classifying a negative sample as\\npositive—in this case, at not classifying a legitimate\\ntransaction as fraudulent.\\nUnfortunately, you can’t make predictions with this model\\nbecause you don’t know the meaning of the numbers in the V1\\nthrough V28 columns, and you can’t generate columnar values\\nfrom a new transaction because you don’t have the transform\\napplied to the original dataset. You don’t even know what\\nthe original dataset looks like. Most likely each row\\ncontains information about the card holder—for example,\\nannual income, credit score, age, country of residence, and\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 129, 'file_type': 'pdf'}, page_content='amount of money spent on the card last year—plus information\\nabout the product that was purchased and where it was\\npurchased. The feature-engineering aspect of machine learning\\n—figuring out what data is relevant to the model you’re\\nattempting to build—is just as challenging, if not more so,\\nthan data preparation and choosing a learning algorithm.\\nIn real life, the models that credit card companies use to\\ndetect fraud are more sophisticated than this, and they often\\nincorporate several models since no single model is 100%\\naccurate. A company might build three models, for example,\\nand allow them to vote on whether a given transaction is\\nlegitimate. Regardless, you have proven the principle that,\\ngiven the right features, you can build a classification\\nmodel that is reasonably accurate at detecting credit card\\nfraud. And you have seen firsthand how easy Scikit makes it\\nto experiment with different learning algorithms to determine\\nwhich produces the most useful model.\\nMulticlass Classification\\nNow it’s time to tackle multiclass classification, in which\\nthere are n possible outcomes rather than just two. A great\\nexample of multiclass classification is performing optical\\ncharacter recognition: examining a handwritten digit and\\npredicting which digit 0 through 9 it corresponds to. Another\\nexample is looking at a facial photo and identifying the\\nperson in the photo by running it through a model trained to\\nrecognize hundreds of people.\\nVirtually everything you learned about binary classification\\napplies to multiclass classification too. In Scikit, any\\nclassifier that works with binary classification also works\\nwith multiclass classification models. The importance of this\\ncan’t be overstated. Some learning algorithms, such as\\nlogistic regression, work only in binary classification\\nscenarios. Many machine learning libraries make you write\\nexplicit code to extend logistic regression to perform'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 130, 'file_type': 'pdf'}, page_content=\"multiclass classification—or use a different form of\\nlogistic regression altogether. Scikit doesn’t. Instead, it\\nmakes sure classifiers such as LogisticRegression work in\\neither scenario, and when necessary, it does extra work\\nbehind the scenes to make it happen.\\nFor logistic regression, Scikit uses one of two strategies to\\nextend the algorithm to work in multiclass scenarios. (You\\ncan specify which strategy to use with the Logisti\\u2060c\\u200bRegression\\nclass’s multi_class parameter, or accept the default of\\n'auto' and allow Scikit to choose.) One is multinomial\\nlogistic regression, which replaces the logistic function\\nwith a softmax function that yields multiple probabilities—\\none per class. The other is one-vs-rest, also known as one-\\nvs-all, which trains n binary classification models, where n\\nis the number of classes that the model can predict. Each of\\nthe n models pairs one class against all the other classes,\\nand when the model is asked to make a prediction, it runs the\\ninput through all n models and uses the output from the one\\nthat yields the highest probability. This strategy is\\ndepicted in Figure\\xa03-6.\\nFigure 3-6. Strategies for extending binary classification algorithms to\\nsupport multiclass classification\\nThe one-vs-rest approach works well for logistic regression,\\nbut for some binary-only classification algorithms, Scikit\\nuses a one-vs-one approach instead. When you use Scikit’s\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 131, 'file_type': 'pdf'}, page_content='SVC class (a support vector machine classifier that you’ll\\nlearn about in Chapter\\xa05) to perform multiclass\\nclassification, for example, Scikit builds one model for each\\npair of classes. If the model includes four possible classes,\\nScikit builds no fewer than seven models under the hood.\\nYou don’t have to know any of this to build a multiclass\\nclassification model. But it does explain why some multiclass\\nclassification models require more memory and train more\\nslowly than others. Some classification algorithms, such as\\nrandom forests and GBMs, support multiclass classification\\nnatively. For algorithms that don’t, Scikit has your back.\\nIt fills the gap and does so as transparently as possible.\\nTo reiterate: all Scikit classifiers are capable of\\nperforming binary classification and multiclass\\nclassification. This simplifies the code you write and lets\\nyou focus on building and training models rather than\\nunderstanding the underlying mechanics of the algorithms.\\nBuilding a Digit Recognition Model\\nWant to experience multiclass classification firsthand? How\\nabout a model that examines scanned, handwritten digits and\\npredicts what digits 0–9 they correspond to? The US Postal\\nService built a similar model many years ago to recognize\\nhandwritten zip codes as part of an effort to automate mail\\nsorting. We’ll use a sample dataset that’s built into\\nScikit: the University of California Irvine’s Optical\\nRecognition of Handwritten Digits dataset, which contains\\nalmost 1,800 handwritten digits. Each digit is represented by\\nan 8 × 8 array of numbers from 0 to 16, with higher numbers\\nindicating darker pixels. We will use logistic regression to\\nmake predictions from the data. Figure\\xa03-7 shows the first\\n50 digits in the dataset.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 132, 'file_type': 'pdf'}, page_content=\"Figure 3-7. First 50 digits in the Optical Recognition of Handwritten Digits\\ndataset\\nStart by creating a Jupyter notebook and executing the\\nfollowing statements in the first cell:\\n\\nfrom\\nsklearn\\nimport\\ndatasets\\n\\ndigits\\ndatasets.load_digits()\\nprint('digits.images: '\\nstr(digits.images.shape))\\nprint('digits.target: '\\nstr(digits.target.shape))\\nHere’s what the first digit looks like in numerical form:\\n\\ndigits.images[0]\\nAnd here’s how it looks to the eye:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 133, 'file_type': 'pdf'}, page_content=\"%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n\\nplt.tick_params(axis='both',\\nwhich='both',\\nbottom=False,\\ntop=False,\\nleft=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 right=False,\\nlabelbottom=False,\\nlabelleft=False)\\nplt.imshow(digits.images[0],\\ncmap=plt.cm.gray_r)\\nIt’s obviously a 0, but you can confirm that from its label:\\n\\ndigits.target[0]\\nPlot the first 50 images and show the corresponding labels:\\n\\nfig,\\naxes\\nplt.subplots(5,\\nfigsize=(12,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n'yticks':\\n\\nfor\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0 ax.imshow(digits.images[i],\\ncmap=plt.cm.gray_r)\\n\\xa0\\xa0\\xa0 ax.text(0.45,\\n1.05,\\nstr(digits.target[i]),\\ntransform=ax.transAxes)\\nClassification models work best with balanced datasets. Use\\nthe following statements to plot the distribution of the\\nsamples:\\n\\nplt.xticks([])\\nplt.hist(digits.target,\\nrwidth=0.9)\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 134, 'file_type': 'pdf'}, page_content='The dataset is almost perfectly balanced, so let’s split it\\nand train a logistic regression model:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 digits.data,\\ndigits.target,\\ntest_size=0.2,\\nrandom_state=0)\\n\\nmodel\\nLogisticRegression(max_iter=5000)\\nmodel.fit(x_train,\\ny_train)\\nUse the score method to quantify the model’s accuracy:\\n\\nmodel.score(x_test,\\ny_test)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 135, 'file_type': 'pdf'}, page_content=\"Use a confusion matrix to see how the model performs on the\\ntest dataset:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nfig,\\nax\\nplt.subplots(figsize=(8,\\nax.grid(False)\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\ncmap='Blues',\\ncolorbar=False,\\nax=ax)\\nThe resulting output paints an encouraging picture: large\\nnumbers and dark colors along the diagonal, and small numbers\\nand light colors outside the diagonal. A perfect model would\\nhave all zeros outside the diagonal, but of course, perfect\\nmodels don’t exist:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 136, 'file_type': 'pdf'}, page_content=\"Pick one of the digits from the dataset and plot it to see\\nwhat it looks like:\\n\\nsns.reset_orig()\\n# Undo sns.set()\\nplt.tick_params(axis='both',\\nwhich='both',\\nbottom=False,\\ntop=False,\\nleft=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 right=False,\\nlabelbottom=False,\\nlabelleft=False)\\nplt.imshow(digits.images[100],\\ncmap=plt.cm.gray_r)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 137, 'file_type': 'pdf'}, page_content='Pass it to the model and see what digit the model predicts it\\nis:\\n\\nmodel.predict([digits.data[100]])[0]\\nWhat probabilities does the model predict for each possible\\ndigit?\\n\\nmodel.predict_proba([digits.data[100]])\\nWhat is the probability that the digit is a 4?\\n\\nmodel.predict_proba([digits.data[100]])[0][4]\\nWhen used for binary classification, predict_proba returns\\ntwo probabilities: one for the negative class and one for the\\npositive class. For multiclass classification, predict_proba\\nreturns probabilities for each possible class. This permits\\nyou to assess the model’s confidence in the prediction\\nreturned by predict. Not surprisingly, predict returns the\\nclass assigned the highest probability.\\nSummary\\nClassification models are widely used in industry to predict\\ncategorical outcomes such as whether a credit card\\ntransaction should be accepted or declined. Binary\\nclassification models predict either of two outcomes, while\\nmulticlass classification models predict more than two'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 138, 'file_type': 'pdf'}, page_content='outcomes. One of the most widely used learning algorithms for\\nclassification models is logistic regression, which fits an\\nequation to training data and uses it to predict outcomes by\\ncomputing the possibility that each is the correct one and\\nselecting the outcome with the highest probability.\\nThere are many ways to score classification models. Which\\nmethod is correct depends on how you intend to use the model.\\nFor example, when the cost of false positives is high, use\\nprecision to assess the model’s accuracy. Precision is\\ncomputed by dividing the number of true positives by the sum\\nof the true positives and false positives. On the other hand,\\nif the cost of false negatives is high, use recall instead.\\nRecall is computed by dividing the number of true positives\\nby the sum of the true positives and false negatives. Closely\\nrelated to precision and recall are sensitivity and\\nspecificity. Sensitivity is identical to recall, while\\nspecificity is recall for the negative class rather than the\\npositive class. Confusion matrices offer a convenient way to\\nvisualize how a model performs on test data without reducing\\nthe accuracy to a single number.\\nSome learning algorithms work only with binary\\nclassification, but Scikit works some magic under the hood to\\nmake sure any learning algorithm can be used for binary or\\nmulticlass classification. This isn’t true of all machine\\nlearning libraries. Some restrict their learning algorithms\\nto specific scenarios or require you to write extra code to\\nuse a binary classification algorithm in a multiclass model.\\nAll the models in this chapter were trained with numerical\\ndata, even though some of the datasets contained categorical\\nvalues—values that are strings rather than numbers—that had\\nto be converted to numbers using one-hot encoding. You may\\nwonder how to build a classification model that works solely\\non text—for example, a model that scores restaurant reviews\\nfor sentiment or classifies emails as spam or not spam'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 138, 'file_type': 'pdf'}, page_content='. You may\\nwonder how to build a classification model that works solely\\non text—for example, a model that scores restaurant reviews\\nfor sentiment or classifies emails as spam or not spam. It’s\\na perfectly legitimate question to ask. And it happens to be\\nthe subject of Chapter\\xa04.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 139, 'file_type': 'pdf'}, page_content='Chapter 4. Text Classification\\nOne of the more novel uses for binary classification is\\nsentiment analysis, which examines a sample of text such as a\\nproduct review, a tweet, or a comment left on a website and\\nscores it on a scale of 0.0 to 1.0, where 0.0 represents\\nnegative sentiment and 1.0 represents positive sentiment. A\\nreview such as “great product at a great price” might score\\n0.9, while “overpriced product that barely works” might\\nscore 0.1. The score is the probability that the text\\nexpresses positive sentiment. Sentiment analysis models are\\ndifficult to build algorithmically but are relatively easy to\\ncraft with machine learning. For examples of how sentiment\\nanalysis is used in business today, see the article “8\\nSentiment Analysis Real-World Use Cases” by Nicholas\\nBianchi.\\nSentiment analysis is one example of a task that involves\\nclassifying textual data rather than numerical data. Because\\nmachine learning works with numbers, you must convert text to\\nnumbers before training a sentiment analysis model, a model\\nthat identifies spam emails, or any other model that\\nclassifies text. A common approach is to build a table of\\nword frequencies called a bag of words. Scikit-Learn provides\\nclasses to help. It also includes support for normalizing\\ntext so that, for example, “awesome” and “Awesome” don’t\\ncount as two different words.\\nThis chapter begins by describing how to prepare text for use\\nin classification models. After building a sentiment analysis\\nmodel, you’ll learn about another popular learning algorithm\\ncalled Naive Bayes that works particularly well with text and\\nuse it to build a model that distinguishes between legitimate\\nemails and spam emails. Finally, you’ll learn about a\\nmathematical technique for measuring the similarity of two'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 140, 'file_type': 'pdf'}, page_content='text samples and use it to build an app that recommends\\nmovies based on other movies you enjoy.\\nPreparing Text for Classification\\nBefore you train a model to classify text, you must convert\\nthe text into numbers, a process known as vectorization.\\nChapter\\xa01 presented the illustration reproduced in\\nFigure\\xa04-1, which demonstrates a common technique for\\nvectorizing text. Each row represents a text sample such as a\\ntweet or a movie review, and each column represents a word in\\nthe training text. The numbers in the rows are word counts,\\nand the final number in each row is a label: 0 for negative\\nand 1 for positive.\\nFigure 4-1. Dataset for sentiment analysis\\nText is typically cleaned before it’s vectorized. Examples\\nof cleaning include converting characters to lowercase (so,\\nfor example, “Excellent” is equivalent to “excellent”),\\nremoving punctuation symbols, and optionally removing stop\\nwords—common words such as the and and that are likely to\\nhave little impact on the outcome. Once cleaned, sentences\\nare divided into individual words (tokenized) and the words\\nare used to produce datasets like the one in Figure\\xa04-1.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 141, 'file_type': 'pdf'}, page_content='Scikit-Learn has three classes that handle the bulk of the\\nwork of cleaning and vectorizing text:\\nCountVectorizer\\nCreates a dictionary (vocabulary) from the corpus of\\nwords in the training text and generates a matrix of\\nword counts like the one in Figure\\xa04-1\\nHashingVectorizer\\nUses word hashes rather than an in-memory vocabulary\\nto produce word counts and is therefore more memory\\nefficient\\nTfidfVectorizer\\nCreates a dictionary from words provided to it and\\ngenerates a matrix similar to the one in Figure\\xa04-1,\\nbut rather than containing integer word counts, the\\nmatrix contains term frequency-inverse document\\nfrequency (TFIDF) values between 0.0 and 1.0\\nreflecting the relative importance of individual\\nwords\\nAll three classes are capable of converting text to\\nlowercase, removing punctuation symbols, removing stop words,\\nsplitting sentences into individual words, and more. They\\nalso support n-grams, which are combinations of two or more\\nconsecutive words (you specify the number n) that should be\\ntreated as a single word. The idea is that words such as\\ncredit and score might be more meaningful if they appear next\\nto each other in a sentence than if they appear far apart.\\nWithout n-grams, the relative proximity of words is ignored.\\nThe downside to using n-grams is that it increases memory\\nconsumption and training time. Used judiciously, however, it\\ncan make text classification models more accurate.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 142, 'file_type': 'pdf'}, page_content=\"NOTE\\nNeural networks have other, more powerful ways of taking word\\norder into account that don’t require related words to occur next\\nto each other. A conventional machine learning model can’t\\nconnect the words blue and sky in the sentence “I like blue, for\\nit is the color of the sky,” but a neural network can. I will\\nshed more light on this in Chapter\\xa013.\\nHere’s an example demonstrating what CountVectorizer does\\nand how it’s used:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nlines\\n\\xa0\\xa0\\xa0 'Four score and 7 years ago our fathers brought forth,',\\n\\xa0\\xa0\\xa0 '... a new NATION, conceived in liberty $$$,',\\n\\xa0\\xa0\\xa0 'and dedicated to the PrOpOsItIoN that all men are created equal',\\n\\xa0\\xa0\\xa0 'One nation\\\\'s freedom equals #freedom for another $nation!'\\n\\n# Vectorize the lines\\nvectorizer\\nCountVectorizer(stop_words='english')\\nword_matrix\\nvectorizer.fit_transform(lines)\\n\\n# Show the resulting word matrix\\nfeature_names\\nvectorizer.get_feature_names_out()\\nline_names\\n[f'Line {(i\\n1):d}'\\nfor\\ni,\\nin\\nenumerate(word_matrix)]\\n\\ndf\\npd.DataFrame(data=word_matrix.toarray(),\\nindex=line_names,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 columns=feature_names)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 143, 'file_type': 'pdf'}, page_content=\"df.head()\\nHere’s the output:\\nThe corpus of text in this case is four strings in a Python\\nlist. CountVectorizer broke the strings into words, removed\\nstop words and symbols, and converted all remaining words to\\nlowercase. Those words comprise the columns in the dataset,\\nand the numbers in the rows show how many times a given word\\nappears in each string. The stop_words='english' parameter\\ntells CountVectorizer to remove stop words using a built-in\\ndictionary of more than 300 English-language stop words. If\\nyou prefer, you can provide your own list of stop words in a\\nPython list. (Or you can leave the stop words in there; it\\noften doesn’t matter.) And if you’re training with text\\nwritten in another language, you can get lists of\\nmultilanguage stop words from other Python libraries such as\\nthe Natural Language Toolkit (NLTK) and Stop-words.\\nObserve from the output that equal and equals count as\\nseparate words, even though they have similar meaning. Data\\nscientists sometimes go a step further when preparing text\\nfor machine learning by stemming or lemmatizing words. If the\\npreceding text were stemmed, all occurrences of equals would\\nbe converted to equal. Scikit lacks support for stemming and\\nlemmatization, but you can get it from other libraries such\\nas NLTK.\\nCountVectorizer removes punctuation symbols, but it doesn’t\\nremove numbers. It ignored the 7 in line 1 because it ignores\\nsingle characters. But if you changed 7 to 777, the term 777\\nwould appear in the vocabulary. One way to fix that is to\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 144, 'file_type': 'pdf'}, page_content=\"define a function that removes numbers and pass it to\\nCountVectorizer via the preprocessor parameter:\\n\\nimport\\nre\\n\\ndef\\npreprocess_text(text):\\n\\xa0\\xa0\\xa0 return\\nre.sub(r'\\\\d+',\\ntext).lower()\\n\\nvectorizer\\nCountVectorizer(stop_words='english',\\npreprocessor=preprocess_text)\\nword_matrix\\nvectorizer.fit_transform(lines)\\nNote the call to lower to convert the text to lowercase.\\nCountVectorizer doesn’t convert text to lowercase if you\\nprovide a preprocessing function, so the preprocessing\\nfunction must convert it itself. It still removes punctuation\\ncharacters, however.\\nAnother useful parameter to CountVectorizer is min_df, which\\nignores words that appear fewer than the specified number of\\ntimes. It can be an integer specifying a minimum count (for\\nexample, ignore words that appear fewer than five times in\\nthe training text, or min_df=5), or it can be a floating-\\npoint value from 0.0 to 1.0 specifying the minimum percentage\\nof samples in which a word must appear—for example, ignore\\nwords that appear in less than 10% of the samples\\n(min_df=0.1). It’s great for filtering out words that\\nprobably aren’t meaningful anyway, and it reduces memory\\nconsumption and training time by decreasing the size of the\\nvocabulary. Count\\u200bVec\\u2060tor\\u2060izer also supports a max_df parameter\\nfor eliminating words that appear too frequently.\\nThe preceding examples use CountVectorizer, which probably\\nleaves you wondering when (and why) you would use\\nHashingVectorizer or TfidfVectorizer instead. HashingVectorizer\\nis useful when dealing with large datasets. Rather than store\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 145, 'file_type': 'pdf'}, page_content='words in memory, it hashes each word and uses the hash as an\\nindex into an array of word counts. It can therefore do more\\nwith less memory and is very useful for reducing the size of\\nvectorizers when serializing them so that you can restore\\nthem later—a topic I’ll say more about in Chapter\\xa07. The\\ndownside to HashingVectorizer is that it doesn’t let you\\nwork backward from vectorized text to the original text.\\nCount\\u200bVec\\u2060tor\\u2060izer does, and it provides an inverse_transform\\nmethod for that purpose.\\nTfidfVectorizer is frequently used to perform keyword\\nextraction: examining a document or set of documents and\\nextracting keywords that characterize their content. It\\nassigns words numerical weights reflecting their importance,\\nand it uses two factors to determine the weights: how often a\\nword appears in individual documents, and how often it\\nappears in the overall document set. Words that appear more\\nfrequently in individual documents but occur in fewer\\ndocuments receive higher weights. I won’t go further into it\\nhere, but if you’re curious to learn more, this book’s\\nGitHub repo contains a notebook that uses Tfidf\\u200bVec\\u2060tor\\u2060izer to\\nextract keywords from the manuscript of Chapter\\xa01.\\nSentiment Analysis\\nTo train a sentiment analysis model, you need a labeled\\ndataset. Several such datasets are available in the public\\ndomain. One of those is the IMDB movie review dataset, which\\ncontains 25,000 samples of negative reviews and 25,000\\nsamples of positive reviews posted on the Internet Movie\\nDatabase website. Each review is meticulously labeled with a\\n0 for negative sentiment or a 1 for positive sentiment. To\\ndemonstrate how sentiment analysis works, let’s build a\\nbinary classification model and train it with this dataset.\\nWe’ll use logistic regression as the learning algorithm. A\\nsentiment analysis score yielded by this model is simply the\\nprobability that the input expresses positive sentiment,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 146, 'file_type': 'pdf'}, page_content=\"which is easily retrieved by calling LogisticRegres\\u2060sion’s\\npredict_proba method.\\nStart by downloading the dataset and copying it to the Data\\nsubdirectory of the directory that hosts your Jupyter\\nnotebooks. Then run the following code in a notebook to load\\nthe dataset and show the first five rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/reviews.csv',\\nencoding='ISO-8859-1')\\ndf.head()\\nThe encoding attribute is necessary because the CSV file uses\\nISO-8859-1 character encoding rather than UTF-8. The output\\nis as follows:\\nFind out how many rows the dataset contains and confirm that\\nthere are no missing values:\\n\\ndf.info()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 147, 'file_type': 'pdf'}, page_content=\"Use the following statement to see how many instances there\\nare of each class (0 for negative and 1 for positive):\\n\\ndf.groupby('Sentiment').describe()\\nHere is the output:\\nThere is an even number of positive and negative samples, but\\nin each case, the number of unique samples is less than the\\nnumber of samples for that class. That means the dataset has\\nduplicate rows, and duplicate rows could bias a machine\\nlearning model. Use the following statements to delete the\\nduplicate rows and check for balance again:\\n\\ndf\\ndf.drop_duplicates()\\ndf.groupby('Sentiment').describe()\\nNow there are no duplicate rows, and the number of positive\\nand negative samples is roughly equal.\\nNext, use CountVectorizer to prepare and vectorize the text\\nin the Text column. Set min_df to 20 to ignore words that\\nappear infrequently in the training text. This reduces the\\nlikelihood of out-of-memory errors and will probably make the\\nmodel more accurate as well. Also use the ngram_range\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 148, 'file_type': 'pdf'}, page_content=\"parameter to allow Count\\u200bVec\\u2060tor\\u2060izer to include word pairs as\\nwell as individual words:\\n\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nvectorizer\\nCountVectorizer(ngram_range=(1,\\nstop_words='english',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 min_df=20)\\n\\nx\\nvectorizer.fit_transform(df['Text'])\\ny\\ndf['Sentiment']\\nNow split the dataset for training and testing. We’ll use a\\n50/50 split since there are almost 50,000 samples in total:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\ntest_size=0.5,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nThe next step is to train a classifier. We’ll use Scikit’s\\nLogisticRegression class, which uses logistic regression to\\nfit a model to the data:\\n\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\n\\nmodel\\nLogisticRegression(max_iter=1000,\\nrandom_state=0)\\nmodel.fit(x_train,\\ny_train)\\nValidate the trained model with the 50% of the dataset set\\naside for testing and show the results in a confusion matrix:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 149, 'file_type': 'pdf'}, page_content=\"%matplotlib\\ninline\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=['Negative',\\n'Positive'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nThe confusion matrix reveals that the model correctly\\nidentified 10,795 negative reviews while misclassifying 1,574\\nof them. It correctly identified 10,966 positive reviews and\\ngot it wrong 1,456 times:\\nNow comes the fun part: analyzing text for sentiment. Use the\\nfollowing statements to produce a sentiment score for the\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 150, 'file_type': 'pdf'}, page_content=\"sentence “The long lines and poor customer service really\\nturned me off”:\\n\\ntext\\n'The long lines and poor customer service really turned me off'\\nmodel.predict_proba(vectorizer.transform([text]))[0][1]\\nHere’s the output:\\n\\n0.09183447847778639\\nNow do the same for “The food was great and the service was\\nexcellent!”:\\n\\ntext\\n'The food was great and the service was excellent!'\\nmodel.predict_proba(vectorizer.transform([text]))[0][1]\\nIf you expected a higher score for this one, you won’t be\\ndisappointed:\\n\\n0.8536277207125618\\nFeel free to try sentences of your own and see if you agree\\nwith the sentiment scores the model predicts. It’s not\\nperfect, but it’s good enough that if you run hundreds of\\nreviews or comments through it, you should get a reliable\\nindication of the sentiment expressed in the text.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 151, 'file_type': 'pdf'}, page_content=\"NOTE\\nSometimes CountVectorizer’s built-in list of stop words lowers the\\naccuracy of a model because the list is so broad. As an\\nexperiment, remove stop_words='english' from CountVectorizer and run\\nthe code again. Check the confusion matrix. Does the accuracy\\nincrease or decrease? Feel free to vary other parameters such as\\nmin_df and ngram_range too. In the real world, data scientists often\\ntry many different parameter combinations to determine which one\\nproduces the best results.\\nNaive Bayes\\nLogistic regression is a go-to algorithm for classification\\nmodels and is often very effective at classifying text. But\\nin scenarios involving text classification, data scientists\\noften turn to another learning algorithm called Naive Bayes.\\nIt’s a classification algorithm based on Bayes’ theorem,\\nwhich provides a means for calculating conditional\\nprobabilities. Mathematically, Bayes’ theorem is stated this\\nway:\\nThis says the probability that A is true given that B is true\\nis equal to the probability that B is true given that A is\\ntrue multiplied by the probability that A is true divided by\\nthe probability that B is true. That’s a mouthful, and while\\naccurate, it doesn’t explain why Naive Bayes is so useful\\nfor classifying text—or how you apply it, for example, to a\\ncollection of emails to determine which ones are spam.\\nLet’s start with a simple example. Suppose 10% of all the\\nemails you receive are spam. That’s P(A). Analysis reveals\\nthat 5% of the spam emails you receive contain the word\\ncongratulations, but just 1% of all your emails contain the\\nsame word. Therefore, P(B|A) is 0.05 and P(B) is 0.01. The\\nprobability of an email being spam if it contains the word\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 152, 'file_type': 'pdf'}, page_content='congratulations is P(A|B), which is (0.05 x 0.10) / 0.01, or\\n0.50.\\nOf course, a spam filter must consider all the words in an\\nemail, not just one. It turns out that if you make some\\nsimple (naive) assumptions—that the order of the words in an\\nemail doesn’t matter, and that every word has equal weight—\\nyou can write Bayes’ equation this way for a spam\\nclassifier:\\nIn plain English, the probability that a message is spam is\\nproportional to the product\\xa0of:\\n\\nThe probability that any message in the dataset\\nis spam, or P(S)\\nThe probability that each word in the message\\nappears in a spam message, or P(word|S)\\nP(S) can be calculated easily enough: it’s simply the\\nfraction of the messages in the dataset that are spam\\nmessages. If you train a machine learning model with 1,000\\nmessages and 500 of them are spam, then P(S) = 0.5. For a\\ngiven word, P(word|S) is simply the number of times the word\\nappears in spam messages divided by the number of words in\\nall the spam messages. The entire problem reduces to word\\ncounts. You can do a similar calculation to compute the\\nprobability that the message is not spam, and then use the\\nhigher of the two probabilities to make a prediction.\\nHere’s an example involving four sample emails. The emails\\nare:\\n\\n\\xa0Text\\xa0\\n\\xa0Spam\\xa0\\n\\xa0Raise your credit score in minutes\\xa0\\n\\xa0Here are the minutes from yesterday’s meeting\\xa0 \\xa00\\xa0\\n\\xa0Meeting tomorrow to review yesterday’s scores\\xa0 \\xa00\\xa0\\n\\xa0Score tomorrow’s meds at yesterday’s prices'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 153, 'file_type': 'pdf'}, page_content='If you remove stop words, convert characters to lowercase,\\nand stem the words such that tomorrow’s becomes tomorrow,\\nyou’re left with this:\\n\\n\\xa0Text\\xa0\\n\\xa0Spam\\xa0\\n\\xa0raise credit score minute\\xa0\\n\\xa0minute yesterday meeting\\xa0\\n\\xa0meeting tomorrow review yesterday score\\xa0 \\xa00\\xa0\\n\\xa0score tomorrow med yesterday price\\xa0\\nBecause two of the four messages are spam and two are not,\\nthe probability that any message is spam (P(S)) is 0.5. The\\nsame goes for the probability that any message is not spam\\n(P(N) = 0.5). In addition, the spam messages contain nine\\nunique words, while the nonspam messages contain a total of\\neight.\\nThe next step is to build the following table of word\\nfrequencies. Take the word yesterday as an example. It\\nappears once in a message that’s labeled as spam, so\\nP(yesterday|S) is 1/9, or 0.111. It appears twice in nonspam\\nmessages, so P(yesterday|N) is 2/8, or 0.250:\\n\\n\\xa0Word\\xa0\\n\\xa0P(word|S\\xa0) \\xa0P(word|N\\xa0)\\n\\xa0raise\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000\\xa0\\n\\xa0credit\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000\\xa0\\n\\xa0score\\xa0\\n\\xa02/9 = 0.222\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0minute\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0yesterday\\xa0 \\xa01/9 = 0.111\\xa0 \\xa02/8 = 0.250\\xa0\\n\\xa0meeting\\xa0\\n\\xa00/9 = 0.000\\xa0 \\xa02/8 = 0.250\\xa0\\n\\xa0tomorrow\\xa0 \\xa01/9 = 0.111\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0review\\xa0\\n\\xa00/9 = 0.000\\xa0 \\xa01/8 = 0.125\\xa0\\n\\xa0med\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000\\xa0\\n\\xa0price\\xa0\\n\\xa01/9 = 0.111\\xa0 \\xa00/8 = 0.000'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 154, 'file_type': 'pdf'}, page_content='This works up to a point, but the zeros in the table are a\\nproblem. Let’s say you want to determine whether “Scores\\nmust be reviewed by tomorrow” is spam. Removing stop words\\nleaves you with “score review tomorrow.” You can compute\\nthe probability that the message is spam this way:\\n\\nThe result is 0 because review doesn’t appear in a spam\\nmessage, and 0 times anything is 0. The algorithm simply\\ncan’t assign a spam probability to “Scores must be reviewed\\nby tomorrow.”\\nA common way to resolve this is to apply Laplace smoothing,\\nalso known as additive smoothing. Typically, this involves\\nadding 1 to each numerator and the number of unique words in\\nthe dataset (in this case, 10) to each denominator. Now,\\nP(review|S) evaluates to (0 + 1) / (9 + 10), which equals\\n0.053. It’s not much, but it’s better than nothing\\n(literally). Here are the word frequencies again, this time\\nrevised with Lap\\u2060lace smoothing:\\n\\n\\xa0Word\\xa0\\n\\xa0P(word|S\\xa0)\\n\\xa0P(word|N\\xa0)\\n\\xa0raise\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056\\xa0\\n\\xa0credit\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056\\xa0\\n\\xa0score\\xa0\\n\\xa0(2 + 1) / (9 + 10) = 0.158\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0minute\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0yesterday\\xa0 \\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(2 + 1) / (8 + 10) = 0.167\\xa0\\n\\xa0meeting\\xa0\\n\\xa0(0 + 1) / (9 + 10) = 0.053\\xa0 \\xa0(2 + 1) / (8 + 10) = 0.167\\xa0\\n\\xa0tomorrow\\xa0 \\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0review\\xa0\\n\\xa0(0 + 1) / (9 + 10) = 0.053\\xa0 \\xa0(1 + 1) / (8 + 10) = 0.111\\xa0\\n\\xa0med\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056\\xa0\\n\\xa0price\\xa0\\n\\xa0(1 + 1) / (9 + 10) = 0.105\\xa0 \\xa0(0 + 1) / (8 + 10) = 0.056'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 155, 'file_type': 'pdf'}, page_content='Now you can determine whether “Scores must be reviewed by\\ntomorrow” is spam by performing two simple calculations:\\n\\nBy this measure, “Scores must be reviewed by tomorrow” is\\nlikely not to be spam. The probabilities are relative, but\\nyou could normalize them and conclude there’s about a 40%\\nchance the message is spam and a 60% chance it’s not based\\non the emails the model was trained with.\\nFortunately, you don’t have to do these computations by\\nhand. Scikit-Learn provides several classes to help out,\\nincluding the MultinomialNB class, which works great with\\ntables of word counts produced by CountVectorizer.\\nSpam Filtering\\nIt’s no coincidence that modern spam filters are remarkably\\nadept at identifying spam. Virtually all of them rely on\\nmachine learning. Such models are difficult to implement\\nalgorithmically because an algorithm that uses keywords such\\nas credit and score to determine whether an email is spam is\\neasily fooled. Machine learning, by contrast, looks at a body\\nof emails and uses what it learns to classify the next email.\\nSuch models often achieve more than 99% accuracy. And they\\nget smarter over time as they’re trained with more and more\\nemails.\\nThe previous example used logistic regression to predict\\nwhether text input to it expresses positive or negative\\nsentiment. It used the probability that the text expresses\\npositive sentiment as a sentiment score, and you saw that\\nexpressions such as “The long lines and poor customer\\nservice really turned me off” score close to 0.0, while\\nexpressions such as “The food was great and the service was\\nexcellent” score close to 1.0. Now let’s build a binary\\nclassification model that classifies emails as spam or not'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 156, 'file_type': 'pdf'}, page_content=\"spam and use Naive Bayes to fit the model to the training\\ndata.\\nThere are several spam classification datasets available in\\nthe public domain. Each contains a collection of emails with\\nsamples labeled with 1s for spam and 0s for not spam. We’ll\\nuse a relatively small dataset containing 1,000 samples.\\nBegin by downloading the dataset and copying it into your\\nnotebooks’ Data subdirectory. Then load the data and display\\nthe first five rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/ham-spam.csv')\\ndf.head()\\nNow check for duplicate rows in the dataset:\\n\\ndf.groupby('IsSpam').describe()\\nThe dataset contains one duplicate row. Let’s remove it and\\ncheck for balance:\\n\\ndf\\ndf.drop_duplicates()\\ndf.groupby('IsSpam').describe()\\nThe dataset now contains 499 samples that are not spam, and\\n500 that are. The next step is to use CountVectorizer to\\nvectorize the emails. Once more, we’ll allow CountVectorizer\\nto consider word pairs as well as individual words and remove\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 157, 'file_type': 'pdf'}, page_content=\"stop words using Scikit’s built-in dictionary of English\\nstop words:\\n\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nvectorizer\\nCountVectorizer(ngram_range=(1,\\nstop_words='english')\\nx\\nvectorizer.fit_transform(df['Text'])\\ny\\ndf['IsSpam']\\nSplit the dataset so that 80% can be used for training and\\n20% for testing:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\ntest_size=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nThe next step is to train a Naive Bayes classifier using\\nScikit’s MultinomialNB class:\\nfrom\\nsklearn.naive_bayes\\nimport\\nMultinomialNB\\n\\nmodel\\nMultinomialNB()\\nmodel.fit(x_train,\\ny_train)\\nValidate the trained model with the 20% of the dataset set\\naside for testing using a confusion matrix:\\n\\n%matplotlib\\ninline\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 158, 'file_type': 'pdf'}, page_content=\"from\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\ncmd.from_estimator(model,\\nx_test,\\ny_test,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=['Not Spam',\\n'Spam'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nThe model correctly identified 101 of 102 legitimate emails\\nas not spam, and 95 of 98 spam emails as spam:\\nUse the score method to get a rough measure of the model’s\\naccuracy:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 159, 'file_type': 'pdf'}, page_content='model.score(x_test,\\ny_test)\\nNow use Scikit’s RocCurveDisplay class to visualize the ROC\\ncurve:\\n\\nfrom\\nsklearn.metrics\\nimport\\nRocCurveDisplay\\nas\\nrcd\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nrcd.from_estimator(model,\\nx_test,\\ny_test)\\nThe results are encouraging. Trained with just 999 samples,\\nthe area under the ROC curve (AUC) indicates the model is\\nmore than 99.9% accurate at classifying emails as spam or not\\nspam:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 160, 'file_type': 'pdf'}, page_content=\"Let’s see how the model classifies a few emails that it\\nhasn’t seen before, starting with one that isn’t spam. The\\nmodel’s predict method predicts a class—0 for not spam, or\\n1 for spam:\\n\\nmsg\\n'Can you attend a code review on Tuesday to make sure the logic is\\nsolid?'\\ninput\\nvectorizer.transform([msg])\\nmodel.predict(input)[0]\\nThe model says this message is not spam, but what’s the\\nprobability that it’s not spam? You can get that from\\npredict_proba, which returns an array containing two values:\\nthe probability that the predicted class is 0, and the\\nprobability that the predicted class is 1, in that order:\\n\\nmodel.predict_proba(input)[0][0]\\nThe model seems very sure that this email is legitimate:\\n\\n0.9999497111473539\\nNow test the model with a spam message:\\n\\nmsg\\n'Why pay more for expensive meds when you can order them online ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'and save $$$?'\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 161, 'file_type': 'pdf'}, page_content='input\\nvectorizer.transform([msg])\\nmodel.predict(input)[0]\\nWhat is the probability that the message is not spam?\\n\\nmodel.predict_proba(input)[0][0]\\nThe answer is:\\n\\n0.00021423891260677753\\nWhat is the probability that the message is spam?\\n\\nmodel.predict_proba(input)[0][1]\\nAnd the answer is:\\n\\n0.9997857610873945\\nObserve that predict and predict_proba accept a list of\\ninputs. Based on that, could you classify an entire batch of\\nemails with one call to either method? How would you get the\\nresults for each email?\\nRecommender Systems'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 162, 'file_type': 'pdf'}, page_content='Another branch of machine learning that has proven its mettle\\nin recent years is recommender systems—systems that\\nrecommend products or services to customers. Amazon’s\\nrecommender system reportedly drives 35% of its sales. The\\ngood news is that you don’t have to be Amazon to benefit\\nfrom a recommender system, nor do you have to have Amazon’s\\nresources to build one. They’re relatively simple to create\\nonce you learn a few basic principles.\\nRecommender systems come in many forms. Popularity-based\\nsystems present options to customers based on what products\\nand services are popular at the time—for example, “Here are\\nthis week’s bestsellers.” Collaborative systems make\\nrecommendations based on what others have selected, as in\\n“People who bought this book also bought these books.”\\nNeither of these systems requires machine learning.\\nContent-based systems, by contrast, benefit greatly from\\nmachine learning. An example of a content-based system is one\\nthat says “if you bought this book, you might like these\\nbooks also.” These systems require a means for quantifying\\nsimilarity between items. If you liked the movie Die Hard,\\nyou might or might not like Monty Python and the Holy Grail.\\nIf you liked Toy Story, you’ll probably like A Bug’s Life\\ntoo. But how do you make that determination algorithmically?\\nContent-based recommenders require two ingredients: a way to\\nvectorize—convert to numbers—the attributes that\\ncharacterize a service or product, and a means for\\ncalculating similarity between the resulting vectors. The\\nfirst one is easy. Count\\u200bVec\\u2060tor\\u2060izer converts text into tables\\nof word counts. All you need is a way to measure similarity\\nbetween rows of word counts and you can build a recommender\\nsystem. And one of the simplest and most effective ways to do\\nthat is a technique called cosine similarity.\\nCosine Similarity'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 163, 'file_type': 'pdf'}, page_content='Cosine similarity is a mathematical means for computing the\\nsimilarity between pairs of vectors (or rows of numbers\\ntreated as vectors). The basic idea is to take each value in\\na sample—for example, word counts in a row of vectorized\\ntext—and use them as endpoint coordinates for a vector, with\\nthe other endpoint at the origin of the coordinate system. Do\\nthat for two samples, and then compute the cosine between\\nvectors in m-dimensional space, where m is the number of\\nvalues in each sample. Because the cosine of 0 is 1, two\\nidentical vectors have a similarity of 1. The more dissimilar\\nthe vectors, the closer the cosine will be to 0.\\nHere’s an example in two-dimensional space to illustrate.\\nSuppose you have three rows containing two values each:\\n\\n\\xa01\\xa0 \\xa02\\xa0\\n\\xa02\\xa0 \\xa03\\xa0\\n\\xa03\\xa0 \\xa01\\xa0\\nYou want to determine whether row 2 is more similar to row 1\\nor row 3. It’s hard to tell just by looking at the numbers,\\nand in real life, there are many more numbers. If you simply\\nadded the numbers in each row and compared the sums, you\\nwould conclude that row 2 is more similar to row 3. But what\\nif you treated each row as a vector, as shown in Figure\\xa04-2?\\n\\nRow 1: (0, 0) → (1, 2)\\nRow 2: (0, 0) → (2, 3)\\nRow 3: (0, 0) → (3, 1)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 164, 'file_type': 'pdf'}, page_content='Figure 4-2. Cosine similarity\\nNow you can plot each row as a vector, compute the cosines of\\nthe angles formed by 1 and 2 and 2 and 3, and determine that\\nrow 2 is more like row 1 than row 3. That’s cosine\\nsimilarity in a nutshell.\\nCosine similarity isn’t limited to two dimensions; it works\\nin higher-dimensional space as well. To help compute cosine\\nsimilarities regardless of the number of dimensions, Scikit\\noffers the cosine_similarity function. The following code\\ncomputes the cosine similarities of the three samples in the\\npreceding example:\\n\\ndata\\ncosine_similarity(data)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 165, 'file_type': 'pdf'}, page_content=\"The return value is a similarity matrix containing the\\ncosines of every vector pair. The width and height of the\\nmatrix equals the number of samples:\\n\\narray([[1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ,\\n0.99227788,\\n0.70710678],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [0.99227788,\\n1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ,\\n0.78935222],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [0.70710678,\\n0.78935222,\\n1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ]])\\nFrom this, you can see that the similarity of rows 1 and 2 is\\n0.992, while the similarity of rows 2 and 3 is 0.789. In\\nother words, row 2 is more similar to row 1 than it is to row\\n3. There is also more similarity between rows 2 and 3 (0.789)\\nthan there is between rows 1 and 3 (0.707).\\nBuilding a Movie Recommendation System\\nLet’s put cosine similarity to work building a content-based\\nrecommender system for movies. Start by downloading the\\ndataset, which is one of several movie datasets available\\nfrom Kaggle.com. This one has information for about 4,800\\nmovies, including title, budget, genres, keywords, cast, and\\nmore. Place the CSV file in your Jupyter notebooks’ Data\\nsubdirectory. Then load the dataset and peruse its contents:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/movies.csv')\\ndf.head()\\nThe dataset contains 24 columns, only a few of which are\\nneeded to describe a movie. Use the following statements to\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 166, 'file_type': 'pdf'}, page_content=\"extract key columns such as title and genres and fill missing\\nvalues with empty strings:\\ndf\\ndf[['title',\\n'genres',\\n'keywords',\\n'cast',\\n'director']]\\ndf\\ndf.fillna('')\\n# Fill missing values with empty strings\\ndf.head()\\nNext, add a column named features that combines all the words\\nin the other columns:\\n\\ndf['features']\\ndf['title']\\ndf['genres']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 df['keywords']\\ndf['cast']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 df['director']\\nUse CountVectorizer to vectorize the text in the features\\ncolumn:\\n\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\n\\nvectorizer\\nCountVectorizer(stop_words='english',\\nmin_df=20)\\nword_matrix\\nvectorizer.fit_transform(df['features'])\\nword_matrix.shape\\nThe table of word counts contains 4,803 rows—one for each\\nmovie—and 918 columns. The next task is to compute cosine\\nsimilarities for each row pair:\\n\\nfrom\\nsklearn.metrics.pairwise\\nimport\\ncosine_similarity\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 167, 'file_type': 'pdf'}, page_content=\"sim\\ncosine_similarity(word_matrix)\\nUltimately, the goal of this system is to input a movie title\\nand identify the n movies that are most similar to that\\nmovie. To that end, define a function named get\\u200b\\n_recom\\u2060mendations that accepts a movie title, a DataFrame\\ncontaining information about all the movies, a similarity\\nmatrix, and the number of movie titles to return:\\n\\ndef\\nget_recommendations(title,\\ndf,\\nsim,\\ncount=10):\\n\\xa0\\xa0\\xa0 # Get the row index of the specified title in the DataFrame\\n\\xa0\\xa0\\xa0 index\\ndf.index[df['title'].str.lower()\\ntitle.lower()]\\n\\n\\xa0\\xa0\\xa0 # Return an empty list if there is no entry for the specified title\\n\\xa0\\xa0\\xa0 if\\n(len(index)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n\\n\\xa0\\xa0\\xa0 # Get the corresponding row in the similarity matrix\\n\\xa0\\xa0\\xa0 similarities\\nlist(enumerate(sim[index[0]]))\\n\\n\\xa0\\xa0\\xa0 # Sort the similarity scores in that row in descending order\\n\\xa0\\xa0\\xa0 recommendations\\nsorted(similarities,\\nkey=lambda\\nx:\\nx[1],\\nreverse=True)\\n\\n\\xa0\\xa0\\xa0 # Get the top n recommendations, ignoring the first entry in the list since\\n\\xa0\\xa0\\xa0 # it corresponds to the title itself (and thus has a similarity of 1.0)\\n\\xa0\\xa0\\xa0 top_recs\\nrecommendations[1:count\\n\\n\\xa0\\xa0\\xa0 # Generate a list of titles from the indexes in top_recs\\n\\xa0\\xa0\\xa0 titles\\n\\n\\xa0\\xa0\\xa0 for\\ni\\nin\\nrange(len(top_recs)):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 title\\ndf.iloc[top_recs[i][0]]['title']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 titles.append(title)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 168, 'file_type': 'pdf'}, page_content=\"return\\ntitles\\nThis function sorts the cosine similarities in descending\\norder to identify the count movies most like the one\\nidentified by the title parameter. Then it returns the titles\\nof those movies.\\nNow use get_recommendations to search the database for\\nsimilar movies. First ask for the 10 movies that are most\\nsimilar to the James Bond thriller Skyfall:\\n\\nget_recommendations('Skyfall',\\ndf,\\nsim)\\nHere is the output:\\n\\n['Spectre',\\n'Quantum of Solace',\\n'Johnny English Reborn',\\n'Clash of the Titans',\\n'Die Another Day',\\n'Diamonds Are Forever',\\n'Wrath of the Titans',\\n'I Spy',\\n'Sanctum',\\n'Blackthorn']\\nCall get_recommendations again to list movies that are like\\nMulan:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 169, 'file_type': 'pdf'}, page_content=\"get_recommendations('Mulan',\\ndf,\\nsim)\\nFeel free to try other movies as well. Note that you can only\\ninput movie titles that are in the dataset. Use the following\\nstatements to print a complete list of titles:\\n\\npd.set_option('display.max_rows',\\nNone)\\nprint(df['title'])\\nI think you’ll agree that the system does a pretty credible\\njob of picking similar movies. Not bad for about 20 lines of\\ncode!\\nSummary\\nMachine learning models that classify text are common and see\\na variety of uses in industry and in everyday life. What\\nrational human being doesn’t wish for a magic wand that\\neradicates all spam mails, for example?\\nText used to train a text classification model must be\\nprepared and vectorized prior to training. Preparation\\nincludes converting characters to lowercase and removing\\npunctuation characters, and may include removing stop words,\\nremoving numbers, and stemming or lemmatizing. Once prepared,\\ntext is vectorized by converting it into a table of word\\nfrequencies. Scikit’s CountVectorizer class makes short work\\nof the vectorization process and handles some of the\\npreparation duties too.\\nLogistic regression and other popular classification\\nalgorithms can be used to classify text once it’s converted\\nto numerical form. For text classification tasks, however,\\nthe Naive Bayes learning algorithm frequently outperforms\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 170, 'file_type': 'pdf'}, page_content='other algorithms. By making a few “naive” assumptions such\\nas that the order in which words appear in a text sample\\ndoesn’t matter, Naive Bayes reduces to a process of word\\ncounting. Scikit’s MultinomialNB class provides a handy\\nNaive Bayes implementation.\\nCosine similarity is a mathematical means for computing the\\nsimilarity between two rows of numbers. One use for it is\\nbuilding systems that recommend products or services based on\\nother products or services that a customer has purchased.\\nWord frequency tables produced from textual descriptions by\\nCountVectorizer can be combined with cosine similarity to\\ncreate intelligent recommender systems intended to supplement\\na company’s bottom line.\\nFeel free to use this chapter’s examples as a starting point\\nfor experiments of your own. For instance, see if you can\\ntweak the parameters passed to CountVectorizer in any of the\\nexamples and increase the accuracy of the resulting model.\\nData scientists call the search for the optimum parameter\\ncombination hyperparameter tuning, and it’s a subject\\nyou’ll learn about in the next chapter.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 171, 'file_type': 'pdf'}, page_content='Chapter 5. Support Vector\\nMachines\\nSupport vector machines (SVMs) represent the cutting edge of\\nmachine learning. They are most often used to solve\\nclassification problems, but they can also be used for\\nregression. Due to the unique way in which they fit\\nmathematical models to data, SVMs often succeed at finding\\nseparation between classes when other models do not. They\\ntechnically perform binary classification only, but Scikit-\\nLearn enables them to do multiclass classification as well\\nusing techniques discussed in Chapter\\xa03.\\nScikit-Learn makes building SVMs easy with classes such as\\nSVC (short for support vector classifier) for classification\\nmodels and SVR (support vector regressor) for regression\\nmodels. You can use these classes without understanding how\\nSVMs work, but you’ll get more out of them if you do\\nunderstand how they work. It’s also important to know how to\\ntune SVMs for individual datasets and how to prepare data\\nbefore you train a model. Toward the end of this chapter,\\nwe’ll build an SVM that performs facial recognition. But\\nfirst, let’s look behind the scenes and discover why SVMs\\nare often the go-to mechanism for modeling real-world\\ndatasets.\\nHow Support Vector Machines Work\\nFirst, why are they called support vector machines? The\\npurpose of an SVM classifier is the same as any other\\nclassifier: to find a decision boundary that cleanly\\nseparates the classes. SVMs do this by finding a line in 2D\\nspace, a plane in 3D space, or a hyperplane in higher-'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 172, 'file_type': 'pdf'}, page_content='dimensional space that allows them to distinguish between\\ndifferent classes with the greatest certainty possible. In\\nthe example in Figure\\xa05-1, there are an infinite number of\\nlines you can draw to separate the two classes, but the best\\nline is the one that produces the widest margin (the one\\nshown on the right). The width of the margin is the distance\\nbetween the points closest to the boundary in each class\\nalong a line perpendicular to the boundary. These points are\\ncalled support vectors and are circled in red.\\nFigure 5-1. Maximum-margin classification\\nOf course, real data rarely lends itself to such clean\\nseparation. Overlap between classes inevitably prevents a\\nperfect fit. To accommodate this, SVMs support a\\nregularization parameter usually referred to as C that can be\\nadjusted to loosen or tighten the fit. Lower values of C\\nproduce a wider margin with more errors on either side of the\\ndecision boundary, as shown in Figure\\xa05-2. Higher values\\nyield a tighter fit to the training data with a\\ncorrespondingly thinner margin and fewer errors. If C is too\\nhigh, the model might not generalize well. The optimum value\\nvaries by dataset. Data scientists typically try different\\nvalues of C to determine which one performs the best against\\ntest data.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 173, 'file_type': 'pdf'}, page_content='All of the aforementioned is true, but none of it explains\\nwhy SVMs are so good at what they do. SVMs aren’t the only\\nmodels that mathematically look for boundaries separating the\\nclasses. What makes SVMs special are kernels, some of which\\nadd dimensions to data to find boundaries that don’t exist\\nat lower dimensions. Consider Figure\\xa05-3. You can’t draw a\\nline that completely separates the red dots from the purple\\ndots. But if you add a third dimension as shown on the right\\n—a z dimension whose value is based on a point’s distance\\nfrom the center—then you can slide a plane between the\\npurples and the reds and achieve 100% separation. In this\\nexample, data that isn’t linearly separable in two\\ndimensions is linearly separable in three dimensions. The\\nprinciple at work is Cover’s theorem, which states that data\\nthat isn’t linearly separable might be linearly separable if\\nprojected into higher-dimensional space using a nonlinear\\ntransform.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 174, 'file_type': 'pdf'}, page_content='Figure 5-2. Effect of C on margin width\\nFigure 5-3. Adding dimensions to achieve linear separability'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 175, 'file_type': 'pdf'}, page_content='The kernel transformation used in this example, which\\nprojects two-dimensional data to three dimensions by adding a\\nz to every x and y, works well with this particular dataset.\\nBut for SVMs to be broadly useful, you need a kernel that\\nisn’t tied to the shape of a specific dataset.\\nKernels\\nScikit-Learn has several general-purpose kernels built in,\\nincluding the linear kernel, the RBF kernel,1 the polynomial\\nkernel, and the sigmoid kernel. The linear kernel doesn’t\\nadd dimensions. It works well with data that is linearly\\nseparable out of the box, but it doesn’t perform very well\\nwith data that isn’t. Applying it to the problem in\\nFigure\\xa05-3 produces the decision boundary on the left in\\nFigure\\xa05-4. Applying the RBF kernel to the same data\\nproduces the decision boundary on the right. The RBF kernel\\nprojects the x and y values into a higher-dimensional space\\nand finds a hyperplane that cleanly separates the purples\\nfrom the reds. When projected back to two dimensions, the\\ndecision boundary roughly forms a circle. Similar results can\\nbe achieved on this dataset with a properly tuned polynomial\\nkernel, but generally speaking, the RBF kernel can find\\ndecision boundaries in nonlinear data that the pol\\u2060ynomial\\nkernel cannot. That’s why RBF is the default kernel type in\\nScikit if you don’t specify otherwise.\\nA logical question to ask is, did the RBF kernel add a z to\\nevery x and y? The short answer is no. It effectively\\nprojected the data points into a space with an infinite\\nnumber of dimensions. The key word is effectively. Kernels\\nuse mathematical shortcuts called kernel tricks to measure\\nthe effect of adding new dimensions without actually\\ncomputing values for them. This is where the math for SVMs\\ngets hairy. Kernels are carefully designed to compute the dot\\nproduct between two n-dimensional vectors in m-dimensional\\nspace (where m is greater than n and can even be infinite)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 176, 'file_type': 'pdf'}, page_content='without generating all those new dimensions, and ultimately,\\nthe dot products are all an SVM needs to compute a decision\\nboundary. It’s the mathematical equivalent of having your\\ncake and eating it too, and it’s the secret sauce that makes\\nSVMs awesome. SVMs can take a long time to train on large\\ndatasets, but one of the benefits of an SVM is that it tends\\nto do better on smaller datasets with fewer rows or samples\\nthan other learning algorithms.\\nFigure 5-4. Linear kernel versus RBF kernel\\nKernel Tricks\\nWant to see an example of how kernel tricks are used to\\ncompute dot products in high-dimensional spaces without\\ncomputing values for the new dimensions? The following\\nexplanation is completely optional. But if you, like me,\\nlearn better from concrete examples, then you might find this\\nsection helpful.\\nLet’s start with the two-dimensional circular dataset\\npresented earlier, but this time let’s project it into\\nthree-dimensional space with the following equations:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 177, 'file_type': 'pdf'}, page_content='In other words, we’ll compute x and y in three-dimensional\\nspace (x′ and y′\\u200a) by squaring x and y in two-dimensional\\nspace, and we’ll add a z that’s the product of the original\\nx and y and the square root of 2. Projecting the data this\\nway produces a clean separation between purples and reds, as\\nshown in Figure\\xa05-5.\\nFigure 5-5. Projecting 2D points to 3D to separate two classes\\nThe efficacy of SVMs depends on their ability to compute the\\ndot product of two vectors (or points, which can be treated\\nas vectors) in higher-dimensional space without projecting\\nthem into that space—that is, using only the values in the\\noriginal space. Let’s manufacture a couple of points to work\\nwith:\\n\\nWe can compute the dot product of these two points this way:\\nOf course, the dot product in two dimensions isn’t very\\nhelpful. An SVM needs the dot product of these points in 3D'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 178, 'file_type': 'pdf'}, page_content=\"space. Let’s use the preceding equations to project a and b\\nto 3D, and then compute the dot product of the result:\\n\\nWe now have the dot product of a pair of 2D points in 3D\\nspace, but we had to generate coordinates in 3D space to get\\nit. Here’s where it gets interesting. The following\\nfunction, or kernel trick, produces the same result using\\nonly the values in the original 2D space:\\n⟨a, b⟩ is simply the dot product of a and b, so ⟨a, b⟩2 is the\\nsquare of the dot product of a and b. We already know how to\\ncompute the dot product of a and b. Therefore:\\nThis agrees with the result computed by explicitly projecting\\nthe points, but with no projection required. That’s the\\nkernel trick in a nutshell. It saves time and memory when\\ngoing from two dimensions to three. Just imagine the savings\\nwhen projecting to an infinite number of dimensions—which,\\nyou’ll recall, is exactly what the RBF kernel does.\\nThe kernel trick used here wasn’t manufactured from thin\\nair. It happens to be the one used by a degree-2 polynomial\\nkernel. With Scikit, you can fit an SVM classifier with a\\ndegree-2 polynomial kernel to a dataset this way:\\n\\nmodel\\nSVC(kernel='poly',\\ndegree=2)\\nmodel.fit(x,\\ny)\\nIf you apply this to the preceding circular dataset and plot\\nthe decision boundary (Figure\\xa05-6, right), the result is\\nalmost identical to the one generated by the RBF kernel.\\nInterestingly, a degree-1 polynomial kernel (Figure\\xa05-6,\\nleft) produces the same decision boundary as the linear\\nkernel since a line is just a first-degree polynomial.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 179, 'file_type': 'pdf'}, page_content='Kernel tricks are special. Each one is designed to simulate a\\nspecific projection into higher dimensions. Scikit gives you\\na handful of kernels to work with, but there are others that\\nScikit doesn’t build in. You can extend Scikit with kernels\\nof your own, but the ones that it provides are sufficient for\\nthe vast majority of use cases.\\nFigure 5-6. Degree-1 versus degree-2 polynomial kernel\\nHyperparameter Tuning\\nAt the outset, it’s difficult to know which of the built-in\\nkernels will produce the most accurate model. It’s also\\ndifficult to know what the right value of C is—that is, the\\nvalue that provides the best balance between underfitting and\\noverfitting the training data and yields the best results\\nwhen the model is run with test data. For the RBF and\\npolynomial kernels, there’s a third value called gamma that\\naffects accuracy. And for polynomial kernels, the degree\\nparameter impacts the model’s ability to learn from the\\ntraining data.\\nThe C parameter controls how aggressively the model fits to\\nthe training data. The higher the value, the tighter the fit'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 180, 'file_type': 'pdf'}, page_content='and the higher the risk of overfitting. Figure\\xa05-7 shows how\\nthe RBF kernel fits a model to a set of training data\\ncontaining three classes with different values of C. The\\ndefault is C=1 in Scikit, but you can specify a different\\nvalue to adjust the fit. You can see the danger of\\noverfitting in the lower-right diagram. A point that lies to\\nthe extreme right would be classified as a blue, even though\\nit probably belongs to the yellow or brown class.\\nUnderfitting is a problem too. In the upper-left example,\\nvirtually any data point that isn’t a brown will be\\nclassified as a blue.\\nFigure 5-7. Effect of C on the RBF kernel\\nAn SVM that uses the RBF kernel isn’t properly tuned until\\nyou have the right value for gamma too. gamma controls how\\nfar the influence of a single data point reaches in computing\\ndecision boundaries. Lower values use more points and produce\\nsmoother decision boundaries; higher values involve fewer'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 181, 'file_type': 'pdf'}, page_content='points and fit more tightly to the training data. This is\\nillustrated in Figure\\xa05-8, where increasing gamma while\\nholding C constant closes the decision boundary more tightly\\naround clusters of classes. gamma can be any nonzero positive\\nvalue, but values between 0 and 1 are the most common. Rather\\nthan hardcode a default value for gamma, Scikit picks a\\ndefault value algorithmically if you don’t specify one.\\nIn practice, data scientists experiment with different\\nkernels and different parameter values to find the\\ncombination that produces the most accurate model, a process\\nknown as hyperparameter tuning. The usefulness of\\nhyperparameter tuning isn’t unique to SVMs, but you can\\nalmost always make an SVM more accurate by finding the\\noptimum combination of kernel type, C, and gamma (and for\\npolynomial kernels, degree).\\nFigure 5-8. Effect of gamma on the RBF kernel'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 182, 'file_type': 'pdf'}, page_content=\"To aid in the process of hyperparameter tuning, Scikit\\nprovides a family of optimizers that includes GridSearchCV,\\nwhich tries all combinations of a specified set of parameter\\nvalues with built-in cross-validation to determine which\\ncombination produces the most accurate model. These\\noptimizers prevent you from having to write code to do a\\nbrute-force search using all the unique combinations of\\nparameter values. To be clear, they do brute-force searches\\nthemselves by training the model multiple times, each time\\nwith a different combination of values. At the end, you can\\nretrieve the most accurate model from the best_estimator_\\nattribute, the parameter values that produced the most\\naccurate model from the best_params_ attribute, and the best\\nscore from the best_score_ attribute.\\nHere’s an example that uses Scikit’s SVC class to implement\\nan SVM classifier. For starters, you can create an SVM\\nclassifier that uses default parameter values and fit it to a\\ndataset with two lines of code:\\n\\nmodel\\nSVC()\\nmodel.fit(x,\\ny)\\nThis uses the RBF kernel with C=1. You can specify the kernel\\ntype and values for C and gamma this way:\\n\\nmodel\\nSVC(kernel='poly',\\nC=10,\\ngamma=0.1)\\nmodel.fit(x,\\ny)\\nSuppose you wanted to try two different kernels and five\\nvalues each for C and gamma to see which combination produces\\nthe best results. Rather than write a nested for loop, you\\ncould do this:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 183, 'file_type': 'pdf'}, page_content=\"model\\nSVC()\\n\\ngrid\\n\\xa0\\xa0\\xa0 'C':\\n[0.01,\\n100],\\n\\xa0\\xa0\\xa0 'gamma':\\n[0.01,\\n0.25,\\n0.75,\\n1.0],\\n\\xa0\\xa0\\xa0 'kernel':\\n['rbf',\\n'poly']\\n\\ngrid_search\\nGridSearchCV(estimator=model,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameter combinations\\nThe call to fit won’t return for a while. It trains the\\nmodel 250 times since there are 50 different combinations of\\nkernel, C, and gamma, and cv=5 says to use fivefold cross-\\nvalidation to assess the results. Once training is complete,\\nyou retrieve the best model this way:\\n\\nbest_model\\ngrid_search.best_estimator_\\nIt is not uncommon to run a search regimen such as this one\\nmultiple times—the first time with course parameter values,\\nand each time thereafter with narrower ranges of values\\ncentered on the values obtained from best_params_. More\\ntraining time up front is the price you pay for an accurate\\nmodel. To reiterate, you can almost always make an SVM more\\naccurate by finding the optimum combination of parameters.\\nAnd for better or worse, brute force is the most effective\\nway to identify the best combination.\\nNOTE\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 184, 'file_type': 'pdf'}, page_content='One nuance to be aware of regarding the SVC class is that it\\ndoesn’t compute probabilities by default. If you want to call\\npredict_proba on an SVC instance, you must set probability to True\\nwhen creating the instance:\\n\\nmodel\\nSVC(probability=True)\\nThe model will train more slowly, but you’ll be able to retrieve\\nprobabilities as well as predictions. Furthermore, the Scikit\\ndocumentation warns that “predict_proba may be inconsistent with\\npredict.” For more information, see Section 1.4.1.2 in the\\ndocumentation.\\nData Normalization\\nIn Chapter\\xa02, I noted that some learning algorithms work\\nbetter with normalized data. Unnormalized data contains\\ncolumns of numbers with vastly different ranges—for example,\\nvalues from 0 to 1 in one column and from 0 to 1,000,000 in\\nanother. SVM is a parametric learning algorithm. Training\\nwith normalized data is important because SVMs use distances\\nto compute margins. If one dimension spans much larger\\ndistances than another, the internal algorithm used to find\\nthe maximum margins might have trouble converging on a\\nsolution.\\nThe importance of training machine learning models with\\nnormalized data isn’t limited to SVMs. Decision trees and\\nlearning algorithms such as random forests and gradient-\\nboosted decision trees that rely on decision trees are\\nnonparametric, so they work equally well with normalized and\\nunnormalized data. They are the exception, however. Most'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 185, 'file_type': 'pdf'}, page_content='other learning algorithms benefit to one degree or another\\nfrom normalized data. That includes k-nearest neighbors,\\nwhich although nonparametric uses distance-based calculations\\ninternally to discriminate between classes.\\nScikit offers several classes for normalizing data. The most\\ncommonly used are MinMaxScaler and StandardScaler. The former\\nnormalizes data by proportionally reducing the values in each\\ncolumn to values from 0.0 to 1.0. Mathematically, it’s\\nsimple. For each column in a dataset, MinMaxScaler subtracts\\nthe minimum value in that column from all the column’s\\nvalues, then it divides each value by the difference between\\nthe minimum and maximum values. In the resulting column, the\\nminimum value is 0.0 and the maximum is 1.0.\\nTo demonstrate, I extracted subsets of two columns with\\nvastly different ranges from the breast cancer dataset built\\ninto Scikit. Each column contains 100 values. Here are the\\nfirst 10 rows:\\n\\n[[1.001e+03 3.001e-01]\\n[1.326e+03 8.690e-02]\\n[1.203e+03 1.974e-01]\\n[3.861e+02 2.414e-01]\\n[1.297e+03 1.980e-01]\\n[4.771e+02 1.578e-01]\\n[1.040e+03 1.127e-01]\\n[5.779e+02 9.366e-02]\\n[5.198e+02 1.859e-01]\\n[4.759e+02 2.273e-01]]\\nThe values in the first column range from 201.9 to 1,878.0;\\nthe values in the second column range from 0.000692 to\\n0.3754. Figure\\xa05-9 shows how the data looks if plotted with\\nthe x- and y-axis equally scaled. Because the values in the\\nfirst column are much larger than the values in the second,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 186, 'file_type': 'pdf'}, page_content='the data points appear to form a line. If you adjust the\\nscale of the axes to match the ranges of values in each\\ncolumn, you get a completely different picture (Figure\\xa05-\\nFigure 5-9. Unnormalized data plotted with equally scaled axes'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 187, 'file_type': 'pdf'}, page_content='Figure 5-10. Unnormalized data plotted with proportionally scaled axes\\nData that is this highly unnormalized can pose a problem for\\nparametric learning algorithms. One way to address that is to\\napply MinMaxScaler to the data:\\n\\nfrom\\nsklearn.preprocessing\\nimport\\nMinMaxScaler\\n\\nscaler\\nMinMaxScaler()\\nnormalized_data\\nscaler.fit_transform(data)\\nHere are the first 10 rows after min-max normalization:\\n\\n[[0.47676153 0.79904352]\\n[0.67066404 0.23006715]\\n[0.5972794\\xa0 0.52496344]'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 188, 'file_type': 'pdf'}, page_content='[0.10989798 0.64238821]\\n[0.65336197 0.52656469]\\n[0.16419068 0.41928115]\\n[0.50002983 0.29892076]\\n[0.22433029 0.24810786]\\n[0.18966649 0.49427287]\\n[0.16347473 0.60475891]]\\nFigure\\xa05-11 shows a plot of the normalized data with equal\\naxes. The shape of the data didn’t change. What did change\\nis that both columns now contain values ranging from 0.0 to\\nFigure 5-11. Data normalized with MinMaxScaler\\nSVMs almost always train better with normalized data, but the\\nsimple normalization performed by MinMaxScaler sometimes\\nisn’t enough. SVMs tend to respond better to data that is\\nnormalized to unit variance using a technique called'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 189, 'file_type': 'pdf'}, page_content='standardization or Z-score normalization. Unit variance is\\nachieved by doing the following to each column in a dataset:\\n\\nComputing the mean and standard deviations of all\\nthe values in the column\\nSubtracting the mean from each value in the\\ncolumn\\nDividing each value in the column by the standard\\ndeviation\\nThis is precisely the transform that Scikit’s StandardScaler\\nclass performs on a dataset. Applying unit variance to a\\ndataset is as simple as this:\\n\\nfrom\\nsklearn.preprocessing\\nimport\\nStandardScaler\\n\\nscaler\\nStandardScaler()\\nnormalized_data\\nscaler.fit_transform(data)\\nThe values in the original dataset may vary wildly from one\\ncolumn to the next, but the transformed dataset will contain\\ncolumns of numbers anchored around 0 with ranges that are\\nproportional to each column’s standard deviation. Applying\\nStandardScaler to the dataset produces the following values\\nin the first 10 rows:\\n\\n[[ 0.93457642\\xa0 2.36212718]\\n[ 1.95483237 -0.35495682]\\n[ 1.56870474\\xa0 1.05328794]\\n[-0.99574783\\xa0 1.61403698]\\n[ 1.86379415\\xa0 1.06093451]\\n[-0.71007617\\xa0 0.5486138 ]'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 190, 'file_type': 'pdf'}, page_content='[ 1.05700714 -0.02615397]\\n[-0.39363986 -0.26880538]\\n[-0.57603023\\xa0 0.90672853]\\n[-0.71384326\\xa0 1.4343424 ]]\\nAnd it produces the distribution shown in Figure\\xa05-12. Once\\nmore, the shape of the data didn’t change, but the values\\nthat define that shape changed substantially.\\nSVMs typically perform best when trained with standardized\\ndata, even if all the columns have similar ranges. (The same\\nis true of neural networks, by the way.) The classic case in\\nwhich columns have similar ranges but benefit from\\nnormalization anyway is image data, where each column holds\\npixel values from 0 to 255. There are exceptions, but it is\\nusually a mistake to throw a bunch of data at an SVM without\\nunderstanding the distribution of the data—specifically,\\nwhether it has unit variance.\\nFigure 5-12. Data normalized with StandardScaler'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 191, 'file_type': 'pdf'}, page_content='Pipelining\\nIf you normalize or standardize the values used to train a\\nmachine learning model, you must apply the same transform to\\nvalues input to the model’s predict method. In other words,\\nif you train a model this way:\\n\\nmodel\\nSVC()\\nscaler\\nStandardScaler()\\nx\\nscaler.fit_transform(x)\\nmodel.fit(x,\\ny)\\nyou make predictions with it this way:\\n\\ninput\\nmodel.predict([scaler.transform([input])\\nOtherwise, you’ll get nonsensical predictions.\\nTo simplify your code and make it harder to forget to\\ntransform training data and prediction data the same way,\\nScikit offers the make_pipeline function. make_pipeline lets\\nyou combine predictive models—what Scikit calls estimators,\\nor instances of classes such as SVC—with transforms applied\\nto data input to those models. Here’s how you use\\nmake_pipeline to ensure that any data input to the model is\\ntransformed with StandardScaler:\\n\\n# Train the model\\npipe\\nmake_pipeline(StandardScaler(),\\nSVC())\\npipe.fit(x,\\ny)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 192, 'file_type': 'pdf'}, page_content=\"# Make a prediction with the model\\ninput\\npipe.predict([input])\\nNow data used to train the model has StandardScaler applied\\nto it, and data input to make predictions is transformed the\\nsame way.\\nWhat if you wanted to use GridSearchCV to find the optimum\\nset of parameters for a pipeline that combines a data\\ntransform and estimator? It’s not hard, but there’s a trick\\nyou need to know about. It involves using class names\\nprefaced with double underscores in the param_grid dictionary\\npassed to GridSearchCV. Here’s an example:\\n\\npipe\\nmake_pipeline(StandardScaler(),\\nSVC())\\n\\ngrid\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.01,\\n100],\\n\\xa0\\xa0\\xa0 'svc__gamma':\\n[0.01,\\n0.25,\\n0.75,\\n1.0],\\n\\xa0\\xa0\\xa0 'svc__kernel':\\n['rbf',\\n'poly']\\n\\ngrid_search\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameter combinations\\nThis example trains the model 250 times to find the best\\ncombination of kernel, C, and gamma for the SVC instance in\\nthe pipeline. Note the “svc__” nomenclature, which maps to\\nthe SVC instance passed to the make_pipeline function.\\nUsing SVMs for Facial Recognition\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 193, 'file_type': 'pdf'}, page_content='Modern facial recognition is often accomplished with neural\\nnetworks, but support vector machines can do a credible job\\ntoo. Let’s demonstrate by building a model that recognizes\\nfaces. The dataset we’ll use is the Labeled Faces in the\\nWild (LFW) dataset, which contains more than 13,000 facial\\nimages of famous people collected from around the web and is\\nbuilt into Scikit as a sample dataset. Of the more than 5,000\\npeople represented in the dataset, 1,680 have two or more\\nfacial images, while only five have 100 or more. We’ll set\\nthe minimum number of faces per person to 100, which means\\nthat five sets of faces corresponding to five famous people\\nwill be imported. Each facial image is labeled with the name\\nof the person the face belongs to.\\nStart by creating a new Jupyter notebook and using the\\nfollowing statements to load the dataset:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\nfetch_lfw_people(min_faces_per_person=100)\\nprint(faces.target_names)\\nprint(faces.images.shape)\\nIn total, 1,140 facial images were loaded. Each image\\nmeasures 47 × 62 pixels for a total of 2,914 pixels per\\nimage. That means the dataset contains 2,914 features. Use\\nthe following code to show the first 24 images in the dataset\\nand the people to whom the faces belong:\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 194, 'file_type': 'pdf'}, page_content=\"fig,\\nax\\nplt.subplots(3,\\nfigsize=(18,\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nHere is the output:\\nCheck the balance in the dataset by generating a histogram\\nshowing how many facial images were imported for each person:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nfrom\\ncollections\\nimport\\nCounter\\ncounts\\nCounter(faces.target)\\nnames\\n\\nfor\\nkey\\nin\\ncounts.keys():\\n\\xa0\\xa0\\xa0 names[faces.target_names[key]]\\ncounts[key]\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 195, 'file_type': 'pdf'}, page_content=\"df\\npd.DataFrame.from_dict(names,\\norient='index')\\ndf.plot(kind='bar')\\nThe output reveals that there are far more images of George\\nW. Bush than of anyone else in the dataset:\\nClassification models are best trained with balanced\\ndatasets. Use the following code to reduce the dataset to 100\\nimages of each person:\\n\\nmask\\nnp.zeros(faces.target.shape,\\ndtype=bool)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 196, 'file_type': 'pdf'}, page_content=\"for\\ntarget\\nin\\nnp.unique(faces.target):\\n\\xa0\\xa0\\xa0 mask[np.where(faces.target\\ntarget)[0][:100]]\\n\\nx\\nfaces.data[mask]\\ny\\nfaces.target[mask]\\nx.shape\\nNote that x contains 500 facial images and y contains the\\nlabels that go with them: 0 for Colin Powell, 1 for Donald\\nRumsfeld, and so on. Now let’s see if an SVM can make sense\\nof the data. We’ll train three different models: one that\\nuses a linear kernel, one that uses a polynomial kernel, and\\none that uses an RBF kernel. In each case, we’ll use\\nGridSearchCV to optimize hyperparameters. Start with a linear\\nmodel and four different values of C:\\n\\nfrom\\nsklearn.svm\\nimport\\nSVC\\nfrom\\nsklearn.model_selection\\nimport\\nGridSearchCV\\n\\nsvc\\nSVC(kernel='linear')\\n\\ngrid\\n\\xa0\\xa0\\xa0 'C':\\n[0.1,\\n\\ngrid_search\\nGridSearchCV(estimator=svc,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameters\\ngrid_search.best_score_\\nThis model achieves a cross-validated accuracy of 84.4%.\\nIt’s possible that accuracy can be improved by standardizing\\nthe image data. Run the same grid search again, but this time\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 197, 'file_type': 'pdf'}, page_content=\"use StandardScaler to apply unit variance to all the pixel\\nvalues:\\n\\nfrom\\nsklearn.pipeline\\nimport\\nmake_pipeline\\nfrom\\nsklearn.preprocessing\\nimport\\nStandardScaler\\n\\nscaler\\nStandardScaler()\\nsvc\\nSVC(kernel='linear')\\npipe\\nmake_pipeline(scaler,\\nsvc)\\n\\ngrid\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.1,\\n\\ngrid_search\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\ngrid_search.best_score_\\nStandardizing the data produced an incremental improvement in\\naccuracy. What value of C produced that accuracy?\\n\\ngrid_search.best_params_\\nIs it possible that a polynomial kernel could outperform a\\nlinear kernel? There’s an easy way to find out. Note the\\nintroduction of the gamma and degree parameters to the\\nparameter grid. These parameters, along with C, can greatly\\ninfluence a polynomial kernel’s ability to fit to the\\ntraining data:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 198, 'file_type': 'pdf'}, page_content=\"scaler\\nStandardScaler()\\nsvc\\nSVC(kernel='poly')\\npipe\\nmake_pipeline(scaler,\\nsvc)\\n\\ngrid\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.1,\\n100],\\n\\xa0\\xa0\\xa0 'svc__gamma':\\n[0.01,\\n0.25,\\n0.75,\\n\\xa0\\xa0\\xa0 'svc__degree':\\n\\ngrid_search\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\n# Train the model with different parameter combinations\\ngrid_search.best_score_\\nThe polynomial kernel achieved the same accuracy as the\\nlinear kernel. What parameter values led to this result?\\n\\ngrid_search.best_params_\\nThe best_params_ attribute reveals that the optimum value of\\ndegree was 1, which means the polynomial kernel acted like a\\nlinear kernel. It’s not surprising, then, that it achieved\\nthe same accuracy. Could an RBF kernel do better?\\n\\nscaler\\nStandardScaler()\\nsvc\\nSVC(kernel='rbf')\\npipe\\nmake_pipeline(scaler,\\nsvc)\\n\\ngrid\\n\\xa0\\xa0\\xa0 'svc__C':\\n[0.1,\\n100],\\n\\xa0\\xa0\\xa0 'svc__gamma':\\n[0.01,\\n0.25,\\n0.75,\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 199, 'file_type': 'pdf'}, page_content='grid_search\\nGridSearchCV(estimator=pipe,\\nparam_grid=grid,\\ncv=5,\\nverbose=2)\\ngrid_search.fit(x,\\ny)\\ngrid_search.best_score_\\nThe RBF kernel didn’t perform as well as the linear and\\npolynomial kernels. There’s a lesson here. The RBF kernel\\noften fits to nonlinear data better than other kernels, but\\nit doesn’t always fit better. That’s why the best strategy\\nwith an SVM is to try different kernels with different\\nparameter values. The best combination will vary from dataset\\nto dataset. For the LFW dataset, it seems that a linear\\nkernel is best. That’s convenient, because the linear kernel\\nis the fastest of all the kernels Scikit provides.\\nNOTE\\nIn addition to the SVC class, Scikit-Learn includes SVM\\nclassifiers named LinearSVC and NuSVC. The latter supports the same\\nassortment of kernels as the SVC class, but it replaces C with a\\nregularization parameter called nu that controls tightness of fit\\ndifferently. NuSVC doesn’t scale as well as SVC to large datasets,\\nand in my experience it is rarely used. LinearSVC implements the\\nlinear kernel only, but it uses a different optimization algorithm\\nthat trains faster. If training is slow with SVC and you determine\\nthat a linear kernel yields the best model, consider swapping SVC\\nfor LinearSVC. Faster training times make a difference even for\\nmodestly sized datasets if you’re using GridSearchCV to train a\\nmodel hundreds of times. For a great summary of the functional\\ndifferences between the two classes, see the article “SVM with\\nScikit-Learn: What You Should Know” by Angela Shi.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 200, 'file_type': 'pdf'}, page_content=\"Confusion matrices are a great way to visualize a model’s\\naccuracy. Let’s split the dataset, train an optimized linear\\nmodel with 80% of the images, test it with the remaining 20%,\\nand show the results in a confusion matrix.\\nThe first step is to split the dataset. Note the stratify=y\\nparameter, which ensures that the training dataset and the\\ntest dataset have the same proportion of samples of each\\nclass as the original dataset. In this example, the training\\ndataset will contain 20 samples of each of the five people:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\ntrain_size=0.8,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y,\\nrandom_state=0)\\nNow train a linear SVM with the optimum C value revealed by\\nthe grid search:\\n\\nscaler\\nStandardScaler()\\nsvc\\nSVC(kernel='linear',\\nC=0.1)\\npipe\\nmake_pipeline(scaler,\\nsvc)\\npipe.fit(x_train,\\ny_train)\\nCross-validate the model to confirm its accuracy:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ncross_val_score\\n\\ncross_val_score(pipe,\\nx,\\ny,\\ncv=5).mean()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 201, 'file_type': 'pdf'}, page_content=\"Use a confusion matrix to see how the model performs against\\nthe test data:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nfig,\\nax\\nplt.subplots(figsize=(6,\\nax.grid(False)\\ncmd.from_estimator(pipe,\\nx_test,\\ny_test,\\ndisplay_labels=faces.target_names,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 202, 'file_type': 'pdf'}, page_content='The model correctly identified Colin Powell 19 times out of\\n20, Donald Rumsfeld 20 times out of 20, and so on. That’s\\nnot bad. And it’s a great example of support vector machines\\nat work.\\nSummary\\nSupport vector machines, or SVMs, frequently fit to datasets\\nbetter than other learning algorithms. SVMs are maximum-\\nmargin classifiers that use kernel tricks to simulate adding\\ndimensions to data. The theory is that data that isn’t\\nlinearly separable in m dimensions might be separable in n'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 203, 'file_type': 'pdf'}, page_content='dimensions if n is higher than m. SVMs are most often used\\nfor classification, but they can perform regression too. As\\nan experiment, try replacing GradientBoostingRegressor in the\\ntaxi-fare example in Chapter\\xa02 with SVR and using\\nGridSearchCV to optimize the model’s hyperparameters. Which\\nmodel produces the highest cross-validated coefficient of\\ndetermination?\\nSVMs usually train better with data that is normalized to\\nunit variance. That’s true even if the values in all the\\ncolumns have similar ranges, but it’s especially true if\\nthey don’t have similar ranges. Scikit’s StandardScaler\\nclass applies unit variance to data. Unit variance is\\nachieved by dividing the values in a column by the mean of\\nall the values in the column and dividing by the standard\\ndeviation. Scikit’s make_pipeline function enables you to\\ncombine transformers such as StandardScaler and classifiers\\nsuch as SVC into one logical unit to ensure that data passed\\nto fit, predict, and predict_proba undergoes the same\\ntransformations.\\nSVMs require tuning in order to achieve optimum accuracy.\\nTuning means finding the right values for parameters such as\\nC, gamma, and kernel, and it entails trying different\\nparameter combinations and assessing the results. Scikit\\nprovides classes such as GridSearchCV to help, but they\\nincrease training time by training the model once for each\\nunique combination of parameter values.\\nSVMs can seem magical in their ability to fit mathematical\\nmodels to complex datasets. But in my view, that magic takes\\na back seat to the numerical gymnastics performed by\\nprincipal component analysis (PCA), which solves a variety of\\nproblems routinely encountered in machine learning. I often\\nintroduce PCA by telling audiences that it’s the best-kept\\nsecret in machine learning. After Chapter\\xa06, it will be a\\nsecret no longer.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 204, 'file_type': 'pdf'}, page_content='RBF is short for radial basis function'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 205, 'file_type': 'pdf'}, page_content='Chapter 6. Principal Component\\nAnalysis\\nPrincipal component analysis, or PCA, is one of the minor\\nmiracles of machine learning. It’s a dimensionality\\nreduction technique that reduces the number of dimensions in\\na dataset without sacrificing a commensurate amount of\\ninformation. While that might seem underwhelming on the face\\nof it, it has profound implications for engineers and\\nsoftware developers working to build predictive models from\\ntheir data.\\nWhat if I told you that you could take a dataset with 1,000\\ncolumns, use PCA to reduce it to 100 columns, and retain 90%\\nor more of the information in the original dataset? That’s\\nrelatively common, believe it or not. And it lends itself to\\na variety of practical uses, including:\\n\\nReducing high-dimensional data to two or three\\ndimensions so that it can be plotted and explored\\nReducing the number of dimensions in a dataset\\nand then restoring the original number of\\ndimensions, which finds application in anomaly\\ndetection and noise filtering\\nAnonymizing datasets so that they can be shared\\nwith others without revealing the nature or\\nmeaning of the data\\nAnd that’s not all. A side effect of applying PCA to a\\ndataset is that less important features—columns of data that\\nhave less relevance to the outcome of a predictive model—are\\nremoved, while dependencies between columns is eliminated.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 206, 'file_type': 'pdf'}, page_content='And in datasets with a low ratio of samples (rows) to\\nfeatures (columns), PCA can be used to increase that ratio.\\nAs a rule of thumb, you typically want a dataset used for\\nmachine learning to have at least five times as many rows as\\nit has columns. If you can’t add rows, an alternative is to\\nuse PCA to shave columns.\\nOnce you learn about PCA, you’ll wonder how you lived\\nwithout it. Let’s take a few moments to understand what it\\nis and how it works. Then we’ll look at some examples\\ndemonstrating why it’s such an indispensable tool.\\nUnderstanding Principal Component Analysis\\nOne way to wrap your head around PCA is to see how it reduces\\na two-dimensional dataset to one dimension. Figure\\xa06-1\\ndepicts a 2D dataset comprising a somewhat random collection\\nof x and y values. If you reduced this dataset to a single\\ndimension by simply dropping the x column or the y column,\\nyou’d be left with a horizontal or vertical line that bears\\nlittle resemblance to the original dataset.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 207, 'file_type': 'pdf'}, page_content='Figure 6-1. Two-dimensional dataset\\nFigure\\xa06-2 adds arrows representing the dataset’s two\\nprincipal components. Essentially, the coordinate system has\\nbeen transformed so that one axis (the longer of the two\\narrows) captures most of the variance in the dataset. This is\\nthe dataset’s primary principal component. The other axis\\ncontains a narrower range of values and represents the\\nsecondary principal component. The number of principal\\ncomponents equals the number of dimensions in a dataset, so\\nin this example, there are two principal components.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 208, 'file_type': 'pdf'}, page_content='Figure 6-2. Arrows depicting the principal components of a two-dimensional\\ndataset\\nTo reduce a two-dimensional dataset to one dimension, PCA\\nfinds the two principal components and eliminates the one\\nwith less variance. This effectively projects the data points\\nonto the primary principal component axis, as shown in\\nFigure\\xa06-3. The red data points don’t retain all of the\\ninformation in the original dataset, but they contain most of\\nit. In this example, the PCAed dataset retains more than 95%\\nof the information in the original. PCA reduced the number of\\ndimensions by 50%, but it sacrificed less than 5% of the\\nmeaningful information in the dataset. That’s the gist of\\nPCA: reducing the number of dimensions without incurring a\\ncommensurate loss of information.\\nUnder the hood, PCA works its magic by building a covariance\\nmatrix that quantifies the variance of each dimension with\\nrespect to the others, and from the matrix computing\\neigenvectors and eigenvalues that identify the dataset’s\\nprincipal components. If you’d like to dig deeper, I suggest'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 209, 'file_type': 'pdf'}, page_content='reading “A Step-by-Step Explanation of Principal Component\\nAnalysis (PCA)” by Zakaria Jaadi. The good news is that you\\ndon’t have to understand the math to make PCA work, because\\nScikit-Learn’s PCA class does the math for you. The\\nfollowing statements reduce the dataset x to five dimensions,\\nregardless of the number of dimensions it originally\\ncontains:\\n\\npca\\nPCA(n_components=5)\\nx\\npca.fit_transform(x)\\nFigure 6-3. Two-dimensional dataset (blue) reduced to one dimension (red)\\nwith PCA\\nYou can also invert a PCA transform to restore the original\\nnumber of dimensions:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 210, 'file_type': 'pdf'}, page_content=\"x\\npca.inverse_transform(x)\\nThe inverse_transform method restores the dataset to its\\noriginal number of dimensions, but it doesn’t restore the\\noriginal dataset. The information that was discarded when the\\nPCA transform was applied will be missing from the restored\\ndataset.\\nYou can visualize the loss of information when a PCA\\ntransform is applied and then inverted using the Labeled\\nFaces in the Wild (LFW) dataset introduced in Chapter\\xa05. To\\ndemonstrate, fire up a Jupyter notebook and run the following\\ncode:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\nfetch_lfw_people(min_faces_per_person=100)\\nfig,\\nax\\nplt.subplots(3,\\nfigsize=(18,\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nThe output shows the first 24 images in the dataset:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 211, 'file_type': 'pdf'}, page_content=\"Each image measures 47 × 62 pixels, for a total of 2,914\\npixels per image. That means the dataset has 2,914\\ndimensions. Now use the following code to reduce the number\\nof dimensions to 150 (roughly 5% of the original number),\\nrestore the original 2,914 dimensions, and plot the restored\\nimages:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\npca\\nPCA(n_components=150,\\nrandom_state=0)\\npca_faces\\npca.fit_transform(faces.data)\\nunpca_faces\\npca.inverse_transform(pca_faces).reshape(1140,\\n\\nfig,\\nax\\nplt.subplots(3,\\nfigsize=(18,\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(unpca_faces[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 212, 'file_type': 'pdf'}, page_content='Even though you removed almost 95% of the dimensions in the\\ndataset, little meaningful information was discarded. The\\nrestored images are slightly blurrier than the originals, but\\nthe faces are still recognizable:\\nTo reiterate, you reduced the number of dimensions from 2,914\\nto 150, but because PCA found 2,914 principal components and\\nremoved the ones that are least important (the ones with the\\nleast variance), you retained the bulk of the information in\\nthe original dataset. Which begs a question: precisely how\\nmuch of the original information was retained?\\nAfter a PCA object is fit to a dataset, you can find out how\\nmuch variance is encoded in each principal component from the\\nexplained_variance_ratio_ attribute. It’s an array with one\\nelement for each principal component in the transformed\\ndataset. Here’s how it looks after the LFW dataset is\\nreduced to 150 dimensions:\\n\\narray([0.18075283,\\n0.15304269,\\n0.07271618,\\n0.05843799,\\n0.05164209,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.03046084,\\n0.02518216,\\n0.02159553,\\n0.02021552,\\n0.01913318,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 213, 'file_type': 'pdf'}, page_content='0.01555639,\\n0.01456686,\\n0.01256744,\\n0.01084539,\\n0.00984127,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.0093953\\n0.00916603,\\n0.00868945,\\n0.00813571,\\n0.00727695,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00677768,\\n0.00641177,\\n0.00598251,\\n0.00584093,\\n0.00560558,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00510269,\\n0.00502014,\\n0.00471686,\\n0.00459556,\\n0.00417527,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00388964,\\n0.00382498,\\n0.00369323,\\n0.00351382,\\n0.00336695,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00334725,\\n0.00322673,\\n0.00311035,\\n0.00296509,\\n0.00291989,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00284639,\\n0.00269707,\\n0.00258668,\\n0.00250783,\\n0.00242382,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.0024114\\n0.00238748,\\n0.00231979,\\n0.00227626,\\n0.00220951,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00208082,\\n0.00205139,\\n0.0020204\\n0.00193762,\\n0.00190936,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00187478,\\n0.00181047,\\n0.00179518,\\n0.0017543\\n0.0017069\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00167668,\\n0.00163358,\\n0.00161024,\\n0.00157452,\\n0.00153261,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00149059,\\n0.00147311,\\n0.00146492,\\n0.00144425,\\n0.00141883,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00138223,\\n0.00134285,\\n0.00131313,\\n0.00129011,\\n0.00125677,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00124584,\\n0.00123509,\\n0.00120691,\\n0.00118097,\\n0.00116907,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00114366,\\n0.00114065,\\n0.00112442,\\n0.00109727,\\n0.00107732,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.0010587\\n0.00103661,\\n0.0010168\\n0.00101077,\\n0.00099613,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00095741,\\n0.00094375,\\n0.00093824,\\n0.00091516,\\n0.00089966,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00089489,\\n0.00088575,\\n0.00086848,\\n0.00085914,\\n0.00085092,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00081995,\\n0.00081599,\\n0.00080637,\\n0.00077363,\\n0.00076949,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00075849,\\n0.00074882,\\n0.00073402,\\n0.00073062,\\n0.00071469,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00070963,\\n0.00070081,\\n0.00069565,\\n0.00068256,\\n0.00066753,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00065527,\\n0.00065092,\\n0.00063932,\\n0.00062695,\\n0.00062127,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00060888,\\n0.0006006\\n0.00058937,\\n0.00058177,\\n0.00057769,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00056761,\\n0.00056366,\\n0.00055619,\\n0.00054269,\\n0.00053866,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00052962,\\n0.00052751,\\n0.00051337,\\n0.00050697,\\n0.00050601,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00049807,\\n0.00049578,\\n0.00048296,\\n0.00047389,\\n0.0004729\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00046955,\\n0.00046098,\\n0.00045423,\\n0.00044775,\\n0.0004431\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.00043341,\\n0.00042795,\\n0.00042292,\\n0.00041978,\\n0.00041305],\\n\\xa0\\xa0\\xa0\\xa0\\xa0 dtype=float32)\\nThis reveals that 18% of the variance in the dataset is\\nexplained by the primary principal component, 15% is'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 213, 'file_type': 'pdf'}, page_content='0.00043341,\\n0.00042795,\\n0.00042292,\\n0.00041978,\\n0.00041305],\\n\\xa0\\xa0\\xa0\\xa0\\xa0 dtype=float32)\\nThis reveals that 18% of the variance in the dataset is\\nexplained by the primary principal component, 15% is\\nexplained by the secondary principal component, and so on.\\nObserve that the numbers decrease as the index increases'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 213, 'file_type': 'pdf'}, page_content='. By\\ndefinition, each principal component in a PCAed dataset\\ncontains more information than the principal component after'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 214, 'file_type': 'pdf'}, page_content=\"it. In this example, the 2,764 principal components that were\\ndiscarded contained so little information that their loss was\\nbarely noticeable when the transform was inverted. In fact,\\nthe sum of the 150 numbers in the preceding example is\\n0.9480211. This means reducing the dataset from 2,914\\ndimensions to 150 retained 94.8% of the information in the\\noriginal dataset. In other words, you reduced the number of\\ndimensions by almost 95%, and yet you retained almost 95% of\\nthe information in the dataset. If that’s not awesome, I\\ndon’t know what is.\\nA logical question to ask is, what is the “right” number of\\ncomponents? In other words, what number of components strikes\\nthe best balance between reducing the number of dimensions in\\nthe dataset and retaining most of the information? One way to\\nfind that number is with a scree plot, which charts the\\nproportion of explained variance for each dimension. The\\nfollowing code produces a scree plot for the PCA transform\\nused on the facial images:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nplt.plot(pca.explained_variance_ratio_)\\nplt.xlabel('Principal Component')\\nplt.ylabel('Explained Variance')\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 215, 'file_type': 'pdf'}, page_content=\"Another way to look at it is to plot the cumulative sum of\\nthe variances as a function of component count:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\\nplt.xlabel('Number of Components')\\nplt.ylabel('Cumulative Explained Variance');\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 216, 'file_type': 'pdf'}, page_content='Either way you look at it, the bulk of the information is\\ncontained in the first 50 to 100 dimensions. Based on these\\nplots, if you reduced the number of dimensions to 50 instead\\nof 150, would you expect the restored facial images to look\\nsubstantially different? If you’re not sure, try it and see.\\nFiltering Noise\\nOne very practical use for PCA is to filter noise from data.\\nNoise is data that is random, corrupt, or otherwise\\nmeaningless, and it’s particularly likely to occur when the\\ndata comes from physical devices such as pressure sensors or\\naccelerometers. The basic approach to using PCA for noise\\nreduction is to PCA-transform the data and then invert the\\ntransform, reducing the dataset from m dimensions to n and\\nthen restoring it to m. Because PCA discards the least\\nimportant information when reducing dimensions and noise\\ntends to have little or no informational value, this ideally'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 217, 'file_type': 'pdf'}, page_content=\"eliminates much of the noise while retaining most of the\\nmeaningful data.\\nYou can test this supposition with the LFW dataset. Use the\\nfollowing statements to add noise to the facial images using\\na random-number generator and plot the first 24 images:\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\nimport\\nnumpy\\nas\\nnp\\n\\nfaces\\nfetch_lfw_people(min_faces_per_person=100)\\n\\nnp.random.seed(0)\\nnoisy_faces\\nnp.random.normal(faces.data,\\n\\nfig,\\nax\\nplt.subplots(3,\\nfigsize=(18,\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(noisy_faces[i].reshape(62,\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nThe resulting facial images resemble a staticky 1960s TV\\nscreen:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 218, 'file_type': 'pdf'}, page_content='Now use PCA to reduce the number of dimensions. Rather than\\nspecify the number of dimensions (components), we’ll specify\\nthat we want to reduce the amount of information in the\\ndataset to 80%. We’ll let Scikit decide how many dimensions\\nwill remain, and then show the count:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\npca\\nPCA(0.8,\\nrandom_state=0)\\npca_faces\\npca.fit_transform(noisy_faces)\\npca.n_components_\\nPCA reduced the number of dimensions from 2,914 to 179, but\\nthe remaining dimensions contain 80% of the information in\\nthe original 2,914. Now reconstruct the facial images from\\nthe PCAed faces and show the results:\\n\\nunpca_faces\\npca.inverse_transform(pca_faces)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 219, 'file_type': 'pdf'}, page_content=\"fig,\\nax\\nplt.subplots(3,\\nfigsize=(18,\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(unpca_faces[i].reshape(62,\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nHere is the output:\\nThe reconstructed dataset isn’t quite as clean as the\\noriginal, but it’s clean enough that you can make out the\\nfaces in the photos.\\nAnonymizing Data\\nChapter\\xa03 demonstrated how to use various learning\\nalgorithms to build a binary classification model that\\ndetects credit card fraud. The dataset used in the example\\ncontained real credit card data that had been anonymized to\\nprotect the card holders (and the credit card company’s\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 220, 'file_type': 'pdf'}, page_content='intellectual property). The first 10 rows of that dataset are\\npictured in Figure\\xa06-4.\\nFigure 6-4. Anonymized fraud detection dataset\\nAnother practical use for PCA is to anonymize data in this\\nmanner. It’s generally a two-step process:\\n\\nUse PCA to “reduce” the dataset from m\\ndimensions to m, where m is the original number\\nof dimensions (as well as the number of\\ndimensions after “reduction”).\\nNormalize the data so that it has unit variance.\\nThe second step isn’t required, but it does make the ranges\\nof values more uniform. Data anonymized this way can still be\\nused to train a machine learning model, but its original\\nmeaning can’t be inferred.\\nTry it with a dataset of your own. First, use the following\\ncode to load Scikit’s breast cancer dataset and display the\\nfirst five rows:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 221, 'file_type': 'pdf'}, page_content=\"import\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nload_breast_cancer\\n\\ndata\\nload_breast_cancer()\\ndf\\npd.DataFrame(data=data.data,\\ncolumns=data.feature_names)\\npd.set_option('display.max_columns',\\ndf.head()\\nThe output is as follows:\\nThe dataset contains 30 columns, not counting the label\\ncolumn. Now use the following statements to find the 30\\nprincipal components and apply StandardScaler to the\\ntransformed data:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\nfrom\\nsklearn.preprocessing\\nimport\\nStandardScaler\\n\\npca\\nPCA(n_components=30,\\nrandom_state=0)\\npca_data\\npca.fit_transform(df)\\n\\nscaler\\nStandardScaler()\\nanon_df\\npd.DataFrame(scaler.fit_transform(pca_data))\\npd.set_option('display.max_columns',\\nanon_df.head()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 222, 'file_type': 'pdf'}, page_content='The result is as follows:\\nThe dataset is unrecognizable after the PCA transform.\\nWithout the transform, it’s impossible to work backward and\\nreconstruct the original data. Yet the sum of the\\nexplained_variance_ratio_ values is 1.0, which means no\\ninformation was lost. You can prove it this way:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nnp.sum(pca.explained_variance_ratio_)\\nThe PCAed dataset is just as useful for machine learning as\\nthe original. Furthermore, if you want to share the dataset\\nwith others so that they can train models of their own, there\\nis no risk of divulging sensitive or proprietary information.\\nVisualizing High-Dimensional Data\\nYet another use for PCA is to reduce a dataset to two or\\nthree dimensions so that it can be plotted with libraries\\nsuch as Matplotlib. You can’t plot a dataset that has 1,000\\ncolumns. You can plot a dataset that has two or three\\ncolumns. The fact that PCA can reduce high-dimensional data\\nto two or three dimensions while retaining much of the'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 223, 'file_type': 'pdf'}, page_content=\"original information makes it a great tool for exploring data\\nand visualizing relationships between classes.\\nSuppose you’re building a classification model and want to\\nassess up front whether there is sufficient separation\\nbetween classes to support such a model. Take the Optical\\nRecognition of Handwritten Digits dataset built into Scikit,\\nfor example. Each digit in the dataset is represented by an 8\\n× 8 array of pixel values, meaning the dataset has 64\\ndimensions. If you could plot a 64-dimensional diagram, you\\nmight be able to inspect the dataset and look for separation\\nbetween classes. But 64 dimensions is 61 too many for most\\nhumans.\\nEnter PCA. The following code loads the dataset, uses PCA to\\nreduce it to two dimensions, and plots the result, with\\ndifferent colors representing different classes (digits):\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\nfrom\\nsklearn.datasets\\nimport\\nload_digits\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndigits\\nload_digits()\\npca\\nPCA(n_components=2,\\nrandom_state=0)\\npca_digits\\npca.fit_transform(digits.data)\\n\\nplt.figure(figsize=(12,\\nplt.scatter(pca_digits[:,\\npca_digits[:,\\nc=digits.target,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap=plt.cm.get_cmap('Paired',\\nplt.colorbar(ticks=range(10))\\nplt.clim(-0.5,\\nThe resulting plot provides an encouraging sign that you\\nmight be able to train a classifier with the data. While\\nthere is clearly some overlap between classes, the different\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 224, 'file_type': 'pdf'}, page_content='classes form rather distinct clusters. There is significant\\noverlap between red (the digit 4) and light purple (the digit\\n6), indicating that a model might have some difficulty\\ndistinguishing between 4s and 6s. However, 0s and 1s lie at\\nthe top and bottom, while 3s and 4s fall on the far left and\\nfar right. A model would presumably be proficient at telling\\nthese digits apart:\\nYou can better visualize relationships between classes with a\\n3D plot. The following code uses PCA to reduce the dataset to\\nthree dimensions and Mplot3D to produce an interactive plot.\\nNote that if you run this code in Jupyter Lab, you’ll\\nprobably have to change the first line to %matplotlib widget:\\n\\n%matplotlib\\nnotebook\\nfrom\\nmpl_toolkits.mplot3d\\nimport\\nAxes3D'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 225, 'file_type': 'pdf'}, page_content=\"digits\\nload_digits()\\npca\\nPCA(n_components=3,\\nrandom_state=0)\\npca_digits\\npca.fit_transform(digits.data)\\n\\nax\\nplt.figure(figsize=(12,\\n8)).add_subplot(111,\\nprojection='3d')\\nax.scatter(xs\\npca_digits[:,\\nys\\npca_digits[:,\\nzs\\npca_digits[:,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 c=digits.target,\\ncmap=plt.cm.get_cmap('Paired',\\nYou can rotate the resulting plot in 3D and look at it from\\ndifferent angles. Here we can see that there is more\\nseparation between 4s and 6s than was evident in two\\ndimensions:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 226, 'file_type': 'pdf'}, page_content='PCA isn’t the only way to reduce a dataset to two or three\\ndimensions for plotting. You can also use Scikit’s Isomap\\nclass or its TSNE class. TSNE implements t-distributed\\nstochastic neighbor embedding, or t-SNE for short. t-SNE is a\\ndimensionality reduction algorithm that is used almost\\nexclusively for visualizing high-dimensional data. Whereas\\nPCA uses a linear function to transform data, t-SNE uses a\\nnonlinear transform that tends to heighten the separation\\nbetween classes by keeping similar data points close together\\nin low-dimensional space. (PCA, by contrast, focuses on\\nkeeping dissimilar points far apart.) Here’s an example that'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 227, 'file_type': 'pdf'}, page_content=\"plots the Digits dataset in two dimensions after reducing it\\nwith t-SNE:\\n\\n%matplotlib\\ninline\\nfrom\\nsklearn.manifold\\nimport\\nTSNE\\n\\ndigits\\nload_digits()\\ntsne\\nTSNE(n_components=2,\\ninit='pca',\\nlearning_rate='auto',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\ntsne_digits\\ntsne.fit_transform(digits.data)\\n\\nplt.figure(figsize=(12,\\nplt.scatter(tsne_digits[:,\\ntsne_digits[:,\\nc=digits.target,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap=plt.cm.get_cmap('Paired',\\nplt.colorbar(ticks=range(10))\\nplt.clim(-0.5,\\nAnd here is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 228, 'file_type': 'pdf'}, page_content='t-SNE does a better job of separating groups of digits into\\nclusters, indicating there are patterns in the data that\\nmachine learning can exploit. The chief drawback is that t-\\nSNE is compute intensive, which means it can take a\\nprohibitively long time to run on large datasets. One way to\\nmitigate that is to run t-SNE on a subset of rows rather than\\nthe entire dataset. Another strategy is to use PCA to reduce\\nthe number of dimensions, and then subject the PCAed dataset\\nto t-SNE.\\nAnomaly Detection\\nAnomaly detection is a branch of machine learning that seeks\\nto identify anomalies in datasets or data streams. Airbus\\nuses it to predict failures in jet engines and detect\\nanomalies in telemetry data beamed down from the\\nInternational Space Station. Credit card companies use it to'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 229, 'file_type': 'pdf'}, page_content='detect credit card fraud. The goal of anomaly detection is to\\nidentify outliers in data—samples that aren’t “normal”\\nwhen compared to others. In the case of credit card fraud,\\nthe assumption is that if transactions are subjected to an\\nanomaly detection algorithm, fraudulent transactions will\\nshow up as anomalous, while legitimate transactions will not.\\nThere are many ways to perform anomaly detection. They go by\\nnames such as isolation forests, one-class SVMs, and local\\noutlier factor (LOF). Most rely on unsupervised learning\\nmethods and therefore do not require labeled data. They\\nsimply look at a collection of samples and determine which\\nones are anomalous. Unsupervised anomaly detection is\\nparticularly interesting because it doesn’t require a priori\\nknowledge of what constitutes an anomaly, nor does it require\\nan unlabeled dataset to be meticulously labeled.\\nOne of the most popular forms of anomaly detection relies on\\nprincipal component analysis. You already know that PCA can\\nbe used to reduce data from m dimensions to n, and that a PCA\\ntransform can be inverted to restore the original m\\ndimensions. You also know that inverting the transform\\ndoesn’t recover the data that was lost when the transform\\nwas applied. The gist of PCA-based anomaly detection is that\\nan anomalous sample should exhibit more loss or\\nreconstruction error (the difference between the original\\ndata and the same data after a PCA transform is applied and\\ninverted) than a normal one. In other words, the loss\\nincurred when an anomalous sample is PCAed and un-PCAed\\nshould be higher than the loss incurred when the same\\noperation is applied to a normal sample. Let’s see if this\\nassumption holds up in the real world.\\nUsing PCA to Detect Credit Card Fraud\\nSupervised learning isn’t the only option for detecting\\ncredit card fraud. Here’s an alternative approach that uses\\nPCA-based anomaly detection to identify fraudulent'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 230, 'file_type': 'pdf'}, page_content='transactions. Begin by loading the dataset, separating the\\nsamples by class into one dataset representing legitimate\\ntransactions and another representing fraudulent\\ntransactions, and dropping the Time and Class columns. If you\\ndidn’t download the dataset in Chapter\\xa03, you can get it\\nnow from the ZIP file.\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv(\\'Data/creditcard.csv\\')\\ndf.head()\\n\\n# Separate the samples by class\\nlegit\\ndf[df[\\'Class\\']\\nfraud\\ndf[df[\\'Class\\']\\n\\n# Drop the \"Time\" and \"Class\" columns\\nlegit\\nlegit.drop([\\'Time\\',\\n\\'Class\\'],\\naxis=1)\\nfraud\\nfraud.drop([\\'Time\\',\\n\\'Class\\'],\\naxis=1)\\nUse PCA to reduce the two datasets from 29 to 26 dimensions,\\nand then invert the transform to restore each dataset to 29\\ndimensions. The transform is fitted to legitimate\\ntransactions only because we need a baseline value for\\nreconstruction error that allows us to discriminate between\\nlegitimate and fraudulent transactions. It is applied,\\nhowever, to both datasets:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\npca\\nPCA(n_components=26,\\nrandom_state=0)\\nlegit_pca\\npd.DataFrame(pca.fit_transform(legit),\\nindex=legit.index)\\nfraud_pca\\npd.DataFrame(pca.transform(fraud),\\nindex=fraud.index)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 231, 'file_type': 'pdf'}, page_content='legit_restored\\npd.DataFrame(pca.inverse_transform(legit_pca),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index=legit_pca.index)\\n\\nfraud_restored\\npd.DataFrame(pca.inverse_transform(fraud_pca),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index=fraud_pca.index)\\nSome information was lost in the transition. Hopefully, the\\nfraudulent transactions incurred more loss than the\\nlegitimate ones, and we can use that to differentiate between\\nthem. The next step is to compute the loss for each row in\\nthe two datasets by summing the squares of the differences\\nbetween the values in the original rows and the restored\\nrows:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\ndef\\nget_anomaly_scores(df_original,\\ndf_restored):\\n\\xa0\\xa0\\xa0 loss\\nnp.sum((np.array(df_original)\\nnp.array(df_restored))\\naxis=1)\\n\\xa0\\xa0\\xa0 loss\\npd.Series(data=loss,\\nindex=df_original.index)\\n\\xa0\\xa0\\xa0 return\\nloss\\n\\nlegit_scores\\nget_anomaly_scores(legit,\\nlegit_restored)\\nfraud_scores\\nget_anomaly_scores(fraud,\\nfraud_restored)\\nNow plot the losses incurred when the legitimate transactions\\nwere transformed and restored:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 232, 'file_type': 'pdf'}, page_content='legit_scores.plot(figsize\\nHere is the result:\\nNext, plot the losses for the fraudulent transactions:\\n\\nfraud_scores.plot(figsize\\nHere is the result:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 233, 'file_type': 'pdf'}, page_content=\"The plots reveal that most of the rows in the dataset\\nrepresenting legitimate transactions incurred a loss of less\\nthan 200, while many of the rows in the dataset representing\\nfraudulent transactions incurred a loss greater than 200.\\nSeparate the rows on this basis—classifying transactions\\nwith a loss of less than 200 as legitimate and transactions\\nwith a higher loss as fraudulent—and use a confusion matrix\\nto visualize the results:\\n\\nthreshold\\n\\ntrue_neg\\nlegit_scores[legit_scores\\nthreshold].count()\\nfalse_pos\\nlegit_scores[legit_scores\\nthreshold].count()\\ntrue_pos\\nfraud_scores[fraud_scores\\nthreshold].count()\\nfalse_neg\\nfraud_scores[fraud_scores\\nthreshold].count()\\n\\nlabels\\n['Legitimate',\\n'Fraudulent']\\nmat\\n[[true_neg,\\nfalse_pos],\\n[false_neg,\\ntrue_pos]]\\n\\nsns.heatmap(mat,\\nsquare=True,\\nannot=True,\\nfmt='d',\\ncbar=False,\\ncmap='Blues',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 xticklabels=labels,\\nyticklabels=labels)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 234, 'file_type': 'pdf'}, page_content=\"plt.xlabel('Predicted label')\\nplt.ylabel('True label')\\nHere is the result:\\nThe results aren’t quite as good as they were with the\\nrandom forest, but the model still caught about 50% of the\\nfraudulent transactions while mislabeling just 76 out of\\n284,315 legitimate transactions. That’s an error rate of\\nless than 0.03% for legitimate transactions, compared to\\n0.007% for the supervised learning model.\\nTwo parameters in this model drive the error rate: the number\\nof dimensions the datasets were reduced to (26), and the\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 235, 'file_type': 'pdf'}, page_content=\"threshold chosen to distinguish between legitimate and\\nfraudulent transactions (200). You can tweak the accuracy by\\nexperimenting with different values. I did some informal\\ntesting and concluded that this was a reasonable combination.\\nPicking a lower threshold improves the model’s ability to\\nidentify fraudulent transactions, but at the cost of\\nmisclassifying more legitimate transactions. In the end, you\\nhave to decide what error rate you’re willing to live with,\\nkeeping in mind that declining a legitimate credit card\\npurchase is likely to anger a customer.\\nUsing PCA to Predict Bearing Failure\\nOne of the classic uses for anomaly detection is to predict\\nfailures in rotating machinery. Let’s apply PCA-based\\nanomaly detection to a subset of a dataset published by NASA\\nto predict failures in bearings. The dataset contains\\nvibration data for four bearings supporting a rotating shaft\\nwith a radial load of 6,000 pounds applied to it. The\\nbearings were run to failure, and vibration data was captured\\nby high-sensitivity quartz accelerometers at regular\\nintervals until failure occurred.\\nFirst, download the CSV file containing the subset that I\\nculled from the larger NASA dataset. Then create a Jupyter\\nnotebook and load the data:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/bearings.csv',\\nindex_col=0,\\nparse_dates=[0])\\ndf.head()\\nHere are the first five rows in the dataset:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 236, 'file_type': 'pdf'}, page_content='The dataset contains 984 samples. Each sample contains\\nvibration data for four bearings, and the samples were taken\\n10 minutes apart. Plot the vibration data for all four\\nbearings as a time series:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\ndf.plot(figsize\\nHere is the output:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 237, 'file_type': 'pdf'}, page_content=\"About four days into the test, vibrations in bearing 1 began\\nincreasing. They spiked a day later, and about two days after\\nthat, bearing 1 suffered a catastrophic failure. Our goal is\\nto build a model that recognizes increased vibration in any\\nbearing as a sign of impending failure, and to do it without\\na labeled dataset.\\nThe next step is to extract samples representing “normal”\\noperation from the dataset (x_train in the following code)\\nand reduce four dimensions to one using PCA—essentially\\ncombining the data from all four bearings. Then apply the\\nsame PCA transform to the remainder of the dataset (x_test),\\ncombine the two partial datasets, and plot the result:\\n\\nfrom\\nsklearn.decomposition\\nimport\\nPCA\\n\\nx_train\\ndf['2004-02-12 10:32:39':'2004-02-13 23:42:39']\\nx_test\\ndf['2004-02-13 23:52:39':]\\n\\npca\\nPCA(n_components=1,\\nrandom_state=0)\\nx_train_pca\\npd.DataFrame(pca.fit_transform(x_train))\\nx_train_pca.index\\nx_train.index\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 238, 'file_type': 'pdf'}, page_content='x_test_pca\\npd.DataFrame(pca.transform(x_test))\\nx_test_pca.index\\nx_test.index\\n\\ndf_pca\\npd.concat([x_train_pca,\\nx_test_pca])\\ndf_pca.plot(figsize\\nplt.legend().remove()\\nThe output is shown here:\\nNow invert the PCA transform and plot the “restored”\\ndataset:\\n\\ndf_restored\\npd.DataFrame(pca.inverse_transform(df_pca),\\nindex=df_pca.index)\\ndf_restored.plot(figsize\\nThe results are as follows:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 239, 'file_type': 'pdf'}, page_content='It is obvious that a loss was incurred by applying and\\ninverting the transform. Let’s define a function that\\ncomputes the loss in a range of samples, then apply that\\nfunction to all of the samples in the original dataset and\\nthe restored dataset and plot the differences over time:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\ndef\\nget_anomaly_scores(df_original,\\ndf_restored):\\n\\xa0\\xa0\\xa0 loss\\nnp.sum((np.array(df_original)\\nnp.array(df_restored))\\naxis=1)\\n\\xa0\\xa0\\xa0 loss\\npd.Series(data=loss,\\nindex=df_original.index)\\n\\xa0\\xa0\\xa0 return\\nloss\\n\\nscores\\nget_anomaly_scores(df,\\ndf_restored)\\nscores.plot(figsize\\nHere is the output:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 240, 'file_type': 'pdf'}, page_content='The loss is very small when all four bearings are operating\\nnormally, but it begins to rise when one or more bearings\\nexhibit greater-than-normal vibration. From the chart, it’s\\napparent that when the loss rises above a threshold value of\\napproximately 0.002, that’s an indication a bearing might\\nfail.\\nNow that you’ve selected a tentative loss threshold, you can\\nuse it to detect anomalous behavior in the bearings. Begin by\\ndefining a function that takes a sample and returns True or\\nFalse indicating whether the sample is anomalous by applying\\nand inverting a PCA transform, measuring the loss for each\\nbearing, and comparing it to a specified loss threshold:\\n\\ndef\\nis_anomaly(row,\\npca,\\nthreshold):\\n\\xa0\\xa0\\xa0 pca_row\\npca.transform(row)\\n\\xa0\\xa0\\xa0 restored_row\\npca.inverse_transform(pca_row)\\n\\xa0\\xa0\\xa0 losses\\nnp.sum((row\\nrestored_row)\\n\\n\\xa0\\xa0\\xa0 for\\nloss\\nin\\nlosses:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nloss\\nthreshold:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\nTrue;'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 241, 'file_type': 'pdf'}, page_content=\"return\\nFalse\\nApply the function to a row early in the time series that\\nrepresents normal behavior and confirm that it returns False:\\n\\nx\\ndf.loc[['2004-02-16 22:52:39']]\\nis_anomaly(x,\\npca,\\n0.002)\\nApply the function to a row later in the time series that\\nrepresents anomalous behavior and confirm that it returns\\nTrue:\\n\\nx\\ndf.loc[['2004-02-18 22:52:39']]\\nis_anomaly(x,\\npca,\\n0.002)\\nNow apply the function to all the samples in the dataset and\\nshade anomalous samples red in order to visualize when\\nanomalous behavior is detected:\\n\\ndf.plot(figsize\\n\\nfor\\nindex,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 if\\nis_anomaly(pd.DataFrame([row]),\\npca,\\n0.002):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 plt.axvline(row.name,\\ncolor='r',\\nalpha=0.2)\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 242, 'file_type': 'pdf'}, page_content=\"Repeat this procedure, but this time use a loss threshold of\\n0.0002 rather than 0.002:\\n\\ndf.plot(figsize\\n\\nfor\\nindex,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 if\\nis_anomaly(pd.DataFrame([row]),\\npca,\\n0.0002):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 plt.axvline(row.name,\\ncolor='r',\\nalpha=0.2)\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 243, 'file_type': 'pdf'}, page_content='You can adjust the sensitivity of the model by adjusting the\\nthreshold value used to detect anomalies. Using a loss\\nthreshold of 0.002 predicts bearing failure about two days\\nbefore it occurs, while a loss threshold of 0.0002 predicts\\nthe failure about three days before. You typically want to\\nchoose a loss threshold that predicts failure as early as\\npossible without raising false alarms.\\nMultivariate Anomaly Detection\\nCould we have predicted failure in the preceding example by\\nsimply monitoring individual bearings? Perhaps. But what if\\nimpending failure is indicated by marginally elevated\\nvibrations in two bearings rather than just one? Engineers\\nfrequently find that it isn’t individual sensors but a\\ncombination of readings from several sensors that signal\\nimpending trouble. These readings may come from sensors of\\ndifferent types: temperature sensors and pressure gauges in\\nautomotive and aerospace applications, for example, or heart\\nmonitors and blood pressure monitors in health-care\\napplications. Reducing the number of dimensions to one with\\nPCA is an attempt to capture relationships between data\\nemanating from individual sensors and treat the readings'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 244, 'file_type': 'pdf'}, page_content='systemically, a technique known as multivariate anomaly\\ndetection.\\nOne limitation of using PCA to detect anomalies in\\nmultivariate systems is that because it uses linear\\ntransforms, PCA is better at modeling linear relationships\\nbetween variables than nonlinear relationships. Neural\\nnetworks, by contrast, excel at modeling nonlinear data.\\nThat’s the primary reason why state-of-the-art multivariate\\nanomaly detection today commonly relies on deep learning.\\nAs the number of variables increases, so too does the\\nchallenge of modeling the interdependencies between them. It\\nis not uncommon for overall system health to be determined by\\ndozens of otherwise independent variables. In September 2020,\\na team of researchers at Microsoft and Peking University\\npublished a paper titled “Multivariate Time-series Anomaly\\nDetection via Graph Attention Network” that proposed a novel\\narchitecture for multivariate anomaly detection. It combines\\ntwo deep-learning models: one that relies on prediction error\\nand another that relies on reconstruction error. Microsoft\\nuses this architecture in its Azure Multivariate Anomaly\\nDetector service, which can model dependencies between up to\\n300 independent data sources and is used by companies such as\\nAirbus and Siemens to detect irregularities in space-station\\ntelemetry and to test medical devices before they’re sent to\\nmarket. The Azure Multivariate Anomaly Detector service is\\npart of Azure Cognitive Services, which is covered in\\nChapter\\xa014.\\nSummary\\nPrincipal component analysis is a technique for reducing the\\nnumber of dimensions in a dataset without incurring a\\ncommensurate loss of information. It enjoys a number of uses\\nin machine learning, including visualizing high-dimensional\\ndata, anonymizing data, reducing noise, and increasing the\\nratio of rows to columns by reducing the number of'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 245, 'file_type': 'pdf'}, page_content='dimensions. It can also be used to perform anomaly detection\\nby measuring the loss incurred when a PCA transform is\\napplied and then inverted. Anomalous samples tend to incur\\nmore loss.\\nWhen I teach classes, I often introduce PCA as “the best-\\nkept secret in machine learning.” It shouldn’t remain a\\nsecret, because it’s an indispensable tool in the hands of\\nmachine learning engineers. Now that you know about it, I can\\njust about guarantee that you’ll find ways to put it to\\nwork.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 246, 'file_type': 'pdf'}, page_content='Chapter 7. Operationalizing\\nMachine Learning Models\\nAll of the machine learning models presented so far in this\\nbook are written in Python. Models don’t have to be written\\nin Python, but many are, thanks in part to the numerous\\nworld-class Python libraries available, including Pandas and\\nScikit-Learn. ML models written in Python are easily consumed\\nin Python apps. Calling them from other languages such as\\nC++, Java, and C# requires a little more work. You can’t\\nsimply call a Python function from C++ as if it were a C++\\nfunction. So how do you invoke models written in Python from\\napps written in other languages? Put another way, how do you\\noperationalize Python models such that they are usable in any\\napp on any platform written in any programming language?\\nThe diagram on the left in Figure\\xa07-1 shows one strategy:\\nwrap the model in a web service and expose its functionality\\nthrough a REST API. Then any client that can generate an\\nHTTP(S) request can invoke the model. It’s relatively easy\\nto do the wrapping with help from Python frameworks such as\\nFlask. The web service can be hosted locally or in the cloud,\\nand it can be containerized for easy deployment using tools\\nsuch as Docker.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 247, 'file_type': 'pdf'}, page_content='Figure 7-1. Architectures for consuming Python models in other languages\\nThe diagram on the right shows another strategy—one that’s\\nrelatively new but rapidly growing in popularity. It involves\\nexporting a Python model to a platform-agnostic format called\\nONNX, short for Open Neural Network Exchange, and then using\\nan ONNX runtime to load the model in Java, C++, C#, and other\\nprogramming languages. Once loaded, the model can be called\\nvia the ONNX runtime.\\nOf course, if the client app and the model are written in the\\nsame language, you need neither a web service nor ONNX. In\\nthis chapter, I’ll walk you through several scenarios:\\n\\nHow to save a trained Python model and invoke it\\nfrom a Python client\\nHow to invoke a Python model from a non-Python\\nclient using a web service\\nHow to containerize a Python model (and web\\nservice) for easy deployment\\nHow to use ONNX to invoke a Python model from\\nother programming languages\\nHow to write machine learning models in C# rather\\nthan Python'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 248, 'file_type': 'pdf'}, page_content=\"I’ll finish up by demonstrating a novel way to\\noperationalize a machine learning model by exposing its\\nfunctionality through Microsoft Excel. There’s a lot to\\ncover, so let’s get started.\\nConsuming a Python Model from a Python Client\\nOstensibly, invoking a Python model from a Python client is\\nsimple: just call predict (or, for a classifier,\\npredict_proba) on the model. Of course, you don’t want to\\nhave to retrain the model every time you use it. You want to\\ntrain it once, and then empower a client app to re-create the\\nmodel in its trained state. For that, Python programmers use\\nthe Python pickle module.\\nTo demonstrate, the following code builds and trains the\\nTitanic model featured in Chapter\\xa03. Rather than use the\\nmodel to make predictions, however, it saves the model to a\\n.pkl file (it “pickles” the model) with a call to\\npickle.dump on the final line:\\n\\nimport\\npickle\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\n\\ndf\\npd.read_csv('Data/titanic.csv')\\ndf\\ndf[['Survived',\\n'Age',\\n'Sex',\\n'Pclass']]\\ndf\\npd.get_dummies(df,\\ncolumns=['Sex',\\n'Pclass'])\\ndf.dropna(inplace=True)\\n\\nx\\ndf.drop('Survived',\\naxis=1)\\ny\\ndf['Survived']\\n\\nmodel\\nLogisticRegression(random_state=0)\\nmodel.fit(x,\\ny)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 249, 'file_type': 'pdf'}, page_content=\"pickle.dump(model,\\nopen('titanic.pkl',\\n'wb'))\\nTo invoke the model, a Python client uses pickle.load to\\ndeserialize the model from the .pkl file, re-creating the\\nmodel in its trained state, and calls predict_proba to\\ncompute the odds of a passenger’s survival:\\n\\nimport\\npickle\\nimport\\npandas\\nas\\npd\\n\\nmodel\\npickle.load(open('titanic.pkl',\\n'rb'))\\n\\nfemale\\npd.DataFrame({\\n'Age':\\n[30],\\n'Sex_female':\\n'Sex_male':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Pclass_1':\\n'Pclass_2':\\n'Pclass_3':\\n\\nprobability\\nmodel.predict_proba(female)[0][1]\\nprint(f'Probability of survival: {probability:.1%}')\\nNow the client can use the model to make a prediction without\\nretraining it. And once the model is loaded, it can persist\\nfor the lifetime of the client and be called upon for\\npredictions whenever needed.\\nChapter\\xa05 introduced Scikit’s make_pipeline function, which\\nallows estimators (objects that make predictions) and\\ntransformers (objects that transform data input to a model)\\nto be combined into a single unit, or pipeline. The pickle\\nmodule can be used to serialize and deserialize pipelines\\ntoo. Example\\xa07-1 recasts the sentiment analysis model\\nfeatured in Chapter\\xa04 to use make_pipeline to combine a\\nCountVec\\u2060tor\\u2060izer for vectorizing text with a LogisticRegression\\nobject for classifying text. The call to pickle.dump saves\\nthe model, CountVectorizer and all.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 250, 'file_type': 'pdf'}, page_content='Example 7-1. Training and saving a sentiment analysis\\npipeline\\n\\nimport\\npickle\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.feature_extraction.text\\nimport\\nCountVectorizer\\nfrom\\nsklearn.linear_model\\nimport\\nLogisticRegression\\nfrom\\nsklearn.pipeline\\nimport\\nmake_pipeline\\n\\ndf\\npd.read_csv(\\'Data/reviews.csv\\',\\nencoding=\"ISO-8859-1\")\\ndf\\ndf.drop_duplicates()\\n\\nx\\ndf[\\'Text\\']\\ny\\ndf[\\'Sentiment\\']\\n\\nvectorizer\\nCountVectorizer(ngram_range=(1,\\nstop_words=\\'english\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 min_df=20)\\n\\nmodel\\nLogisticRegression(max_iter=1000,\\nrandom_state=0)\\npipe\\nmake_pipeline(vectorizer,\\nmodel)\\npipe.fit(x,\\ny)\\n\\npickle.dump(pipe,\\nopen(\\'sentiment.pkl\\',\\n\\'wb\\'))\\nA Python client can deserialize the pipeline and call\\npredict_proba to score a line of text for sentiment with a\\nfew simple lines of code:\\n\\nimport\\npickle\\n\\npipe\\npickle.load(open(\\'sentiment.pkl\\',\\n\\'rb\\'))\\nscore\\npipe.predict_proba([\\'Great food and excellent service!\\'])[0][1]\\nprint(score)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 251, 'file_type': 'pdf'}, page_content=\"Pickling in this manner works not just with CountVectorizer\\nbut with other transformers as well, such as StandardScaler.\\nNOTE\\nPickling a pipeline containing a CountVectorizer can produce a\\nlarge .pkl file. In this example, sentiment.pkl is 50 MB in length\\nbecause it contains the entire vocabulary built by CountVectorizer\\nfrom the training text. Remove the min_df=20 parameter and the file\\nswells to nearly 90 MB.\\nThe solution is to replace CountVectorizer with Hashing\\u200bVec\\u2060tor\\u2060izer,\\nwhich doesn’t create a vocabulary but instead uses word hashes to\\nindex a table of word frequencies. That reduces sentiment.pkl to 8\\nMB—without the min_df parameter, which HashingVectorizer doesn’t\\nsupport anyway.\\nIf you’d like to write a standalone Python client that\\nperforms sentiment analysis, run the code in Example\\xa07-1 in\\na Jupyter notebook to generate sentiment.pkl. Optionally, you\\ncan change CountVectorizer to HashingVectorizer and remove the\\nmin_df parameter to reduce the size of the .pkl file. Then\\ncreate a Python script named sentiment.py containing the\\nfollowing code:\\n\\nimport\\npickle,\\nsys\\n\\n# Get the text to analyze\\nif\\nlen(sys.argv)\\n\\xa0\\xa0\\xa0 text\\nsys.argv[1]\\nelse:\\n\\xa0\\xa0\\xa0 text\\ninput('Text to analyze: ')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 252, 'file_type': 'pdf'}, page_content='# Load the pipeline containing the model and the vectorizer\\npipe\\npickle.load(open(\\'sentiment.pkl\\',\\n\\'rb\\'))\\n\\n# Pass the input text to the pipeline and print the result\\nscore\\npipe.predict_proba([text])[0][1]\\nprint(score)\\nCopy sentiment.pkl into the same directory as sentiment.py,\\nand then pop out to the command line and run the script:\\n\\npython\\nsentiment.py\\n\"Great food and excellent service!\"\\nThe output should look something like this, which is proof\\nthat you succeeded in re-creating the model in its trained\\nstate and invoking it to analyze the input text for\\nsentiment:\\nNote that the sentiment score will differ slightly if you\\nreplaced CountVectorizer with HashingVectorizer, in part due'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 253, 'file_type': 'pdf'}, page_content='to the omission of the min_df parameter.\\nVersioning Pickle Files\\nGenerally speaking, a model pickled (saved) with one version\\nof Scikit can’t be unpickled with another version. Sometimes\\nthe result is warning messages; other times, it doesn’t work\\nat all. Be sure to save models with the same version of\\nScikit that you use to consume them. This requires a bit of\\nplanning from an engineering perspective, because if you\\nstore serialized models in a centralized repository and\\nupdate the version of Scikit used in your apps, you’ll need\\nto update the saved models too.\\nWhich prompts some interesting questions: How do you set up a\\nrepository for machine learning models? How do you deploy\\nmodels from the repository to the devices that host them? For\\nthat matter, how do you version those models as well as the\\ndatasets you train them with?\\nThe answers come from a nascent field known as MLOps, which\\nis short for ML operations. I don’t cover MLOps in this book\\nbecause it’s a rich subject that is a book unto itself. If\\nyou want to learn more, I recommend reading Practical MLOps:\\nOperationalizing Machine Learning Models by Noah Gift and\\nAlfredo Deza (O’Reilly).\\nConsuming a Python Model from a C# Client\\nSuppose you wanted to invoke the sentiment analysis model in\\nthe previous section from an app written in another language\\n—say, C#. You can’t directly call a Python function from\\nC#, but you can wrap a Python model in a web service and\\nexpose its predict (or predict_proba) method using a REST\\nAPI. One way to code the web service is to use Flask, a\\npopular framework for building websites and web services in\\nPython.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 254, 'file_type': 'pdf'}, page_content=\"To see for yourself, make sure Flask is installed on your\\ncomputer. Then create a file named app.py and paste in the\\nfollowing code. This code uses Flask to implement a Python\\nweb service that listens on port 5000:\\n\\nimport\\npickle\\nfrom\\nflask\\nimport\\nFlask,\\nrequest\\n\\napp\\nFlask(__name__)\\npipe\\npickle.load(open('sentiment.pkl',\\n'rb'))\\n\\n@app.route('/analyze',\\nmethods=['GET'])\\ndef\\nanalyze():\\n\\xa0\\xa0\\xa0 if\\n'text'\\nin\\nrequest.args:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\nrequest.args.get('text')\\n\\xa0\\xa0\\xa0 else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n'No string to analyze'\\n\\n\\xa0\\xa0\\xa0 score\\npipe.predict_proba([text])[0][1]\\n\\xa0\\xa0\\xa0 return\\nstr(score)\\n\\nif __name__ ==\\n'__main__':\\n\\xa0\\xa0\\xa0 app.run(debug=True,\\nport=5000,\\nhost='0.0.0.0')\\nAt startup, the service deserializes the pipeline comprising\\nthe sentiment analysis model in sentiment.pkl. The @app.route\\nstatement decorating the analyze function tells Flask to call\\nthe function when the service’s analyze method is called. If\\nthe service is hosted locally, the following request invokes\\nanalyze and returns a string containing a sentiment score for\\nthe text in the query string:\\n\\nhttp://localhost:5000/analyze?text=Great food and excellent service!\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 255, 'file_type': 'pdf'}, page_content='To demonstrate, go to the directory where app.py is located\\n(make sure sentiment.pkl is there too) and start Flask by\\ntyping:\\n\\nflask run\\nThen go to a separate command prompt and use a curl command\\nto fire off a request to the URL:\\ncurl -G -w \"\\\\n\" http://localhost:5000/analyze --data-urlencode \"text=Great food\\nand excellent service!\"\\nHere’s the output:\\nIf you have Visual Studio or Visual Studio Code installed on\\nyour computer and are set up to compile and run C# apps, you\\ncan use the following code in a C# console app to invoke the\\nweb service and score a text string for sentiment. Of course,\\nyou’re not limited to invoking the web service (and by'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 256, 'file_type': 'pdf'}, page_content='extension, the model) from C#. Any language will do, because\\nvirtually all modern programming languages provide a means\\nfor sending HTTP requests.\\n\\nusing\\nSystem;\\nusing\\nSystem.Net.Http;\\nusing\\nSystem.Threading.Tasks;\\n\\nclass\\nProgram\\n\\xa0\\xa0\\xa0 static\\nasync\\nTask\\nMain(string[]\\nargs)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 string\\ntext;\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Get the text to analyze\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(args.Length\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\nargs[0];\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 else\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.Write(\"Text to analyze: \");\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\nConsole.ReadLine();\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Pass the text to the web service\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nclient\\nnew\\nHttpClient();\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nurl\\n$\"http://localhost:5000/analyze?text={text}\";\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nresponse\\nawait\\nclient.GetAsync(url);\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nscore\\nawait\\nresponse.Content.ReadAsStringAsync();\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Show the sentiment score\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.WriteLine(score);'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 257, 'file_type': 'pdf'}, page_content='This web service is a simple one that reads input from a\\nquery string and returns a string. For more complex input and\\noutput, you can serialize the input into JSON and transmit it\\nin the body of an HTTP POST, and you can return a JSON\\npayload in the response. For a tutorial demonstrating how to\\ndo it in Python, see the article “Python Post JSON Using\\nRequests Library”.\\nContainerizing a Machine Learning Model\\nOne downside to wrapping a machine learning model in a web\\nservice and running it locally is that the client computer\\nmust have Python installed, as well as all the packages that\\nthe model and web service require. An alternative is to host\\nthe web service in the cloud where it can be called via the\\ninternet. It’s not hard to go out to Azure or AWS, spin up a\\nvirtual machine (VM), and install the software there. But\\nthere’s a better way. That better way is containers.\\nContainers have revolutionized the way software is built and\\ndeployed. A container includes an app and everything the app\\nneeds to run, including a runtime (for example, Python), the\\npackages the app relies on, and even a virtual filesystem. If\\nyou’re not familiar with containers, think of them as\\nlightweight VMs that start quickly and consume far less\\nmemory. Docker is the world’s most popular container\\nplatform, although it is rapidly being supplanted by\\nKubernetes.\\nContainers are created from container images, which serve as\\nblueprints for containers in the same way that classes in\\nobject-oriented programming languages constitute blueprints\\nfor objects. The first step in creating a Docker container\\nimage containing the sentiment analysis model and web service\\nis to create a file named Dockerfile (no filename extension)\\nin the same directory as app.py and sentiment.pkl and then\\npaste the following statements into it:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 258, 'file_type': 'pdf'}, page_content='FROM\\npython:3.8\\nRUN\\npip\\ninstall\\nflask\\nnumpy\\nscipy\\nscikit-learn\\n\\xa0\\xa0\\xa0 mkdir\\n/app\\nCOPY\\napp.py\\n/app\\nCOPY\\nsentiment.pkl\\n/app\\nWORKDIR\\n/app\\nEXPOSE\\nENTRYPOINT\\n[\"python\"]\\nCMD\\n[\"app.py\"]\\nA Dockerfile contains instructions for building a container\\nimage. This one creates a container image that includes a\\nPython runtime, several Python packages such as Flask and\\nScikit-Learn, and app.py and sentiment.pkl. It also instructs\\nthe Docker runtime that hosts the container to open port 5000\\nfor HTTP requests and to execute app.py when the container\\nstarts.\\nThere are several ways to build a container image from a\\nDockerfile. If Docker is installed on your computer, you can\\nuse a docker build command:\\ndocker build -t sentiment-server .\\nAlternatively, you can upload the Dockerfile to a cloud\\nplatform such as Microsoft Azure and build it there. This\\nprevents you from having to have Docker installed on the\\nlocal machine, and it makes it easy to store the resulting\\ncontainer image in the cloud. Container images are stored in\\ncontainer registries, and modern cloud platforms host\\ncontainer registries as well as containers. If you launch a\\ncontainer instance in Azure, the web service in the container\\ncan be invoked with a URL similar to this one:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 259, 'file_type': 'pdf'}, page_content='http://wintellect.northcentralus.azurecontainer.io:5000/analyze?text=Great food\\nand excellent service!\\nOne of the benefits of hosting the container instance in the\\ncloud is that it can be reached from any client app running\\non any machine and any operating system, and the computer\\nthat hosts the app needs nothing special installed.\\nContainers can be beneficial even if you host the web service\\nlocally rather than in the cloud. As long as you deploy a\\ncontainer stack such as the Docker runtime to the local\\nmachine, you don’t have to install Python and all the\\npackages that the web service requires. You just launch a\\ncontainer instance and direct HTTP requests to it via\\nlocalhost.\\nUsing ONNX to Bridge the Language Gap\\nIs it possible to bridge the gap between a C# client and a\\nPython ML model without using a web service as a middleman?\\nIn a word, yes! The solution lies in a four-letter acronym:\\nONNX. As mentioned earlier, ONNX stands for Open Neural\\nNetwork Exchange, and it was originally devised to allow\\ndeep-learning models written with one framework—for example,\\nTensorFlow—to be used with other frameworks such as PyTorch.\\nBut today it can be used with Scikit models too. I’ll say\\nmore about ONNX in Chapter\\xa012, but for now, let’s use it to\\ncall a Python model directly from an app written in C#—no\\nweb service required.\\nThe first step in using ONNX to bridge the language gap is to\\ninstall Skl2onnx in your Python environment. Then use that\\npackage’s convert_sklearn method to save a trained Scikit\\nmodel to a .onnx file. Here’s a short code snippet that\\nsaves the sentiment analysis model in Example\\xa07-1 to a file\\nnamed sentiment.onnx. The initial_types parameter specifies'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 260, 'file_type': 'pdf'}, page_content=\"that the model expects one input value: a string containing\\nthe text to score for sentiment:\\n\\nfrom\\nskl2onnx\\nimport\\nconvert_sklearn\\nfrom\\nskl2onnx.common.data_types\\nimport\\nStringTensorType\\n\\ninitial_type\\n[('string_input',\\nStringTensorType([None,\\n1]))]\\nonnx\\nconvert_sklearn(pipe,\\ninitial_types=initial_type)\\n\\nwith\\nopen('sentiment.onnx',\\n'wb')\\nas\\nf:\\n\\xa0\\xa0\\xa0 f.write(onnx.SerializeToString())\\nIf you wish to consume the model from Python, you first\\ninstall a Python package named Onnxruntime containing the\\nONNX runtime, also known as the ORT. This provides support\\nfor loading and running ONNX models. Then you call the\\nruntime’s InferenceSession method with a path to a .onnx\\nfile to deserialize the model, and call run on the returned\\nsession object to call the model’s predict or predict_proba\\nmethod. Here’s how it looks in code:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\nonnxruntime\\nas\\nrt\\n\\nsession\\nrt.InferenceSession('sentiment.onnx')\\ninput_name\\nsession.get_inputs()[0].name\\nlabel_name\\nsession.get_outputs()[1].name\\n# 0 = predict, 1 = predict_proba\\n\\ninput\\nnp.array('Great food and excellent service!').reshape(1,\\nscore\\nsession.run([label_name],\\ninput_name:\\ninput\\n})[0][0][1]\\nprint(score)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 261, 'file_type': 'pdf'}, page_content='Note that the string input to the model is passed as a NumPy\\narray. The value returned from the run method reveals the\\nsentiment score returned by predict_proba. If you’d rather\\ncall predict to predict a class, change ses\\u2060sion\\u200b.get_outputs()\\n[1].name to session.get\\u200b_outputs()[0].name.\\nNOTE\\nIn real life, you wouldn’t load the model every time you call it.\\nYou’d load it once, allow it to persist for the lifetime of the\\nclient, and call it whenever you want to make a prediction.\\nOf course, the whole point of this discussion is to call the\\nmodel from C# instead of Python. That’s not difficult\\neither, thanks to a NuGet package from Microsoft called\\nMicrosoft.ML.OnnxRuntime. You can install it from the command\\nline or using an integrated development environment such as\\nVisual Studio. Then it’s a relatively simple matter to write\\na C# console app that re-creates the trained sentiment\\nanalysis model from the .onnx file and calls it to score a\\ntext string:\\n\\nusing\\nSystem;\\nusing\\nMicrosoft.ML.OnnxRuntime;\\nusing\\nMicrosoft.ML.OnnxRuntime.Tensors;\\nusing\\nSystem.Collections.Generic;\\nusing\\nSystem.Linq;\\n\\nclass\\nProgram\\n\\xa0\\xa0\\xa0 static\\nvoid\\nMain(string[]\\nargs)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 262, 'file_type': 'pdf'}, page_content='string\\ntext;\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Get the text to analyze\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(args.Length\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\nargs[0];\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 else\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.Write(\"Text to analyze: \");\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\nConsole.ReadLine();\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Create the model and pass the text to it\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\ntensor\\nnew\\nDenseTensor<string>(new\\nstring[]\\ntext\\nnew\\nint[]\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\ninput\\nnew\\nList<NamedOnnxValue>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 NamedOnnxValue.CreateFromTensor<string>(\"string_input\",\\ntensor)\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nsession\\nnew\\nInferenceSession(\"sentiment.onnx\");\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\noutput\\nsession.Run(input)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 .ToList().Last().AsEnumerable<NamedOnnxValue>();\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 var\\nscore\\noutput.First().AsDictionary<Int64,\\nfloat>()[1];\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 // Show the sentiment score\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Console.WriteLine(score);\\nThis code assumes that sentiment.onnx is present in the\\ncurrent directory. Instantiating the InferenceSession object'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 263, 'file_type': 'pdf'}, page_content=\"creates the model, and calling Run on the object invokes the\\nmodel. Under the hood, Run calls the model’s predict and\\npredict_proba methods and makes the results of both\\navailable. There’s a little work required to convert C#\\ntypes into ONNX types and vice versa, but once you get the\\nhang of it, it’s pretty remarkable that you can call a\\nmachine learning model written in Python directly from C#.\\nHere’s another example. Suppose you wanted to consume\\nChapter\\xa02’s taxi-fare regression model in C#. Recall that\\nthe model accepts three floating-point values as input—the\\nday of the week (0–6), the hour of day (0–23), and the\\ndistance to travel in miles—and returns a predicted taxi\\nfare. The following Python code saves the trained model in an\\nONNX file:\\n\\nfrom\\nskl2onnx\\nimport\\nconvert_sklearn\\nfrom\\nskl2onnx.common.data_types\\nimport\\nFloatTensorType\\n\\ninitial_type\\n[('float_input',\\nFloatTensorType([None,\\n3]))]\\nonnx\\nconvert_sklearn(model,\\ninitial_types=initial_type)\\n\\nwith\\nopen('taxi.onnx',\\n'wb')\\nas\\nf:\\n\\xa0\\xa0\\xa0 f.write(onnx.SerializeToString())\\nThe following C# code loads the model and makes a prediction.\\nThe big difference between this and the previous example is\\nthat you pass the model an array of three floating-point\\nvalues rather than a string:\\n\\n// Package the input\\nvar\\ninput\\nnew\\nfloat[]\\n\\xa0\\xa0\\xa0 4.0f,\\xa0 // Day of week\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 264, 'file_type': 'pdf'}, page_content='17.0f,\\n// Pickup time (hour of day)\\n\\xa0\\xa0\\xa0 2.0f\\xa0\\xa0 // Distance to travel\\n\\nvar\\ntensor\\nnew\\nDenseTensor<float>(input,\\nnew\\nint[]\\n\\n// Create the model and pass the input to it\\nvar\\nsession\\nnew\\nInferenceSession(\"taxi.onnx\");\\n\\nvar\\noutput\\nsession.Run(new\\nList<NamedOnnxValue>\\n\\xa0\\xa0\\xa0 NamedOnnxValue.CreateFromTensor(\"float_input\",\\ntensor)\\n\\nvar\\nscore\\noutput.First().AsTensor<float>().First();\\n\\n// Show the predicted fare\\nConsole.WriteLine($\"{score:#.00}\");\\nSo which is faster? Calling a machine learning model wrapped\\nin a web service, or calling the same model using ONNX? I\\nwrote a simple test harness to answer that question. On my\\ncomputer, calling the sentiment analysis model in a Flask web\\nservice running locally required slightly more than 2 seconds\\nper round trip. Calling the same model through the Python\\nONNX runtime took 0.001 seconds on average. That’s a\\ndifference of more than three orders of magnitude. And you\\nwould incur additional latency if the web service was hosted\\non a remote server.\\nSignificantly, ONNX isn’t limited to C# and Python. ONNX\\nruntimes are available for Python, C, C++, C#, Java,\\nJavaScript, and Objective-C, and they run on Windows, Linux,\\nmacOS, Android, and iOS. When it comes to projecting machine\\nlearning models written in Python to other platforms and\\nlanguages, ONNX is a game changer. For more information,\\ncheck out the ONNX runtime website.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 265, 'file_type': 'pdf'}, page_content='Building ML Models in C# with ML.NET\\nScikit-Learn is arguably the world’s most popular machine\\nlearning framework. The efficacy of the library, the\\ndocumentation that accompanies it, and the mindshare that\\nsurrounds it are the primary reasons more ML models are\\nwritten in Python than any other language. But Scikit isn’t\\nthe only machine learning framework. Others exist for other\\nlanguages, and if you can code an ML model in the same\\nprogramming language as the client that consumes it, you can\\navoid jumping through hoops to operationalize the model.\\nML.NET is Microsoft’s free, open source, cross-platform\\nmachine learning library for .NET developers. It does most of\\nwhat Scikit does and a few things that Scikit doesn’t do.\\nAnd when it comes to writing ML/AI solutions in C#, there is\\nno better tool for the job.\\nML.NET derives from an internal library that was developed by\\nMicrosoft—and has been used in Microsoft products—for more\\nthan a decade. The ML algorithms that it implements have been\\ntried and tested in the real world and tuned to optimize\\nperformance and accuracy. Because ML.NET is consumed from C#,\\nyou get all the benefits of a compiled programming language,\\nincluding type safety and fast execution.\\nA paper published by the ML.NET team at Microsoft in 2019\\ndiscusses the motivations and design goals behind ML.NET. It\\nalso compares ML.NET’s accuracy and performance to that of\\nScikit-Learn and another machine learning framework named\\nH2O. Using a 9 GB Amazon review dataset, ML.NET trained a\\nsentiment analysis model to 95% accuracy. Neither Scikit nor\\nH2O could process the dataset due to its size. When all three\\nframeworks were trained on 10% of the dataset, ML.NET\\nachieved the highest accuracy, and trained six times faster\\nthan Scikit and almost 10 times faster than H2O.\\nML.NET is compatible with Windows, Linux, and macOS. Thanks\\nto an innovation called IDataView, it can handle datasets of\\nvirtually unlimited size. While it can’t be used to build'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 266, 'file_type': 'pdf'}, page_content='neural networks from scratch, it does have the ability to\\nload existing neural networks and use a technique called\\ntransfer learning to repurpose those networks to solve\\ndomain-specific problems. (Transfer learning will be covered\\nin Chapter\\xa010.) It also builds in ONNX support that allows\\nit to load sophisticated models, including deep-learning\\nmodels, stored in ONNX format. Finally, it can be consumed in\\nPython and even combined with Scikit-Learn using a set of\\nPython bindings called NimbusML.\\nIf you’re a .NET developer who is interested in machine\\nlearning, there has never been a better time to get\\nacquainted with ML.NET. This section isn’t meant to provide\\nan exhaustive treatment of ML.NET but to introduce it, show\\nthe basics of building ML models with it, and hopefully whet\\nyour appetite enough to motivate you to learn more. There are\\nplenty of great resources available online, including the\\nofficial ML.NET documentation, a GitHub repo containing\\nML.NET samples, and the ML.NET cookbook.\\nSentiment Analysis with ML.NET\\nThe following C# code uses ML.NET to build and train a\\nsentiment analysis model. It’s equivalent to the Python\\nimplementation in Example\\xa07-1:\\n\\nvar\\ncontext\\nnew\\nMLContext(seed:\\n\\n// Load the data\\nvar\\ndata\\ncontext.Data.LoadFromTextFile<Input>(\"reviews.csv\",\\n\\xa0\\xa0\\xa0 hasHeader:\\ntrue,\\nseparatorChar:\\nallowQuoting:\\ntrue);\\n\\n// Split the data into a training set and a test set\\nvar\\ntrainTestData\\ncontext.Data.TrainTestSplit(data,\\n\\xa0\\xa0\\xa0 testFraction:\\nseed:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 267, 'file_type': 'pdf'}, page_content='var\\ntrainData\\ntrainTestData.TrainSet;\\nvar\\ntestData\\ntrainTestData.TestSet;\\n\\n// Build and train the model\\nvar\\npipeline\\ncontext.Transforms.Text.FeaturizeText\\n\\xa0\\xa0\\xa0 (outputColumnName:\\n\"Features\",\\ninputColumnName:\\n\"Text\")\\n\\xa0\\xa0\\xa0 .Append(context.BinaryClassification.Trainers.SdcaLogisticRegression());\\n\\nvar\\nmodel\\npipeline.Fit(trainData);\\n\\n// Evaluate the model\\nvar\\npredictions\\nmodel.Transform(testData);\\nvar\\nmetrics\\ncontext.BinaryClassification.Evaluate(predictions);\\nConsole.WriteLine($\"AUC: {metrics.AreaUnderPrecisionRecallCurve:P2}\");\\n\\n// Score a line of text for sentiment\\nvar\\npredictor\\ncontext.Model.CreatePredictionEngine<Input,\\nOutput>(model);\\nvar\\ninput\\nnew\\nInput\\nText\\n\"Among the best movies I have ever seen\"};\\nvar\\nprediction\\npredictor.Predict(input);\\n\\n// Show the score\\nConsole.WriteLine($\"Sentiment score: {prediction.Probability}\");\\nEvery ML.NET app begins by creating an instance of the\\nMLContext class. The optional seed parameter initializes the\\nrandom-number generator used by ML.NET so that you get\\nrepeatable results from one run to the next. MLContext\\nexposes a number of properties through which large parts of\\nthe ML.NET API are accessed. One example of this is the call\\nto LoadFromTextFile, which is a DataOperationsCatalog method\\naccessed through MLContext’s Data property.\\nLoadFromTextFile is one of several methods ML.NET provides\\nfor loading data from text files, databases, and other data\\nsources. It returns a data view, which is an object that\\nimplements the IDataView interface. Data views in ML.NET are\\nsimilar to DataFrames in Pandas, with one important'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 268, 'file_type': 'pdf'}, page_content='difference: whereas DataFrames have to fit in memory, data\\nviews do not. Internally, data views use a SQL-like cursor to\\naccess data. This means they can wrap a theoretically\\nunlimited amount of data. That’s why ML.NET was able to\\nprocess the entire 9 GB Amazon dataset, while Scikit and H2O\\nwere not.\\nAfter loading the data and splitting it for training and\\ntesting, the preceding code creates a pipeline containing a\\nTextFeaturizingEstimator object—created with the\\nFeaturizeText method—and an\\nSdcaLogisticRegressionBinaryTrainer object—created with the\\nSdca\\u200bLo\\u2060gisticRegression method. This is analogous in Scikit to\\ncreating a pipeline containing a CountVectorizer object for\\nvectorizing input text and a LogisticRegression object for\\nfitting a model to the data. Calling Fit on the pipeline\\ntrains the model, just like calling fit in Scikit. It’s no\\ncoincidence that ML.NET employs some of the same patterns as\\nScikit. This was done intentionally to impart a sense of\\nfamiliarity to programmers who use Scikit.\\nAfter evaluating the model’s accuracy by computing the area\\nunder the precision-recall curve, a call to\\nModelOperationsCatalog.CreatePredictionEngine creates a\\nprediction engine whose Predict method makes predictions.\\nUnlike Scikit, which has you call predict on the estimator\\nitself, ML.NET encapsulates prediction capability in a\\nseparate object, in part so that multiple prediction engines\\ncan be created to achieve scalability in high-traffic\\nscenarios.\\nIn this example, Predict accepts an Input object as input and\\nreturns an Output object. One of the benefits of building\\nmodels with ML.NET is strong typing. LoadFromTextFile is a\\ngeneric method that accepts a class name as a type parameter\\n—in this case, Input. Similarly, CreatePredictionEngine uses\\ntype parameters to specify schemas for input and output. The\\nInput and Output classes are application specific and in this\\ninstance are defined as follows:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 269, 'file_type': 'pdf'}, page_content='public\\nclass\\nInput\\n\\xa0\\xa0\\xa0 [LoadColumn(0)]\\n\\xa0\\xa0\\xa0 public\\nstring\\nText;\\n\\n\\xa0\\xa0\\xa0 [LoadColumn(1), ColumnName(\"Label\")]\\n\\xa0\\xa0\\xa0 public\\nbool\\nSentiment;\\n\\npublic\\nclass\\nOutput\\n\\xa0\\xa0\\xa0 [ColumnName(\"PredictedLabel\")]\\n\\xa0\\xa0\\xa0 public\\nbool\\nPrediction\\nget;\\nset;\\n\\n\\xa0\\xa0\\xa0 public\\nfloat\\nProbability\\nget;\\nset;\\nThe LoadColumn attributes map columns in the data file to\\nproperties in the Input class. Here, they tell ML.NET that\\nvalues for the Text field come from column 0 in the input\\nfile, and values for Sentiment (the 1s and 0s indicating\\nwhether the sentiment expressed in the text is positive or\\nnegative) come from column 1. The Colum\\u2060n\\u200bName(\"Label\")\\nattribute identifies the second column as the label column—\\nthe one containing the values that the model will attempt to\\npredict.\\nThe Output class defines the output schema. In this example,\\nit contains properties named Prediction and Probability,\\nwhich, following a prediction, hold the predicted label (0 or\\n1) and the probability that the sample belongs to the\\npositive class, which doubles as a sentiment score. The\\nColumnName(\"PredictedLabel\") attribute maps the value returned\\nby Predict to the Output object’s Prediction property.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 270, 'file_type': 'pdf'}, page_content='NOTE\\nThere is nothing magic about the class names Input and Output. You\\ncould name them SentimentData and SentimentPrediction and the code\\nwould work just the same.\\nSaving and Loading ML.NET Models\\nEarlier, you learned how to use Python’s pickle module to\\nsave and load trained models. You do the same in ML.NET by\\ncalling ModelOperationsCatalog.Save and\\nModelOperationsCatalog.Load through the MLContext object’s\\nModel property:\\n\\nSave\\na\\ntrained\\nmodel\\nto\\na\\nzip\\nfile\\ncontext.Model.Save(model,\\ndata.Schema,\\n\"model.zip\");\\n\\nLoad\\na\\ntrained\\nmodel\\nfrom\\na\\nzip\\nfile\\nvar\\nmodel\\ncontext.Model.Load(\"model.zip\",\\nout\\nDataViewSchema\\nschema);\\nThis enables clients to re-create a model in its trained\\nstate and use it to make predictions without having to train\\nthe model repeatedly.\\nAdding Machine Learning Capabilities to Excel\\nWant to see a novel way to operationalize a machine learning\\nmodel? Imagine that you’re a software developer at an\\ninternet vacation rentals firm. The company’s communications\\ndepartment has asked you to create a spreadsheet that lets\\nthem analyze text for sentiment. The idea is that if'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 271, 'file_type': 'pdf'}, page_content='sentiment toward the company turns negative on social media,\\nthe communications team can get out in front of it.\\nYou already know how to build, train, and save a sentiment\\nanalysis model in Python using Scikit. Excel supports user-\\ndefined functions (UDFs), which enable users to write custom\\nfunctions that are called just like SUM(), AVG(), and other\\nfunctions built into Excel. But UDFs are written in Visual\\nBasic for Applications (VBA), not Python. To marry Scikit\\nwith Excel, you need to write UDFs in Python.\\nFortunately, there are libraries that let you do just that.\\nOne of them is Xlwings, an open source library that combines\\nthe power of Excel with the versatility of Python. With it,\\nyou can write Python code that loads or creates Excel\\nspreadsheets and manipulates their content, write Python\\nmacros triggered by button clicks in Excel, access Excel\\nspreadsheets from Jupyter notebooks, and more. You can also\\nuse Xlwings to write Python UDFs for Excel for Windows.\\nThe first step in building the spreadsheet the communications\\nteam wants is to configure Excel to trust VBA add-ins. Launch\\nMicrosoft Excel and use the File → Options command to open\\nthe Excel Options dialog. Click Trust Center in the menu on\\nthe left, and then click the Trust Center Settings button.\\nClick Macro Settings on the left, and check the “Trust\\naccess to the VBA project object model” box, as shown in\\nFigure\\xa07-2. Then click OK to dismiss the Trust Center\\ndialog, followed by OK to dismiss the Excel Options dialog.\\nThe next step is to install Xlwings on your computer using a\\npip install xlwings command or equivalent for your Python\\nenvironment. Afterward, go to the command line and use the\\nfollowing command to install the Xlwings add-in in Excel:\\n\\nxlwings addin install'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 272, 'file_type': 'pdf'}, page_content='Now create a project directory on your computer and cd to it.\\nThen execute the following command:\\n\\nxlwings quickstart sentiment\\nFigure 7-2. Configuring Excel to trust VBA add-ins\\nThis command creates a subdirectory named sentiment in the\\ncurrent directory and initializes it with a pair of files: a\\nspreadsheet named sentiment.xlsm and a Python file named\\nsentiment.py. It is the latter of these in which you will\\nwrite a UDF that analyzes text for sentiment.\\nNext, copy sentiment.pkl into the sentiment directory. (If\\nyou didn’t generate that file earlier, run the code in\\nExample\\xa07-1 in a Jupyter notebook to generate it now.) Then\\nopen sentiment.py in your favorite text editor and replace\\nits contents with the following code. This code loads the\\nsentiment analysis model from the .pkl file and stores a\\nreference to the model in the variable named model. Then it'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 273, 'file_type': 'pdf'}, page_content=\"defines a UDF named analyze_text that can be called from\\nExcel. analyze_text vectorizes the text passed to it and\\ninputs it to the model’s predict_proba method. That method\\nreturns a number from 0.0 to 1.0, with 0.0 representing\\nnegative sentiment and 1.0 representing positive sentiment.\\nWhen you’re done, save your changes to sentiment.py. The UDF\\nis written. Now it’s time to call it from Excel.\\n\\nimport\\npickle,\\nos\\nimport\\nxlwings\\nas\\nxw\\n\\n# Load the model and the vocabulary and create a CountVectorizer\\nmodel_path\\nos.path.abspath(os.path.join(os.path.dirname(__file__),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'sentiment.pkl'))\\n\\nmodel\\npickle.load(open(model_path,\\n'rb'))\\n\\n@xw.func\\ndef\\nanalyze_text(text):\\n\\xa0\\xa0\\xa0 score\\nmodel.predict_proba([text])[0][1]\\n\\xa0\\xa0\\xa0 return\\nscore\\nThe elegance of Xlwings is that once you’ve written a UDF\\nsuch as analyze_text, you can call it the same way you call\\nfunctions built into Excel. But first you must import the\\nUDF. To do that, open sentiment.xlsm in Excel. Go to the\\n“xlwings” tab and click Import Functions to import the\\nanalyze_text function, as shown in Figure\\xa07-3. Excel\\ndoesn’t tell you if the import was successful, but it does\\nlet you know if the import failed—if, for example, you\\nforgot to copy sentiment.pkl into the directory where\\nsentiment.py is stored.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 274, 'file_type': 'pdf'}, page_content='Figure 7-3. Importing the analyze_text function\\nType Great food and excellent service into cell A1 (Figure\\xa07-\\nFigure 7-4. Entering a string of text to analyze\\nType the following expression into cell B1 to pass the text\\nin cell A1 to the analyze_text function imported from\\nsentiment.py:\\n\\n=analyze_text(A1)\\nConfirm that a number from 0.0 to 1.0 appears in cell B1, as\\nshown in Figure\\xa07-5. This is the score that the machine'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 275, 'file_type': 'pdf'}, page_content='learning model assigned to the text “Great food and\\nexcellent service.” Do you agree with the score? Finish up\\nby entering some text strings of your own to see how they\\nscore for sentiment.\\nFigure 7-5. Sentiment analysis in Excel\\nUDFs written in Python present Excel users with a new whole\\nnew world of possibilities thanks to the rich ecosystem of\\nPython libraries available for machine learning, statistical\\nanalysis, and other tasks. And they provide a valuable\\nopportunity for Excel users to operationalize machine\\nlearning models written in Python.\\nSummary\\nWriting a Python client that invokes a Python machine\\nlearning model requires little more than an extra line of\\ncode to deserialize the model from a .pkl file. One way for a\\nnon-Python client to invoke a Python model is to wrap the\\nmodel in a Python web service and invoke the model using REST\\nAPIs. The web service can be hosted locally or in the cloud,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 276, 'file_type': 'pdf'}, page_content='and containerizing the web service (and the model) simplifies\\ndeployment and makes the software more portable.\\nAn alternative approach is to use ONNX to bridge the language\\ngap. With ONNX, you can save a Scikit model to a .onnx file\\nand load the model from a variety of programming languages,\\nincluding C, C++, C#, Java, and JavaScript. Once loaded, the\\nmodel can be invoked just as if it were called from Python.\\nAnother option for invoking machine learning models from non-\\nPython clients is to write the model in the same language as\\nthe client. Microsoft’s ML.NET, which is free, cross-\\nplatform, and open source, is a great option for C#\\ndevelopers. Other libraries include Java-ML for Java\\ndevelopers and Caret and Tidymodels for R developers. The\\nAPIs supported by these libraries are different from the APIs\\nin Scikit, but the principles embodied in them are the same.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 277, 'file_type': 'pdf'}, page_content='Part II. Deep Learning with\\nKeras and TensorFlow'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 278, 'file_type': 'pdf'}, page_content='Chapter 8. Deep Learning\\nEvery model in Part\\xa0I\\xa0of this book employed classic machine\\nlearning algorithms that form the core of ML itself: logistic\\nregression, random forests, and so on. Such models are often\\nreferred to as traditional machine learning models to\\ndifferentiate them from deep-learning models. Recall from\\nChapter\\xa01 that deep learning is a subset of machine learning\\nthat relies primarily on neural networks, and that most of\\nwhat’s considered AI today is accomplished with deep\\nlearning. From recognizing objects in photos to real-time\\nspeech translation to using computers to generate art, music,\\npoetry, and photorealistic faces, deep learning allows\\ncomputers to perform feats that traditional machine learning\\ndoes not.\\nI frequently introduce deep learning to software developers\\nby challenging them to devise an algorithmic means for\\ndetermining whether a photo contains a dog. If they offer a\\nsolution, I’ll counter with a dog picture that foils the\\nalgorithm. Traditional ML models can partially solve the\\nproblem, but when it comes to recognizing objects in images,\\ndeep learning represents the state of the art. It’s not\\nterribly difficult to train a neural network to recognize dog\\npictures, sometimes more accurately than humans. Once you\\nlearn how to do that, it’s a small step forward to\\nrecognizing defective parts coming off an assembly line or\\nbicycles passing in front of a self-driving car.\\nNeural networks have been around for decades, but it’s only\\nin the past 10 years or so that sufficient compute power has\\nbeen available to train sophisticated networks. Cutting-edge\\nneural networks are trained on graphics processing units\\n(GPUs) and tensor processing units (TPUs), often attached to\\nhigh-performance computing clusters. GPUs are great for'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 279, 'file_type': 'pdf'}, page_content='gaming because they deliver high-performance graphics. They\\nare also efficient parallel processing machines that allow\\ndata scientists to train neural networks in a fraction of the\\ntime required on ordinary CPUs. Today, any researcher with a\\ncredit card can purchase an NVIDIA GPU or spin up GPUs in\\nAzure or AWS and have access to compute power that\\nresearchers 20 years ago could only have dreamed of. This,\\nmore than anything else, has driven AI’s resurgence and\\nprecipitated continual advances in the state of the art.\\nThis chapter is the first of several focused on deep\\nlearning. In it, you’ll learn:\\n\\nWhat a neural network is and where the “deep”\\nin deep learning comes from\\nHow a neural network transforms input into output\\nusing simple mathematical operations\\nWhat happens when a neural network is trained, as\\nwell as the challenges that training entails\\nYou won’t start building and training neural networks just\\nyet; that begins in Chapter\\xa09. Before you build a house, you\\nneed a foundation to build upon. That foundation begins right\\nnow.\\nUnderstanding Neural Networks\\nNeural networks come in many varieties. Convolutional neural\\nnetworks (CNNs), for example, excel at computer-vision tasks\\nsuch as classifying images. Recurrent neural networks (RNNs)\\nfind application in handwriting recognition and natural\\nlanguage processing (NLP), while generative adversarial\\nnetworks, or GANs, enable computers to create art, music, and\\nother content. But the first step in wrapping your head\\naround deep learning is to understand what a neural network\\nis and how it works.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 280, 'file_type': 'pdf'}, page_content='The simplest type of neural network is the multilayer\\nperceptron. It consists of nodes or neurons arranged in\\nlayers. The depth of the network is the number of layers; the\\nwidth is the number of neurons in each layer, which can be\\ndifferent for every layer. State-of-the-art neural networks\\nsometimes contain 100 or more layers and thousands of neurons\\nin individual layers. A deep neural network is one that\\ncontains many layers, and it’s where the term deep learning\\nis derived from.\\nThe multilayer perceptron in Figure\\xa08-1 contains three\\nlayers: an input layer with two neurons, a middle layer (also\\nknown as a hidden layer) with three neurons, and an output\\nlayer with one neuron. Because the input layer is often\\nignored when counting layers, some would argue that this\\nnetwork contains two layers, not three. Regardless, the\\nnetwork’s job is to take two floating-point values as input\\nand produce a single floating-point number as output. Neural\\nnetworks work with floating-point numbers. They only work\\nwith floating-point numbers. As with traditional machine\\nlearning models, a neural network can only process non-\\nnumeric data—for example, text strings—if the data is first\\nconverted to numbers.\\nFigure 8-1. Multilayer perceptron\\nThe orange arrows in Figure\\xa08-1 represent connections\\nbetween neurons. Each neuron in each layer is connected to\\neach neuron in the next layer, giving rise to the term fully'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 281, 'file_type': 'pdf'}, page_content='connected layers. Each connection is assigned a weight, which\\nis typically a small floating-point number. In addition, each\\nneuron outside the input layer is assigned a bias, which is\\nalso a small floating-point number. Figure\\xa08-2 shows a set\\nof weights and biases that enable the network to sum two\\ninputs (for example, to add 2 and 2). The blocks labeled\\n“ReLU” represent activation functions, which apply simple\\nnonlinear transforms to values propagated through the\\nnetwork. The most commonly used activation function is the\\nrectified linear units (ReLU) function, which passes positive\\nnumbers through unchanged while converting negative numbers\\nto 0s. Without activation functions, neural networks would\\nstruggle to model nonlinear data. And it’s no secret that\\nreal-world data tends to be nonlinear.\\nFigure 8-2. Weights and biases\\nNeurons perform simple linear transformations on data input\\nto them. For a neuron with a single input x, the neuron’s\\nvalue y is computed by multiplying x by the weight m assigned\\nto the input and adding b, the neuron’s bias:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 282, 'file_type': 'pdf'}, page_content='Look familiar? That’s the equation for linear regression.\\nScikit-Learn has a Perceptron class that models this behavior\\nand can be used to build neural linear regression models. It\\neven offers classes named MLPRegressor and MLPClassifier for\\nbuilding simple multilayer perceptrons. Scikit is not,\\nhowever, a deep-learning library. Real deep-learning\\nlibraries do more to support advanced neural networks.\\nNOTE\\nThe combination of neurons that perform linear transformations and\\nactivation functions that apply nonlinear transforms is an\\nembodiment of the universal approximation theorem, which states\\nthat you can approximate any function f by summing the output from\\nlinear functions and transforming it with a nonlinear function.\\nTextbooks often say that activation functions “add nonlinearity”\\nto neural networks. Now you know why.\\nTo turn inputs into outputs, a neural network assigns the\\ninput values to the neurons in the input layer. Then it\\nmultiplies the values of the input neurons by the weights\\nconnecting them to the neurons in the next layer, sums the\\ninputs for each neuron, and adds the biases. It repeats this\\nprocess to propagate values from left to right all the way to'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 283, 'file_type': 'pdf'}, page_content='the output layer. Figure\\xa08-3 shows what happens in the first\\ntwo layers when the network in Figure\\xa08-2 adds 2 and 2.\\nFigure 8-3. Flow of data from the input layer to the hidden layer when\\nadding 2 and 2\\nValues propagate from the hidden layer to the output layer\\nthe same way, with one exception: they are transformed by an\\nactivation function before they’re multiplied by weights.\\nRemember that the ReLU activation function turns negative\\nnumbers into 0s. In Figure\\xa08-4, the –1.83 calculated for\\nthe middle neuron in the hidden layer is converted to 0 when\\nforwarded to the output layer, effectively eliminating that\\nneuron’s contribution to the output.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 284, 'file_type': 'pdf'}, page_content='Figure 8-4. Flow of data from the hidden layer to the output layer when\\nadding 2 and 2\\nGiven a set of weights and biases, it isn’t difficult to\\ncode a neural network by hand. The following Python code\\nmodels the network in Figure\\xa08-2:\\n\\n# Weights\\nw0\\n0.9907079\\nw1\\n1.0264927\\nw2\\n0.01417504\\nw3\\n-0.8950311\\nw4\\n0.88046944\\nw5\\n0.7524377\\nw6\\n0.794296\\nw7\\n1.1687347\\nw8\\n0.2406084\\n\\n# Biases\\nb0\\n-0.00070612\\nb1\\n-0.06846002'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 285, 'file_type': 'pdf'}, page_content='b2\\n-0.00055442\\nb3\\n-0.00000929\\n\\ndef\\nrelu(x):\\n\\xa0\\xa0\\xa0 return\\nmax(0,\\nx)\\n\\ndef\\npredict(x1,\\nx2):\\n\\xa0\\xa0\\xa0 h1\\n(x1\\nw0)\\n(x2\\nw1)\\nb0\\n\\xa0\\xa0\\xa0 h2\\n(x1\\nw2)\\n(x2\\nw3)\\nb1\\n\\xa0\\xa0\\xa0 h3\\n(x1\\nw4)\\n(x2\\nw5)\\nb2\\n\\xa0\\xa0\\xa0 y\\n(relu(h1)\\nw6)\\n(relu(h2)\\nw7)\\n(relu(h3)\\nw8)\\nb3\\n\\xa0\\xa0\\xa0 return\\ny\\nIf you’d like to see for yourself, paste the code into a\\nJupyter notebook and call the predict function with the\\ninputs 2 and 2. The answer should be very close to the actual\\nsum of 2 and 2.\\nFor a given problem, there is an infinite combination of\\nweights and biases that produces the desired outcome.\\nFigure\\xa08-5 shows the same network with a completely\\ndifferent set of weights and biases. Yet, if you plug the\\nvalues into the preceding code (or propagate values through\\nthe network by hand), you’ll find that the network is\\nequally capable of adding 2 and 2—or other small values, for\\nthat matter.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 286, 'file_type': 'pdf'}, page_content='Figure 8-5. Adding 2 and 2 with a different set of weights and biases\\nGiven a set of weights and biases, using a neural network to\\nmake predictions is simplicity itself. It’s little more than\\nmultiplication and addition. But coming up with a set of\\nweights and biases to begin with is a challenge. It’s why\\nneural networks must be trained.\\nTraining Neural Networks\\nTraining a traditional machine learning model fits it to a\\ndataset. Neural networks require training too, and it is\\nduring training that weights and biases are calculated.\\nWeights are typically initialized with small random numbers.\\nBiases are usually initialized with 0s. In its untrained\\nstate, a neural network can do little more than generate\\nrandom outputs. Once training is complete, the weights and\\nbiases enable the network to distinguish dogs from cats,\\ntranslate a book review to another language, or do whatever\\nelse it was designed to do.\\nWhat happens when a neural network is trained? At a high\\nlevel, training samples are fed through the network, the\\nerror (the difference between the computed output and the\\ncorrect output) is computed using a loss function, and a'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 287, 'file_type': 'pdf'}, page_content='backpropagation algorithm goes backward through the network\\nadjusting the weights and biases (Figure\\xa08-6). This is done\\nrepeatedly until the error is sufficiently small. With each\\niteration, the weights and biases become incrementally more\\nrefined and the error commensurately smaller.\\nFigure 8-6. Adjusting weights and biases during training\\nThe most critical component of the backpropagation regimen is\\nthe optimizer, which on each backward pass decides how much\\nand in which direction, positive or negative, to adjust the\\nweights and biases. Data scientists work constantly to find\\nbetter and more efficient optimizers to train networks more\\naccurately and in less time.\\nDo a search on “neural networks” and you’ll turn up lots\\nof articles with lots of complex math. Most of the math is\\nrelated to optimization. An optimizer can’t just guess how\\nto adjust the weights and biases due to their sheer numbers.\\nA neural network containing two hidden layers with 1,000\\nneurons each has 1,000,000 connections between layers, and\\ntherefore 1,000,000 weights to adjust. Training would take\\nforever if the optimization strategy were simply randomly\\nguessing. An optimizer must be intelligent enough to make'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 288, 'file_type': 'pdf'}, page_content='adjustments that reduce the error in each successive\\niteration.\\nData scientists use plots like the one in Figure\\xa08-7 to\\nvisualize what optimizers do. The plot is called a loss\\nlandscape. It has been reduced to three dimensions for\\nvisualization purposes, but in reality, it contains many\\ndimensions—sometimes millions of them. The multicolored\\ncontour charts the error for different combinations of\\nweights and biases. The optimizer’s goal is to navigate the\\ncontour and find the combination that produces the least\\nerror, which corresponds to the lowest point, or global\\nminimum, in the loss landscape.\\nFigure 8-7. Loss landscape (Source: Alexander Amini, Ava Soleimany, Sertac\\nKaraman, and Daniela Rus, “Spatial Uncertainty Sampling for End-to-End'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 289, 'file_type': 'pdf'}, page_content='Control,” NeurIPS Bayesian Deep Learning [2017],\\nhttps://arxiv.org/pdf/1805.04829)\\nThe optimizer’s job isn’t an easy one. It involves partial\\nderivatives (calculating the slope of the contour with\\nrespect to each weight and bias), gradient descent (adjusting\\nthe weights and biases to go down the slope rather than up it\\nor sideways), and learning rates, which drive the fractional\\nadjustments made to the weights and biases in each\\nbackpropagation pass. If the learning rate is too great, the\\noptimizer might miss the global minimum. If it’s too small,\\nthe network will take a long time to train. Modern optimizers\\nuse adaptive learning rates that take smaller steps as they\\napproach a minimum, where the slope of the contour is 0. To\\ncomplicate matters, the optimizer must avoid getting trapped\\nin local minima so that it can continue traversing the\\ncontour toward the global minimum where the error is the\\nsmallest. It also has to be wary of “saddle points” where\\nthe slope increases in one direction but falls off in a\\nperpendicular direction.\\nNOTE\\nIf you research gradient descent, you’ll encounter terms such as\\nstochastic gradient descent (SGD) and mini-batch gradient descent\\n(MBGD). Optimization via gradient descent is an iterative process\\nin which samples are fed forward through the network, gradients\\nare computed, and the gradients are combined with the learning\\nrate to update weights and biases. Updating the weights and biases\\nafter every sample is fed forward through the network is\\ncomputationally expensive, so training typically involves running\\nbatches of perhaps 30 to 40 samples through the network, averaging\\nthe error, and then performing a backpropagation pass. That’s\\nMBGD. It speeds training and helps the optimizer bypass local\\nminima. For more information, and for a very readable introduction'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 290, 'file_type': 'pdf'}, page_content='to the challenges inherent to training neural networks, see the\\narticle “How Neural Networks Are Trained”.\\nNeural networks are fundamentally simple. Training them is\\nmathematically complex. Fortunately, you don’t have to\\nunderstand everything that happens during training in order\\nto build them. Deep-learning libraries such as Keras and\\nTensorFlow insulate you from the math and provide cutting-\\nedge optimizers to do the heavy lifting. But now when you use\\none of these libraries and it asks you to pick a loss\\nfunction and an optimizer, you’ll understand what it’s\\nasking for and why.\\nSummary\\nDeep learning is a subset of machine learning that relies on\\ndeep neural networks, and it is the root of modern AI. It’s\\nhow computers identify objects in images, translate text and\\nspeech into other languages, generate artwork and music, and\\nperform other tasks that were virtually impossible a few\\nyears ago.\\nThe multilayer perceptron is a simple neural network\\ncomprising layers of neurons. Each neuron turns input into\\noutput using a simple mathematical formula. Activation\\nfunctions further transform the data as it passes between\\nlayers by introducing nonli\\u2060nearities, enabling neural\\nnetworks to fit to a variety of datasets. Hidden layers\\nbetween the input layer and the output layer perform the bulk\\nof the computational work, and a multilayer perceptron with\\nmany hidden layers is referred to as a deep neural network.\\nTraining a neural network fits it to a dataset by iteratively\\nadjusting weights and biases—the weights connecting neurons\\nin adjacent layers and the biases assigned to the neurons'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 291, 'file_type': 'pdf'}, page_content='themselves—to produce the desired outcome. The\\nbackpropagation passes that adjust the weights and biases are\\nthe heart of the training regimen. The component responsible\\nfor making adjustments is the optimizer; its ultimate goal is\\nto find the optimum combination of weights and biases with as\\nfew backpropagation passes as possible.\\nNow that you understand how neural networks work, the next\\nstep is to learn how to build and train them. For that, data\\nscientists rely on frameworks such as Keras and TensorFlow.\\nChapter\\xa09 begins a deep dive into both.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 292, 'file_type': 'pdf'}, page_content='Chapter 9. Neural Networks\\nMachine learning isn’t hard when you have a properly\\nengineered dataset to work with. The reason it’s not hard is\\nlibraries such as Scikit-Learn and ML.NET, which reduce\\ncomplex learning algorithms to a few lines of code. Deep\\nlearning isn’t difficult either, thanks to libraries such as\\nthe Microsoft Cognitive Toolkit (CNTK), Theano, and PyTorch.\\nBut the library\\xa0 that most of the world has settled on for\\nbuilding neural networks is TensorFlow, an open source\\nframework created by Google that was released under the\\nApache License 2.0 in 2015.\\nTensorFlow isn’t limited to building neural networks. It is\\na framework for performing fast mathematical operations at\\nscale using tensors, which are generalized arrays. Tensors\\ncan represent scalar values (0-dimensional tensors), vectors\\n(1D tensors), matrices (2D tensors), and so on. A neural\\nnetwork is basically a workflow for transforming tensors. The\\nthree-layer perceptron featured in Chapter\\xa08 takes a 1D\\ntensor containing two values as input, transforms it into a\\n1D tensor containing three values, and produces a 0D tensor\\nas output. TensorFlow lets you define directed graphs that in\\nturn define how tensors are computed. And unlike Scikit, it\\nsupports GPUs.\\nThe learning curve for TensorFlow is rather steep. Another\\nlibrary, named Keras, provides a simplified Python interface\\nto TensorFlow and has emerged as the Scikit of deep learning.\\nKeras is all about neural networks. It began life as a\\nstandalone project in 2015 but was integrated into TensorFlow\\nin 2019. Any code that you write using TensorFlow’s built-in\\nKeras module ultimately executes in (and is optimized for)\\nTensorFlow. Even Google recommends using the Keras API.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 293, 'file_type': 'pdf'}, page_content=\"Keras offers two APIs for building neural networks: a\\nsequential API and a functional API. The former is simpler\\nand is sufficient for most neural networks. The latter is\\nuseful in more advanced scenarios such as networks with\\nmultiple inputs or outputs—for example, a classification\\noutput and a regression output, which is common in neural\\nnetworks that perform object detection—or shared layers.\\nMost of the examples in this book use the sequential API. If\\ncuriosity compels you to learn more about the functional API,\\nsee “How to Use the Keras Functional API for Deep Learning”\\nby Jason Brownlee for a very readable introduction.\\nBuilding Neural Networks with Keras and\\nTensorFlow\\nCreating a neural network using Keras’s sequential API is\\nsimple. You first create an instance of the Sequential class.\\nThen you call add on the Sequential object to add layers. The\\nlayers are instances of classes such as Dense, which\\nrepresents a fully connected layer with a specified number of\\nneurons. The following statements create the three-layer\\nnetwork featured in Chapter\\xa08:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Dense(3,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(1))\\nThis network contains an input layer with two neurons, a\\nhidden layer with three neurons, and an output layer with one\\nneuron. Values passed from the hidden layer to the output\\nlayer are transformed by the rectified linear units (ReLU)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 294, 'file_type': 'pdf'}, page_content=\"activation function, which, you’ll recall, turns negative\\nnumbers into 0s and helps the model fit to nonlinear\\ndatasets. Observe that you don’t have to add the input layer\\nexplicitly. The input_dim=2 parameter in the first hidden\\nlayer implicitly creates an input layer with two neurons.\\nNOTE\\nrelu is one of several activation functions included in Keras.\\nOthers include tanh, sigmoid, and softmax. You will rarely if ever\\nuse anything other than relu in the hidden layers. Later in this\\nchapter, you’ll see why functions such as sigmoid and softmax are\\nuseful in the output layers of networks that perform\\nclassification rather than regression.\\nOnce all the layers are added, the next step is to call\\ncompile and specify important attributes such as which\\noptimizer and loss function to use during training. Here’s\\nan example:\\n\\nmodel.compile(optimizer='adam',\\nloss='mae',\\nmetrics=['mae'])\\nLet’s walk through the parameters one at a time:\\noptimizer='adam'\\nTells Keras to use the Adam optimizer to adjust\\nweights and biases in each backpropagation pass\\nduring training. Adam is one of eight optimizers\\nbuilt into Keras, and it is among the most advanced.\\nIt employs an adaptive learning rate and is always\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 295, 'file_type': 'pdf'}, page_content=\"the one I start with in the absence of a compelling\\nreason to do otherwise.\\nloss='mae'\\nTells Keras to use mean absolute error (MAE) to\\nmeasure loss. This is common for neural networks\\nintended to solve regression problems. Another\\nfrequently used option for regression models is\\nloss='mse' for mean squared error (MSE).\\nmetrics=['mae']\\nTells Keras to capture MAE values as the network is\\ntrained. This information is used after training is\\ncomplete to judge the efficacy of the training.\\nString values such as 'adam' and 'mae' are shortcuts for\\nfunctions built into Keras. For example, optimizer='adam' is\\nequivalent to optimizer=Adam(). The longhand form is useful\\nfor calling the function with nondefault parameter values—\\nfor example, optimizer=Adam(learning_rate=2e-5) to create an\\nAdam optimizer with a custom learning rate. You’ll see an\\nexample of this in Chapter\\xa013 when we fine-tune a model by\\ntraining it with a low learning rate.\\nInside the compile method, Keras creates a TensorFlow object\\ngraph to speed execution. Once the network is compiled, you\\ntrain it by calling fit:\\n\\nhist\\nmodel.fit(x,\\ny,\\nepochs=100,\\nbatch_size=100,\\nvalidation_split=0.2)\\nThe fit method accepts many parameters. Here are the ones\\nused in this example:\\nx\\nThe dataset’s feature columns.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 296, 'file_type': 'pdf'}, page_content='y\\nThe dataset’s label column—the one containing the\\nvalues the network will attempt to predict.\\nepochs=100\\nTells Keras to train the network for 100 iterations,\\nor epochs. In each epoch, all of the training data\\npasses through the network one time.\\nbatch_size=100\\nTells Keras to pass 100 training samples through the\\nnetwork before making a backpropagation pass to\\nadjust the weights and biases. Training takes less\\ntime if the batch size is large, but accuracy could\\nsuffer. You typically experiment with different batch\\nsizes to find the right balance between training time\\nand accuracy. Do not assume that lowering the batch\\nsize will improve accuracy. It frequently does, but\\nsometimes does not.\\nvalidation_split=0.2\\nTells Keras that in each epoch, it should train with\\n80% of the rows in the dataset and validate the\\nnetwork’s accuracy with the remaining 20%. If you\\nprefer, you can split the dataset yourself and use\\nthe validation_data parameter to pass the validation\\ndata to fit. Keras doesn’t offer an explicit\\nfunction for splitting a dataset, but you can use\\nScikit’s train_test_split function to do it. One\\ndifference between train_test_split and\\nvalidation_split is that the former splits the data\\nrandomly and includes an option for performing a\\nstratified split. validation_split, by contrast,\\nsimply divides the dataset into two partitions and\\ndoes not attempt to shuffle or stratify. Don’t use\\nvalidation_split on ordered data without shuffling\\nthe data first.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 297, 'file_type': 'pdf'}, page_content=\"It might surprise you to learn that if you train the same\\nnetwork on the same dataset several times, the results will\\nbe different each time. By default, weights are initialized\\nwith random values, and different starting points produce\\ndifferent outcomes. Additional randomness baked into the\\ntraining process means the network will train differently\\neven if it’s initialized with the same random weights.\\nRather than fight it, data scientists learn to “embrace the\\nrandomness.” If you work the tutorial in the next section,\\nyour results will differ from mine. They shouldn’t differ by\\na lot, but they will differ.\\nNOTE\\nThe random weights assigned to the connections between neurons\\naren’t perfectly random. Keras includes about a dozen\\ninitializers, each of which initializes parameters in a different\\nway. By default, Dense layers use the Zeroes initializer to\\ninitialize biases and the GlorotUniform initializer to initialize\\nweights. The latter generates random numbers that fall within a\\nuniform distribution whose limits are computed from the network\\ntopology.\\nYou judge the efficacy of training by examining information\\nreturned by the fit method. fit returns a history object\\ncontaining the training and validation metrics specified in\\nthe metrics parameter passed to the compile method. For\\nexample, metrics=['mae'] captures MAE at the end of each\\nepoch. Charting these metrics lets you determine whether you\\ntrained for the right number of epochs. It also lets you know\\nif the network is underfitting or overfitting. Figure\\xa09-1\\nplots MAE over the course of 30 training epochs.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 298, 'file_type': 'pdf'}, page_content='Figure 9-1. Training and validation accuracy during training\\nThe blue curve in Figure\\xa09-1 reveals how the network fit to\\nthe training data. The orange curve shows how it tested\\nagainst the validation data. Most of the learning was done in\\nthe first 20 epochs, but MAE continued to drop as training\\nprogressed. The validation MAE nearly matched the training\\nMAE at the end, which is an indication that the network\\nisn’t overfitting. You typically don’t care how well the\\nnetwork fits to the training data. You care about the fit to\\nthe validation data because that indicates how the network\\nperforms with data it hasn’t seen before. The greater the\\ngap between the training and validation accuracy, the greater\\nthe likelihood that the network is overfitting.\\nOnce a neural network is trained, you call its predict method\\nto make a prediction:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 299, 'file_type': 'pdf'}, page_content='prediction\\nmodel.predict(np.array([[2,\\n2]]))\\nIn this example, the network accepts two floating-point\\nvalues as input and returns a single floating-point value as\\noutput. The value returned by predict is that output.\\nSizing a Neural Network\\nA neural network is characterized by the number of layers\\n(the depth of the network), the number of neurons in each\\nlayer (the widths of the layers), the types of layers (in\\nthis example, Dense layers of fully connected neurons), and\\nthe activation functions used. There are other layer types,\\nmany of which I will introduce in later chapters. Dropout\\nlayers, for example, can increase a network’s ability to\\ngeneralize by randomly dropping connections between layers\\nduring weight updates, while Conv2D layers enable us to build\\nconvolutional neural networks (CNNs) that excel at image\\nprocessing.\\nWhen designing a network, how do you pick the right number of\\nlayers and the right number of neurons for each layer? The\\nshort answer is that the “right” width and depth depends on\\nthe problem you’re trying to solve, the dataset you’re\\ntraining with, and the accuracy you desire. As a rule, you\\nwant the minimum width and depth required to achieve that\\naccuracy, and you get there using a combination of intuition\\nand experimentation. That said, here are a few guidelines to\\nkeep in mind:\\n\\nGreater widths and depths give the network more\\ncapacity to “learn” by fitting more tightly to\\nthe training data. They also increase the\\nlikelihood of overfitting. It’s the validation\\nresults that matter, and sometimes loosening the'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 300, 'file_type': 'pdf'}, page_content='fit to the training data allows the network to\\ngeneralize better. The simplest way to loosen the\\nfit is to reduce the number of neurons.\\nGenerally speaking, you prefer greater width to\\ngreater depth in part to avoid the vanishing\\ngradient problem, which diminishes the impact of\\nadded layers. The ReLU activation function\\nprovides some protection against vanishing\\ngradients, but that protection isn’t absolute.\\nFor an explanation, see “How to Fix the\\nVanishing Gradients Problem Using the ReLU”. In\\naddition, a network with, say, 100 neurons in one\\nlayer trains faster than a network with five\\nlayers of 20 neurons each because the former has\\nfewer weights. Think about it: there are no\\nconnections between neurons in one layer, but\\nthere are 1,600 connections (202 × 4) between\\nfive layers containing 20 neurons each.\\nFewer neurons means less training time. State-of-\\nthe-art neural networks trained with large\\ndatasets sometimes take days or weeks to train on\\nhigh-end GPUs, so training time is important.\\nIn real life, data scientists experiment with various widths\\nand depths to find the right balance between training time,\\naccuracy, and the network’s ability to generalize. For a\\nmultilayer perceptron, you rarely ever need more than two\\nhidden layers, and one is often sufficient. A network with\\none or two hidden layers has the capacity to solve even\\ncomplex nonlinear problems. Two layers with 128 neurons each,\\nfor example, gives you 16,384 (1282) weights that can be\\nadjusted, plus 256 biases. That’s a lot of fitting power. I\\nfrequently start with one or, at most, two layers of 512\\nneurons each and halve the width or depth until the\\nvalidation accuracy drops below an acceptable threshold.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 301, 'file_type': 'pdf'}, page_content=\"Using a Neural Network to Predict Taxi Fares\\nLet’s put this knowledge to work building and training a\\nneural network. The problem that we’ll solve is the same one\\npresented in Chapter\\xa02: using data from the New York City\\nTaxi and Limousine Commission to predict taxi fares. We’ll\\nuse a neural network as a regression model to make the\\npredictions.\\nDownload the CSV file containing the dataset if you didn’t\\ndownload it in Chapter\\xa02 and copy it into the Data directory\\nwhere your Jupyter notebooks are hosted. Then use the\\nfollowing code to load the dataset and show the first five\\nrows. It contains about 55,000 rows and is a subset of a much\\nlarger dataset that was recently used in Kaggle’s New York\\nCity Taxi Fare Prediction competition:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/taxi-fares.csv',\\nparse_dates=['pickup_datetime'])\\ndf.head()\\nThe data requires a fair amount of prep work before it’s\\nuseful—something that’s quite common in machine learning\\nand in deep learning too. Use the following statements to\\ntransform the raw dataset into one suitable for training, and\\nrefer to the taxi-fare example in Chapter\\xa02 for a step-by-\\nstep explanation of the transformations applied:\\n\\nfrom\\nmath\\nimport\\nsqrt\\n\\ndf\\ndf[df['passenger_count']\\ndf\\ndf.drop(['key',\\n'passenger_count'],\\naxis=1)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 302, 'file_type': 'pdf'}, page_content=\"for\\ni,\\nrow\\nin\\ndf.iterrows():\\n\\xa0\\xa0\\xa0 dt\\nrow['pickup_datetime']\\n\\xa0\\xa0\\xa0 df.at[i,\\n'day_of_week']\\ndt.weekday()\\n\\xa0\\xa0\\xa0 df.at[i,\\n'pickup_time']\\ndt.hour\\n\\xa0\\xa0\\xa0 x\\n(row['dropoff_longitude']\\nrow['pickup_longitude'])\\n\\xa0\\xa0\\xa0 y\\n(row['dropoff_latitude']\\nrow['pickup_latitude'])\\n\\xa0\\xa0\\xa0 distance\\nsqrt(x**2\\ny**2)\\n\\xa0\\xa0\\xa0 df.at[i,\\n'distance']\\ndistance\\n\\ndf.drop(['pickup_datetime',\\n'pickup_longitude',\\n'pickup_latitude',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'dropoff_longitude',\\n'dropoff_latitude'],\\naxis=1,\\ninplace=True)\\n\\ndf\\ndf[(df['distance']\\n(df['distance']\\n10.0)]\\ndf\\ndf[(df['fare_amount']\\n(df['fare_amount']\\n50.0)]\\ndf.head()\\nThe resulting dataset contains columns for the day of the\\nweek (0–6, where 0 corresponds to Monday), the hour of the\\nday (0–23), and the distance traveled in miles, and from\\nwhich outliers have been removed:\\nThe next step is to create the neural network. Use the\\nfollowing statements to create a network with an input layer\\nthat accepts three values (day, time, and distance), two\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 303, 'file_type': 'pdf'}, page_content=\"hidden layers with 512 neurons each, and an output layer with\\na single neuron (the predicted fare amount):\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\\ninput_dim=3))\\nmodel.add(Dense(512,\\nactivation='relu'))\\nmodel.add(Dense(1))\\nmodel.compile(optimizer='adam',\\nloss='mae',\\nmetrics=['mae'])\\nmodel.summary()\\nThe call to summary in the last statement produces a concise\\nsummary of the network topology, including the number of\\ntrainable parameters—weights and biases that can be adjusted\\nto fit the network to a dataset (Figure\\xa09-2). For a given\\nlayer, the parameter count is the product of the number of\\nneurons in that layer and the previous layer (the number of\\nweights connecting the neurons in the two layers) plus the\\nnumber of neurons in the layer (the biases associated with\\nthose neurons). This network is a relatively simple one, and\\nyet it features more than a quarter million knobs and dials\\nthat can be adjusted to fit it to a dataset.\\nNow separate the feature columns from the label column and\\nuse them to train the network. Set validation_split to 0.2 to\\nvalidate the network using 20% of the training data. Train\\nfor 100 epochs and use a batch size of 100. Given that the\\ndataset contains more than 38,000 samples, this means that\\nabout 380 backpropagation passes will be performed in each\\nepoch:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 304, 'file_type': 'pdf'}, page_content=\"x\\ndf.drop('fare_amount',\\naxis=1)\\ny\\ndf['fare_amount']\\n\\nhist\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=100)\\nFigure 9-2. Trainable parameters in a simple neural network\\nUse the history object returned by fit to plot the training\\nand validation accuracy for each epoch:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nerr\\nhist.history['mae']\\nval_err\\nhist.history['val_mae']\\nepochs\\nrange(1,\\nlen(err)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 305, 'file_type': 'pdf'}, page_content=\"plt.plot(epochs,\\nerr,\\nlabel='Training MAE')\\nplt.plot(epochs,\\nval_err,\\nlabel='Validation MAE')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Mean Absolute Error')\\nplt.legend(loc='upper right')\\nplt.plot()\\nYour results will be slightly different from mine, but they\\nshould look something like this:\\nThe final validation MAE was about 2.25, which means that on\\naverage, a taxi fare predicted by this network should be\\naccurate to within about $2.25.\\nRecall from Chapter\\xa02 that a common accuracy measure for\\nregression models is the coefficient of determination, or R2\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 306, 'file_type': 'pdf'}, page_content='score. Keras doesn’t have a function for computing R2\\nscores, but Scikit does. To that end, use the following\\nstatements to compute R2 for the network:\\n\\nfrom\\nsklearn.metrics\\nimport\\nr2_score\\n\\nr2_score(y,\\nmodel.predict(x))\\nAgain, your results will differ from mine but will probably\\nland at around 0.75.\\nFinish up by using the model to predict what it will cost to\\nhire a taxi for a 2-mile trip at 5:00 p.m. on Friday:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nmodel.predict(np.array([[4,\\n2.0]]))\\nNow predict the fare amount for a 2-mile trip taken at 5:00\\np.m. one day later (on Saturday):\\n\\nmodel.predict(np.array([[5,\\n2.0]]))\\nDoes the model predict a higher or lower fare amount for the\\nsame trip on Saturday afternoon? Do the results make sense\\ngiven that the data comes from New York City cabs?\\nBefore you close out this notebook, use it as a basis for\\nfurther experimentation. Here are a few things you can try in\\norder to gain further insights into neural networks:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 307, 'file_type': 'pdf'}, page_content='Run the notebook from start to finish a few times\\nand note the differences in R2 scores as well as\\nthe MAE curves. Remember that neural networks are\\ninitialized with random weights each time\\nthey’re created, and additional randomness\\nduring the training process further ensures that\\nthe results will vary from run to run.\\nVary the width of the hidden layers. I used 512\\nneurons in each layer and found that doing so\\nproduced acceptable results. Would 128, 256, or\\n1,024 neurons per layer improve the accuracy? Try\\nit and find out. Since the results will vary\\nslightly from one run to the next, it might be\\nuseful to train the network several times in each\\nconfiguration and average the results.\\nVary the batch size. What effect does that have\\non training time, and why? How about the effect\\non accuracy?\\nFinally, try reducing the network to one hidden layer\\ncontaining just 16 neurons. Train it again and check the R2\\nscore. Does the result surprise you? How many traina\\u2060ble\\nparameters does this network contain?\\nBinary Classification with Neural Networks\\nOne of the common uses for machine learning is binary\\nclassification, which looks at an input and predicts which of\\ntwo possible classes it belongs to. Practical uses include\\nsentiment analysis, spam filtering, and fraud detection. Such\\nmodels are trained with datasets labeled with 1s and 0s\\nrepresenting the two classes, employ popular learning\\nalgorithms such as logistic regression and Naive Bayes, and\\nare frequently built with libraries such as Scikit-Learn.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 308, 'file_type': 'pdf'}, page_content=\"Deep learning can be used for binary classification too. In\\nfact, building a neural network that acts as a binary\\nclassifier is not much different than building one that acts\\nas a regressor. In the previous section, you built a neural\\nnetwork that solved a regression problem. That network had an\\ninput layer that accepted three values—distance to travel,\\nhour of the day, and day of the week—and output a predicted\\ntaxi fare. Building a neural network that performs binary\\nclassification involves making two simple changes:\\n\\nAdd an activation function—specifically, the\\nsigmoid activation function—to the output layer.\\nsigmoid produces a value from 0.0 to 1.0\\nrepresenting the probability that the input\\nbelongs to the positive class. For a reminder of\\nwhat a sigmoid function does, refer to the\\ndiscussion of logistic regression in Chapter\\xa03.\\nChange the loss function to binary_crossentropy,\\nwhich is purpose-built for binary classifiers.\\nAccordingly, change metrics to '[accuracy]' so\\nthat accuracies computed by the loss function are\\ncaptured in the history object returned by fit.\\nHere’s a network designed to perform binary classification\\nrather than regression:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\\ninput_dim=3))\\nmodel.add(Dense(512,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 309, 'file_type': 'pdf'}, page_content=\"model.compile(optimizer='adam',\\nloss='binary_crossentropy',\\nmetrics=\\n['accuracy'])\\nThat’s it. That’s all it takes to create a neural network\\nthat serves as a binary classifier. You still call fit to\\ntrain the network, and you use the returned history object to\\nplot the training and validation accuracy to determine\\nwhether you trained for a sufficient number of epochs and see\\nhow well the network fit to the data.\\nNOTE\\nWhat is binary cross-entropy, and what does it do to help a binary\\nclassifier converge on a solution? During training, the cross-\\nentropy loss function exponentially increases the penalty for\\nwrong outputs to drive the weights and biases more aggressively in\\nthe right direction.\\nLet’s say a sample belongs to the positive class (its label is\\n1), and the network predicts that the probability it’s a 1 is\\n0.9. The cross-entropy loss, also known as log loss, is –log(0.9),\\nwhich is 0.04. But if the network outputs a probability of 0.1 for\\nthe same sample, the error is –log(0.1), which equals 1. What’s\\nsignificant is that if the predicted probability is really wrong,\\nthe penalty is much higher. If the sample is a 1 and the network\\nsays the probability it’s a 1 is a mere 0.0001, the cross-entropy\\nloss is –log(0.0001), or 4. Cross-entropy loss basically pats the\\noptimizer on the back when it’s close to the right answer and\\nslaps it on the hand when it’s not. The worse the prediction, the\\nharder the slap.\\nTo sum up, you build a neural network that performs binary\\nclassification by including a single neuron with sigmoid\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 310, 'file_type': 'pdf'}, page_content='activation in the output layer and specifying\\nbinary_crossentropy as the loss function. The output from the\\nnetwork is a probability from 0.0 to 1.0 that the input\\nbelongs to the positive class. Doesn’t get much simpler than\\nthat!\\nMaking Predictions\\nOne of the benefits of a neural network is that it can easily\\nfit nonlinear datasets. You don’t have to worry about trying\\ndifferent learning algorithms as you do with conventional\\nmachine learning models; the network is the learning\\nalgorithm. As an example, consider the dataset in Figure\\xa09-\\n3, in which each data point consists of an x–y coordinate\\npair and belongs to one of two classes.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 311, 'file_type': 'pdf'}, page_content=\"Figure 9-3. Nonlinear dataset containing two classes\\nThe following code trains a neural network to predict a class\\nbased on a point’s x and y coordinates:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(optimizer='adam',\\nloss='binary_crossentropy',\\nmetrics=\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 312, 'file_type': 'pdf'}, page_content=\"['accuracy'])\\nhist\\nmodel.fit(x,\\ny,\\nepochs=40,\\nbatch_size=10,\\nvalidation_split=0.2)\\nThis network contains just one hidden layer with 128 neurons,\\nand yet a plot of the training and validation accuracy\\nreveals that it is remarkably successful in separating the\\nclasses:\\nOnce a binary classifier is trained, you make predictions by\\ncalling its predict method. Thanks to the sigmoid activation\\nfunction, predict returns a number from 0.0 to 1.0\\nrepresenting the probability that the input belongs to the\\npositive class. In this example, purple data points represent\\nthe negative class (0), while red data points represent the\\npositive class (1). Here the network is asked to predict the\\nprobability that a data point at (–0.5, 0.0) belongs to the\\nred class:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 313, 'file_type': 'pdf'}, page_content=\"model.predict(np.array([[-0.5,\\n0.0]]))\\nThe answer is 0.57, which indicates that (–0.5, 0.0) is more\\nlikely to be red than purple. If you simply want to know\\nwhich class the point belongs to, do it this way:\\n\\n(model.predict(np.array([[-0.5,\\n0.0]]))\\n0.5).astype('int32')\\nThe answer is 1, which corresponds to red. Older versions of\\nKeras included a predict_classes method that did the same\\nwithout the astype cast, but that method was recently\\ndeprecated and removed.\\nTraining a Neural Network to Detect Credit\\nCard Fraud\\nLet’s train a neural network to detect credit card fraud.\\nBegin by downloading a ZIP file containing the dataset if you\\nhaven’t already and copying creditcard.csv from the ZIP file\\ninto your notebooks’ Data subdirectory. It’s the same one\\nused in Chapters 3 and 6. It contains information about\\n284,808 credit card transactions, including the amount of\\neach transaction and a label: 0 for legitimate transactions\\nand 1 for fraudulent transactions. It also contains 28\\ncolumns named V1 through V28 whose meaning has been\\nobfuscated with principal component analysis. The dataset is\\nhighly imbalanced, containing fewer than 500 examples of\\nfraudulent transactions.\\nNow load the dataset:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 314, 'file_type': 'pdf'}, page_content=\"import\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/creditcard.csv')\\ndf.head(10)\\nUse the following statements to drop the Time column, divide\\nthe dataset into features x and labels y, and split the\\ndataset into two datasets: one for training and one for\\ntesting. Rather than allow Keras to do the split for us,\\nwe’ll do it ourselves so that we can later run the test data\\nthrough the network and use a confusion matrix to analyze the\\nresults:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nx\\ndf.drop(['Time',\\n'Class'],\\naxis=1)\\ny\\ndf['Class']\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 x,\\ny,\\ntest_size=0.2,\\nstratify=y,\\nrandom_state=0)\\nCreate a neural network for binary classification:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\n\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=29))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\n\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\nmetrics=\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 315, 'file_type': 'pdf'}, page_content=\"['accuracy'])\\n\\nmodel.summary()\\nThe next step is to train the model. Notice the\\nvalidation_data parameter passed to fit, which uses the test\\ndata split off from the larger dataset to assess the model’s\\naccuracy as training takes place:\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=10,\\nbatch_size=100)\\nNow plot the training and validation accuracy using the per-\\nepoch values in the history object:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 316, 'file_type': 'pdf'}, page_content='The result looked like this for me. Remember that your\\nresults will be different thanks to the randomness inherent\\nto training neural networks:\\nOn the surface, the validation accuracy (around 0.9994)\\nappears to be very high. But remember that the dataset is\\nimbalanced. Fraudulent transactions represent less than 0.2%\\nof all the samples, which means that the model could simply\\nguess that every transaction is legitimate and get it right\\nabout 99.8% of the time. Use a confusion matrix to visualize\\nhow the model performs during testing with data it wasn’t\\ntrained with:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\ny_predicted\\nmodel.predict(x_test)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 317, 'file_type': 'pdf'}, page_content=\"labels\\n['Legitimate',\\n'Fraudulent']\\n\\ncmd.from_predictions(y_test,\\ny_predicted,\\ndisplay_labels=labels,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical')\\nHere’s how it turned out for me:\\nYour results will probably vary. Indeed, train the model\\nseveral times and you’ll get different results each time. In\\nthis run, the model correctly identified 56,858 transactions\\nas legitimate while misclassifying legitimate transactions\\njust six times. This means legitimate transactions are\\nclassified correctly about 99.99% of the time. Meanwhile, the\\nmodel caught more than 73% of the fraudulent transactions.\\nThat’s acceptable, because credit card companies would\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 318, 'file_type': 'pdf'}, page_content='rather allow 100 fraudulent transactions to go through than\\ndecline one legitimate transaction.\\nNOTE\\nData scientists often use three datasets, not two, to train and\\nassess the accuracy of a neural network: a training dataset for\\ntraining, a validation dataset for validating the network (and\\nscoring its progress) as training takes place, and a test dataset\\nfor evaluating the network’s accuracy once training is complete.\\nThe preceding example used the same dataset for validation and\\ntesting—the 20% split off from the original dataset with\\ntrain_test_split. That’s ostensibly fine because validation data is\\nnot used to adjust the network’s weights and biases during\\ntraining. However, if you really want to have confidence in the\\nnetwork’s accuracy, it is never a bad idea to test it with a\\nthird dataset not used for training or validation. In the real\\nworld, the ultimate test of a deep-learning model’s accuracy is\\nhow it performs against data that it has never seen before.\\nA final note regarding this example has to do with an extra\\nparameter you can pass to the fit method that is particularly\\nuseful when dealing with imbalanced datasets. As an\\nexperiment, try replacing the call to fit with the following\\nstatement:\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\nepochs=10,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=100,\\nclass_weight={\\nThen run the notebook again from start to finish. In all\\nlikelihood, the resulting confusion matrix will show zero (or'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 319, 'file_type': 'pdf'}, page_content=\"at most, one or two) misclassified legitimate transactions,\\nbut the percentage of correctly identified fraudulent\\ntransactions will decrease too. The class_weight parameter in\\nthis example tells the model that you care a lot more about\\nclassifying legitimate samples correctly than correctly\\nidentifying fraudulent samples. You can experiment with\\ndifferent weights for the two classes and find the balance\\nthat best suits the business requirements that prompted you\\nto build the model in the first place.\\nMulticlass Classification with Neural\\nNetworks\\nHere again is a simple binary classifier that accepts two\\ninputs, has a hidden layer with 128 neurons, and outputs a\\nvalue from 0.0 to 1.0 representing the probability that the\\ninput belongs to the positive class:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(optimizer='adam',\\nloss=' binary_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nKey elements include an output layer with one neuron assigned\\nthe sigmoid activation function, and binary_crossentropy as\\nthe loss function. Three simple modifications repurpose this\\nnetwork to do multiclass classification:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 320, 'file_type': 'pdf'}, page_content=\"from\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(4,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nThe changes are as follows:\\n\\nThe output layer contains one neuron per class\\nrather than just one neuron. If the dataset\\ncontains four classes, then the output layer has\\nfour neurons. If the dataset contains 10 classes,\\nthen the output layer has 10 neurons. Each neuron\\ncorresponds to one class.\\nThe output layer uses the softmax activation\\nfunction rather than the sigmoid activation\\nfunction. Each neuron in the output layer yields\\na probability for the corresponding class, and\\nthanks to the softmax function, the sum of all\\nthe probabilities is 1.0.\\nThe loss function is\\nsparse_categorical_crossentropy. During training,\\nthis loss function exponentially penalizes error\\nin the probabilities predicted by a multiclass\\nclassifier, just as binary_crossentropy does for\\nbinary classifiers.\\nAfter defining the network, you call fit to train it and\\npredict to make predictions. Since an example is worth a\\nthousand words, let’s fit a neural network to a two-\\ndimensional dataset comprising four classes (Figure\\xa09-4).\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 321, 'file_type': 'pdf'}, page_content=\"Figure 9-4. Nonlinear dataset containing four classes\\nThe following code trains a neural network to predict a class\\nbased on a point’s x and y coordinates:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=2))\\nmodel.add(Dense(4,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 322, 'file_type': 'pdf'}, page_content='hist\\nmodel.fit(x,\\ny,\\nepochs=40,\\nbatch_size=10,\\nvalidation_split=0.2)\\nA plot of the training and validation accuracy reveals that\\nthe network had little trouble separating the classes:\\nYou make predictions by calling the classifier’s predict\\nmethod. For each input, predict returns an array of\\nprobabilities—one per class. The predicted class is the one\\nassigned the highest probability. In this example, purple\\ndata points represent class 0, light blue represent class 1,\\ntaupe represent class 2, and red represent class 3. Here the\\nnetwork is asked to classify a point that lies at (0.2, 0.8):\\n\\nmodel.predict(np.array([[0.2,\\n0.8]]))'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 323, 'file_type': 'pdf'}, page_content='The answer is an array of four probabilities corresponding to\\nclasses 0, 1, 2, and 3, in that order:\\n\\n[2.1877741e-02, 5.3804164e-05, 5.0240371e-02, 9.2782807e-01]\\nThe network predicted there’s a 2% chance that (0.2, 0.8)\\ncorresponds to class 0, a 0% chance that it corresponds to\\nclass 1, a 5% chance that it corresponds to class 2, and a\\n93% chance that it corresponds to class 3. Looking at the\\nplot, that seems like a reasonable answer.\\nIf you simply want to know which class the point belongs to,\\nyou can do it this way:\\n\\nnp.argmax(model.predict(np.array([[0.2,\\n0.8]])),\\naxis=1)\\nThe answer is 3, which corresponds to red. Older versions of\\nKeras included a predict_classes method that did the same\\nwithout the call to argmax, but that method has since been\\ndeprecated and removed.\\nNOTE\\nKeras also includes a loss function named categorical_cross\\u200ben\\u2060tropy\\nthat is frequently used for multiclass classification. It works\\nlike sparse_categorical_cross\\u200ben\\u2060tropy, but it requires labels to be\\none-hot-encoded. Rather than pass fit a label column containing\\nvalues from 0 to 3, for example, you pass it four columns\\ncontaining 0s and 1s. Keras provides a utility function named\\nto_categorical to do the encoding. If you use\\nsparse_categorical_crossentropy, however, you can use the label\\ncolumn as is.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 324, 'file_type': 'pdf'}, page_content='Training a Neural Network to Recognize Faces\\nChapter\\xa05 documented the steps for training a support vector\\nmachine to recognize faces. Let’s train a neural network to\\ndo the same. We’ll use the same dataset as before: the\\nLabeled Faces in the Wild (LFW) dataset, which contains more\\nthan 13,000 facial images of famous people and is built into\\nScikit as a sample dataset. Recall that of the more than\\n5,000 people represented in the dataset, 1,680 have two or\\nmore facial images, while only 5 have 100 or more. We’ll set\\nthe minimum number of faces per person to 100, which means\\nthat five sets of faces corresponding to five famous people\\nwill be imported.\\nStart by creating a new Jupyter notebook and using the\\nfollowing statements to load the dataset:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\nfetch_lfw_people(min_faces_per_person=100)\\nimage_count\\nfaces.images.shape[0]\\nimage_height\\nfaces.images.shape[1]\\nimage_width\\nfaces.images.shape[2]\\nclass_count\\nlen(faces.target_names)\\n\\nfaces\\nfetch_lfw_people(min_faces_per_person=100)\\nprint(faces.target_names)\\nprint(faces.images.shape)\\nIn total, 1,140 facial images were loaded. Each measures 47\\n× 62 pixels. Use the following code to show the first 24\\nimages in the dataset and the people to whom the faces\\nbelong:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 325, 'file_type': 'pdf'}, page_content=\"%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n\\nfig,\\nax\\nplt.subplots(3,\\nfigsize=(18,\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i],\\ncmap='gist_gray')\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])\\nCheck the balance in the dataset by generating a histogram\\nshowing how many facial images were imported for each person:\\n\\nfrom\\ncollections\\nimport\\nCounter\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\ncounts\\nCounter(faces.target)\\nnames\\n\\nfor\\nkey\\nin\\ncounts.keys():\\n\\xa0\\xa0\\xa0 names[faces.target_names[key]]\\ncounts[key]\\n\\ndf\\npd.DataFrame.from_dict(names,\\norient='index')\\ndf.plot(kind='bar')\\nThere are far more images of George W. Bush than of anyone\\nelse in the dataset. Classification models are best trained\\nwith balanced datasets. Use the following code to reduce the\\ndataset to 100 images of each person:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nmask\\nnp.zeros(faces.target.shape,\\ndtype=bool)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 326, 'file_type': 'pdf'}, page_content='for\\ntarget\\nin\\nnp.unique(faces.target):\\n\\xa0\\xa0\\xa0 mask[np.where(faces.target\\ntarget)[0][:100]]\\n\\nx_faces\\nfaces.data[mask]\\ny_faces\\nfaces.target[mask]\\nx_faces.shape\\nx_faces contains 500 facial images, and y_faces contains the\\nlabels that go with them: 0 for Colin Powell, 1 for Donald\\nRumsfeld, and so on.\\nThe next step is to divide the pixel values by 255. After\\nthat, split the data for training and testing. We’ll set\\naside 20% of the data for testing, let Keras use it to\\nvalidate the model during training, and later use it to\\nassess the results with a confusion matrix:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\n\\nface_images\\nx_faces\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(face_images,\\ny_faces,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 train_size=0.8,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 stratify=y_faces,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nNOTE\\nWhy divide the pixel values by 255? Neural networks frequently\\ntrain better with normalized data, and dividing by 255 is a simple\\nway to normalize pixel values. It’s not uncommon to use Scikit’s\\nStandardScaler class to apply unit variance instead, and sometimes\\na network trains better that way. The only way to find out is to'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 327, 'file_type': 'pdf'}, page_content=\"try. In this example, I tried both and found that the results were\\nabout the same either way.\\nCreate a neural network containing one hidden layer with 512\\nneurons. Use sparse_categorical_crossentropy as the loss\\nfunction and softmax as the activation function in the output\\nlayer since this is a multiclass classification task:\\n\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\n\\nmodel\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_shape=(image_width\\nimage_height,)))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary()\\nNow train the network:\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=100,\\nbatch_size=20)\\nPlot the training and validation accuracy:\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 328, 'file_type': 'pdf'}, page_content=\"epochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nFinally, use a confusion matrix to visualize how the network\\nperforms against test data:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\ny_pred\\nmodel.predict(x_test)\\nfig,\\nax\\nplt.subplots(figsize=(5,\\nax.grid(False)\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=faces.target_names,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHow many times did the model correctly identify George W.\\nBush? How many times did it identify him as someone else?\\nWould the network be just as accurate with 128 neurons in the\\nhidden layer as it is with 512?\\nDropout\\nThe goal\\xa0of any machine learning model is to make accurate\\npredictions. In a perfect world, the gap between training\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 329, 'file_type': 'pdf'}, page_content=\"accuracy and validation accuracy would be close to 0 in the\\nlater stages of training a neural network. In the real world,\\nit rarely happens that way. Training accuracy for the model\\nin the previous section approached 100%, but validation\\naccuracy probably peaked between 80% and 85%. This means the\\nmodel isn’t generalizing as well as you’d like. It learned\\nthe training data very well, but when presented with data it\\nhadn’t seen before (the validation data), it underperformed.\\nThis may be a sign that the model is overfitting. In the end,\\nit’s not training accuracy that matters; it’s how\\naccurately the model responds to new data.\\nOne way to combat overfitting is to reduce the depth of the\\nnetwork, the width of individual layers, or both. Fewer\\nneurons means fewer trainable parameters, and fewer\\nparameters makes it harder for the network to fit too tightly\\nto the training data.\\nAnother way to guard against overfitting is to introduce\\ndropout to the network. Dropout randomly drops connections\\nbetween layers during training to prevent the network from\\nlearning the training data too well. It’s like reading a\\nbook but skipping every other page in hopes that you’ll\\nlearn high-level concepts without getting bogged down in the\\ndetails. Dropout was introduced in a 2014 paper titled\\n“Dropout: A Simple Way to Prevent Neural Networks from\\nOverfitting”.\\nKeras’s Dropout class makes adding dropout to a network\\ndead-simple. To demonstrate, go back to the example in the\\nprevious section and redefine the network this way:\\n\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nDropout\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\n\\nmodel\\nSequential()\\nmodel.add(Dense(512,\\nactivation='relu',\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 330, 'file_type': 'pdf'}, page_content=\"input_shape=(image_width\\nimage_height,)))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nNow train the network again and plot the training and\\nvalidation accuracy. Here’s how it turned out for me:\\nThe gap didn’t close much (if at all), but sometimes adding\\ndropout in this manner will increase the validation accuracy.\\nThe key statement in this example is model.add(Dropout(0.2)),\\nwhich adds a Dropout layer that randomly drops (ignores) 20%\\nof the connections between the neurons in the hidden layer\\nand the neurons in the output layer in each backpropagation\\npass. You can be more aggressive by dropping more connections\\n—increasing 0.2 to 0.4, for example—but you can also reach\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 331, 'file_type': 'pdf'}, page_content=\"a point of diminishing returns. Note that if you’re super-\\naggressive with the dropout rate (for example, 0.5), you’ll\\nprobably need to train the model for more epochs. Making it\\nharder to learn means taking longer to learn too.\\nIn practice, the only way to know whether dropout will\\nimprove a model’s ability to generalize is to try it. In\\naddition to trying different dropout percentages, you can try\\nintroducing dropout between two or more layers in hopes of\\nfinding a combination that works.\\nSaving and Loading Models\\nIn Chapter\\xa07, you learned how to serialize (save) a trained\\nScikit model and load it in a client app. The same\\nrequirement applies to neural networks: you need a way to\\nsave a trained network and load it later in order to\\noperationalize it.\\nYou can get the weights and biases from a model with Keras’s\\nget_weights method, and you can restore them with\\nset_weights. But saving a trained model so that you can re-\\ncreate it later requires an additional step. Specifically,\\nyou must save the network architecture: the number of and\\ntypes of layers, the number of neurons in each layer, the\\nactivation functions used in each layer, and so on.\\nFortunately, all that requires just one line of code. That\\nline of code differs depending on which of two formats you\\nwant the model saved in:\\n\\nmodel.save('my_model.h5')\\n# Save the model in Keras's H5 format\\nmodel.save('my_model')\\xa0\\xa0\\xa0 # Save the model in TensorFlow's native format\\nLoading a saved model is equally simple:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 332, 'file_type': 'pdf'}, page_content=\"from\\ntensorflow.keras.models\\nimport\\nload_model\\n\\nmodel\\nload_model('my_model.h5')\\n# Load model saved in H5 format\\nmodel\\nload_model('my_model')\\n# Load model saved in TensorFlow format\\nSaving the model in H5 format produces a single .h5 file that\\nencapsulates the entire model. Saving it in TensorFlow’s\\nnative format, also known as the SavedModel format, produces\\na series of files and subdirectories containing the\\nserialized model. Google recommends using the latter,\\nalthough it’s still common to see Keras’s H5 format used.\\nThere is no functional difference between the two, but .h5\\nfiles can only be read by Keras apps written in Python, while\\nmodels saved in SavedModel format can be loaded by other\\nframeworks. Apps written in C# with Microsoft’s ML.NET, for\\nexample, can load models saved in SavedModel format\\nregardless of the programming language in which the model was\\ncrafted.\\nNOTE\\nThe H5 format was originally devised so that Keras models could be\\nsaved in a manner independent of the deep-learning framework used\\nas the backend. Keras is still available in a standalone version\\nthat supports backends other than TensorFlow (specifically, CNTK\\nand Theano), but those frameworks have been deprecated—they are\\nno longer being developed—and are rarely used today other than in\\nlegacy models. The version of Keras built into TensorFlow supports\\nonly TensorFlow backends.\\nOnce a saved model is loaded, it acts identically to the\\noriginal. The predictions that it makes, for example, are\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 333, 'file_type': 'pdf'}, page_content='identical to the predictions made by the original model. You\\ncan even further train the model by running additional\\ntraining samples through it. This highlights one of the major\\ndifferences between neural networks and traditional machine\\nlearning models built with Scikit. Since the state of a\\nnetwork is defined by its weights and biases and loading a\\nmodel restores the weights and biases, neural networks\\ninherently support incremental training, also known as\\ncontinual learning, so that they can become smarter over\\ntime. Most Scikit models do not because serializing the\\nmodels doesn’t save the internal state accumulated as\\ntraining takes place.\\nTo recap: you can run a million training samples through a\\nneural network, save it, load it, and run another million\\ntraining samples through it and the network picks up right\\nwhere it left off. The results are identical to running 2\\nmillion training samples through the network to begin with\\nsave for minor differences that result from the randomness\\nthat is always inherent to training.\\nKeras Callbacks\\nAs you train a neural network and it achieves peak validation\\naccuracy, the peak is hard to capture. Rather than nicely\\nlevel out in later epochs, the validation accuracy may go\\ndown or oscillate between peaks and valleys. Given the\\nstochastic (random) nature of neural networks, if you mark\\nthe epoch that achieved maximum validation accuracy and train\\nagain for exactly that number of epochs, you won’t get the\\nsame results the second time. How do you train for exactly\\nthe right number of epochs to produce the best (most\\naccurate) network possible?\\nAn elegant solution is Keras’s callbacks API, which lets you\\nwrite callback functions that are called at various points\\nduring training—for example, at the end of each epoch—and\\nthat have the ability to alter and even stop the training'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 334, 'file_type': 'pdf'}, page_content=\"process. Here’s an example that creates a child class named\\nStopCallback that inherits from Keras’s Callback class. The\\nchild class implements the on_epoch_end function that’s\\ncalled at the end of each training epoch and stops training\\nif the validation accuracy reaches 95%:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nCallback\\n\\nclass\\nStopCallback(Callback):\\n\\xa0\\xa0\\xa0 accuracy_threshold\\nNone\\n\\n\\xa0\\xa0\\xa0 def __init__(self,\\nthreshold):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.accuracy_threshold\\nthreshold\\n\\n\\xa0\\xa0\\xa0 def\\non_epoch_end(self,\\nepoch,\\nlogs=None):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(logs.get('val_accuracy')\\nself.accuracy_threshold):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.model.stop_training\\nTrue\\n\\ncallback\\nStopCallback(0.95)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nmodel.save('best_model.h5')\\nNote the validation accuracy threshold (0.95) passed to\\nStopCallback’s constructor. The call to fit ostensibly\\ntrains the network for 100 epochs, but if the validation\\naccuracy reaches 0.95 before that, training stops in its\\ntracks. The final statement saves the model that achieved\\nthat accuracy.\\non_epoch_end is one of several functions you can implement in\\nclasses that inherit from Callback to receive a callback when\\na predetermined checkpoint is reached in the training\\nprocess. Others include on_epoch_begin, on_train_begin,\\non_train_end, on_train_batch_begin, and on_train_batch_end.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 335, 'file_type': 'pdf'}, page_content=\"You’ll find a complete list, along with examples, in\\n“Writing Your Own Callbacks” in the Keras documentation.\\nIn addition to providing a base Callback class from which you\\ncan create your own callback classes, Keras provides several\\ncallback classes of its own. One of them is the EarlyStopping\\nclass, which lets you stop training based on a specified\\ncriterion such as decreasing validation accuracy or\\nincreasing training loss without writing a lot of code. In\\nthe following example, training stops early if the validation\\naccuracy fails to improve for five consecutive epochs\\n(patience=5). When training is halted, the network’s weights\\nand biases are automatically restored to what they were when\\nvalidation accuracy peaked in the final five epochs\\n(restore_best_weights=True):\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nEarlyStopping\\n\\ncallback\\nEarlyStopping(monitor='val_accuracy',\\npatience=5,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 restore_best_weights=True)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nStopping the training process based on rising training loss\\nrather than decreasing validation accuracy at the end of each\\nepoch requires a minor code change:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nEarlyStopping\\n\\ncallback\\nEarlyStopping(monitor='loss',\\npatience=5,\\nrestore_best_weights=True)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 336, 'file_type': 'pdf'}, page_content=\"The callback that is used perhaps more than any other is\\nModelCheckpoint, which saves a model at specified intervals\\nduring training or, if you set save_best_only to True, saves\\nthe most accurate model. The next example trains a model for\\n100 epochs and saves the one that exhibits the highest\\nvalidation accuracy in best_model.h5:\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nModelCheckpoint\\n\\ncallback\\nModelCheckpoint(filepath='best_model.h5',\\nmonitor='val_accuracy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 save_best_only=True)\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nAnother frequently used callback is TensorBoard, which logs a\\nvariety of information to a specified location in the\\nfilesystem as a model is trained. The following example logs\\nto the logs subdirectory of the current directory:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nTensorBoard\\n\\ncallback\\nTensorBoard(log_dir='logs',\\nhistogram_freq=1)\\nmodel.fit(x_train,\\ny_train,\\nvalidation_split=0.2,\\nepochs=100,\\nbatch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nYou can use a tool called TensorBoard to monitor accuracy and\\nloss, changes in the model’s weights and biases, and more\\nwhile training takes place or after it has completed. You can\\nlaunch TensorBoard from a Jupyter notebook and point it to\\nthe logs subdirectory with a command like this one:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 337, 'file_type': 'pdf'}, page_content='%tensorboard --logdir logs\\nOr you can launch it from a command prompt by executing the\\nsame command without the percent sign. Then point your\\nbrowser to http://localhost:6006 to open the TensorBoard\\nconsole (Figure\\xa09-5). “Get Started with TensorBoard” in\\nthe TensorFlow documentation contains a helpful tutorial on\\nthe basics of TensorBoard. It’s an indispensable tool in the\\nhands of professionals, especially when training complex\\nmodels that require hours, days, or even weeks to fully\\ntrain.\\nFigure 9-5. TensorBoard showing the results of training a neural network\\nOther Keras callback classes include LearningRateScheduler\\nfor adjusting the learning rate at the beginning of each\\nepoch and CSV\\u200bLog\\u2060ger for capturing the results of each'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 338, 'file_type': 'pdf'}, page_content='training epoch in a CSV file. Refer to the callbacks API\\ndocumentation for a complete list. In addition, observe that\\nthe fit method’s callbacks parameter is a Python list, which\\nmeans you can specify multiple callbacks when you train a\\nmodel. You could use one callback to stop training if certain\\nconditions are met, for example, and another callback to log\\ntraining metrics in a CSV file.\\nSummary\\nKeras and TensorFlow are widely used open source frameworks\\nthat facilitate building, training, saving, loading, and\\nconsuming (making predictions with) neural networks. You can\\nbuild neural networks in a variety of programming languages\\nusing native TensorFlow APIs, but Keras abstracts those APIs\\nand makes deep learning much more approachable. Keras apps\\nare written in Python.\\nNeural networks, like traditional machine learning models,\\ncan be used to solve regression problems and classification\\nproblems. A network that performs regression has one neuron\\nin the output layer with no activation function; the output\\nfrom the network is a floating-point number. A network that\\nperforms binary classification also has one neuron in the\\noutput layer, but the sigmoid activation function ensures\\nthat the output is a value from 0.0 to 1.0 representing the\\nprobability that the input represents the positive class. For\\nmulticlass classification, the number of neurons in the\\noutput layer equals the number of classes the network can\\npredict. The softmax activation function transforms the raw\\nvalues assigned to the output neurons into an array of\\nprobabilities for each class.\\nWhen you find that a neural network is fitting too tightly to\\nthe training data, one way to combat overfitting and increase\\nthe network’s ability to generalize is to reduce the\\ncomplexity of the network: reduce the number of layers, the\\nnumber of neurons in individual layers, or both. Another'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 339, 'file_type': 'pdf'}, page_content='approach is to add dropout to the network. Dropout purposely\\nimpedes a network’s ability to learn the training data by\\nrandomly ignoring a subset of the connections between layers\\nwhen updating weights and biases.\\nKeras’s callbacks API lets you customize the training\\nprocess. By processing the callbacks that occur at the end of\\neach training epoch, for example, you can check the model’s\\naccuracy and halt training if it has reached an acceptable\\nlevel. Keras also includes a simple and easy-to-use API for\\nsaving and loading trained models. This is essential for\\noperationalizing the models that you train—deploying them to\\nproduction and using the predictive powers developed during\\ntraining.\\nThe facial recognition model in this chapter exceeded 80% in\\nvalidation accuracy, but modern deep-learning models often\\nachieve 99% accuracy on the same dataset. It won’t surprise\\nyou to learn that there is more to deep learning than\\nmultilayer perceptrons. We’ll take a deep dive into facial\\nrecognition in Chapter\\xa011, but first we’ll explore a\\ndifferent type of neural network—one that’s particularly\\nadept at solving computer-vision problems. It’s called the\\nconvolutional neural network, and it is the subject of\\nChapter\\xa010.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 340, 'file_type': 'pdf'}, page_content='Chapter 10. Image Classification\\nwith Convolutional Neural\\nNetworks\\nComputer vision is a branch of deep learning in which\\ncomputers discern information from images. Real-world uses\\ninclude identifying objects in photos, removing inappropriate\\nimages from social media sites, counting the cars in line at\\na tollbooth, and recognizing faces in photos. Computer-vision\\nmodels can even be combined with natural language processing\\n(NLP) models to caption photos. I snapped a photo while on\\nvacation and asked Azure’s Computer Vision service to\\ncaption it. The result is shown in Figure\\xa010-1. It’s\\nsomewhat remarkable given that no human intervention was\\nrequired.\\nFigure 10-1. “A body of water with a dock and a building in the\\nbackground”—Azure AI'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 341, 'file_type': 'pdf'}, page_content='The field of computer vision has advanced rapidly in recent\\nyears, mostly due to convolutional neural networks, also\\nknown as CNNs or ConvNets. In 2012, an eight-layer CNN called\\nAlexNet outperformed traditional machine learning models\\nentered in the annual ImageNet Large Scale Visual Recognition\\nChallenge (ILSVRC) by achieving an error rate of 15.3% when\\nidentifying objects in photos. In 2015, ResNet-152 featuring\\na whopping 152 layers won the challenge with an error rate of\\njust 3.5%, which exceeds a human’s ability to classify\\nimages featured in the competition.\\nCNNs are magical because they treat images as images rather\\nthan just arrays of pixel values. They use a decades-old\\ntechnology called convolution kernels to extract “features”\\nfrom images, allowing them to recognize the shape of a cat’s\\nhead or the outline of a dog’s tail. Moreover, they are easy\\nto build with Keras and TensorFlow.\\nState-of-the-art CNNs such as ResNet-152 are trained at great\\nexpense with millions of images on GPUs, but there’s a lot\\nyou can do with an ordinary CPU. In this chapter, you’ll\\nlearn what CNNs are and how they work, and you’ll build and\\ntrain a few CNNs of your own. You’ll also learn how to\\nleverage advanced CNNs published for public consumption by\\ncompanies such as Google and Microsoft, and how to use a\\ntechnique called transfer learning to repurpose those CNNs to\\nsolve domain-specific problems.\\nUnderstanding CNNs\\nFigure\\xa010-2 shows the topology of a basic CNN. It begins\\nwith one or more sets of convolution layers and pooling\\nlayers. Convolution layers extract features from images,\\ngenerating transformed images that are commonly referred to\\nas feature maps because they highlight distinguishing\\nfeatures such as shapes and contours. Pooling layers reduce\\nthe feature maps’ size by half so that features can be\\nextracted at various resolutions and are less sensitive to'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 342, 'file_type': 'pdf'}, page_content='small changes in position. Output from the final pooling\\nlayer is flattened to one dimension and input to one or more\\ndense layers for classification. The convolution and pooling\\nlayers are called bottleneck layers since they reduce the\\ndimensionality of images input to them. They also account for\\nthe bulk of the computation time during training.\\nConvolution layers extract features from images by passing\\nconvolution kernels over them—the same technique used by\\nimage editing tools to blur, sharpen, and emboss images. A\\nkernel is simply a matrix of values. It usually measures 3 ×\\n3, but it can be larger. To process an image, you place the\\nkernel in the upper-left corner of the image, multiply the\\nkernel values by the pixel values underneath, and compute a\\nnew value for the center pixel by summing the products, as\\nshown in Figure\\xa010-3. Then you move the kernel one pixel to\\nthe right and repeat the process, continuing row by row and\\ncolumn by column until the entire image has been processed.\\nFigure 10-2. Convolutional neural network'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 343, 'file_type': 'pdf'}, page_content='Figure 10-3. Processing image pixels with a 3 × 3 convolution kernel\\nFigure\\xa010-4 shows what happens when you apply a 3 × 3\\nkernel to a hot dog image. This particular kernel is called a\\nbottom Sobel kernel, and it’s designed to do edge detection\\nby highlighting edges as if a light were shined from the\\nbottom. The convolution layers of a CNN use kernels like this\\none to extract features that help distinguish one class from\\nanother.\\nFigure 10-4. Processing an image with a bottom Sobel kernel\\nA convolution layer doesn’t use just one kernel to process\\nimages. It uses many—sometimes 100 or more. The kernel\\nvalues aren’t determined ahead of time. They are initialized\\nwith random values and then learned (adjusted) as the CNN is\\ntrained, just as the weights connecting neurons in dense\\nlayers are learned. Each kernel also has a bias associated'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 344, 'file_type': 'pdf'}, page_content='with it, just like a neuron in a dense layer. The images in\\nFigure\\xa010-5 were generated by the first convolution layer in\\na trained CNN. You can see how the various convolution\\nkernels allow the network to view the same hot dog image in\\ndifferent ways, and how certain features such as the shape of\\nthe bun and the ribbon of mustard on top are highlighted.\\nFigure 10-5. Images generated by convolution kernels in a CNN\\nPooling layers downsample images to reduce their size. The\\nmost common resizing technique is max pooling, which divides\\nimages into 2 × 2 blocks of pixels and selects the highest\\nof the four values in each block. An alternative is average\\npooling, which averages the values in each block.\\nFigure\\xa010-6 shows how an image contracts as it passes\\nthrough successive pooling layers. The first row came from\\nthe first pooling layer, the second row came from the second\\npooling layer, and so on.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 345, 'file_type': 'pdf'}, page_content='Figure 10-6. Images generated by pooling layers in a CNN\\nPooling isn’t the only way to downsize an image. While less\\ncommon, reduction can be accomplished without pooling layers\\nby setting a convolution layer’s stride to 2. Stride is the\\nnumber of pixels a convolution kernel moves as it passes over\\nan image. It defaults to 1, but setting it to 2 halves the\\nimage size by ignoring every other row and every other column\\nof pixels.\\nThe dense layers at the end of the network classify features\\nextracted from the bottleneck layers and are referred to as\\nthe CNN’s classification layers. They are no different than\\nthe multilayer perceptrons featured in Chapter\\xa09. For binary\\nclassification, the output layer contains one neuron and uses\\nthe sigmoid activation function. For multiclass\\nclassification, the output layer contains one neuron per\\nclass and uses the softmax activation function.\\nNOTE\\nThere’s no law that says bottleneck layers have to be paired with\\nclassification layers. You could take the feature maps output from\\nthe bottleneck layers and classify them with a support vector\\nmachine rather than a multilayer perceptron. It’s not as far-\\nfetched as it sounds. In Chapter\\xa012, I’ll introduce one well-\\nknown model that does just that.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 346, 'file_type': 'pdf'}, page_content=\"Using Keras and TensorFlow to Build CNNs\\nTo simplify building CNNs that classify images, Keras offers\\nthe Conv2D class, which models convolution layers, and the\\nMaxPooling2D class, which implements max pooling layers. The\\nfollowing statements create a CNN with two pairs of\\nconvolution and pooling layers, a flatten layer to reshape\\nthe output into a 1D array for input to a dense layer, a\\ndense layer to classify the features extracted from the\\nbottleneck layers, and a softmax output layer for\\nclassification:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nFlatten,\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Conv2D(32,\\nactivation='relu',\\ninput_shape=(28,\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(64,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(10,\\nactivation='softmax'))\\nThe first parameter passed to the Conv2D function is the\\nnumber of convolution kernels to include in the layer. More\\nkernels means more fitting power, similar to the number of\\nneurons in a dense layer. The second parameter is the\\ndimensions of each kernel. You sometimes get greater accuracy\\nfrom 5 × 5 kernels, but a kernel that size increases\\ntraining time by requiring 25 multiplication operations for\\neach pixel as opposed to nine for a 3 × 3 kernel. The\\ninput_shape parameter in the first layer specifies the size\\nof the images input to the CNN: in this case, one-channel\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 347, 'file_type': 'pdf'}, page_content='(grayscale) 28 × 28 images. All the images used to train a\\nCNN must be the same size.\\nNOTE\\nConv2D processes images, which are two-dimensional. Keras also\\noffers the Conv1D class for processing 1D data and Conv3D for 3D\\ndata. The former finds use processing text and time-series data.\\nCanonical use cases for Conv3D include analyzing video and 3D\\nmedical images.\\nGiven a set of images with a relatively high degree of\\nseparation between classes, it’s perfectly feasible to train\\na CNN to classify those images on a typical laptop or PC. A\\ngreat example is the MNIST dataset, which contains 60,000\\ntraining images of scanned, handwritten digits, each\\nmeasuring 28 × 28 pixels, plus 10,000 test images.\\nFigure\\xa010-7 shows the first 50 scans in the training set.\\nFigure 10-7. The MNIST digits dataset'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 348, 'file_type': 'pdf'}, page_content=\"Let’s train a CNN to recognize digits in the MNIST dataset,\\nwhich conveniently is one of several sample datasets built\\ninto Keras. Begin by creating a new Jupyter notebook and\\nusing the following statements to load the dataset, reshape\\nthe 28 × 28 images into 28 × 28 × 1 arrays (28 × 28\\nimages containing a single color channel), and divide the\\npixel values by 255 as a simple form of normalization:\\n\\nfrom\\ntensorflow.keras.datasets\\nimport\\nmnist\\n\\n(train_images,\\ny_train),\\n(test_images,\\ny_test)\\nmnist.load_data()\\nx_train\\ntrain_images.reshape(60000,\\nx_test\\ntest_images.reshape(10000,\\nNext, define a CNN that accepts 28 × 28 × 1 arrays of pixel\\nvalues as input, contains two pairs of convolution and\\npooling layers, and has a softmax output layer with 10\\nneurons since the dataset contains scans of 10 different\\ndigits:\\n\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nDense,\\nFlatten\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\n\\nmodel\\nSequential()\\nmodel.add(Conv2D(32,\\nactivation='relu',\\ninput_shape=(28,\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(64,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(10,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 349, 'file_type': 'pdf'}, page_content=\"metrics=['accuracy'])\\nmodel.summary(line_length=80)\\nFigure\\xa010-8 shows the output from the call to summary on the\\nfinal line. The summary reveals a lot about how this CNN\\nprocesses images. Each pooling layer reduces the image size\\nby half, while each convolution layer reduces the image’s\\nheight and width by two pixels. Why is that? By default, a\\nconvolution kernel doesn’t start with its center cell over\\nthe pixel in the upper-left corner of the image; rather, its\\nupper-left corner is aligned with the image’s upper-left\\ncorner. For a 3 × 3 kernel, there’s a 1-pixel-wide border\\naround the edges that doesn’t survive the convolution. (For\\na 5 × 5 kernel, the border that doesn’t survive is 2 pixels\\nwide.) The term for this is padding, and if you’d like, you\\ncan override the default behavior to push the kernel’s\\ncenter cell right up to the edges of the image. In Keras,\\nthis is accomplished by including a padding='same' parameter\\nin the call to Conv2D.\\nFigure 10-8. Output from the summary method\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 350, 'file_type': 'pdf'}, page_content='Another takeaway is that each 28 × 28 image exits the first\\nconvolution layer as a 3D array or tensor measuring 26 × 26\\n× 32: one 26 × 26 feature map for each of the 32 kernels.\\nAfter max pooling, the tensor is reduced to 13 × 13 × 32\\nand input to the second convolution layer, where 64 more\\nkernels filter features from the thirty-two 13 × 13 feature\\nmaps and combine them to produce 64 new feature maps (a\\ntensor measuring 11 × 11 × 64). A final pooling layer\\nreduces that to 5 × 5 × 64. These values are flattened into\\na 1D tensor containing 1,600 values and fed into a dense\\nlayer for classification.\\nNOTE\\nThe big picture here is that the CNN transforms each 28 × 28\\nimage comprising 784 pixel values into an array of 1,600 floating-\\npoint numbers that (hopefully) distinguishes the contents of the\\nimage more clearly than ordinary pixel values do. That’s what\\nbottleneck layers do: they transform matrices of integer pixel\\nvalues into tensors of floating-point numbers that better\\ncharacterize the images input to them. As you’ll see in\\nChapter\\xa013, NLP networks use word embeddings to create dense\\nvector representations of the words in a document. Dense vector\\nrepresentation is a term you’ll encounter a lot in deep learning.\\nIt’s nothing more than arrays of floating-point numbers that do\\nmore to characterize the input than the input data itself.\\nThe output from summary would look exactly the same if the\\nimages input to the network were three-channel color images\\nrather than one-channel grayscale images. Applying a\\nconvolution layer with n kernels to an image produces n\\nfeature maps regardless of image depth, just as applying a\\nconvolution layer featuring n kernels to the feature maps\\noutput by preceding layers produces n new feature maps'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 351, 'file_type': 'pdf'}, page_content=\"regardless of input depth. Internally, CNNs use tensor dot\\nproducts to produce 2D feature maps from 3D feature maps.\\nPython’s NumPy library includes a function named tensordot\\nfor computing tensor dot products quickly.\\nNow train the network and plot the training and validation\\naccuracy:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=10,\\nbatch_size=50)\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nOnce trained, this simple CNN can achieve 99% accuracy\\nclassifying handwritten digits:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 352, 'file_type': 'pdf'}, page_content='One reason it can attain such accuracy is the number of\\ntraining samples—roughly 6,000 per class. (As a test, I\\ntrained the network with just 100 samples of each class and\\ngot 92% accuracy.) Another factor is that a 2 looks very\\ndifferent from, say, an 8. If a person can rather easily\\ndistinguish between the two, then a CNN can too.\\nTraining a CNN to Recognize Arctic Wildlife\\nA basic CNN can easily achieve 99% accuracy on the MNIST\\ndataset. But it isn’t as easy when the problem is more\\nperceptual—for example, when the goal is to determine\\nwhether a photo contains a dog or a cat. One reason is that\\nmost 8s look a lot alike, while dogs and cats come in many\\nvarieties. Another factor is that each digit in the MNIST\\ndataset is carefully cropped to precisely fill the frame,\\nwhereas dogs and cats can appear anywhere in the frame and'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 353, 'file_type': 'pdf'}, page_content='can be photographed in different poses and from an infinite\\nnumber of angles.\\nTo demonstrate, let’s train a CNN to distinguish between\\nArctic foxes, polar bears, and walruses. For context, imagine\\nyou’ve been tasked with creating a system that uses AI to\\nexamine pictures snapped by motion-activated cameras deployed\\nin the Arctic to document polar bear activity.\\nStart by downloading a ZIP file containing images for\\ntraining and testing the CNN. Unpack the ZIP file and place\\nits contents in a subdirectory named Wildlife where your\\nJupyter notebooks are hosted. The ZIP file contains folders\\nnamed train, test, and samples. Each folder contains\\nsubfolders named arctic_fox, polar_bear, and walrus. The\\ntraining folders contain 100 images each, while the test\\nfolders contain 40 images each. Figure\\xa010-9 shows some of\\nthe polar bear training images. These are public images that\\nwere downloaded from the internet and cropped and resized to\\n224 × 224 pixels.\\nFigure 10-9. Polar bear images\\nNow create a Jupyter notebook and use the following code to\\ndefine a pair of helper functions—one to load a batch of\\nimages from a specified location in the filesystem and assign\\nthem labels, and another to show the first eight images in a\\nbatch of images:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 354, 'file_type': 'pdf'}, page_content=\"import\\nos\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndef\\nload_images_from_path(path,\\nlabel):\\n\\xa0\\xa0\\xa0 images,\\nlabels\\n\\n\\xa0\\xa0\\xa0 for\\nfile\\nin\\nos.listdir(path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 img\\nimage.load_img(os.path.join(path,\\nfile),\\ntarget_size=(224,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 images.append(image.img_to_array(img))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 labels.append((label))\\n\\n\\xa0\\xa0\\xa0 return\\nimages,\\nlabels\\n\\ndef\\nshow_images(images):\\n\\xa0\\xa0\\xa0 fig,\\naxes\\nplt.subplots(1,\\nfigsize=(20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n'yticks':\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.imshow(images[i]\\n\\nx_train,\\ny_train,\\nx_test,\\ny_test\\nUse the following statements to load 100 Arctic fox training\\nimages and plot a subset of them:\\n\\nimages,\\nlabels\\nload_images_from_path('Wildlife/train/arctic_fox',\\nshow_images(images)\\n\\nx_train\\nimages\\ny_train\\nlabels\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 355, 'file_type': 'pdf'}, page_content=\"Do the same to load and label the polar bear training images:\\n\\nimages,\\nlabels\\nload_images_from_path('Wildlife/train/polar_bear',\\nshow_images(images)\\n\\nx_train\\nimages\\ny_train\\nlabels\\nAnd then the walrus training images:\\n\\nimages,\\nlabels\\nload_images_from_path('Wildlife/train/walrus',\\nshow_images(images)\\n\\nx_train\\nimages\\ny_train\\nlabels\\nYou also need to load the images used to validate the CNN.\\nStart with 40 Arctic fox test images:\\n\\nimages,\\nlabels\\nload_images_from_path('Wildlife/test/arctic_fox',\\nshow_images(images)\\n\\nx_test\\nimages\\ny_test\\nlabels\\nThen the polar bear test images:\\n\\nimages,\\nlabels\\nload_images_from_path('Wildlife/test/polar_bear',\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 356, 'file_type': 'pdf'}, page_content=\"show_images(images)\\n\\nx_test\\nimages\\ny_test\\nlabels\\nAnd finally the walrus test images:\\n\\nimages,\\nlabels\\nload_images_from_path('Wildlife/test/walrus',\\nshow_images(images)\\n\\nx_test\\nimages\\ny_test\\nlabels\\nThe next step is to normalize the training and testing images\\nby dividing their pixel values by 255:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nx_train\\nnp.array(x_train)\\nx_test\\nnp.array(x_test)\\n\\ny_train\\nnp.array(y_train)\\ny_test\\nnp.array(y_test)\\nNow it’s time to build a CNN. Since the images measure 224\\n× 224 and we want the final feature maps to compress as much\\ninformation as possible into a small space, we’ll use five\\npairs of convolution and pooling layers to extract features\\nfrom the training images at five resolutions: 224 × 224, 111\\n× 111, 54 × 54, 26 × 26, and 12 × 12. We’ll follow those\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 357, 'file_type': 'pdf'}, page_content=\"with a dense layer and a softmax output layer containing\\nthree neurons—one for each of the three classes:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nFlatten,\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Conv2D(32,\\nactivation='relu',\\ninput_shape=(224,\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(64,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(64,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(128,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(128,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary(line_length=80)\\nCall fit to train the model:\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=20)\\nIf you train the model on a CPU, training will probably\\nrequire from 10 to 20 seconds per epoch. (Think of all those\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 358, 'file_type': 'pdf'}, page_content=\"pixel calculations taking place on all those images with all\\nthose convolution kernels.) When training is complete, use\\nthe following statements to plot the training and validation\\naccuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nHere is the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 359, 'file_type': 'pdf'}, page_content='Were the results what you expected? The validation accuracy\\nis decent, but it’s not state of the art. It probably landed\\nbetween 60% and 70%. Modern CNNs often do 95% or better\\nclassifying images such as these. You might be able to\\nsqueeze more out of this model by stacking convolution layers\\nor increasing the number of kernels, and you might get it to\\ngeneralize slightly better by introducing a dropout layer.\\nBut you won’t reach 95% with this network and this dataset.\\nOne of the reasons modern CNNs can do image classification so\\naccurately is that they’re trained with millions of images.\\nYou don’t need millions of samples of each class, but you\\nprobably need at least an order of magnitude more—if not two\\norders of magnitude more—than the 300 you trained with here.\\nYou could scour the internet for more images, but more images\\nmeans more training time. If the goal is to achieve an\\naccuracy of 95% or more, you’ll quickly get to the point'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 360, 'file_type': 'pdf'}, page_content='where the CNN takes too long to train—or find yourself\\nshopping for an NVIDIA GPU.\\nThat doesn’t mean CNNs aren’t practical for solving\\nbusiness problems. It just means that there’s more to learn.\\nThe next section is the first step in understanding how to\\nattain high levels of accuracy without training a CNN from\\nscratch.\\nPretrained CNNs\\nMicrosoft, Google, and other tech companies use a subset of\\nthe ImageNet dataset containing more than 1 million images to\\ntrain state-of-the-art CNNs to recognize hundreds of objects,\\nincluding Arctic foxes and polar bears. Then they make them\\navailable for public consumption. Called pretrained CNNs,\\nthey are more sophisticated than anything you’re likely to\\ntrain yourself. And if that’s not awesome enough, Keras\\nreduces the process of loading a pretrained CNN to one line\\nof code.\\nKeras provides classes that wrap more than two dozen popular\\npretrained CNNs. The full list is documented on the Keras\\nwebsite. Most of these CNNs are documented in scholarly\\npapers such as “Deep Residual Learning for Image\\nRecognition” and “EfficientNet: Rethinking Model Scaling\\nfor Convolutional Neural Networks”. Some have won\\nprestigious competitions such as the ImageNet Large Scale\\nVisual Recognition Challenge and the COCO Detection\\nChallenge. Among the most notable are the ResNet family of\\nnetworks from Microsoft and the Inception networks from\\nGoogle. Also noteworthy is MobileNet, which trades size for\\naccuracy and is ideal for mobile devices due to its small\\nmemory footprint. You can learn more about it in the Google\\nAI blog.\\nPRETRAINED CNN ARCHITECTURES'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 361, 'file_type': 'pdf'}, page_content='Pretrained CNNs achieve impressive levels of accuracy not\\njust because they’re trained with millions of images but\\nalso because they are deeper and architecturally more\\nadvanced than the CNNs presented thus far. For example,\\nthey use consecutive convolution layers to further refine\\nthe features extracted from each image, and many use\\nbatch normalization to normalize (standardize to unit\\nvariance) values propagated between layers during\\ntraining.\\nBut pretrained CNNs are more sophisticated for other\\nreasons as well. ResNets, for example, pioneered a\\nconcept called residual layers, which add their input to\\ntheir output. This simple innovation helped mitigate the\\nvanishing-gradient problem that causes updates to become\\nincreasingly smaller as the optimizer goes backward\\nthrough the network updating weights and biases, and it\\nmade much deeper networks possible.\\nAnd then there is the Inception family of CNNs, whose\\nchief innovation was the use of 1 × 1 convolution\\nkernels to minimize the computational overhead of larger\\nconvolution kernels by reducing the depth of the feature\\nmaps input to them. Recent versions of Inception have\\nincorporated features of ResNets as well, and a\\nderivative architecture known as Xception improved on\\nInception by introducing depthwise separable\\nconvolutions. If you’d like to learn more, check out “A\\nSimple Guide to the Versions of the Inception Network”\\nand “Xception: Deep Learning with Depthwise Separable\\nConvolutions”.\\nThe following statement instantiates Keras’s MobileNetV2\\nclass and initializes it with the weights, biases, and kernel\\nvalues arrived at when the network was trained on the\\nImageNet dataset:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 362, 'file_type': 'pdf'}, page_content=\"from\\ntensorflow.keras.applications\\nimport\\nMobileNetV2\\n\\nmodel\\nMobileNetV2(weights='imagenet')\\nThe weights='imagenet' parameter tells Keras what parameters\\nto load to re-create the network in its trained state. You\\ncan also pass a path to a file containing custom weights, but\\nimagenet is the only set of predefined weights that are\\ncurrently supported.\\nBefore an image is submitted to a pretrained CNN for\\nclassification, it must be sized to the dimensions the CNN\\nexpects—typically 224 × 224—and preprocessed. Different\\nCNNs expect images to be preprocessed in different ways, so\\nKeras provides a preprocess_input function for each\\npretrained CNN. It also includes utility functions for\\nloading and resizing images. The following statements load an\\nimage from the filesystem and preprocess it for input to the\\nMobileNetV2 network:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.mobilenet\\nimport\\npreprocess_input\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\nx\\nimage.load_img('arctic_fox.jpg',\\ntarget_size=(224,\\n224))\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\nIn most cases, preprocess_input does all the work that’s\\nneeded, which often involves applying unit variance to pixel\\nvalues and converting RGB images to BGR format. In some\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 363, 'file_type': 'pdf'}, page_content=\"cases, however, you still need to divide the pixel values by\\n255. ResNet50V2 is one example:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\nx\\nimage.load_img('arctic_fox.jpg',\\ntarget_size=(224,\\n224))\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\nOnce an image is preprocessed, making a prediction is as\\nsimple as calling the network’s predict method:\\n\\ny\\nmodel.predict(x)\\nTo help you interpret the output, Keras also provides a\\nnetwork-specific decode\\u200b_pre\\u2060dictions method. Figure\\xa010-10\\nshows what that method returned for a photo submitted to\\nResNet50V2.\\nFigure 10-10. Output from decode_predictions\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 364, 'file_type': 'pdf'}, page_content=\"ResNet50V2 is 89% sure the photo contains an Arctic fox—\\nwhich, it so happens, it does. MobileNetV2 predicted with 92%\\ncertainty that the photo contains an Arctic fox. Both\\nnetworks were trained on the same dataset, but different\\npretrained CNNs classify images slightly differently.\\nUsing ResNet50V2 to Classify Images\\nLet’s use Keras to load a pretrained CNN and classify a pair\\nof images. Fire up a notebook and use the following\\nstatements to load ResNet50V2:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50V2\\n\\nmodel\\nResNet50V2(weights='imagenet')\\nmodel.summary()\\nNext, load an Arctic fox image and show it in the notebook:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\nx\\nimage.load_img('Wildlife/samples/arctic_fox/arctic_fox_140.jpeg',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nNow preprocess the image (remember that for ResNet50V2, you\\nalso have to divide all the pixel values by 255 after calling\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 365, 'file_type': 'pdf'}, page_content=\"Keras’s preprocess_input method) and pass it to the CNN for\\nclassification:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\ndecode_predictions\\n\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\n\\ny\\nmodel.predict(x)\\ndecode_predictions(y)\\nThe output should look like this:\\n\\n[[('n02120079', 'Arctic_fox', 0.9999944),\\n\\xa0 ('n02114548', 'white_wolf', 4.760021e-06),\\n\\xa0 ('n02119789', 'kit_fox', 2.3306782e-07),\\n\\xa0 ('n02442845', 'mink', 1.2460312e-07),\\n\\xa0 ('n02111889', 'Samoyed', 1.1914468e-07)]]\\nResNet50V2 is virtually certain that the image contains an\\nArctic fox. But now load a walrus image:\\n\\nx\\nimage.load_img('Wildlife/samples/walrus/walrus_143.png',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 366, 'file_type': 'pdf'}, page_content=\"Ask ResNet50V2 to classify it:\\n\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\n\\ny\\nmodel.predict(x)\\ndecode_predictions(y)\\nHere’s the output:\\n\\n[[('n02454379', 'armadillo', 0.63758147),\\n\\xa0 ('n01704323', 'triceratops', 0.16057032),\\n\\xa0 ('n02113978', 'Mexican_hairless', 0.07795086),\\n\\xa0 ('n02398521', 'hippopotamus', 0.022284042),\\n\\xa0 ('n01817953', 'African_grey', 0.016944142)]]\\nResNet50V2 thinks the image is most likely an armadillo, but\\nit’s not even very sure about that. Can you guess why?\\nResNet50V2 was trained with almost 1.3 million images. None\\nof them, however, contained a walrus. The ImageNet 1000 Class\\nList shows a complete list of classes it was trained to\\nrecognize. A pretrained CNN is great when you need it to\\nclassify images using the classes it was trained with, but it\\nis powerless to handle domain-specific tasks that it wasn’t\\ntrained for.\\nBut all is not lost. A technique called transfer learning\\nenables pretrained CNNs to be repurposed to solve domain-\\nspecific problems. The repurposing can be done on an ordinary\\nCPU; no GPU required. Transfer learning sometimes achieves\\n95% accuracy with just a few hundred training images. Once\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 367, 'file_type': 'pdf'}, page_content='you learn about it, you’ll have a completely different\\nperspective on the efficacy of using CNNs to solve business\\nproblems.\\nTransfer Learning\\nEarlier, you used a dataset with photos of Arctic foxes,\\npolar bears, and walruses to train a CNN to recognize Artic\\nwildlife. Trained with 300 images—100 for each of the three\\nclasses—the CNN achieved an accuracy of around 60%. That’s\\nnot sufficient for most purposes.\\nOne solution is to train the CNN with tens of thousands of\\nphotos. A better solution—one that can deliver world-class\\naccuracy with the 300 photos you have and doesn’t require\\nexpensive hardware—is transfer learning. In the hands of\\nsoftware developers and engineers, transfer learning makes\\nCNNs a practical solution for a variety of computer-vision\\nproblems. And it requires orders of magnitude less time and\\ncompute power than CNNs trained from scratch. Let’s take a\\nmoment to understand what transfer learning is and how it\\nworks—and then put it to work identifying Arctic wildlife.\\nPretrained CNNs trained on the ImageNet dataset can identify\\nArctic foxes and polar bears, but they can’t identify\\nwalruses because they weren’t trained with walrus images.\\nTransfer learning lets you repurpose pretrained CNNs to\\nidentify objects they weren’t originally trained to\\nidentify. It leverages the intelligence baked into pretrained\\nCNNs, but it repurposes that intelligence to solve new\\nproblems.\\nRecall that a CNN has two groups of layers: bottleneck layers\\ncontaining the convolution and pooling layers that extract\\nfeatures from images at various resolutions, and\\nclassification layers, which classify features output from\\nthe bottleneck layers as belonging to an Arctic fox, a polar\\nbear, or something else. Convolution layers use convolution'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 368, 'file_type': 'pdf'}, page_content='kernels to extract features, and the values in the\\nconvolution kernels are learned during training. This\\nlearning accounts for the bulk of the training time. When\\nsophisticated CNNs are trained with millions of images, the\\nconvolution kernels become very efficient at extracting\\nfeatures. But that efficiency comes at a cost.\\nThe premise behind transfer learning is shown in Figure\\xa010-\\n11. You load the bottleneck layers of a pretrained CNN, but\\nyou don’t load the classification layers. Instead, you\\nprovide your own, which train orders of magnitude more\\nquickly than an entire CNN. Then you pass the training images\\nthrough the bottleneck layers for feature extraction and\\ntrain the classification layers on the resulting features.\\nThe pretrained CNN might have been trained to extract\\nfeatures from pictures of apples and oranges, but those same\\nlayers are probably pretty good at extracting features from\\nphotos of dogs and cats too. By using the pretrained\\nbottleneck layers to extract features and then using those\\nfeatures to train your own classification layers, you can\\nteach the model that a certain feature extracted from an\\nimage might be indicative of a dog rather than an apple.\\nFigure 10-11. Neural network architecture for transfer learning\\nTransfer learning is relatively simple to implement with\\nKeras and TensorFlow. Recall that the following statement'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 369, 'file_type': 'pdf'}, page_content=\"loads ResNet50V2 and initializes it with the weights\\n(including kernel values) and biases that were arrived at\\nwhen the network was trained on a subset of the ImageNet\\ndataset:\\n\\nbase_model\\nResNet50V2(weights='imagenet')\\nTo load ResNet50V2 (or any other pretrained CNN that Keras\\nsupports) without the classification layers, you simply add\\nan include_top=False attribute:\\n\\nbase_model\\nResNet50V2(weights='imagenet',\\ninclude_top=False)\\nFrom that point, there are two ways to go about transfer\\nlearning. The first involves appending classification layers\\nto the base model’s bottleneck layers and setting each base\\nlayer’s trainable attribute to False so that the weights,\\nbiases, and convolution kernels won’t be updated when the\\nnetwork is trained:\\n\\nfor\\nlayer\\nin\\nbase_model.layers:\\n\\xa0\\xa0\\xa0 layer.trainable\\nFalse\\n\\nmodel\\nSequential()\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 370, 'file_type': 'pdf'}, page_content=\"model.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=10,\\nbatch_size=10)\\nThe second technique is to run all the training images\\nthrough the base model for feature extraction, and then run\\nthe features through a separate network containing your\\nclassification layers:\\n\\nfeatures\\nbase_model.predict(x)\\n\\nmodel\\nSequential()\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\n\\nmodel.fit(features,\\ny,\\nvalidation_split=0.2,\\nepochs=10,\\nbatch_size=10)\\nWhich technique is better? The second is faster because the\\ntraining images go through the bottleneck layers for feature\\nextraction just one time rather than once per epoch. It’s\\nthe technique you should use in the absence of a compelling\\nreason to do otherwise. The first technique is slightly\\nslower, but it lends itself to fine-tuning, in which you\\nunfreeze one or more bottleneck layers after training is\\ncomplete and train for a few more epochs with a very low\\nlearning rate. It also facilitates data augmentation, which\\nI’ll introduce in the next section.\\nNOTE\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 371, 'file_type': 'pdf'}, page_content='Fine-tuning is frequently applied to transfer-learning models\\nafter training is complete in an effort to squeeze out an extra\\npercentage point or two of accuracy. We will use fine-tuning in\\nChapter\\xa013 to increase the accuracy of an NLP model that utilizes\\na pretrained neural network.\\nIf you use the first technique to implement transfer\\nlearning, you make predictions by preprocessing the images\\nand passing them to the model’s predict method. For the\\nsecond technique, making predictions is a two-step process.\\nAfter preprocessing the images, you pass them to the base\\nmodel’s predict method, and then you pass the output from\\nthat method to your model’s predict method:\\n\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\n\\nfeatures\\nbase_model.predict(x)\\npredictions\\nmodel.predict(features)\\nAnd with that, transfer learning is complete. All that\\nremains is to put it in practice.\\nUsing Transfer Learning to Identify Arctic\\nWildlife\\nLet’s use transfer learning to solve the same problem that\\nwe attempted to solve earlier with a scratch-built CNN:\\nbuilding a model that determines whether a photo contains an\\nArctic fox, a polar bear, or a walrus.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 372, 'file_type': 'pdf'}, page_content=\"Create a Jupyter notebook and use the same code you used\\nearlier to load the training and test images and assign\\nlabels to them: 0 for Arctic foxes, 1 for polar bears, and 2\\nfor walruses. Once that’s done, the next step is to\\npreprocess the images. We’ll use ResNet50V2 as our\\npretrained CNN, so use the ResNet version of preprocess_input\\nto preprocess the pixels. Then divide the pixel values by\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nx_train\\npreprocess_input(np.array(x_train))\\nx_test\\npreprocess_input(np.array(x_test))\\n\\ny_train\\nnp.array(y_train)\\ny_test\\nnp.array(y_test)\\nThe next step is to load ResNet50V2, being careful to load\\nthe bottleneck layers but not the classification layers, and\\nuse it to extract features from the training and test images:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50V2\\n\\nbase_model\\nResNet50V2(weights='imagenet',\\ninclude_top=False)\\n\\nx_train\\nbase_model.predict(x_train)\\nx_test\\nbase_model.predict(x_test)\\nNow train a neural network to classify features extracted\\nfrom the training images:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 373, 'file_type': 'pdf'}, page_content=\"from\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\nHow well did the network train? Plot the training accuracy\\nand validation accuracy for each epoch:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 374, 'file_type': 'pdf'}, page_content=\"Your results will differ from mine, but I got about 97%\\naccuracy. If you didn’t quite get there, try training the\\nnetwork again:\\nFinally, use a confusion matrix to visualize how well the\\nnetwork distinguishes between classes:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\nfig,\\nax\\nplt.subplots(figsize=(4,\\nax.grid(False)\\n\\ny_pred\\nmodel.predict(x_test)\\nclass_labels\\n['arctic fox',\\n'polar bear',\\n'walrus']\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 375, 'file_type': 'pdf'}, page_content=\"cmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=class_labels,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHere’s how it turned out for me:\\nTo see transfer learning at work, load one of the Arctic fox\\nimages from the samples folder. That folder contains wildlife\\nimages with which the model was neither trained nor\\nvalidated:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 376, 'file_type': 'pdf'}, page_content=\"x\\nimage.load_img('Wildlife/samples/arctic_fox/arctic_fox_140.jpeg',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nNow preprocess the image, run it through ResNet50V2’s\\nfeature extraction layers, and run the output through the\\nnewly trained classification layers:\\n\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\n\\ny\\nbase_model.predict(x)\\npredictions\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nFor me, the network predicted with almost 100% confidence\\nthat the image contains an Arctic fox:\\n\\narctic fox: 1.0\\npolar bear: 0.0\\nwalrus: 0.0\\nPerhaps that’s not surprising, since ResNet50V2 was trained\\nwith Arctic fox images. But now let’s load a walrus image,\\nwhich, you’ll recall, ResNet50V2 was unable to classify:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 377, 'file_type': 'pdf'}, page_content=\"x\\nimage.load_img('Wildlife/samples/walrus/walrus_143.png',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nPreprocess the image and make a prediction:\\n\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\n\\ny\\nbase_model.predict(x)\\npredictions\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nHere’s how it turned out this time:\\n\\narctic fox: 0.0\\npolar bear: 0.0\\nwalrus: 1.0\\nResNet50V2 wasn’t trained to recognize walruses, but your\\nnetwork was. That’s transfer learning in a nutshell. It’s\\nthe deep-learning equivalent of having your cake and eating\\nit too. And it’s the secret sauce that makes CNNs a viable\\ntool for anyone with a laptop and a few hundred training\\nimages.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 378, 'file_type': 'pdf'}, page_content='That’s not to say that transfer learning will always get you\\n97% accuracy with 100 images per class. It won’t. If a\\ndataset lacks the information to achieve that level of\\nseparation, neither scratch-built CNNs nor transfer learning\\nwill magically make it happen. That’s always true in machine\\nlearning and AI. You can’t get water from a rock. And you\\ncan’t build an accurate model from data that doesn’t\\nsupport it.\\nData Augmentation\\nThe previous example demonstrated how to use transfer\\nlearning to build a model that, with just 300 training\\nimages, can classify photos of three different types of\\nArctic wildlife with 97% accuracy. One of the benefits of\\ntransfer learning is that it can do more with fewer images.\\nThis feature is also a bug, however. With just 100 or so\\nsamples of each class, there is little diversity among\\nimages. A model might be able to recognize a polar bear if\\nthe bear’s head is perfectly aligned in the center of the\\nphoto. But if the training images don’t include photos with\\nthe bear’s head aligned differently or tilted at different\\nangles, the model might have difficulty classifying the\\nphoto.\\nOne solution is data augmentation. Rather than scare up more\\ntraining images, you can rotate, translate, and scale the\\nimages you have. It doesn’t always increase a CNN’s\\naccuracy, but it frequently does, especially with small\\ndatasets. Keras makes it easy to randomly transform training\\nimages provided to a network. Images are transformed\\ndifferently in each epoch, so if you train for 10 epochs, the\\nnetwork sees 10 different variations of each training image.\\nThis can increase a model’s ability to generalize with\\nlittle impact on training time. Figure\\xa010-12 shows the\\neffect of applying random transforms to a hot dog image. You\\ncan see why presenting the same image to a model in different'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 379, 'file_type': 'pdf'}, page_content='ways might make the model more adept at recognizing hot dogs,\\nregardless of how the hot dog is framed.\\nFigure 10-12. Hot dog image with random transforms applied\\nKeras has built-in support for data augmentation with images.\\nLet’s look at a couple of ways to put image augmentation to\\nwork, and then apply it to the Arctic wildlife model.\\nImage Augmentation with ImageDataGenerator\\nOne way to apply image augmentation when training a model is\\nto use Keras’s ImageDataGenerator class. ImageDataGenerator\\ngenerates batches of training images on the fly, either from\\nimages you’ve loaded (for example, with Keras’s load_img\\nfunction) or from a specified location in the filesystem. The\\nlatter is especially useful when training CNNs with millions\\nof images because it loads images into memory in batches\\nrather than all at once. Regardless of where the images come\\nfrom, however, ImageDataGenerator is happy to apply\\ntransforms as it serves them up.\\nHere’s a simple example that you can try yourself. Use the\\nfollowing code to load an image from your filesystem, wrap an\\nImageDataGenerator around it, and generate 24 versions of the\\nimage:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nfrom\\ntensorflow.keras.preprocessing.image\\nimport\\nImageDataGenerator\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 380, 'file_type': 'pdf'}, page_content=\"# Load an image\\nx\\nimage.load_img('Wildlife/train/polar_bear/polar_bear_010.jpeg')\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\n\\n# Wrap an ImageDataGenerator around it\\nidg\\nImageDataGenerator(rescale=1./255,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 horizontal_flip=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rotation_range=30,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 width_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 height_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 zoom_range=0.2)\\nidg.fit(x)\\n\\n# Generate 24 versions of the image\\ngenerator\\nidg.flow(x,\\nbatch_size=1,\\nseed=0)\\nfig,\\naxes\\nplt.subplots(3,\\nfigsize=(16,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n'yticks':\\n\\nfor\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0 img,\\nlabel\\ngenerator.next()\\n\\xa0\\xa0\\xa0 ax.imshow(img[0])\\nHere’s the result:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 381, 'file_type': 'pdf'}, page_content='The parameters passed to ImageDataGenerator tell it how to\\ntransform each image it delivers:\\nrescale=1./255\\nDivides each pixel value by 255\\nhorizontal_flip=True\\nRandomly flips the image horizontally (around the\\nvertical axis)\\nrotation_range=30\\nRandomly rotates the image by –30 to 30 degrees\\nwidth_shift_range=0.2 and height_shift_range=0.2\\nRandomly translates the image by –20% to 20%\\nzoom_range=0.2\\nRandomly scales the image by –20% to 20%\\nThere are other parameters that you can use, such as\\nvertical_flip,\\nshear_range, and brightness_range, but you get\\nthe picture. The flow method used in this example generates\\nimages from the images you pass to fit. The related flow_from\\u200b\\n_direc\\u2060tory method loads images from the filesystem and\\noptionally labels them based on the subdirectories they’re\\nin.\\nThe generator returned by flow can be passed directly to a\\nmodel’s fit method to provide randomly transformed images to\\nthe model as it is trained. Assume that x_train and y_train\\nhold a collection of training images and labels. The\\nfollowing code wraps an ImageDataGenerator around them and\\nuses them to train a model:\\n\\nidg\\nImageDataGenerator(rescale=1./255,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 382, 'file_type': 'pdf'}, page_content='horizontal_flip=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rotation_range=30,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 width_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 height_shift_range=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 zoom_range=0.2)\\n\\nidg.fit(x_train)\\nimage_batch_size\\ngenerator\\nidg.flow(x_train,\\ny_train,\\nbatch_size=image_batch_size,\\nseed=0)\\n\\nmodel.fit(generator,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 steps_per_epoch=len(x_train)\\nimage_batch_size,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=10)\\nThe steps_per_epoch parameter is key because an\\nImageDataGenerator can provide an infinite number of versions\\nof each image. In this example, the batch_size parameter\\npassed to flow tells the generator to create 10 images in\\neach batch. Dividing the number of images by the image batch\\nsize to calculate steps_per_epoch ensures that in each\\ntraining epoch, the model is provided with one transformed\\nversion of each image in the dataset.\\nNOTE\\nVersions of Keras prior to 2.1 didn’t allow a generator to be\\npassed to the fit method. Instead, they provided a separate method\\nnamed fit_generator. That method is deprecated and will be removed\\nin a future release.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 383, 'file_type': 'pdf'}, page_content=\"Observe that the call to fit includes a validation_data\\nparameter identifying a separate set of images and labels for\\nvalidating the network during training. You generally don’t\\nwant to augment validation images, so you should avoid using\\nvalidation_split when passing a generator to fit.\\nImage Augmentation with Augmentation Layers\\nYou can use ImageDataGenerator to provide transformed images\\nto a model, but recent versions of Keras provide an\\nalternative in the form of image preprocessing layers and\\nimage augmentation layers. Rather than transform training\\nimages separately, you can integrate the transforms directly\\ninto the model. Here’s an example:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D,\\nFlatten,\\nDense\\nfrom\\ntensorflow.keras.layers\\nimport\\nRescaling,\\nRandomFlip,\\nRandomRotation\\nfrom\\ntensorflow.keras.layers\\nimport\\nRandomTranslation,\\nRandomZoom\\n\\nmodel\\nSequential()\\nmodel.add(Rescaling(1./255))\\nmodel.add(RandomFlip(mode='horizontal'))\\nmodel.add(RandomTranslation(0.2,\\n0.2))\\nmodel.add(RandomRotation(0.2))\\nmodel.add(RandomZoom(0.2))\\nmodel.add(Conv2D(32,\\nactivation='relu',\\ninput_shape=(224,\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(128,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 384, 'file_type': 'pdf'}, page_content='Each image used to train the CNN has its pixel values divided\\nby 255 and is then randomly flipped, translated, rotated, and\\nscaled. Significantly, the RandomFlip, RandomTranslation,\\nRandomRotation, and RandomZoom layers operate only on training\\nimages. They are inactive when the network is validated or\\nasked to make predictions. Consequently, it’s fine to use\\nvalidation_split when training a model that contains image\\naugmentation layers. The Rescaling layer is active at all\\ntimes, meaning you no longer have to remember to divide pixel\\nvalues by 255 before training the model or submitting an\\nimage for classification.\\nApplying Image Augmentation to Arctic\\nWildlife\\nWould image augmentation make transfer learning even better?\\nThere’s one way to find out.\\nCreate a Jupyter notebook and copy the code that loads the\\ntraining and test images from the transfer learning example.\\nThen use the following statements to prepare the data. Note\\nthat there is no need to divide by 255 this time because a\\nRescaling layer will take care of that:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nx_train\\npreprocess_input(np.array(x_train))\\nx_test\\npreprocess_input(np.array(x_test))\\n\\ny_train\\nnp.array(y_train)\\ny_test\\nnp.array(y_test)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 385, 'file_type': 'pdf'}, page_content=\"Now load ResNet50V2 without the classification layers and\\ninitialize it with the ImageNet weights. A key element here\\nis preventing the bottleneck layers from training when the\\nnetwork is trained by setting their trainable attributes to\\nFalse, effectively freezing those layers. Rather than setting\\neach individual layer’s trainable attribute to False, we’ll\\nset trainable to False on the model itself and allow that\\nsetting to be “inherited” by the individual layers:\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50V2\\n\\nbase_model\\nResNet50V2(weights='imagenet',\\ninclude_top=False)\\nbase_model.trainable\\nFalse\\nDefine a network that incorporates rescaling and augmentation\\nlayers, ResNet50V2’s bottleneck layers, and dense layers for\\nclassification. Then train the network:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense,\\nRescaling,\\nRandomFlip\\nfrom\\ntensorflow.keras.layers\\nimport\\nRandomRotation,\\nRandomTranslation,\\nRandomZoom\\n\\nmodel\\nSequential()\\nmodel.add(Rescaling(1./255))\\nmodel.add(RandomFlip(mode='horizontal'))\\nmodel.add(RandomTranslation(0.2,\\n0.2))\\nmodel.add(RandomRotation(0.2))\\nmodel.add(RandomZoom(0.2))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 386, 'file_type': 'pdf'}, page_content=\"metrics=['accuracy'])\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\nHow well did the network train? Plot the training accuracy\\nand validation accuracy for each epoch:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nWith a little luck, the accuracy slightly exceeded that of\\nthe model trained without data augmentation:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 387, 'file_type': 'pdf'}, page_content='NOTE\\nYou may find that the version of the model that uses data\\naugmentation is less accurate than the version that doesn’t. To\\nbe sure, I trained each version 10 times and averaged the results.\\nI found that the data augmentation version delivered, on average,\\nabout 0.5% more accuracy than the version that lacks augmentation.\\nThat’s not a lot, but data scientists frequently go to great\\nlengths to improve accuracy by just a fraction of a percentage\\npoint.\\nUse a confusion matrix to visualize how well the network\\nperformed during testing:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 388, 'file_type': 'pdf'}, page_content=\"from\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\nfig,\\nax\\nplt.subplots(figsize=(4,\\nax.grid(False)\\n\\ny_pred\\nmodel.predict(x_test)\\nclass_labels\\n['arctic fox',\\n'polar bear',\\n'walrus']\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=class_labels,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nHere’s how it turned out for me:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 389, 'file_type': 'pdf'}, page_content='Data scientists sometimes employ data augmentation even when\\nthey’re training a CNN from scratch rather than employing\\ntransfer learning, especially when the dataset is relatively\\nsmall. It’s a useful tool to know about, and one that could\\nmake a difference when you’re trying to squeeze every last\\nounce of accuracy out of a deep-learning model.\\nGlobal Pooling\\nThe purpose of including a Flatten layer in a CNN is to\\nreshape the 3D tensors containing the final feature maps into\\n1D tensors suitable for input to a Dense layer. But Flatten\\nisn’t the only way to do it. Flattening sometimes leads to'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 390, 'file_type': 'pdf'}, page_content=\"overfitting by providing too much information to the\\nclassification layers.\\nOne way to combat overfitting is to introduce a Dropout\\nlayer. Another strategy is to reduce the width of the Dense\\nlayer. A third option is to replace the Flatten layer with a\\nGlobalMaxPooling2D layer or a GlobalAverage\\u200bPool\\u2060ing2D layer.\\nThey, too, output 1D tensors, but they generate them in a\\ndifferent way. And that way is less prone to overfitting.\\nTo demonstrate, modify the MNIST dataset example earlier in\\nthis chapter to use a GlobalMaxPooling2D layer rather than a\\nFlatten layer:\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv2D,\\nMaxPooling2D, \\\\\\n\\xa0\\xa0\\xa0 GlobalMaxPooling2D,\\nDense\\n\\nmodel\\nSequential()\\nmodel.add(Conv2D(32,\\nactivation='relu',\\ninput_shape=(28,\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(64,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(GlobalMaxPooling2D())\\n# In lieu of Flatten()\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(10,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary(line_length=120)\\nThe summary (Figure\\xa010-13) shows that the output from the\\nGlobalMaxPooling2D layer is a tensor containing 64 values—\\none per feature map emitted by the final MaxPooling2D layer—\\nrather than 5 × 5 × 64, or 1,600, values, as it was for the\\nFlatten layer. Each value is the maximum of the 25 values in\\neach 5 × 5 feature map. Had you used GlobalAveragePooling2D\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 391, 'file_type': 'pdf'}, page_content='instead, each value would have been the average of the 25\\nvalues in each feature map.\\nFigure 10-13. Output from the summary method\\nGlobal pooling sometimes increases a CNN’s ability to\\ngeneralize and sometimes does not. For the MNIST dataset, it\\nslightly diminishes accuracy. As is so often the case in\\nmachine learning, the only way to know is to try. And due to\\nthe randomness inherent in training neural networks, it’s\\nalways advisable to train the network several times in each\\nconfiguration and average the results before drawing\\nconclusions.\\nAudio Classification with CNNs\\nImagine that you’re the leader of a group of climate\\nscientists concerned about the planet’s dwindling\\nrainforests. The world loses up to 10 million acres of old-\\ngrowth rainforests each year, much of it due to illegal\\nlogging. Your team plans to convert thousands of discarded\\nsmartphones into solar-powered listening devices and position\\nthem throughout the Amazon to transmit alerts in response to\\nthe sounds of chainsaws and truck engines. You need software\\nthat uses AI to identify such sounds in real time. And you\\nneed it fast, because climate change won’t wait.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 392, 'file_type': 'pdf'}, page_content='An effective way to perform audio classification is to\\nconvert audio streams into spectrogram images, which provide\\nvisual representations of spectrums of frequencies as they\\nvary over time, and use CNNs to classify the spectrograms.\\nThe spectrograms in Figure\\xa010-14 were generated from WAV\\nfiles containing chainsaw sounds. Let’s use transfer\\nlearning to create a model that can identify the telltale\\nsounds of logging operations and distinguish them from\\nambient sounds such as wildlife and thunderstorms.\\nFigure 10-14. Spectrograms generated from audio files containing chainsaw\\nsounds\\nNOTE\\nThe tutorial in this section was inspired by the Rainforest\\nConnection, which uses recycled Android phones to monitor\\nrainforests for sounds of illegal activity. A TensorFlow CNN\\nhosted in the cloud analyzes audio from the phones and may one day\\nrun on the phones themselves with an assist from TensorFlow Lite,\\na smaller version of TensorFlow designed for mobile, embedded, and\\nedge devices. For more information, see “The Fight Against\\nIllegal Deforestation with TensorFlow” in the Google AI blog.\\nIt’s just one example of how AI is making the world a better\\nplace.\\nBegin by downloading a ZIP file containing a dataset of\\nrainforest sounds. (Warning: it’s a 666 MB download.) Create\\na subdirectory named Sounds in the directory where your\\nnotebooks are hosted, and copy the contents of the ZIP file\\ninto the subdirectory. Sounds now contains subdirectories'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 393, 'file_type': 'pdf'}, page_content='named background, chainsaw, engine, and storm. Each\\nsubdirectory contains 100 WAV files. The WAV files in the\\nbackground directory contain rainforest background noises\\nonly, while the files in the other subdirectories include the\\nsounds of chainsaws, engines, and thunderstorms overlaid on\\nthe background noises. I generated these files by using a\\nsoundscape synthesis package named Scaper to combine sounds\\nin the public UrbanSound8K dataset with rainforest sounds.\\nPlay a few of the WAV files on your computer to get a feel\\nfor the sounds they contain.\\nNow create a Jupyter notebook and paste the following code\\ninto the first cell:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\nlibrosa.display,\\nos\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndef\\ncreate_spectrogram(audio_file,\\nimage_file):\\n\\xa0\\xa0\\xa0 fig\\nplt.figure()\\n\\xa0\\xa0\\xa0 ax\\nfig.add_subplot(1,\\n\\xa0\\xa0\\xa0 fig.subplots_adjust(left=0,\\nright=1,\\nbottom=0,\\ntop=1)\\n\\n\\xa0\\xa0\\xa0 y,\\nsr\\nlibrosa.load(audio_file)\\n\\xa0\\xa0\\xa0 ms\\nlibrosa.feature.melspectrogram(y=y,\\nsr=sr)\\n\\xa0\\xa0\\xa0 log_ms\\nlibrosa.power_to_db(ms,\\nref=np.max)\\n\\xa0\\xa0\\xa0 librosa.display.specshow(log_ms,\\nsr=sr)\\n\\n\\xa0\\xa0\\xa0 fig.savefig(image_file)\\n\\xa0\\xa0\\xa0 plt.close(fig)\\n\\ndef\\ncreate_pngs_from_wavs(input_path,\\noutput_path):\\n\\xa0\\xa0\\xa0 if\\nnot\\nos.path.exists(output_path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 os.makedirs(output_path)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 394, 'file_type': 'pdf'}, page_content=\"dir\\nos.listdir(input_path)\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nfile\\nin\\nenumerate(dir):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_file\\nos.path.join(input_path,\\nfile)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_file\\nos.path.join(output_path,\\nfile.replace('.wav',\\n'.png'))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 create_spectrogram(input_file,\\noutput_file)\\nThis code defines a pair of functions to help convert WAV\\nfiles into spectrogram images. create_spectrogram uses a\\nPython package named Librosa to create a spectrogram image\\nfrom a WAV file. create_pngs_from_wavs converts all the WAV\\nfiles in a specified directory into spectrogram images. You\\nwill need to install Librosa if it isn’t installed already.\\nUse the following statements to create PNG files containing\\nspectrograms from all the WAV files in the Sounds\\ndirectory’s subdirectories:\\n\\ncreate_pngs_from_wavs('Sounds/background',\\n'Spectrograms/background')\\ncreate_pngs_from_wavs('Sounds/chainsaw',\\n'Spectrograms/chainsaw')\\ncreate_pngs_from_wavs('Sounds/engine',\\n'Spectrograms/engine')\\ncreate_pngs_from_wavs('Sounds/storm',\\n'Spectrograms/storm')\\nCheck the Spectrograms directory for subdirectories\\ncontaining spectrograms and confirm that each subdirectory\\ncontains 100 PNG files. Then use the following code to define\\ntwo new helper functions for loading and displaying\\nspectrograms, and declare two Python lists—one to store\\nspectrogram images and another to store class labels:\\n\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\n\\ndef\\nload_images_from_path(path,\\nlabel):\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 395, 'file_type': 'pdf'}, page_content=\"images,\\nlabels\\n\\n\\xa0\\xa0\\xa0 for\\nfile\\nin\\nos.listdir(path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 images.append(image.img_to_array(image.load_img(os.path.join(path,\\nfile),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n3))))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 labels.append((label))\\n\\n\\xa0\\xa0\\xa0 return\\nimages,\\nlabels\\n\\ndef\\nshow_images(images):\\n\\xa0\\xa0\\xa0 fig,\\naxes\\nplt.subplots(1,\\nfigsize=(20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n'yticks':\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.imshow(images[i]\\n\\nx,\\ny\\nUse the following statements to load the background\\nspectrogram images, add them to the list named x, and label\\nthem with 0s:\\n\\nimages,\\nlabels\\nload_images_from_path('Spectrograms/background',\\nshow_images(images)\\n\\nx\\nimages\\ny\\nlabels\\nRepeat this process to load chainsaw spectrograms from the\\nSpectrograms/chainsaw directory, engine spectrograms from the\\nSpectrograms/engine directory, and thunderstorm spectrograms\\nfrom the Spectrograms/storm directory. Label chainsaw\\nspectrograms with 1s, engine spectrograms with 2s, and\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 396, 'file_type': 'pdf'}, page_content='thunderstorm spectrograms with 3s. Here are the labels for\\nthe four classes of images:\\n\\n\\xa0Spectrogram type\\xa0 \\xa0Label\\xa0\\n\\xa0Background\\xa0\\n\\xa0Chainsaw\\xa0\\n\\xa0Engine\\xa0\\n\\xa0Storm\\xa0\\nSince this model may one day run on mobile phones, we’ll use\\nMobileNetV2 as the base network. Use the following code to\\npreprocess the pixels and split the images and labels into\\ntwo datasets—one for training and one for testing:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\nfrom\\ntensorflow.keras.applications.mobilenet\\nimport\\npreprocess_input\\n\\nx\\npreprocess_input(np.array(x))\\ny\\nnp.array(y)\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(x,\\ny,\\nstratify=y,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 test_size=0.3,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 random_state=0)\\nCall Keras’s MobileNetV2 function to instantiate MobileNetV2\\nwithout the classification layers. Then run the training data\\nand test data through MobileNetV2 to extract features from\\nthe spectrogram images:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nMobileNetV2'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 397, 'file_type': 'pdf'}, page_content=\"base_model\\nMobileNetV2(weights='imagenet',\\ninclude_top=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_shape=(224,\\n\\ntrain_features\\nbase_model.predict(x_train)\\ntest_features\\nbase_model.predict(x_test)\\nDefine a neural network to classify features extracted by\\nMobileNetV2:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten\\n\\nmodel\\nSequential()\\nmodel.add(Flatten())\\nmodel.add(Dense(512,\\nactivation='relu'))\\nmodel.add(Dense(4,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nNOTE\\nAs an experiment, I replaced the Flatten layer with a Global\\u200b\\nAvera\\u2060gePooling2D layer. Validation accuracy improved slightly, but\\nthe model didn’t generalize as well when tested with audio\\nextracted from a documentary video. This underscores an important\\npoint from Chapter\\xa09: you can have full trust and confidence in a\\nmodel only when it’s tested with data it has never seen before—\\npreferably data that comes from a different source.\\nTrain the network with the features:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 398, 'file_type': 'pdf'}, page_content=\"hist\\nmodel.fit(train_features,\\ny_train,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 validation_data=(test_features,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\nPlot the training and validation accuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe validation accuracy should reach 95% or higher:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 399, 'file_type': 'pdf'}, page_content=\"Run the test images through the network and use a confusion\\nmatrix to assess the results:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\nfig,\\nax\\nplt.subplots(figsize=(4,\\nax.grid(False)\\n\\ny_pred\\nmodel.predict(test_features)\\nclass_labels\\n['background',\\n'chainsaw',\\n'engine',\\n'storm']\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=class_labels,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 400, 'file_type': 'pdf'}, page_content='The network is reasonably adept at identifying clips that\\ndon’t contain the sounds of chainsaws or engines. It\\nsometimes confuses chainsaw sounds and engine sounds. That’s\\nOK, because the presence of either might indicate illicit\\nactivity in a rainforest:\\nThe Sounds directory has a subdirectory named samples\\ncontaining WAV files with which the CNN was neither trained\\nnor validated. The WAV files bear no relation to the samples\\nused for training and testing; they come from a YouTube video\\ndocumenting Brazil’s efforts to curb illegal logging. Let’s\\nuse the model you just trained to analyze these files for\\nsounds of logging activity.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 401, 'file_type': 'pdf'}, page_content=\"Start by creating a spectrogram from the first sample WAV\\nfile, which contains audio of loggers cutting down trees in\\nthe Amazon:\\n\\ncreate_spectrogram('Sounds/samples/sample1.wav',\\n'Spectrograms/sample1.png')\\n\\nx\\nimage.load_img('Spectrograms/sample1.png',\\ntarget_size=(224,\\n224))\\nplt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nPreprocess the spectrogram image, pass it to MobileNetV2 for\\nfeature extraction, and classify the features:\\n\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\n\\ny\\nbase_model.predict(x)\\npredictions\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nNow create a spectrogram from a WAV file that features the\\nsound of a logging truck rumbling through the rainforest:\\n\\ncreate_spectrogram('Sounds/samples/sample2.wav',\\n'Spectrograms/sample2.png')\\n\\nx\\nimage.load_img('Spectrograms/sample2.png',\\ntarget_size=(224,\\n224))\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 402, 'file_type': 'pdf'}, page_content=\"plt.xticks([])\\nplt.yticks([])\\nplt.imshow(x)\\nPreprocess the image, pass it to MobileNetV2 for feature\\nextraction, and classify the features:\\n\\nx\\nimage.img_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\nx\\npreprocess_input(x)\\n\\ny\\nbase_model.predict(x)\\npredictions\\nmodel.predict(y)\\n\\nfor\\ni,\\nlabel\\nin\\nenumerate(class_labels):\\n\\xa0\\xa0\\xa0 print(f'{label}: {predictions[0][i]}')\\nIf the network got either of the samples wrong, try training\\nit again. Remember that a neural network will train\\ndifferently every time, in part because Keras initializes the\\nweights with small random values. In the real world, data\\nscientists often train a neural network several times and\\naverage the results to quantify its accuracy.\\nSummary\\nConvolutional neural networks excel at image classification\\nbecause they use convolution kernels to extract features from\\nimages at different resolutions—features intended to\\naccentuate differences between classes. Convolution layers\\nuse convolution kernels to extract features, and pooling\\nlayers reduce the size of the feature maps output from the\\nconvolution layers. Output from these layers is input to\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 403, 'file_type': 'pdf'}, page_content='fully connected layers for classification. Keras provides\\nimplementations of convolution and pooling layers in classes\\nsuch as Conv2D and MaxPooling2D.\\nTraining a CNN from scratch when there is a relatively high\\ndegree of separation between classes—for example, the MNIST\\ndataset—is feasible on an ordinary laptop or PC. Training a\\nCNN to solve a more perceptual problem requires more training\\nimages and commensurately more compute power. Transfer\\nlearning is a practical alternative to training CNNs from\\nscratch. It uses the intelligence already present in the\\nbottleneck layers of pretrained CNNs to extract features from\\nimages, and then uses its own classification layers to\\ninterpret the results.\\nData augmentation can increase the accuracy of a CNN trained\\nwith a relatively small number of images and is especially\\nuseful with transfer learning. Augmentation involves applying\\nrandom transforms such as translations and rotations to the\\ntraining images. You can transform images before inputting\\nthem to the network with Keras’s ImageDataGenerator class,\\nor you can build the transforms into the network with layers\\nsuch as RandomRotation and RandomTranslation. Layers that\\ntransform images are active at training time but inactive\\nwhen the network makes predictions.\\nCNNs are applicable to a wide variety of computer-vision\\nproblems and are almost single-handedly responsible for the\\nrapid advancements made in that field in the past decade.\\nThey play an important role in modern facial recognition\\nsystems too. Want to know more? Detecting and identifying\\nfaces in photographs is the subject of the next chapter.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 404, 'file_type': 'pdf'}, page_content='Chapter 11. Face Detection and\\nRecognition\\nNot long ago, I boarded a flight to Europe and was surprised\\nthat I didn’t have to show my passport. I passed in front of\\na camera and was promptly welcomed aboard the flight. It was\\npart of an early pilot for Delta Air Lines’ effort to push\\nforward with facial recognition and offer a touchless curb-\\nto-gate travel experience.\\nFacial recognition is everywhere. It’s one of the most\\ncommon, and sometimes controversial, applications for AI.\\nMeta, formerly known as Facebook, uses it to tag friends in\\nphotos—at least it did until it killed the feature due to\\nprivacy concerns. Apple uses it to allow users to unlock\\ntheir iPhones, while Microsoft uses it to unlock Windows PCs.\\nUber uses it to confirm the identity of its drivers. Used\\nproperly, facial recognition has vast potential to make the\\nworld a better, safer, and more secure place.\\nSuppose you want to build a system that identifies people in\\nphotos or video frames. Perhaps it’s part of a security\\nsystem that restricts access to college dorms to students and\\nstaff who are authorized to enter. Or perhaps you’re writing\\nan app that searches your hard disk for photos of people you\\nknow. (“Show me all the photos of me and my daughter.”)\\nBuilding systems such as these requires algorithms or models\\ncapable of:\\n\\nFinding faces in photos or video frames, a\\nprocess known as face detection\\nIdentifying the faces detected, a process known\\nas facial recognition or face identification'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 405, 'file_type': 'pdf'}, page_content='Numerous well-known algorithms exist for finding and\\nidentifying faces in photos. Some rely on deep learning—in\\nparticular, convolutional neural networks—while some do not.\\nFacial recognition, after all, predated the explosion of deep\\nlearning by decades. But deep learning has supercharged the\\nscience of facial recognition and made it more practical than\\never before.\\nThis chapter begins by introducing two popular face detection\\nmethods. Then it moves on to facial recognition and\\nintroduces transfer learning as a means for recognizing\\nfaces. It concludes with a tutorial in which you put the\\npieces together and build a facial recognition system of your\\nown. Sound like fun? Then let’s get started.\\nFace Detection\\nThe sections that follow introduce two widely used algorithms\\nfor face detection—one that relies on machine learning and\\nanother that uses deep learning—as well as libraries that\\nimplement them. The goal is to be able to find all the faces\\nin a photo or video frame like the one in Figure\\xa011-1.\\nAfterward, I’ll present an easy-to-use function that you can\\ncall to extract all the facial images from a photo and save\\nthem to disk or submit them to a facial recognition model.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 406, 'file_type': 'pdf'}, page_content='Figure 11-1. Face detection\\nFace Detection with Viola-Jones\\nOne of the fastest and most popular algorithms for detecting\\nfaces in photos stems from a paper published in 2001 titled\\n“Rapid Object Detection Using a Boosted Cascade of Simple\\nFeatures”. Sometimes known as Viola-Jones (the authors of\\nthe paper), the algorithm keys on the relative intensities of\\nadjacent blocks of pixels. For example, the average pixel\\nintensity in a rectangle around the eyes is typically darker\\nthan the average pixel intensity in a rectangle immediately\\nbelow that area. Similarly, the bridge of the nose is usually\\nlighter than the region around the eyes, so two dark\\nrectangles with a bright rectangle in the middle might\\nrepresent two eyes and a nose. The presence of many such\\nHaar-like features in a frame at the right locations is an\\nindicator that the frame contains a face (Figure\\xa011-2).'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 407, 'file_type': 'pdf'}, page_content='Figure 11-2. Face detection using Haar-like features\\nViola-Jones works by sliding windows of various sizes over an\\nimage looking for frames with Haar-like features in the right\\nplaces. At each stop, the pixels in the window are scaled to\\na specified size (typically 24 × 24), and features are\\nextracted and fed into a binary classifier that returns\\npositive indicating the frame contains a face or negative\\nindicating it does not. Then the window slides to the next\\nlocation and the detection regimen begins again.\\nThe key to Viola-Jones’s performance is the binary\\nclassifier. A frame that is 24 pixels wide and 24 pixels high\\ncontains more than 160,000 combinations of rectangles\\nrepresenting potential Haar-like features. Rather than\\ncompute values for every combination, Viola-Jones computes\\nonly those that the classifier requires. Furthermore, how\\nmany features the classifier requires depends on the content\\nof the frame. The classifier is actually several binary\\nclassifiers arranged in stages. The first stage might require\\njust one feature. The second stage might require 10, the\\nthird might require 20, and so on. Features are extracted and\\npassed to stage n only if stage n – 1 returns positive,\\ngiving rise to the term cascade classifier.\\nFigure\\xa011-3 depicts a three-stage cascade classifier. Each\\nstage is carefully tuned to achieve a 100% detection rate\\nusing a limited number of features even if the false-positive\\nrate is high. In the first stage, one feature determines\\nwhether the frame contains a face. A positive response means\\nthe frame might contain a face; a negative response means'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 408, 'file_type': 'pdf'}, page_content='that it most certainly doesn’t, in which case no further\\nchecks are performed. If stage 1 returns positive, however,\\n10 other features are extracted and passed to stage 2. A\\nframe is judged to contain a face only if all stages return\\npositive, yielding a cumulative false-positive rate near\\nzero. In machine learning, this is a design pattern known as\\nhigh recall then precision because while individual stages\\nare tuned for high recall, the cumulative effect is one of\\nhigh precision.\\nFigure 11-3. Face detection using a cascade classifier\\nOne benefit of this architecture is that frames lacking faces\\ntend to fall out fast because they evoke a negative response\\nearly in the cascade. Because most frames don’t contain\\nfaces, the algorithm runs very quickly until it encounters a\\nframe that does. In testing with a 38-stage classifier\\ntrained on 6,061 features from 4,916 facial images, Viola and\\nJones found that, on average, just 10 features were extracted\\nfrom each frame.\\nThe efficacy of Viola-Jones depends on the cascade\\nclassifier, which is essentially a machine learning model\\ntrained with facial and nonfacial images. Training is slow,\\nbut predictions are fast. In some respects, Viola-Jones acts\\nlike a CNN handcrafted to extract the minimum number of\\nfeatures needed to determine whether a frame contains a face.\\nTo speed feature extraction, Viola-Jones uses a clever\\nmathematical trick called integral images to rapidly compute\\nthe difference in intensity between two blocks of pixels. The\\nresult is a system that can identify bounding boxes'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 409, 'file_type': 'pdf'}, page_content=\"surrounding faces in an image with a relatively high degree\\nof accuracy, and it can do so quickly enough to detect faces\\nin live video frames.\\nUsing the OpenCV Implementation of Viola-\\nJones\\nOpenCV is a popular open source computer-vision library\\nthat’s free for commercial use. It provides an\\nimplementation of Viola-Jones in its CascadeClassifier class,\\nalong with an XML file containing a cascade classifier\\ntrained to detect faces. The following statements use Cascade\\u200b\\nClassi\\u2060fier in a Jupyter notebook to detect faces in an image\\nand draw rectangles around the faces. You can use an image of\\nyour own or download the one featured in my example from\\nGitHub:\\n\\nimport\\ncv2\\nfrom\\ncv2\\nimport\\nCascadeClassifier\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\nimage\\nplt.imread('Data/Amsterdam.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\n\\nmodel\\nCascadeClassifier(cv2.data.haarcascades\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'haarcascade_frontalface_default.xml')\\nfaces\\nmodel.detectMultiScale(image)\\n\\nfor\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\nface\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 410, 'file_type': 'pdf'}, page_content=\"rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\nHere’s the output with a photo of a mother and her daughter\\ntaken in Amsterdam a few years ago:\\nCascadeClassifier detected the two faces in the photo, but it\\nalso suffered a number of false positives. One way to\\nmitigate that is to use the minNeighbors parameter. It\\ndefaults to 3, but higher values make CascadeClassifier more\\nselective. With minNeighbors=20, detectMultiScale finds just\\nthe faces of the two people:\\n\\nfaces\\nmodel.detectMultiScale(image,\\nminNeighbors=20)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 411, 'file_type': 'pdf'}, page_content='Here is the output:\\nAs detectMultiScale analyzes an image, it typically detects a\\nface multiple times, each defined by a bounding box that’s\\naligned slightly differently. The minNeighbors parameter\\nspecifies the minimum number of times a face must be detected\\nto be reported as a face. Higher values deliver higher\\nprecision (fewer false positives), but at the cost of lower\\nrecall, which means some faces might not be detected.\\nCascadeClassifier frequently requires tuning in this manner\\nto strike the right balance between finding too many faces\\nand finding too few. With that in mind, it is among the\\nfastest face detection algorithms in existence. It can also\\nbe used to detect objects other than faces by loading XML\\nfiles containing other pretrained classifiers. In OpenCV’s\\nGitHub repo, you’ll find XML files for detecting silverware\\nand other objects using Haar-like features, and XML files'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 412, 'file_type': 'pdf'}, page_content='that detect objects using a different type of discriminator\\ncalled local binary patterns.\\nFace Detection with Convolutional Neural\\nNetworks\\nWhile more computationally expensive, deep-learning methods\\noften do a better job of detecting faces in images than\\nViola-Jones. In particular, multitask cascaded convolutional\\nneural networks, or MTCNNs, have proven adept at face\\ndetection in a variety of benchmarks. They also identify\\nfacial landmarks such as the eyes, the nose, and the mouth.\\nFigure\\xa011-4 is adapted from a diagram in the 2016 paper\\ntitled “Joint Face Detection and Alignment Using Multitask\\nCascaded Convolutional Networks” that proposed MTCNNs. An\\nMTCNN uses three CNNs arranged in a series to detect faces.\\nThe first one, called the Proposal Network, or P-Net, is a\\nshallow CNN that searches the image at various resolutions\\nlooking for features indicative of faces. Rectangles\\nidentified by P-Net are combined to form candidate face\\nrectangles and are input to the Refine Network, or R-Net,\\nwhich is a deeper CNN that examines each rectangle more\\nclosely and rejects those that lack faces. Finally, output\\nfrom R-Net is input to the Output Network (O-Net), which\\nfurther filters candidate rectangles and identifies facial\\nlandmarks. MTCNNs are multitask CNNs because they produce\\nthree outputs each—a classification output indicating the\\nconfidence level that the rectangle contains a face, and two\\nregression outputs locating the face and facial landmarks—\\nrather than just one. And they’re cascaded like Viola-Jones\\nclassifiers to quickly rule out frames that don’t contain\\nfaces.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 413, 'file_type': 'pdf'}, page_content='Figure 11-4. Multitask cascaded convolutional neural network\\nA handy MTCNN implementation is available in the Python\\npackage named MTCNN. The following statements use it to\\ndetect faces in the same photo featured in the previous\\nexample:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\nfrom\\nmtcnn.mtcnn\\nimport\\nMTCNN\\n%matplotlib\\ninline'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 414, 'file_type': 'pdf'}, page_content=\"image\\nplt.imread('Data/Amsterdam.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\n\\ndetector\\nMTCNN()\\nfaces\\ndetector.detect_faces(image)\\n\\nfor\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\nface['box']\\n\\xa0\\xa0\\xa0 rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\nHere’s the result:\\nThe MTCNN detected not only the faces of the two people but\\nalso the face of a statue reflected in the door behind them.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 415, 'file_type': 'pdf'}, page_content=\"Here’s what detect_faces actually returned—a list\\ncontaining three dictionaries, each corresponding to one of\\nthe faces in the photo:\\n\\xa0\\xa0\\xa0 'box': [723, 248, 204, 258],\\n\\xa0\\xa0\\xa0 'confidence': 0.9997798800468445,\\n\\xa0\\xa0\\xa0 'keypoints': {\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'left_eye': (765, 341),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'right_eye': (858, 343),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'nose': (800, 408),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_left': (770, 432),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_right': (864, 433)\\n\\xa0\\xa0\\xa0 'box': [538, 258, 183, 232],\\n\\xa0\\xa0\\xa0 'confidence': 0.9997591376304626,\\n\\xa0\\xa0\\xa0 'keypoints': {\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'left_eye': (601, 353),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'right_eye': (685, 344),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'nose': (662, 394),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_left': (614, 433),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_right': (689, 424)\\n\\xa0\\xa0\\xa0 'box': [1099, 84, 40, 41],\\n\\xa0\\xa0\\xa0 'confidence': 0.8863282203674316,\\n\\xa0\\xa0\\xa0 'keypoints': {\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'left_eye': (1108, 101),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'right_eye': (1123, 96),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'nose': (1116, 102),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_left': (1114, 115),\\n\\xa0\\xa0\\xa0\\xa0\\xa0 'mouth_right': (1127, 111)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 416, 'file_type': 'pdf'}, page_content=\"You can eliminate the face in the reflection in either of two\\nways: by ignoring faces with a confidence level below a\\ncertain threshold, or by passing a min_face_size parameter to\\nthe MTCNN function so that detect_faces ignores faces smaller\\nthan a specified size. Here’s a modified for loop that does\\nthe former:\\n\\nfor\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0 if\\nface['confidence']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\nface['box']\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\nAnd here’s the result:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 417, 'file_type': 'pdf'}, page_content='The facial rectangles in Figure\\xa011-1 were generated using\\nMTCNN’s default settings—that is, without any filtering\\nbased on confidence levels or face sizes. Generally speaking,\\nit does a better job out of the box than CascadeClassifier at\\ndetecting faces.\\nExtracting Faces from Photos\\nOnce you know how to find faces in photos, it’s a simple\\nmatter to extract facial images in order to train a model or\\nsubmit them to a trained model for identification.\\nExample\\xa011-1 presents a Python function that accepts a path\\nto an image file and returns a list of facial images. By\\ndefault, it crops facial images so that they’re square\\n(perfect for passing them to a CNN), but you can disable\\ncropping by passing the function a crop=False parameter. You'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 418, 'file_type': 'pdf'}, page_content='can also specify a minimum confidence level with a\\nmin_confidence parameter, which defaults to 0.9.\\nExample 11-1. Function for extracting facial images from a\\nphoto\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\nPIL\\nimport\\nImage,\\nImageOps\\nfrom\\nmtcnn.mtcnn\\nimport\\nMTCNN\\n\\ndef\\nextract_faces(input_file,\\nmin_confidence=0.9,\\ncrop=True):\\n\\xa0\\xa0\\xa0 # Load the image and orient it correctly\\n\\xa0\\xa0\\xa0 pil_image\\nImage.open(input_file)\\n\\xa0\\xa0\\xa0 exif\\npil_image.getexif()\\n\\n\\xa0\\xa0\\xa0 for\\nk\\nin\\nexif.keys():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nk\\n0x0112:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 exif[k]\\nNone\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 del\\nexif[k]\\n\\n\\xa0\\xa0\\xa0 pil_image.info[\"exif\"]\\nexif.tobytes()\\n\\xa0\\xa0\\xa0 pil_image\\nImageOps.exif_transpose(pil_image)\\n\\xa0\\xa0\\xa0 image\\nnp.array(pil_image)\\n\\n\\xa0\\xa0\\xa0 # Find the faces in the image\\n\\xa0\\xa0\\xa0 detector\\nMTCNN()\\n\\xa0\\xa0\\xa0 faces\\ndetector.detect_faces(image)\\n\\xa0\\xa0\\xa0 faces\\n[face\\nfor\\nface\\nin\\nfaces\\nif\\nface[\\'confidence\\']\\nmin_confidence]\\n\\xa0\\xa0\\xa0 results\\n\\n\\xa0\\xa0\\xa0 for\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x1,\\ny1,\\nw,\\nh\\nface[\\'box\\']\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(crop):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Compute crop coordinates\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nw\\nh:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 419, 'file_type': 'pdf'}, page_content=\"x1\\nx1\\n((w\\nh)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 w\\nh\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 elif\\nh\\nw:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y1\\ny1\\n((h\\nw)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 h\\nw\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Extract the facial image and add it to the list\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x2\\nx1\\nw\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y2\\ny1\\nh\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 results.append(Image.fromarray(image[y1:y2,\\nx1:x2]))\\n\\n\\xa0\\xa0\\xa0 # Return all the facial images\\n\\xa0\\xa0\\xa0 return\\nresults\\nI passed the photo in Figure\\xa011-5 to the function, and it\\nreturned the faces underneath. The items returned from\\nextract_faces are Python Imaging Library (PIL) images, so you\\ncan resize them or save them to disk with a single line of\\ncode. Here’s a code snippet that extracts all the faces from\\na photo, resizes them to 224 × 224 pixels, and saves the\\nresized images:\\nfaces\\nextract_faces('PATH_TO_IMAGE_FILE')\\n\\nfor\\ni,\\nface\\nin\\nenumerate(faces):\\n\\xa0\\xa0\\xa0 face.resize((224,\\n224)).save(f'face{i}.jpg')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 420, 'file_type': 'pdf'}, page_content='Figure 11-5. Facial images extracted from a photo\\nWith extract_faces to lend a hand, it’s a relatively simple\\nmatter to generate a set of facial images for training a CNN\\nfrom a batch of photos on your hard disk, or to extract faces\\nfrom a photo and submit them to a CNN for identification.\\nFacial Recognition\\nNow that you know how to detect faces in photos, the next\\nstep is to learn how to identify them. Several algorithms for\\nrecognizing faces in photos have been developed over the\\nyears. Some rely on biometrics, such as the distance between\\nthe eyes or the texture of the skin, while others take a more\\nholistic approach by treating facial identification as a\\npattern recognition problem. State-of-the-art models today'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 421, 'file_type': 'pdf'}, page_content='frequently rely on deep convolutional neural networks.\\nOne\\xa0of the primary benchmarks for facial recognition models\\nis the Labeled Faces in the Wild (LFW) dataset pictured in\\nFigure\\xa011-6, which contains more than 13,000 facial images\\nof more than 5,000 people collected from the web. Deep-\\nlearning models such as MobiFace and FaceNet routinely\\nachieve greater than 99% accuracy on the dataset. This equals\\nor exceeds a human’s ability to identify faces in LFW\\nphotos.\\nFigure 11-6. The Labeled Faces in the Wild dataset\\nChapter\\xa05 presented a support vector machine (SVM) that\\nachieved 85% accuracy using a subset of 500 images—100 each\\nof five famous people—from the dataset. Chapter\\xa09 tackled\\nthe same problem with a neural network, with similar results.\\nThese models merely scratch the surface of what modern facial\\nrecognition can accomplish. Let’s apply CNNs and transfer\\nlearning to the same LFW subset and see if they can do better\\nat recognizing faces in photos. Along the way, you’ll learn\\na valuable lesson about pretrained CNNs and the specificity\\nof the weights that are generated when those CNNs are\\ntrained.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 422, 'file_type': 'pdf'}, page_content='Applying Transfer Learning to Facial\\nRecognition\\nThe first step in exploring CNN-based facial recognition is\\nto load the LFW dataset. This time, we’ll load full-size\\ncolor images and crop them to 128 × 128 pixels. Here’s the\\ncode:\\n\\nimport\\npandas\\nas\\npd\\nfrom\\nsklearn.datasets\\nimport\\nfetch_lfw_people\\n\\nfaces\\nfetch_lfw_people(min_faces_per_person=100,\\nresize=1.0,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 slice_=(slice(60,\\n188),\\nslice(60,\\n188)),\\ncolor=True)\\nclass_count\\nlen(faces.target_names)\\n\\nprint(faces.target_names)\\nprint(faces.images.shape)\\nBecause we set min_faces_per_person to 100, a total of 1,140\\nfacial images corresponding to five people were loaded. Use\\nthe following statements to show the first several images and\\nthe labels that go with them:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\nfig,\\nax\\nplt.subplots(3,\\nfigsize=(18,\\n\\nfor\\ni,\\naxi\\nin\\nenumerate(ax.flat):\\n\\xa0\\xa0\\xa0 axi.imshow(faces.images[i]\\n\\xa0\\xa0\\xa0 axi.set(xticks=[],\\nyticks=[],\\nxlabel=faces.target_names[faces.target[i]])'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 423, 'file_type': 'pdf'}, page_content='The dataset is imbalanced, containing almost as many photos\\nof George W. Bush as of everyone else combined. Use the\\nfollowing code to reduce the dataset to 100 images of each\\nperson, for a total of 500 facial images:\\n\\nimport\\nnumpy\\nas\\nnp\\n\\nmask\\nnp.zeros(faces.target.shape,\\ndtype=bool)\\n\\nfor\\ntarget\\nin\\nnp.unique(faces.target):\\n\\xa0\\xa0\\xa0 mask[np.where(faces.target\\ntarget)[0][:100]]\\n\\nx_faces\\nfaces.data[mask]\\ny_faces\\nfaces.target[mask]\\nx_faces\\nnp.reshape(x_faces,\\n(x_faces.shape[0],\\nfaces.images.shape[1],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 faces.images.shape[2],\\nfaces.images.shape[3]))\\nx_faces.shape\\nNow preprocess the pixel values for input to a pretrained\\nResNet50 CNN and use Scikit-Learn’s train_test_split\\nfunction to split the dataset, yielding 400 training samples\\nand 100 test samples:\\n\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nface_images\\npreprocess_input(np.array(x_faces))\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 face_images,\\ny_faces,\\ntrain_size=0.8,\\nstratify=y_faces,\\n\\xa0\\xa0\\xa0 random_state=0)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 424, 'file_type': 'pdf'}, page_content=\"If you wanted, you could divide the preprocessed pixel values\\nby 255 and train a CNN from scratch right now with this data.\\nHere’s how you’d go about it (in case you care to give it a\\ntry):\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nConv2D,\\nMaxPooling2D\\n\\nmodel\\nSequential()\\nmodel.add(Conv2D(32,\\nactivation='relu',\\ninput_shape=\\n(x_train.shape[1:])))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(64,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Conv2D(64,\\nactivation='relu'))\\nmodel.add(MaxPooling2D(2,\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\n\\nmodel.fit(x_train\\ny_train,\\nvalidation_data=(x_test\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 epochs=20,\\nbatch_size=10)\\nI did it and then plotted the training and validation\\naccuracy:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 425, 'file_type': 'pdf'}, page_content='The validation accuracy is better than that of an SVM or a\\nconventional neural network, but it’s nowhere near what\\nmodern CNNs achieve on the LFW dataset. So clearly there is a\\nbetter way.\\nThat better way, of course, is transfer learning, which we\\ncovered in Chapter\\xa010. ResNet50 was trained with more than 1\\nmillion images from the ImageNet dataset, so it should be\\nadept at extracting features from photos—more so than a\\nhandcrafted CNN trained with 400 images. Let’s see if\\nthat’s the case. Use the following statements to load\\nResNet50’s feature extraction layers, initialize them with\\nthe ImageNet weights, and freeze them so that the weights\\naren’t adjusted during training:\\n\\nfrom\\ntensorflow.keras.applications\\nimport\\nResNet50'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 426, 'file_type': 'pdf'}, page_content=\"base_model\\nResNet50(weights='imagenet',\\ninclude_top=False)\\nbase_model.trainable\\nFalse\\nNow add classification layers to the base model and include a\\nResizing layer to resize images input to the network to the\\nsize that ResNet50 expects:\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense,\\nResizing\\n\\nmodel\\nSequential()\\nmodel.add(Resizing(224,\\n224))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nTrain the model and plot the training and validation\\naccuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 427, 'file_type': 'pdf'}, page_content=\"plt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nResults will vary, but my run produced a validation accuracy\\naround 94%:\\nThis is an improvement over a CNN trained from scratch, and\\nit’s an indication that ResNet50 does a better job of\\nextracting features from facial images. But it’s still not\\nstate of the art. Is it possible to do even better?\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 428, 'file_type': 'pdf'}, page_content=\"Boosting Transfer Learning with Task-Specific\\nWeights\\nInitialized with ImageNet weights, ResNet50 does a credible\\njob of feature extraction. Those weights were arrived at when\\nResNet50 was trained on more than 1 million photos of objects\\nranging from basketballs to butterflies. It was not, however,\\ntrained with facial images. Would it be better at extracting\\nfeatures from facial images if it were trained with facial\\nimages?\\nIn 2017, a group of researchers at the University of\\nOxford’s Visual Geometry Group published a paper titled\\n“VGGFace2: A Dataset for Recognising Faces Across Pose and\\nAge”. After assembling a dataset comprising several million\\nfacial images, they trained two variations of ResNet50 with\\nit and published the results. They also published the\\nweights, which are wrapped in a handy Python library named\\nKeras-vggface. That library includes a class named VGGFace\\nthat encapsulates ResNet50 with TensorFlow-compatible\\nweights. Out of the box, the VGGFace model is capable of\\nrecognizing the faces of thousands of celebrities ranging\\nfrom Brie Larson to Jennifer Aniston. But its real value lies\\nin using transfer learning to repurpose it to recognize faces\\nit wasn’t trained to recognize before.\\nTo simplify matters, I installed Keras-vggface, created an\\ninstance of VGGFace without the classification layers,\\ninitialized the weights, and saved the model to an H5 file\\nnamed vggface.h5. Download that file and drop it into your\\nnotebooks’ Data subdirectory. Then use the following code to\\ncreate an instance of VGGFace built on top of ResNet50, and\\nadd custom classification layers:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nload_model\\n\\nbase_model\\nload_model('Data/vggface.h5')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 429, 'file_type': 'pdf'}, page_content=\"base_model.trainable\\nFalse\\n\\nmodel\\nSequential()\\nmodel.add(Resizing(224,\\n224))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(1024,\\nactivation='relu'))\\nmodel.add(Dense(class_count,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nNext, train the model and plot the training and validation\\naccuracy:\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=10,\\nepochs=10)\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe results are spectacular:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 430, 'file_type': 'pdf'}, page_content=\"To be sure, run the test data through the network and use a\\nconfusion matrix to assess the results:\\n\\nfrom\\nsklearn.metrics\\nimport\\nConfusionMatrixDisplay\\nas\\ncmd\\n\\nsns.reset_orig()\\ny_pred\\nmodel.predict(x_test)\\nfig,\\nax\\nplt.subplots(figsize=(5,\\nax.grid(False)\\n\\ncmd.from_predictions(y_test,\\ny_pred.argmax(axis=1),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 display_labels=faces.target_names,\\ncolorbar=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 cmap='Blues',\\nxticks_rotation='vertical',\\nax=ax)\\nBecause VGGFace was tuned to extract features from facial\\nimages, it achieves a perfect score on the 100 test images.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 431, 'file_type': 'pdf'}, page_content='That’s not to say that it will never fail to recognize a\\nface. It does indicate that, on the dataset you trained it\\nwith, it is remarkably adept at extracting features from\\nfacial images:\\nAnd therein lies an important lesson. CNNs that are trained\\nin task-specific ways frequently provide a better base for\\ntransfer learning than CNNs trained in a more generic\\nfashion. If the goal is to perform facial recognition,\\nyou’ll almost always do better with a CNN trained with\\nfacial images than a CNN trained with photos of thousands of'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 432, 'file_type': 'pdf'}, page_content='dissimilar objects. For a neural network, it’s all about the\\nweights.\\nArcFace\\nVGGFace isn’t the only pretrained CNN that excels at\\nextracting features from facial images. Another is ArcFace,\\nwhich was introduced in a 2019 paper titled “ArcFace:\\nAdditive Angular Margin Loss for Deep Face Recognition”. A\\nhandy implementation is available in a Python package named\\nArcface.\\nEach facial image submitted to ArcFace is transformed into a\\ndense vector of 512 values known as a face embedding. The\\ncode for creating an embedding is simple:\\n\\nfrom\\narcface\\nimport\\nArcFace\\n\\naf\\nArcFace.ArcFace()\\nembedding\\naf.calc_emb(image)\\nEmbeddings can be used to train machine learning models, and\\nthey can be used to make predictions with those models.\\nThanks to the loss function named in the title of the paper,\\nembeddings created by ArcFace often do a better job of\\ncapturing the uniqueness of a face than embeddings generated\\nby conventional CNNs.\\nAnother use for the embeddings created by ArcFace is face\\nverification, which compares two facial images and computes\\nthe probability that they represent the same person. The\\nfollowing statements generate embeddings for two facial\\nimages and use the cosine_similarity function introduced in\\nChapter\\xa04 to quantify the similarity between the two:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 433, 'file_type': 'pdf'}, page_content='af\\nArcFace.ArcFace()\\nface_emb1\\naf.calc_emb(image1)\\nface_emb2\\naf.calc_emb(image2)\\nsim\\ncosine_similarity([face_emb1,\\nface_emb2])[0][1]\\nThe result is a value from 0.0 to 1.0, with higher values\\nreflecting greater similarity between the faces.\\nPutting It All Together: Detecting and\\nRecognizing Faces in Photos\\nAny time a model scores perfectly in testing, you should be\\nskeptical. No model is perfect, and even if it achieves 100%\\naccuracy against a test dataset, it won’t duplicate that in\\nthe wild. Given that VGGFace was trained with images of some\\nof the same famous people found in the LFW dataset, is it\\npossible that it’s biased toward those people? That transfer\\nlearning with VGGFace wouldn’t do as well if trained with\\nimages of ordinary people? And how would it perform with just\\na handful of training images?\\nTo answer these questions, let’s build a notebook that\\ntrains a facial recognition model based on VGGFace, uses an\\nMTCNN to detect faces in photos, and uses the model to\\nidentify the faces it detects. The dataset you’ll use\\ncontains eight pictures each of three ordinary people in\\nslightly different poses, at ages up to 20 years apart, with\\nand without glasses (Figure\\xa011-7). These images were\\nextracted from photos using the extract_faces function in\\nExample\\xa011-1 and resized to 224 × 224.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 434, 'file_type': 'pdf'}, page_content='Figure 11-7. Photos for training a facial recognition model\\nBegin by downloading a ZIP file containing the facial images\\nand copying the contents of the ZIP file into a subdirectory\\nnamed Faces where your notebooks are hosted. The ZIP file\\ncontains four folders: one named Jeff, one named Lori, one\\nnamed Abby, and one named Samples that contains uncropped\\nphotos for testing.\\nNow create a new notebook and run the following code in the\\nfirst cell to define helper functions for loading and\\ndisplaying facial images from the subdirectories you copied\\nthem to, and declare a pair of Python lists to hold the\\nimages and labels:\\n\\nimport\\nos\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\ndef\\nload_images_from_path(path,\\nlabel):\\n\\xa0\\xa0\\xa0 images,\\nlabels\\n\\n\\xa0\\xa0\\xa0 for\\nfile\\nin\\nos.listdir(path):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 images.append(image.img_to_array(image.load_img(os.path.join(path,\\nfile),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_size=(224,\\n3))))'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 435, 'file_type': 'pdf'}, page_content=\"labels.append((label))\\n\\n\\xa0\\xa0\\xa0 return\\nimages,\\nlabels\\n\\ndef\\nshow_images(images):\\n\\xa0\\xa0\\xa0 fig,\\naxes\\nplt.subplots(1,\\nfigsize=(20,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n'yticks':\\n\\n\\xa0\\xa0\\xa0 for\\ni,\\nax\\nin\\nenumerate(axes.flat):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.imshow(images[i]\\n\\nx,\\ny\\nNext, load the images of Jeff and label them with 0s:\\n\\nimages,\\nlabels\\nload_images_from_path('Faces/Jeff',\\nshow_images(images)\\n\\nx\\nimages\\ny\\nlabels\\nLoad the images of Lori and label them with 1s:\\n\\nimages,\\nlabels\\nload_images_from_path('Faces/Lori',\\nshow_images(images)\\n\\nx\\nimages\\ny\\nlabels\\nLoad the images of Abby and label them with 2s:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 436, 'file_type': 'pdf'}, page_content=\"images,\\nlabels\\nload_images_from_path('Faces/Abby',\\nshow_images(images)\\n\\nx\\nimages\\ny\\nlabels\\nFinally, preprocess the pixels for the ResNet50 version of\\nVGGFace and split the data fifty-fifty so that the network\\nwill be trained with four randomly selected images of each\\nperson and validated with the same number of images:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\nsklearn.model_selection\\nimport\\ntrain_test_split\\nfrom\\ntensorflow.keras.applications.resnet50\\nimport\\npreprocess_input\\n\\nfaces\\npreprocess_input(np.array(x))\\nlabels\\nnp.array(y)\\n\\nx_train,\\nx_test,\\ny_train,\\ny_test\\ntrain_test_split(\\n\\xa0\\xa0\\xa0 faces,\\nlabels,\\ntrain_size=0.5,\\nstratify=labels,\\n\\xa0\\xa0\\xa0 random_state=0)\\nThe next step is to load the saved VGGFace model and freeze\\nthe bottleneck layers. If you didn’t download vggface.h5\\nearlier, download it now and drop it into your notebooks’\\nData subdirectory. Then execute the following code:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nload_model\\n\\nbase_model\\nload_model('Data/vggface.h5')\\nbase_model.trainable\\nFalse\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 437, 'file_type': 'pdf'}, page_content=\"Now define a network that uses transfer learning with VGGFace\\nto identify faces. The Resizing layer ensures that each image\\nmeasures exactly 224 × 224 pixels. The Dense layer contains\\njust eight neurons because the training dataset is small and\\nwe don’t want the network to fit too tightly to it:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nFlatten,\\nDense,\\nResizing\\n\\nmodel\\nSequential()\\nmodel.add(Resizing(224,\\n224))\\nmodel.add(base_model)\\nmodel.add(Flatten())\\nmodel.add(Dense(8,\\nactivation='relu'))\\nmodel.add(Dense(3,\\nactivation='softmax'))\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nTrain the model:\\n\\nhist\\nmodel.fit(x_train,\\ny_train,\\nvalidation_data=(x_test,\\ny_test),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 batch_size=2,\\nepochs=10)\\nPlot the training and validation accuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval_acc\\nhist.history['val_accuracy']\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 438, 'file_type': 'pdf'}, page_content=\"epochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training Accuracy')\\nplt.plot(epochs,\\nval_acc,\\nlabel='Validation Accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nHopefully you got something like this:\\nNow comes the fun part: using an MTCNN to detect the faces in\\na photo and the trained model to identify those faces. First\\nmake sure the MTCNN package is installed in your environment.\\nThen define a pair of helper functions—one that retrieves a\\nface from a specified location in an image (get_face), and\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 439, 'file_type': 'pdf'}, page_content=\"another that loads a photo and annotates faces in the photo\\nwith names and confidence levels (label_faces):\\n\\nfrom\\nmtcnn.mtcnn\\nimport\\nMTCNN\\nfrom\\nPIL\\nimport\\nImage,\\nImageOps\\nfrom\\ntensorflow.keras.preprocessing\\nimport\\nimage\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\n\\ndef\\nget_face(image,\\nface):\\n\\xa0\\xa0\\xa0 x1,\\ny1,\\nw,\\nh\\nface['box']\\n\\n\\xa0\\xa0\\xa0 if\\nw\\nh:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x1\\nx1\\n((w\\nh)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 w\\nh\\n\\xa0\\xa0\\xa0 elif\\nh\\nw:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y1\\ny1\\n((h\\nw)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 h\\nw\\n\\n\\xa0\\xa0\\xa0 x2\\nx1\\nh\\n\\xa0\\xa0\\xa0 y2\\ny1\\nw\\n\\n\\xa0\\xa0\\xa0 return\\nimage[y1:y2,\\nx1:x2]\\n\\ndef\\nlabel_faces(path,\\nmodel,\\nnames,\\nface_threshold=0.9,\\nprediction_threshold=0.9,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 show_outline=True,\\nsize=(12,\\n\\xa0\\xa0\\xa0 # Load the image and orient it correctly\\n\\xa0\\xa0\\xa0 pil_image\\nImage.open(path)\\n\\xa0\\xa0\\xa0 exif\\npil_image.getexif()\\n\\n\\xa0\\xa0\\xa0 for\\nk\\nin\\nexif.keys():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nk\\n0x0112:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 exif[k]\\nNone\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 del\\nexif[k]\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 440, 'file_type': 'pdf'}, page_content='pil_image.info[\"exif\"]\\nexif.tobytes()\\n\\xa0\\xa0\\xa0 pil_image\\nImageOps.exif_transpose(pil_image)\\n\\xa0\\xa0\\xa0 np_image\\nnp.array(pil_image)\\n\\n\\xa0\\xa0\\xa0 fig,\\nax\\nplt.subplots(figsize=size,\\nsubplot_kw={\\'xticks\\':\\n\\'yticks\\':\\n\\xa0\\xa0\\xa0 ax.imshow(np_image)\\n\\n\\xa0\\xa0\\xa0 detector\\nMTCNN()\\n\\xa0\\xa0\\xa0 faces\\ndetector.detect_faces(np_image)\\n\\xa0\\xa0\\xa0 faces\\n[face\\nfor\\nface\\nin\\nfaces\\nif\\nface[\\'confidence\\']\\nface_threshold]\\n\\n\\xa0\\xa0\\xa0 for\\nface\\nin\\nfaces:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\nface[\\'box\\']\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Use the model to identify the face\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 face_image\\nget_face(np_image,\\nface)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 face_image\\nimage.array_to_img(face_image)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 face_image\\npreprocess_input(np.array(face_image))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 predictions\\nmodel.predict(np.expand_dims(face_image,\\naxis=0))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 confidence\\nnp.max(predictions)\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\n(confidence\\nprediction_threshold):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Optionally draw a box around the face\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nshow_outline:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor=\\'red\\',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 # Label the face\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index\\nint(np.argmax(predictions))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n= f\\'{names[index]} ({confidence:.1%})\\'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.text(x\\n(w\\ny,\\ntext,\\ncolor=\\'white\\',\\nbackgroundcolor=\\'red\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ha=\\'center\\',\\nva=\\'bottom\\',\\nfontweight=\\'bold\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 bbox=dict(color=\\'red\\'))'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 441, 'file_type': 'pdf'}, page_content=\"Now pass the first sample image in the Samples folder to\\nlabel_faces:\\n\\nlabels\\n['Jeff',\\n'Lori',\\n'Abby']\\nlabel_faces('Faces/Samples/Sample-1.jpg',\\nmodel,\\nlabels)\\nThe output should look like this, although your percentages\\nmight be different:\\nTry it again, but this time with a different photo:\\n\\nlabel_faces('Faces/Samples/Sample-2.jpg',\\nmodel,\\nlabels)\\nHere’s the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 442, 'file_type': 'pdf'}, page_content=\"Finally, submit a photo containing all three individuals that\\nthe model was trained with:\\n\\nlabel_faces('Faces/Samples/Sample-3.jpg',\\nmodel,\\nlabels)\\nTrained with just 12 facial images—four of each person—the\\nmodel does a credible job of identifying faces in photos. Of\\ncourse, you could generate a dataset of your own by passing\\nphotos of friends and family members through the function in\\nExample\\xa011-1 and training the model with the resulting\\nimages.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 443, 'file_type': 'pdf'}, page_content='Handling Unknown Faces: Closed-Set Versus\\nOpen-Set Classification\\nNow for some bad news. A VGGFace facial recognition model is\\nadept at identifying faces it was trained with, but it\\ndoesn’t know what to do when it encounters a face it wasn’t\\ntrained with. Try it: pass in a photo of yourself. The model\\nwill probably identify you as Jeff, Lori, or Abby, and it\\nmight do so with a high level of confidence. It literally\\ndoesn’t know what it doesn’t know. This is especially true\\nwhen the dataset is small and the network is given room to\\noverfit.\\nThe reason why has nothing to do with VGGFace. It has\\neverything to do with the fact that a neural network with a\\nsoftmax output layer is a closed-set classifier, meaning it\\nclassifies any sample presented to it for predictions as one\\nof the classes it was trained with. (Remember that softmax\\nensures that the sum of the probabilities for all classes is\\n1.0.) The alternative is an open-set classifier (Figure\\xa011-\\n8), which has the ability to say “this sample doesn’t\\nbelong to any of the classes I was trained with.”\\nFigure 11-8. Closed-set versus open-set classification\\nThere is not a one-size-fits-all solution for building open-\\nset classifiers in the deep-learning community today. A 2016\\npaper titled “Towards Open Set Deep Networks” proposed one\\nsolution in the form of openmax output layers, which replace\\nsoftmax output layers and “estimate the probability of an'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 444, 'file_type': 'pdf'}, page_content='input being from an unknown class.” Essentially, if the\\nnetwork is trained with 10 classes, the openmax output layer\\nadds an 11th output representing the unknown class. It works\\nby taking the activations from the final classification layer\\nand adjusting them using a Weibull distribution rather than\\nnormalizing the probabilities as softmax does.\\nAnother potential solution was put forth in a 2018 paper\\ntitled “Reducing Network Agnostophobia” from researchers at\\nthe University of Colorado. It proposed replacing cross-\\nentropy loss with a new loss function called entropic open-\\nset loss that drives softmax scores for unknown classes\\ntoward a uniform probability distribution. Using this\\ntechnique, you could more reliably detect samples belonging\\nto an unknown class using probability thresholds paired with\\nconventional softmax output layers. For a great summary of\\nthe problems posed by open-set classification in deep\\nlearning and an overview of openmax and entropic open-set\\nloss, see “Does a Neural Network Know When It Doesn’t\\nKnow?” by Tivadar Danka.\\nYet another solution is to use ArcFace to verify each face\\nthe model identifies by comparing an embedding generated from\\nthat face to a reference embedding for the same person. You\\ncould reject the model’s conclusion if cosine similarity\\nfalls below a predetermined threshold.\\nA more naive approach is to prevent the network from learning\\nthe training data too well in hopes that unknown classes will\\nyield lower softmax probabilities. That’s why I included\\njust eight neurons in the classification layer in the\\nprevious example. (You could go even further by introducing a\\ndropout layer.) It works up to a point, but it isn’t\\nperfect. The label_faces function has a default prediction\\nthreshold of 0.9, meaning it labels a face only if the model\\nclassifies it with at least 90% confidence'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 444, 'file_type': 'pdf'}, page_content='. The label_faces function has a default prediction\\nthreshold of 0.9, meaning it labels a face only if the model\\nclassifies it with at least 90% confidence. You could set\\nprediction_threshold to 0.99 to rule out more unknown faces,\\nbut at the cost of failing to identify more known faces.\\nTuning in this manner to strike the right balance between'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 445, 'file_type': 'pdf'}, page_content='recognizing known faces while ignoring unknowns is an\\ninevitable part of readying a facial recognition model for\\nproduction.\\nSummary\\nBuilding an end-to-end facial recognition system requires a\\nmeans for detecting faces in photos as well as a means for\\nclassifying (identifying) those faces. One way to detect\\nfaces is the Viola-Jones algorithm, for which the OpenCV\\nlibrary provides a convenient implementation. An alternative\\nthat relies on deep learning is a multitask cascaded\\nconvolutional neural network, or MTCNN. The Python package\\nnamed MTCNN contains a ready-to-use MTCNN implementation.\\nViola-Jones is faster and more suitable for real-time\\napplications (for example, identifying faces in a live webcam\\nfeed), but MTCNNs are generally more accurate and incur fewer\\nfalse positives.\\nDeep learning can be applied to the task of facial\\nrecognition by employing convolutional neural networks.\\nTransfer learning with a pretrained CNN such as ResNet50 can\\nidentify faces with a relatively high degree of accuracy, but\\ntransfer learning with a CNN that was trained with millions\\nof facial images delivers unparalleled accuracy. The primary\\nreason is that the CNN’s bottleneck layers are optimized for\\nextracting features from facial images.\\nWith a highly optimized set of weights, a neural network can\\ndo almost anything. Generic weights will suffice when nothing\\nbetter is available, but given a task-specific set of weights\\nto start with, facial recognition via transfer learning can\\nachieve human-like accuracy.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 446, 'file_type': 'pdf'}, page_content='Chapter 12. Object Detection\\nThe previous chapter introduced two popular algorithms for\\ndetecting faces in photographs: Viola-Jones, which relies on\\nmachine learning, and MTCNNs, which rely on deep learning.\\nFace detection is a special case of object detection, in\\nwhich computers detect and identify objects in images.\\nIdentifying an object is an image classification problem,\\nsomething at which CNNs excel. But finding objects to\\nidentify poses a different challenge.\\nObject detection is challenging because objects aren’t\\nassumed to be perfectly cropped and aligned as they are for\\nimage classification tasks. Nor are they limited to one per\\nimage. Figure\\xa012-1 shows what a self-driving car might see\\nas it scans video frames from a forward-pointing camera. A\\nCNN trained to do conventional image classification using\\ncarefully prepared training images is powerless to help. It\\nmight be able to classify the image as one of a city street,\\nbut it can’t determine that the image contains cars, people,\\nand traffic lights, much less pinpoint their locations.\\nObject detection has grown in speed and accuracy in recent\\nyears, and state-of-the-art methods rely on deep learning. In\\nparticular, they employ CNNs, which were introduced in\\nChapter\\xa010. Let’s discuss how CNNs do object detection and\\nidentification and try our hand at it using cutting-edge\\nobject detection models.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 447, 'file_type': 'pdf'}, page_content='Figure 12-1. Detecting and identifying objects in a scene\\nR-CNNs\\nOne way to apply deep learning to the task of object\\ndetection is to use region-based CNNs, also known as region\\nCNNs or simply R-CNNs. The first R-CNN was introduced in a\\n2014 paper titled “Rich Feature Hierarchies for Accurate\\nObject Detection and Semantic Segmentation”. The model\\ndescribed in the paper comprises three stages. The first\\nstage scans the image and identifies up to 2,000 bounding\\nboxes representing regions of interest—regions that might\\ncontain objects. The second stage is a deep CNN that extracts\\nfeatures from regions of interest. The third is a support\\nvector machine that classifies the features. The output is a\\ncollection of bounding boxes with class labels and confidence\\nscores. An algorithm called non-maximum suppression (NMS)\\nfilters the output and selects the best bounding box for each\\nobject.\\nNMS is a crucial element of virtually all modern object\\ndetection systems. A detector invariably emits several\\nbounding boxes for each object. If a photo contains one\\ninstance of a given class—for example, one zebra—NMS'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 448, 'file_type': 'pdf'}, page_content='selects the bounding box with the highest confidence score.\\nIf the photo contains two zebras (Figure\\xa012-2), NMS divides\\nthe bounding boxes into two groups and selects the box with\\nthe highest confidence score in each group. It groups boxes\\nbased on the amount of overlap between them. Overlap is\\ncomputed by dividing the area of intersection between two\\nboxes by the area formed by the union of the boxes. If the\\nresulting intersection-over-union (IoU) score is greater than\\na predetermined threshold (typically 0.5), NMS assigns the\\nboxes to the same group. Otherwise, it assigns them to\\nseparate groups.\\nWhen two objects of the same class have little or no overlap,\\nNMS easily separates the two. When two instances of the same\\nclass overlap significantly (picture one zebra standing\\nbehind the other), the IoU threshold might have to be\\nadjusted to achieve separation. IoU threshold is a\\nhyperparameter that can be tuned to strike the right balance\\nbetween being overly aggressive at separating overlapping\\nobjects and not aggressive enough.\\nFigure 12-2. Non-maximum suppression\\nThe first stage of most R-CNN implementations uses an\\nalgorithm called selective search to identify regions of\\ninterest by keying on similarities in color, texture, shape,\\nand size. Figure\\xa012-3 shows the first 500 bounding boxes\\ngenerated when OpenCV’s implementation of selective search'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 449, 'file_type': 'pdf'}, page_content='examines an image. Submitting a targeted list of regions to\\nthe CNN is faster than a brute-force sliding-window approach\\nthat inputs the contents of the window to the CNN at every\\nstop.\\nFigure 12-3. Bounding boxes generated by selective search\\nEven with selective search narrowing the list of candidate\\nregions input to stage 2, an R-CNN can’t do object detection\\nin real time. Why? Because the CNN individually processes the\\n2,000 or so regions of interest identified in stage 1. These\\nregions invariably overlap, so the CNN processes the same\\npixels multiple times.\\nA 2015 paper titled “Fast R-CNN” addressed this by\\nproposing a modified architecture in which the entire image\\npasses through the CNN one time (Figure\\xa012-4). Selective\\nsearch or a similar algorithm identifies regions of interest\\nin the image, and those regions are projected onto the\\nfeature map generated by the CNN. A region of interest (ROI)\\npooling layer then uses a form of max pooling to reduce the\\nfeatures in each region of interest to a fixed-length vector,\\nindependent of the region’s size and shape. (By contrast, R-\\nCNN scales each region to an image of predetermined size'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 450, 'file_type': 'pdf'}, page_content='before submitting it to the CNN, which is substantially more\\nexpensive than ROI pooling.) Classification of the feature\\nvectors is performed by fully connected layers rather than\\nSVMs, and the output is split to include both a softmax\\nclassifier and a bounding-box regressor. NMS filters the\\nbounding boxes down to the ones that matter. The result is a\\nsystem that trains an order of magnitude faster than R-CNN,\\nmakes predictions two orders of magnitude faster, and is\\nslightly more accurate than R-CNN.\\nFigure 12-4. Fast R-CNN architecture\\nNOTE\\nROI pooling reduces any region of interest to a feature vector of\\na specified size, regardless of the region’s height and width.\\nImagine that you have an 8 × 16 region of a feature map and you\\nwant to reduce it to 4 × 4. You can divide the 8 × 16 region\\ninto a 4 × 4 grid, with each cell in the grid measuring 2 × 4.\\nYou can then take the maximum of the eight values in each cell and\\nplug them into the 4 × 4 grid. That’s ROI pooling. It’s simple,\\nfast, and effective. And it works with regions of any size and\\naspect ratio.\\nA 2016 paper titled “Faster R-CNN: Towards Real-Time Object\\nDetection with Region Proposal Networks” further boosted\\nperformance by replacing selective search with a region'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 451, 'file_type': 'pdf'}, page_content='proposal network, or RPN. The RPN is a shallow CNN that\\nshares layers with the main CNN (Figure\\xa012-5). To generate\\nregion proposals, it slides a window over the feature map\\ngenerated by the last shared layer. At each stop in the\\nwindow’s travel, the RPN evaluates n candidate regions\\ncalled anchors or anchor boxes and computes an objectness\\nscore for each based on IoU scores with ground-truth boxes.\\nObjectness scores and anchor boxes are input to fully\\nconnected layers for classification (does the anchor contain\\nan object, and if so, what type?) and regression. The output\\nfrom these layers ultimately determines the regions of\\ninterest projected onto the feature map generated by the main\\nCNN and forwarded to the ROI pooling layer.\\nFigure 12-5. Faster R-CNN architecture\\nFaster R-CNN can perform 10 times faster than Fast R-CNN and\\ndo object detection in near real time. It’s typically more\\naccurate than Fast R-CNNs too, thanks to the RPN’s superior\\nability to identify candidate regions. Selective search is\\nstatic, but an RPN gets smarter as the network is trained.\\nRPNs also execute in parallel to the main CNN, which means\\nregion proposals have little impact on performance.\\nMask R-CNN\\nThe next chapter in the R-CNN story came in 2017 in a paper\\ntitled “Mask R-CNN”. Mask R-CNNs extend Faster R-CNNs by\\nadding instance segmentation, which identifies the shapes of\\nobjects detected in an image using segmentation masks like'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 452, 'file_type': 'pdf'}, page_content='the ones in Figure\\xa012-6. Performance impact is minimal\\nbecause instance segmentation is performed in parallel with\\nregion evaluation. The benefit of Mask R-CNNs is that they\\nprovide more detail about the objects they detect. For\\nexample, you can tell whether a person’s arms are extended—\\nsomething you can’t discern from a simple bounding box. They\\nare also slightly more accurate than Faster R-CNNs because\\nthey replace ROI pooling with ROI alignment, which discards\\nless information when generating feature vectors whose\\nboundaries don’t perfectly align with the boundaries of the\\nregions they represent.\\nFigure 12-6. Bounding boxes and segmentation masks generated by a Mask R-CNN\\nZoom uses instance segmentation to display custom backgrounds\\nbehind you in video feeds. Instance segmentation also powers\\nAI-based image editing tools that crop figures from the\\nforeground and place them on different backgrounds. Let’s\\ndemonstrate using a pretrained Mask R-CNN.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 453, 'file_type': 'pdf'}, page_content='Start by downloading a ZIP file containing the files needed\\nfor this exercise. The files are:\\nadam.jpg\\nA photo of a young man wearing a backpack\\nmaui.jpg\\nA photo taken poolside in Hawaii\\nmask.py\\nContains helper functions for preprocessing images\\nbefore they’re input to a Mask R-CNN and\\npostprocessing functions for interpreting the results\\nMaskRCNN-12-int8.onnx\\nContains a pretrained Mask R-CNN saved in ONNX format\\nPlace mask.py in the directory where your Jupyter notebooks\\nare hosted, and the remaining files in your notebooks’ Data\\nsubdirectory.\\nThe critical file in this ensemble is MaskRCNN-12-int8.onnx.\\nChapter\\xa07 introduced Open Neural Network Exchange (ONNX) as\\na means for loading Scikit-Learn models written in Python and\\nconsuming them in other languages. But ONNX does more than\\nbridge the language gap to Scikit. It’s a neutral format\\nthat enables neural networks written with one deep-learning\\nframework to be exported to others. For example, you can save\\na PyTorch model to a .onnx file, and then convert the .onnx\\nfile to a TensorFlow model and use it as if it had been\\nwritten with TensorFlow from the outset. Or you can install\\nan ONNX runtime and load and call the model without\\nconverting it to TensorFlow. Chapter\\xa07 demonstrated how to\\nsave a Scikit model in ONNX format and use the Python ONNX\\nruntime to load the model and call it from Python. It also\\nused the .NET ONNX runtime to load the model and call it from'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 454, 'file_type': 'pdf'}, page_content=\"C#. As noted in Chapter\\xa07, ONNX runtimes are available for a\\nvariety of platforms and programming languages.\\nMaskRCNN-12-int8.onnx contains a sophisticated Mask R-CNN\\nimplementation from Facebook Research. It uses a ResNet50\\nbackbone to extract features from the images input to it, and\\nit includes a branch that computes instance segmentation\\nmasks in parallel with bounding box computations. It was\\ntrained with more than 200,000 images from the COCO dataset.\\nThose images contain more than 1.5 million objects that fall\\ninto 80 categories ranging from cats and dogs to people,\\nbicycles, and cars. And because the trained model was\\npublished in the ONNX model zoo on GitHub, you can load it\\nand pass images to it with just a few lines of code.\\nThe first step in using the model in Python is to make sure\\nthe Python ONNX runtime is installed in your environment. For\\nthis exercise, make sure OpenCV is installed too. Then create\\na Jupyter notebook and run the following code in the first\\ncell to load and display adam.jpg:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nPIL\\nimport\\nImage\\n%matplotlib\\ninline\\n\\nimage\\nImage.open('Data/adam.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\nHere’s the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 455, 'file_type': 'pdf'}, page_content=\"The goal is to run this image through Mask R-CNN, detect the\\nobjects in it, and annotate the image accordingly. To that\\nend, use the following code to preprocess the image pixels\\nthe way the model expects, load Mask R-CNN from the ONNX\\nfile, and submit the preprocessed image to it:\\n\\nfrom\\nmask\\nimport\\nimport\\nonnxruntime\\nas\\nrt\\n\\nimage_data\\npreprocess(image)\\nsession\\nrt.InferenceSession('Data/MaskRCNN-12-int8.onnx')\\ninput_name\\nsession.get_inputs()[0].name\\nresult\\nsession.run(None,\\ninput_name:\\nimage_data\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 456, 'file_type': 'pdf'}, page_content='The value returned by the run method is an array of four\\narrays. The first array contains bounding boxes for the\\nobjects detected in the image. In this example, it detected a\\ntotal of 13 objects:\\n\\narray([[\\n377.50537,\\xa0 174.36874,\\xa0 801.177\\xa0 ,\\xa0 787.7125\\n428.15994,\\xa0 344.70105,\\xa0 699.99097,\\xa0 599.8499\\n757.1859\\n,\\xa0 529.4938\\n,\\xa0 814.2776\\n,\\xa0 658.8041\\n432.57123,\\xa0 351.229\\xa0 ,\\xa0 672.2242\\n,\\xa0 484.63702],\\n435.53653,\\xa0 357.34235,\\xa0 494.73557,\\xa0 516.4939\\n672.56714,\\xa0 516.2991\\n,\\xa0 822.82153,\\xa0 663.68726],\\n608.1904\\n,\\xa0 361.0429\\n,\\xa0 686.1207\\n,\\xa0 552.70105],\\n629.84894,\\xa0 355.3375\\n,\\xa0 799.4963\\n,\\xa0 718.9458\\n437.99078,\\xa0 331.19318,\\xa0 709.4969\\n,\\xa0 748.2785\\n438.44028,\\xa0 348.14404,\\xa0 579.8056\\n,\\xa0 546.0588\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [1152.0518\\n,\\xa0 203.89978,\\n1163.7583\\n,\\xa0 223.2485\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [1151.8087\\n,\\xa0 132.45256,\\n1164.0013\\n,\\xa0 150.6876\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 [1151.8087\\n,\\xa0 404.0875\\n1164.0013\\n,\\xa0 423.058\\xa0 ],\\n416.03802,\\xa0 396.8123\\n,\\xa0 694.8975\\n,\\xa0 511.93896],\\n683.35236,\\xa0 765.0078\\n,\\xa0 701.3403\\n,\\xa0 784.47546]],\\ndtype=float32)\\nThe second array is an array of integer class identifiers\\ncorresponding to classes in the COCO dataset: 1 for a person,\\n2 for a bicycle, and so on. These identify the objects in the\\nimage:\\n\\narray([\\n25,\\xa0 1,\\xa0 1,\\xa0 1,\\n\\xa0\\xa0\\xa0\\xa0\\xa0 dtype=int64)\\nThe third array contains confidence scores for the 13\\nobjects. They range from a high of 99.9% for the person in\\nthe image to a low of 5.5% for the logo on his cap, which the'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 457, 'file_type': 'pdf'}, page_content='model thinks might be a clock. Observe that only two objects\\nscored a confidence level of 70% or higher:\\narray([0.99911565,\\n0.8009716\\n0.4624603\\n0.19400069,\\n0.17042953,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.11972303,\\n0.11626374,\\n0.10352837,\\n0.08243025,\\n0.08089489,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 0.07266886,\\n0.07266886,\\n0.07266886,\\n0.06278796,\\n0.0549276\\n\\xa0\\xa0\\xa0\\xa0\\xa0 dtype=float32)\\nFinally, the fourth array contains segmentation masks for\\neach object. Each mask is a 28 × 28 array of floating-point\\nvalues sometimes referred to as a soft mask due to the soft\\nedges. Figure\\xa012-7 shows the mask for the person detected in\\nthe image. When mapped back to the image’s original size and\\naspect ratio, it does a reasonable job of defining which\\npixels correspond to that person.\\nFigure 12-7. A 28 × 28 soft mask corresponding to the person in the\\npreceding photo\\nThe next step is to grab the bounding boxes, predicted class\\nlabels, confidence scores, and segmentation masks and pass\\nthem to the annotate_image function to visualize the results:\\n\\nboxes\\nresult[0]\\xa0 # Bounding boxes\\nlabels\\nresult[1]\\n# Class labels'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 458, 'file_type': 'pdf'}, page_content='scores\\nresult[2]\\n# Confidence scores\\nmasks\\nresult[3]\\xa0 # Segmentation masks\\n\\nannotate_image(image,\\nboxes,\\nlabels,\\nscores,\\nmasks)\\nHere’s the result:\\nBy default, annotate_image ignores objects identified with\\nless than 70% confidence. You can override that by including\\na min_confidence parameter in the call. The model detected\\ntwo objects in the image with a confidence that equaled or\\nexceeded 70%: a person and a backpack. annotate_image draws\\nthe bounding boxes and annotates them with class names and\\nconfidence levels. It also shades the objects by overlaying\\nthem with partially transparent pixels. These are the\\nsegmentation masks returned by run.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 459, 'file_type': 'pdf'}, page_content=\"annotate_image is one of several helper functions found in\\nmask.py. Another is preprocess, which preps each image in the\\nway Mask R-CNN expects by resizing the image, making sure the\\nwidth and height are multiples of 32, converting the image to\\nBGR format, and normalizing the pixel values. I brought both\\nof these functions over from the model’s GitHub page with\\nsome minor modifications. Since Mask R-CNNs are often used to\\ncrop objects from images, I also added a change_background\\nfunction that extracts all the objects detected in an image\\nwith a specified confidence level (default = 0.7) and\\ncomposites them onto a new background. To see for yourself,\\nrun the following statements in the notebook’s next cell:\\n\\nfg_image\\nImage.open('Data/adam.jpg')\\nbg_image\\nImage.open('Data/maui.jpg')\\n\\nchange_background(session,\\nfg_image,\\nbg_image)\\nThe output should look like this:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 460, 'file_type': 'pdf'}, page_content='change_background submits the foreground image (the one\\npassed in the function’s second parameter) to the model and\\ncopies all the pixels inside the segmentation masks to the\\nbackground image. It works best when the foreground and\\nbackground images are the same size, but it will resize the\\nbackground image if needed to match the foreground image. The\\nbackground will be distorted if its aspect ratio differs from\\nthat of the foreground image.\\nThe fact that segmentation masks produced by Mask R-CNN\\nmeasure just 28 × 28 explains the “tearing” around the\\npixels copied from the foreground image: the resolution of\\nthe mask is lower than that of the person in the photo. You\\ncould modify Mask R-CNN to use higher-resolution masks, but\\nyou’d have to retrain the model from scratch. That’s a\\ntime-consuming endeavor with a model as complex as this one\\nand a dataset as large as COCO, even if you do the training\\non GPUs.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 461, 'file_type': 'pdf'}, page_content='YOLO\\nWhile the R-CNN family of object detection systems delivers\\nunparalleled accuracy, it leaves something to be desired when\\nit comes to real-time object detection of the type required\\nby, say, self-driving cars. A paper titled “You Only Look\\nOnce: Unified, Real-Time Object Detection” published in 2015\\nproposed an alternative to R-CNNs known as YOLO (You Only\\nLook Once) that revolutionized the way engineers think about\\nobject detection. From the paper’s introduction:\\nHumans glance at an image and instantly know what objects\\nare in the image, where they are, and how they interact.\\nThe human visual system is fast and accurate, allowing us\\nto perform complex tasks like driving with little\\nconscious thought. Fast, accurate algorithms for object\\ndetection would allow computers to drive cars without\\nspecialized sensors, enable assistive devices to convey\\nreal-time scene information to human users, and unlock\\nthe potential for general purpose, responsive robotic\\nsystems.\\nCurrent detection systems repurpose classifiers to\\nperform detection. To detect an object, these systems\\ntake a classifier for that object and evaluate it at\\nvarious locations and scales in a test image. Systems\\nlike deformable parts models (DPM) use a sliding window\\napproach where the classifier is run at evenly spaced\\nlocations over the entire image.\\nMore recent approaches like R-CNN use region proposal\\nmethods to first generate potential bounding boxes in an\\nimage and then run a classifier on these proposed boxes.\\nAfter classification, post-processing is used to refine\\nthe bounding boxes, eliminate duplicate detections, and\\nrescore the boxes based on other objects in the scene.\\nThese complex pipelines are slow and hard to optimize\\nbecause each individual component must be trained\\nseparately.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 462, 'file_type': 'pdf'}, page_content='We reframe object detection as a single regression\\nproblem, straight from image pixels to bounding box\\ncoordinates and class probabilities. Using our system,\\nyou only look once (YOLO) at an image to predict what\\nobjects are present and where they are.\\nYOLO systems are characterized by their performance:\\nOur base network runs at 45 frames per second with no\\nbatch processing on a Titan X GPU and a fast version runs\\nat more than 150 fps. This means we can process streaming\\nvideo in real-time with less than 25 milliseconds of\\nlatency. Furthermore, YOLO achieves more than twice the\\nmean average precision of other real-time systems.\\nAt a high level, YOLO works by dividing feature maps into\\ngrids of cells and evaluating each cell for the presence of\\nan object, as shown in Figure\\xa012-8. Using the anchor-box\\nconcept borrowed from Faster R-CNN, YOLO analyzes bounding\\nboxes of various shapes and sizes around each cell and\\nassigns classes and probabilities to the boxes. One CNN\\nhandles everything, including feature extraction,\\nclassification, and regression designating the sizes and\\nlocations of the bounding boxes, and an image goes through\\nthe CNN just once. At the end, NMS reduces the number of\\nbounding boxes to one per object, and each bounding box is\\nattributed with a class as well as a confidence level—the\\nprobability that the box actually contains an object of the\\nspecified class.\\nFigure 12-8. YOLO’s approach to analyzing an image'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 463, 'file_type': 'pdf'}, page_content='There are currently seven versions of YOLO referred to as\\nYOLOv1 through YOLOv7. Each new version improves on the\\nprevious version in terms of accuracy and performance. There\\nare also variations such as PP-YOLO and YOLO9000. YOLOv3 was\\nthe last version that YOLO creator Joseph Redmon contributed\\nto and is considered a reference implementation of sorts. By\\nextracting feature maps from certain layers of the CNN,\\nYOLOv3 analyzes the image using a 13 × 13 grid, a 26 × 26\\ngrid, and a 52 × 52 grid in an effort to detect objects of\\nvarious sizes. It uses anchors to predict nine bounding boxes\\nper cell. YOLO’s primary weakness is that it has difficulty\\ndetecting very small objects that are close together,\\nalthough YOLOv3 improved on YOLOv1 and YOLOv2 in this regard.\\nMore information about YOLO can be found on its creator’s\\nwebsite. A separate article titled “Digging Deep into YOLO\\nV3” offers a deep dive into the YOLOv3 architecture.\\nYOLOv3 and Keras\\nYOLO was originally written using a deep-learning framework\\ncalled Darknet, but it can be implemented with other\\nframeworks as well. The keras-yolo3 project on GitHub\\ncontains a Keras implementation of YOLOv3 that can be trained\\nfrom scratch or initialized with predefined weights and used\\nfor inference—that is, to make predictions. This version\\naccepts 416 × 416 images. To simplify usage, I created a\\nfile named yolov3.py containing helper classes and helper\\nfunctions. It is a modified version of a file that’s\\navailable in the keras-yolo3 project. I removed elements that\\nweren’t needed for making predictions, rewrote some of the\\ncode for improved utility and performance, and added a little\\ncode of my own, but most of the credit goes to Huynh Ngoc\\nAnh, whose GitHub ID is experiencor. With yolov3.py and a set\\nof weights to lend a hand, you can load YOLOv3 and make\\npredictions with just a few lines of code.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 464, 'file_type': 'pdf'}, page_content=\"To see YOLOv3 in action, begin by downloading a ZIP file\\ncontaining the files you need. Open the ZIP file and place\\nyolov3.py in the directory with your notebooks. Place the\\nother files in your notebooks’ Data subdirectory. Next,\\ndownload yolov3.weights, which contains the weights arrived\\nat when YOLOv3 was trained on the COCO dataset, and place it\\nin the Data subdirectory as well.\\nNow create a new Jupyter notebook and paste the following\\ncode into the first cell:\\n\\nfrom\\nyolov3\\nimport\\n\\nmodel\\nmake_yolov3_model()\\nweight_reader\\nWeightReader('Data/yolov3.weights')\\nweight_reader.load_weights(model)\\nmodel.summary()\\nRun the code to import the helper classes and functions in\\nyolov3.py, create the model, and initialize it with the COCO\\nweights. Then use the following statements to load a photo of\\na couple biking the city wall surrounding Xian, China:\\n\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\n\\nimage\\nplt.imread('Data/xian.jpg')\\nwidth,\\nheight\\nimage.shape[1],\\nimage.shape[0]\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 465, 'file_type': 'pdf'}, page_content=\"Now let’s see if YOLOv3 can detect objects in the image. Use\\nthe following code to load the image again, resize it to 416\\n× 416, preprocess the pixels, and submit the resulting image\\nto the model for prediction:\\n\\nimport\\nnumpy\\nas\\nnp\\nfrom\\ntensorflow.keras.preprocessing.image\\nimport\\nload_img,\\nimg_to_array\\n\\nx\\nload_img('Data/xian.jpg',\\ntarget_size=(YOLO3.width,\\nYOLO3.height))\\nx\\nimg_to_array(x)\\nx\\nnp.expand_dims(x,\\naxis=0)\\ny\\nmodel.predict(x)\\npredict returns arrays containing information about objects\\ndetected in the image at three resolutions (that is, with\\ngrid cells measuring 8, 16, and 32 pixels square), but the\\narrays need to be decoded into bounding boxes and the boxes\\nfiltered with NMS. To help, yolov3.py contains a function\\ncalled decode_predictions (inspired by Keras’s\\ndecode_predictions function) to do the post-processing. decode\\u200b\\n_pre\\u2060dic\\u2060tions requires the width and height of the original\\nimage as input so that it can scale the bounding boxes to\\nmatch the original image dimensions. The return value is a\\nlist of BoundingBox objects, each containing the pixel\\ncoordinates of a box surrounding an object detected in the\\nscene, along with a label identifying the object and a\\nconfidence value from 0.0 to 1.0.\\nThe next step, then, is to pass the predictions returned by\\npredict to decode\\u200b_pre\\u2060dic\\u2060tions and list the bounding boxes:\\n\\nboxes\\ndecode_predictions(y,\\nwidth,\\nheight)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 466, 'file_type': 'pdf'}, page_content=\"for\\nbox\\nin\\nboxes:\\n\\xa0\\xa0\\xa0 print(f'({box.xmin}, {box.ymin}), ({box.xmax}, {box.ymax}), '\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 f'{box.label}, {box.score}')\\nHere’s the output:\\n\\n(692, 232), (1303, 1490), person, 0.9970048069953918\\n(1314, 327), (1920, 1496), person, 0.9957388639450073\\n(716, 786), (1277, 1634), bicycle, 0.9924144744873047\\n(1210, 845), (2397, 1600), bicycle, 0.9957170486450195\\nThe model detected four objects in the image: two people and\\ntwo bikes. The labels come from the COCO dataset. There are\\n80 in all, and they’re built into the YOLO3 class in\\nyolov3.py. Use the following command to list them and see all\\nthe different types of objects the model can detect:\\n\\nYOLO3.labels\\nyolov3.py also contains a helper function named\\nannotate_image that loads an image from the filesystem and\\ndraws the bounding boxes returned by decode_predictions as\\nwell as labels and confidence values. Use the following\\nstatement to visualize what the model found in the image:\\n\\nannotate_image('Data/xian.jpg',\\nboxes)\\nThe output should look like this:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 467, 'file_type': 'pdf'}, page_content=\"Now let’s try it with another image—this time, a photo of a\\nyoung woman and the family dog. First show the image and save\\nits width and height:\\n\\nimage\\nplt.imread('Data/abby-lady.jpg')\\nwidth,\\nheight\\nimage.shape[1],\\nimage.shape[0]\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\nPreprocess the image and pass it to the model’s predict\\nmethod:\\n\\nx\\nload_img('Data/abby-lady.jpg',\\ntarget_size=(YOLO3.width,\\nYOLO3.height))\\nx\\nimg_to_array(x)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 468, 'file_type': 'pdf'}, page_content=\"x\\nnp.expand_dims(x,\\naxis=0)\\ny\\nmodel.predict(x)\\nShow the image again, this time annotated with the objects\\ndetected in it:\\n\\nboxes\\ndecode_predictions(y,\\nwidth,\\nheight)\\nannotate_image('Data/abby-lady.jpg',\\nboxes)\\nHere is the output:\\nThe model detected the girl and her laptop, but it didn’t\\ndetect the dog even though the COCO training images included\\ndogs. Under the hood, it did detect the dog, but with less\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 469, 'file_type': 'pdf'}, page_content=\"than 90% confidence. The model’s predict method typically\\nreturns information about thousands of bounding boxes, most\\nof which can be ignored because the confidence levels are so\\nlow. By default, decode_predictions ignores bounding boxes\\nwith confidence scores less than 0.9, but you can override\\nthat by including a min_score parameter in the call. Use the\\nfollowing statements to decode the predictions and visualize\\nthem again, this time with a minimum confidence level of 55%:\\n\\nboxes\\ndecode_predictions(y,\\nwidth,\\nheight,\\nmin_score=0.55)\\nannotate_image('Data/abby-lady.jpg',\\nboxes)\\nWith the confidence threshold lowered to 0.55, the model not\\nonly detected the dog, but also the sofa:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 470, 'file_type': 'pdf'}, page_content='Because the model was trained on the COCO dataset, it can\\ndetect lots of other objects too, including traffic lights,\\nstop signs, various types of food and animals, and even\\nbottles and wine glasses. Try it with some images of your own\\nto experience state-of-the-art object detection firsthand.\\nCustom Object Detection\\nAn object detection model trained on the COCO dataset can\\nreadily detect and identify 80 different types of objects.\\nBut what if you need an object detection model that\\nidentifies other objects? What if, for example, you need a\\nmodel that detects license plates on cars, packages left on\\nyour front porch, or cancer cells in tissue samples? All of\\nthese are ways in which object detection is employed in\\nindustry today. And none of them rely on models trained\\nsolely with COCO images.\\nTraining an object detection model from scratch is a heavy\\nlift even for researchers at Microsoft, Facebook, and Google.\\nYou can short-circuit the process a bit by starting with the\\nCOCO weights and incrementally training the network with new\\nclasses, but even that is compute and time intensive.\\nThere is an easier way. The premise underlying this entire\\nbook is that you’re a software developer or engineer, not a\\nPhD data scientist. You want to use machine learning (or deep\\nlearning) to add value to your business. You want results,\\nnot equations on a blackboard. You may or may not own a GPU,\\nbut you have a job to do, and in this case, that job involves\\ncustom object detection.\\nThe “easier way” is Azure Cognitive Services—specifically,\\na member of the Cognitive Services family called the Custom\\nVision service. I will formally introduce Azure Cognitive\\nServices in Chapter\\xa014, but if your goal is to build a\\ncustom object detector, now’s the right time for your first\\nforay into the world of AI as a service. In my opinion,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 471, 'file_type': 'pdf'}, page_content='nothing makes crafting custom object detection models easier\\nthan the Custom Vision service. Little expertise is required,\\nand at the end, you’re left with a trained model that you\\ncan easily consume in Python or C# or other programming\\nlanguages such as Swift.\\nAzure’s Custom Vision service is one of the best-kept\\nsecrets in AI. Let’s use it to train a custom object\\ndetection model—and then don our marine biologist cap and\\nput the model to work.\\nTraining a Custom Object Detection Model with\\nthe Custom Vision Service\\nThe Custom Vision service is one of more than 20 services and\\nAPIs that make up Azure Cognitive Services. Cognitive\\nServices enables you to build intelligence into apps without\\nrequiring deep expertise in machine learning and AI. The\\nCustom Vision service lets you build image classification\\nmodels and object detection models and train them in the\\ncloud on GPUs. When training is complete, you can either\\ndeploy the models as web services and call them using REST\\nAPIs, or download them in various formats, including\\nTensorFlow and Core ML, and consume them locally. Included in\\nthe download are sample source code files demonstrating how\\nto consume the model.\\nTo use the Custom Vision service, you need a Microsoft\\naccount and an Azure subscription. If you don’t have a\\nMicrosoft account, you can create one for free. If you don’t\\nhave an Azure subscription, you can create a free trial\\nsubscription. You have to provide a credit card or debit\\ncard, but you get $200 in free credits to use for 30 days and\\naccess to many free service tiers. Even if you have a paid\\nsubscription or let your free trial roll over into a paid\\nsubscription, you won’t necessarily incur any charges for\\nusing the Custom Vision service—especially if you export the\\nmodels you create.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 472, 'file_type': 'pdf'}, page_content='NOTE\\nIf you’d prefer not to create an Azure subscription, you can skip\\nthe remainder of this section and pick up again with the next\\nsection, where we use the trained model to detect objects. All the\\nfiles you need in order to utilize the model are present in the\\nChapter 12 folder of this book’s GitHub repo.\\nReady to build a custom object detection model? How about one\\nthat spots sea turtles in the wild? Start by downloading a\\ndataset of sea turtle images. I used Bing image search to\\nfind these photos and limited the search to images that are\\nfree to be shared and used commercially. Copy the images from\\nthe ZIP file to a convenient location on your hard disk.\\nNext, navigate to the Custom Vision portal in your browser\\nand log in with your Microsoft account. Click “+ NEW\\nPROJECT,” and in the “Create new project” dialog, click\\n“create new” to create a new Azure resource for the project\\n(Figure\\xa012-9).\\nFigure 12-9. Creating a new Azure resource in a Custom Vision service\\nproject'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 473, 'file_type': 'pdf'}, page_content='Enter a name such as “turtle-detector” for the resource and\\nselect your Azure subscription, as shown in Figure\\xa012-10.\\nSelect an existing Azure resource group for the new resource\\nor click “create new” and create a new one. Make sure\\nCognitiveServi\\u2060ces is selected as the resource type and either\\naccept the default Azure location or choose one that’s\\ncloser to you. Finally, select F0 as the pricing tier if\\nit’s available, or S1 if it’s not. (F0 is a free tier and\\nis generally limited to one per Azure subscription.) Then\\nclick the “Create resource” button.\\nFigure 12-10. Creating a new Cognitive Services resource'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 474, 'file_type': 'pdf'}, page_content='NOTE\\nIn Azure, a resource group is a collection of related resources\\nthat serve a common purpose and share a common lifetime. Deleting\\na resource group deletes all the resources in that group. In\\naddition, you can easily get billing information for the group as\\na whole rather than add up the costs of the individual resources.\\nIn the “Create new project” dialog, enter a name for the\\nproject and select the resource that you just created\\n(Figure\\xa012-11). Select Object Detection as the project type\\nand “General (compact)” as the domain. Selecting one of the\\n“compact” domains is essential if the goal is to train a\\nmodel that can be exported and consumed locally. Make sure\\n“Basic platforms” is selected under Export Capabilities,\\nand then click “Create project.”'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 475, 'file_type': 'pdf'}, page_content='Figure 12-11. Creating a project containing an object detector'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 476, 'file_type': 'pdf'}, page_content='Now that the Custom Vision project has been created, the next\\nstep is to upload the images that you’ll use to train the\\nmodel and annotate them with class names and bounding boxes.\\nTo begin, click “Add images.” Then navigate to the folder\\nwhere your sea turtle images are stored and upload all 50.\\nAfter the upload is complete, click the first image to open\\nit in the image detail editor. Use your mouse to draw a box\\naround the sea turtle, as shown in Figure\\xa012-12. Then type\\nsea-turtle into the tag editor that pops up, and click the\\nplus sign to the right of it. Now click the right arrow at\\nthe far right to move to the next image, and repeat this\\nprocess for the remaining training images. You don’t have to\\nenter the tag name again; you simply select it from a list of\\nexisting tags. In addition, you usually don’t have to draw\\nboxes around the sea turtles. Hover your cursor over a sea\\nturtle and the editor will use a little AI of its own to\\nsuggest a bounding box.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 477, 'file_type': 'pdf'}, page_content='Figure 12-12. Identifying sea turtles in training images\\nNOTE\\nThe most tedious part of training a custom object detection model\\nis identifying objects and bounding boxes in all the training\\nimages. The Custom Vision service helps by providing an\\nintelligent editor to minimize the amount of clicking and dragging\\nrequired. Still, imagine doing this for hundreds of thousands of\\nimages!\\nIn cases in which a photo contains two or more sea turtles,\\nbe sure to tag them all, as shown in Figure\\xa012-13. It’s\\nfine if the bounding boxes overlap. Objects frequently do\\noverlap in the real world, after all. Data scientists refer\\nto overlapping bounding boxes as occlusions.\\nFigure 12-13. Sea turtles with overlapping bounding boxes'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 478, 'file_type': 'pdf'}, page_content='After you’ve tagged all 50 training images, click the green\\nTrain button in the upper-right corner of the page. When\\nasked to choose a training type (Figure\\xa012-14), select Quick\\nTraining to conserve time and money, and then click the Train\\nbutton in the lower-right corner of the dialog to commence\\ntraining. The Advanced Training option usually produces a\\nmore accurate model, but it takes longer and charges you for\\nup to the number of hours you budget. Advanced training might\\nnot be an option if you selected the free F0 tier when you\\ncreated the model. F0 provides one hour of training per month\\nat no charge.\\nFigure 12-14. Choosing a training option\\nQuick training usually takes 5 to 10 minutes. When training\\nis complete, you’ll see a summary like the one in'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 479, 'file_type': 'pdf'}, page_content='Figure\\xa012-15. You already know what precision and recall\\nare. The third metric, mean average precision (mAP), is a\\ncommon metric for gauging the accuracy of object detection\\nmodels. It reflects the model’s ability to classify objects\\nand identify the objects’ bounding boxes. The higher the\\npercentage, the better. If the number isn’t sufficiently\\nhigh, then you have two choices: train with more images, or\\ntry advanced training rather than quick training (or both).\\nState-of-the-art object detection models are usually trained\\nwith thousands of examples of each class.\\nFigure 12-15. Training results'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 480, 'file_type': 'pdf'}, page_content='NOTE\\nAs an experiment, I trained the same model using the advanced\\ntraining option and budgeted one hour for training. Training took\\nless than 30 minutes and mAP reached 97.2%. At the time of this\\nwriting, advanced training costs $10 per hour, so the cost to my\\nAzure subscription was less than $5. The Custom Vision pricing\\npage has more information on pricing.\\nYou can test the model’s accuracy by clicking the Quick Test\\nbutton at the top of the page and selecting a photo or two\\nthat the model wasn’t trained with. Or you can click Publish\\nin the upper-left corner of the page and deploy the model as\\na web service. But our goal is to download the model and run\\nit locally. To that end, click Export at the top of the page.\\nChoose TensorFlow as the export type and then click the\\nExport button. Wait for the export to be prepared, and then\\nclick the Download button (Figure\\xa012-16).\\nFigure 12-16. Downloading the trained TensorFlow model'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 481, 'file_type': 'pdf'}, page_content='When the download completes, open the downloaded ZIP file.\\nCopy the following files from the ZIP file to the directory\\nwhere your Jupyter notebooks are hosted:\\nlabels.txt\\nContains the label you created (sea-turtle) when you\\nlabeled the images.\\nmodel.pb\\nContains the trained and serialized TensorFlow model.\\nobject_detection.py\\nFound in the ZIP file’s python folder, this file\\ncontains helper functions for utilizing the model in\\nPython apps.\\nOnce these files are downloaded, delete the project in the\\nCustom Vision portal unless you want to go back and refine\\nthe model by training it with more images (or granting it\\nmore training time). To ensure that no additional charges to\\nyour Azure subscription will be incurred, delete the Azure\\nresource group containing the model. You can delete resource\\ngroups through the Azure portal.\\nNote that when preparing a model with the Custom Vision\\nservice, you aren’t limited to one class of object. You can\\nupload images with dozens of different classes and tag them\\naccordingly in the image detail editor. In that case,\\nlabels.txt will contain all the labels rather than just one.\\nUsing the Exported Model\\nNow you’re ready to put the model to work. Begin by creating\\na new Jupyter notebook and pasting the following code into\\nthe first cell. These statements define a class named\\nSeaTurtleDetector that represents a TensorFlow model and a\\nfunction named annotate_image that annotates an image to show'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 482, 'file_type': 'pdf'}, page_content='what the model detected. The SeaTurtleDetector class is a\\nchild of the ObjectDetection class implemented in\\nobject_detection.py. It has a predict method that you can\\ncall to detect objects in an image:\\n\\nimport\\nsys\\nimport\\ntensorflow\\nas\\ntf\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\nfrom\\nobject_detection\\nimport\\nObjectDetection\\nfrom\\nPIL\\nimport\\nImage\\nimport\\nnumpy\\nas\\nnp\\n\\nclass\\nSeaTurtleDetector(ObjectDetection):\\n\\xa0\\xa0\\xa0 def __init__(self,\\ngraph_def,\\nlabels):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 super(SeaTurtleDetector,\\nself).__init__(labels)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 self.graph\\ntf.compat.v1.Graph()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\nself.graph.as_default():\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_data\\ntf.compat.v1.placeholder(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tf.float32,\\nNone,\\nNone,\\nname=\\'Placeholder\\')\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tf.import_graph_def(graph_def,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input_map={\"Placeholder:0\":\\ninput_data},\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 name=\"\")\\n\\n\\xa0\\xa0\\xa0 def\\npredict(self,\\npreprocessed_image):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 inputs\\nnp.array(preprocessed_image,\\ndtype=float)[:,\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 with\\ntf.compat.v1.Session(graph=self.graph)\\nas\\nsess:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_tensor\\nsess.graph.get_tensor_by_name(\\'model_outputs:0\\')\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 outputs\\nsess.run(output_tensor,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 {\\'Placeholder:0\\':\\ninputs[np.newaxis,\\n...]})\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\noutputs[0]\\n\\ndef\\nannotate_image(image,\\npredictions,\\nmin_score=0.7,\\nfigsize=(12,\\n\\xa0\\xa0\\xa0 fig,\\nax\\nplt.subplots(figsize=figsize,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 483, 'file_type': 'pdf'}, page_content=\"subplot_kw={'xticks':\\n'yticks':\\n\\xa0\\xa0\\xa0 img_width,\\nimg_height\\nimage.size\\n\\xa0\\xa0\\xa0 ax.imshow(image)\\n\\n\\xa0\\xa0\\xa0 for\\np\\nin\\npredictions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 score\\np['probability']\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nscore\\nmin_score:\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x1\\np['boundingBox']['left']\\nimg_width\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 y1\\np['boundingBox']['top']\\nimg_height\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 width\\np['boundingBox']['width']\\nimg_width\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 height\\np['boundingBox']['height']\\nimg_height\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 label\\np['tagName']\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\nRectangle((x1,\\ny1),\\nwidth,\\nheight,\\nfill=False,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 color='red',\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 label\\n= f'{label} ({score:.0%})'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.text(x1\\n(width\\ny1,\\nlabel,\\ncolor='white',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 backgroundcolor='red',\\nha='center',\\nva='bottom',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 fontweight='bold',\\nbbox=dict(color='red'))\\nNext, use the following statements to load the serialized\\nmodel in model.pb and the label in labels.txt and initialize\\nan object detection model:\\n\\n# Load the serialized model graph\\ngraph_def\\ntf.compat.v1.GraphDef()\\nwith\\ntf.io.gfile.GFile('model.pb',\\n'rb')\\nas\\nf:\\n\\xa0\\xa0\\xa0 graph_def.ParseFromString(f.read())\\n\\n# Load the labels\\nwith\\nopen('labels.txt',\\n'r')\\nas\\nf:\\n\\xa0\\xa0\\xa0 labels\\n[l.strip()\\nfor\\nl\\nin\\nf.readlines()]\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 484, 'file_type': 'pdf'}, page_content=\"# Create the model\\nmodel\\nSeaTurtleDetector(graph_def,\\nlabels)\\nFinally, go out to the internet and download a sea turtle\\nphoto. Then load the photo into your notebook and see if the\\nmodel detects the sea turtle in it:\\n\\n%matplotlib\\ninline\\n\\nimage\\nImage.open('PATH_TO_IMAGE_FILE')\\npredictions\\nmodel.predict_image(image)\\nannotate_image(image,\\npredictions)\\nDid the model get it right? It did for me:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 485, 'file_type': 'pdf'}, page_content='Keep in mind that state-of-the-art accuracy in object\\ndetection is a product of training with lots of labeled\\nimages. The 50 you trained with here are a good start, but to\\ntrain a model to a sufficient level of accuracy for real-\\nworld use, you’ll probably need a minimum of 10 times more.\\nThe good news is that once you’ve collected the images, the\\nCustom Vision service makes the process of labeling the\\nimages and training the model relatively easy.\\nSummary\\nObject detection represents the tip of the spear in the world\\nof computer vision. It’s how self-driving cars “see”\\nobjects in front of them, and it’s the technology underlying\\nnumerous real-world applications. State-of-the-art object\\ndetection is accomplished with deep learning using specially\\ncrafted CNNs that do more than mere image classification. You\\ncan use pretrained models such as YOLO and Mask R-CNN to\\ndetect objects in images and video frames. With Mask R-CNN,\\nyou can even generate segmentation masks revealing additional\\ninformation about those objects.\\nCustom object detection requires you to train models like\\nthese with images of your own carefully labeled to denote\\nobjects and bounding boxes. Azure’s Custom Vision service\\nsimplifies the process of training custom object detection\\nmodels, and allows the models you train to be hosted in the\\ncloud and called using REST APIs or downloaded and consumed\\nlocally. It’s one of several services that comprise Azure\\nCognitive Services, and it’s the right tool for the job when\\nthe job calls for detecting objects that pretrained models\\nweren’t trained to detect.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 486, 'file_type': 'pdf'}, page_content='Chapter 13. Natural Language\\nProcessing\\nIt’s not difficult to use Scikit-Learn to build machine\\nlearning models that analyze text for sentiment, identify\\nspam, and classify text in other ways. But today, state-of-\\nthe-art text classification is most often performed with\\nneural networks. You already know how to build neural\\nnetworks that accept numbers and images as input. Let’s\\nbuild on that to learn how to construct deep-learning models\\nthat process text—a segment of deep learning known as\\nnatural language processing, or NLP for short.\\nNLP encompasses a variety of activities including text\\nclassification, named-entity recognition, keyword extraction,\\nquestion answering, and language translation. The accuracy of\\nNLP models has improved in recent years for a variety of\\nreasons, not the least of which are newer and better ways of\\nconverting words and sentences into dense vector\\nrepresentations that incorporate meaning, and a relatively\\nnew neural network architecture called the transformer that\\ncan zero in on the most meaningful words and even\\ndifferentiate between different meanings of the same word.\\nOne element that virtually all neural networks that process\\ntext have in common is an embedding layer, which uses word\\nembeddings to transform arrays, or sequences, of scalar\\nvalues representing words into arrays of floating-point\\nnumbers called word vectors. These vectors encode information\\nabout the meanings of words and the relationships between\\nthem. Output from an embedding layer can be input to a\\nclassification layer, or it can be input to other types of\\nneural network layers to tease more meaning from it before\\nsubjecting it to further processing.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 487, 'file_type': 'pdf'}, page_content=\"Transformers? Embedding layers? Word vectors? There’s a lot\\nto unpack here, but once you wrap your head around a few\\nbasic concepts, neural networks that process language are\\npure magic. Let’s dive in. We’ll start by learning how to\\nprepare text for processing by a deep-learning model and how\\nto create word embeddings. Then we’ll put that knowledge to\\nwork building neural networks that classify text, translate\\ntext from one language to another, and more—all classic\\napplications of NLP.\\nText Preparation\\nChapter\\xa04 introduced Scikit-Learn’s CountVectorizer class,\\nwhich converts rows of text into rows of word counts that a\\nmachine learning model can consume. Count\\u200bVec\\u2060torizer also\\nconverts characters to lowercase, removes numbers and\\npunctuation symbols, and optionally removes stop words—\\ncommon words such as and and the that are likely to have\\nlittle influence on the outcome.\\nText must be cleaned and vectorized before it’s used to\\ntrain a neural network too, but vectorization is typically\\nperformed differently. Rather than create a table of word\\ncounts, you create a table of sequences containing tokens\\nrepresenting individual words. Tokens are often indices into\\na dictionary, or vocabulary, built from the corpus of words\\nin the dataset. To help, Keras provides the Tokenizer class,\\nwhich you can think of as the deep-learning equivalent of\\nCount\\u200bVec\\u2060torizer. Here’s an example that uses Tokenizer to\\ncreate sequences from four lines of text:\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\n\\nlines\\n\\xa0\\xa0\\xa0 'The quick brown fox',\\n\\xa0\\xa0\\xa0 'Jumps over $$$ the lazy brown dog',\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 488, 'file_type': 'pdf'}, page_content=\"'Who jumps high into the blue sky after counting 123',\\n\\xa0\\xa0\\xa0 'And quickly returns to earth'\\n\\ntokenizer\\nTokenizer()\\ntokenizer.fit_on_texts(lines)\\nsequences\\ntokenizer.texts_to_sequences(lines)\\nfit_on_texts creates a dictionary containing all the words in\\nthe input text. texts_to_sequences returns a list of\\nsequences, which are simply arrays of indices into the\\ndictionary:\\n\\n[[1, 4, 2, 5],\\n[3, 6, 1, 7, 2, 8],\\n[9, 3, 10, 11, 1, 12, 13, 14, 15, 16],\\n[17, 18, 19, 20, 21]]\\nThe word brown appears in lines 1 and 2 and is represented by\\nthe index 2. Therefore, 2 appears in both sequences.\\nSimilarly, a 3 representing the word jumps appears in\\nsequences 2 and 3. The index 0 isn’t used to denote words;\\nit’s reserved to serve as padding. More on this in a moment.\\nYou can use Tokenizer’s sequences_to_texts method to reverse\\nthe process and convert the sequences back into text:\\n\\n['the quick brown fox',\\n'jumps over the lazy brown dog',\\n'who jumps high into the blue sky after counting 123',\\n'and quickly returns to earth']\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 489, 'file_type': 'pdf'}, page_content=\"One revelation that comes from this is that Tokenizer\\nconverts text to lowercase and removes symbols, but it\\ndoesn’t remove stop words or numbers. If you want to remove\\nstop words, you can use a separate library such as the\\nNatural Language Toolkit (NLTK). You can also remove words\\ncontaining numbers while you’re at it:\\n\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\nfrom\\nnltk.tokenize\\nimport\\nword_tokenize\\nfrom\\nnltk.corpus\\nimport\\nstopwords\\n\\nlines\\n\\xa0\\xa0\\xa0 'The quick brown fox',\\n\\xa0\\xa0\\xa0 'Jumps over $$$ the lazy brown dog',\\n\\xa0\\xa0\\xa0 'Who jumps high into the blue sky after counting 123',\\n\\xa0\\xa0\\xa0 'And quickly returns to earth'\\n\\ndef\\nremove_stop_words(text):\\n\\xa0\\xa0\\xa0 text\\nword_tokenize(text.lower())\\n\\xa0\\xa0\\xa0 stop_words\\nset(stopwords.words('english'))\\n\\xa0\\xa0\\xa0 text\\n[word\\nfor\\nword\\nin\\ntext\\nif\\nword.isalpha()\\nand\\nnot\\nword\\nin\\nstop_words]\\n\\xa0\\xa0\\xa0 return\\n' '.join(text)\\n\\nlines\\nlist(map(remove_stop_words,\\nlines))\\n\\ntokenizer\\nTokenizer()\\ntokenizer.fit_on_texts(lines)\\ntokenizer.texts_to_sequences(lines)\\nThe resulting sequences look like this:\\n\\n[[3, 1, 4],\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 490, 'file_type': 'pdf'}, page_content=\"[2, 5, 1, 6],\\n[2, 7, 8, 9, 10],\\n[11, 12, 13]]\\nwhich, converted back to text, are as follows:\\n\\n['quick brown fox',\\n'jumps lazy brown dog',\\n'jumps high blue sky counting',\\n'quickly returns earth']\\nThe sequences range from three to five values in length, but\\na neural network expects all sequences to be the same length.\\nKeras’s pad_sequences function performs this final step,\\ntruncating sequences longer than the specified length and\\npadding sequences shorter than the specified length with 0s:\\n\\nfrom\\ntensorflow.keras.preprocessing.sequence\\nimport\\npad_sequences\\n\\npadded_sequences\\npad_sequences(sequences,\\nmaxlen=4)\\nThe resulting padded sequences look like this:\\n\\narray([[\\n0,\\xa0 3,\\xa0 1,\\xa0 4],\\n2,\\xa0 5,\\xa0 1,\\xa0 6],\\n7,\\xa0 8,\\xa0 9,\\n13]])\\nConverting these sequences back to text yields this:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 491, 'file_type': 'pdf'}, page_content='[\\'quick brown fox\\',\\n\\'jumps lazy brown dog\\',\\n\\'high blue sky counting\\',\\n\\'quickly returns earth\\']\\nBy default, pad_sequences pads and truncates on the left, but\\nyou can include a padding=\\'post\\' parameter if you prefer to\\npad and truncate on the right. Padding on the right is\\nsometimes important when using neural networks to translate\\ntext.\\nNOTE\\nRemoving stop words frequently has little or no effect on text\\nclassification tasks. If you simply want to remove numbers from\\ntext input to a neural network without removing stop words, create\\na Tokenizer this way:\\n\\ntokenizer\\nTokenizer(\\n\\xa0\\xa0\\xa0 filters=\\'!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_` \\' \\\\\\n\\xa0\\xa0\\xa0 \\'{|}~\\\\t\\\\n0123456789\\')\\nThe filters parameter tells Tokenizer what characters to remove. It\\ndefaults to \\'!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_`{|}~\\\\t\\\\n\\'. This code simply\\nadds 0–9 to the list. Note that Tokenizer does not remove\\napostrophes by default, but you can remove them by adding an\\napostrophe to the filters list.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 492, 'file_type': 'pdf'}, page_content='It’s important that text input to a neural network for\\npredictions be tokenized and padded in the same way as text\\ninput to the model for training. If you’re thinking it sure\\nwould be nice not to have to do the tokenization and\\nsequencing manually, there is a way around it. Rather than\\nwrite a lot of code, you can include a TextVectorization\\nlayer in the model. I’ll demonstrate how momentarily. But\\nfirst, you need to learn about embedding layers.\\nWord Embeddings\\nOnce text is tokenized and converted into padded sequences,\\nit is ready for training a neural network. But you probably\\nwon’t get very far training on the raw padded sequences.\\nOne of the crucial elements of a neural network that\\nprocesses text is an embedding layer whose job is to convert\\npadded sequences of word tokens into arrays of word vectors,\\nwhich represent each word with an array (vector) of floating-\\npoint numbers rather than a single integer. Each word in the\\ninput text is represented by a vector in the embedding layer,\\nand as the network is trained, vectors representing\\nindividual words are adjusted to reflect their relationship\\nto one another. If you’re building a sentiment analysis\\nmodel and words such as excellent and amazing have similar\\nconnotations, then the vectors representing those words in\\nthe embedding space should be relatively close together so\\nthat phrases such as “excellent service” and “amazing\\nservice” score similarly.\\nImplementing an embedding layer by hand is a complex\\nundertaking (especially the training aspect), so Keras offers\\nthe Embedding class. With Keras, creating a trainable\\nembedding layer requires just one line of code:\\n\\nEmbedding(input_dim=10000,\\noutput_dim=32,\\ninput_length=100)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 493, 'file_type': 'pdf'}, page_content='In order, the three parameters passed to the Embedding\\nfunction are:\\n\\nThe vocabulary size, or the number of words in\\nthe vocabulary built by Tokenizer\\nThe number of dimensions m in the embedding space\\nThe length n of each padded sequence\\nYou pick the number of dimensions m, and each word gets\\nencoded in the embedding space as an m-dimensional vector.\\nMore dimensions provide more fitting power, but also increase\\ntraining time. In practice, m is usually a number from 32 to\\nThe vectors that represent individual words in an embedding\\nlayer are learned during training, just as the weights\\nconnecting neurons in adjacent dense layers are learned. If\\nthe number of training samples is sufficiently high, training\\nthe network usually creates effective vector representations\\nof all the words. However, if you have only a few hundred\\ntraining samples, the embedding layer might not have enough\\ninformation to properly vectorize the corpus of text.\\nIn that case, you can elect to initialize the embedding layer\\nwith pretrained word embeddings rather than rely on it to\\nlearn the word embeddings on its own. Several popular\\npretrained word embeddings exist in the public domain,\\nincluding the GloVe word vectors developed by Stanford and\\nGoogle’s own Word2Vec. Pretrained embeddings tend to model\\nsemantic relationships between words, recognizing, for\\nexample, that king and queen are related terms while stairs\\nand zebra are not. While that can be beneficial, a network\\ntrained to classify text usually performs better when word\\nembeddings are learned from the training data because such\\nembeddings are task specific. For an example showing how to\\nuse pretrained embeddings, see “Using Pre-trained Word\\nEmbeddings” by the author of Keras.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 494, 'file_type': 'pdf'}, page_content='Text Classification\\nFigure\\xa013-1 shows the baseline architecture for a neural\\nnetwork that classifies text. Tokenized text sequences are\\ninput to the embedding layer. The output from the embedding\\nlayer is a 2D matrix of floating-point values measuring m by\\nn, where m is the number of dimensions in the embedding space\\nand n is the sequence length. The Flatten layer following the\\nembedding layer “flattens” the 2D output into a 1D array\\nsuitable for input to a dense layer, and the dense layer\\nclassifies the values emitted from the flatten layer. You can\\nexperiment with different dimensions in the embedding layer\\nand different widths of the dense layer to maximize accuracy.\\nYou can also add more dense layers if needed.\\nFigure 13-1. Neural network for classifying text\\nA neural network like the one in Figure\\xa013-1 can be\\nimplemented this way:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\\n\\nmodel\\nSequential()\\n\\nmodel.add(Embedding(10000,\\ninput_length=100))\\n\\nmodel.add(Flatten())'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 495, 'file_type': 'pdf'}, page_content=\"model.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(optimizer='adam',\\nloss='binary_crossentropy',\\nmetrics=\\n['accuracy'])\\nOne application for a neural network that classifies text is\\nspam filtering. Chapter\\xa04 demonstrated how to use Scikit to\\nbuild a machine learning model that separates spam from\\nlegitimate emails. Let’s build an equivalent deep-learning\\nmodel with Keras and TensorFlow. We’ll use the same dataset\\nwe used before: one containing 1,000 emails, half of which\\nare spam (indicated by 1s in the label column) and half of\\nwhich are not (indicated by 0s in the label column).\\nBegin by downloading the dataset and copying it into the Data\\nsubdirectory where your Jupyter notebooks are hosted. Then\\nuse the following statements to load the dataset and shuffle\\nthe rows to distribute positive and negative samples\\nthroughout. Shuffling is important because rather than use\\ntrain_test_split to create a validation dataset, we’ll use\\nfit’s validation_split parameter. It doesn’t shuffle the\\ndata as train_test_split does:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/ham-spam.csv')\\ndf\\ndf.sample(frac=1,\\nrandom_state=0)\\ndf.head()\\nUse the following statements to remove any duplicate rows\\nfrom the dataset and check for balance:\\n\\ndf\\ndf.drop_duplicates()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 496, 'file_type': 'pdf'}, page_content=\"df.groupby('IsSpam').describe()\\nNext, extract the emails from the DataFrame’s Text column\\nand labels from the IsSpam column. Then use Keras’s\\nTokenizer class to tokenize the text and convert it into\\nsequences, and pad_sequences to produce sequences of equal\\nlength. There’s no need to remove stop words because doing\\nso doesn’t impact the outcome:\\n\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\nfrom\\ntensorflow.keras.preprocessing.sequence\\nimport\\npad_sequences\\n\\nx\\ndf['Text']\\ny\\ndf['IsSpam']\\n\\nmax_words\\n10000\\n# Limit the vocabulary to the 10,000 most common words\\nmax_length\\n\\ntokenizer\\nTokenizer(num_words=max_words)\\ntokenizer.fit_on_texts(x)\\nsequences\\ntokenizer.texts_to_sequences(x)\\nx\\npad_sequences(sequences,\\nmaxlen=max_length)\\nDefine a binary classification model that contains an\\nembedding layer with 32 dimensions, a flatten layer to\\nflatten output from the embedding layer, a dense layer for\\nclassification, and an output layer with a single neuron and\\nsigmoid activation:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 497, 'file_type': 'pdf'}, page_content=\"model\\nSequential()\\n\\nmodel.add(Embedding(max_words,\\ninput_length=max_length))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\nmetrics=\\n['accuracy'])\\n\\nmodel.summary()\\nTrain the network and allow Keras to use 20% of the training\\nsamples for validation:\\n\\nhist\\nmodel.fit(x,\\ny,\\nvalidation_split=0.2,\\nepochs=5,\\nbatch_size=20)\\nUse the history object returned by fit to plot the training\\nand validation accuracy in each epoch:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 498, 'file_type': 'pdf'}, page_content=\"plt.legend(loc='lower right')\\nplt.plot()\\nHopefully, the network achieved a validation accuracy\\nexceeding 95%. If it didn’t, train it again. Here’s how it\\nturned out for me:\\nOnce you’re satisfied with the accuracy, use the following\\nstatements to compute the probability that an email regarding\\na code review is spam:\\n\\ntext\\n'Can you attend a code review on Tuesday? ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Need to make sure the logic is rock solid.'\\n\\nsequence\\ntokenizer.texts_to_sequences([text])\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 499, 'file_type': 'pdf'}, page_content=\"padded_sequence\\npad_sequences(sequence,\\nmaxlen=max_length)\\nmodel.predict(padded_sequence)[0][0]\\nThen do the same for another email:\\n\\ntext\\n'Why pay more for expensive meds when ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'you can order them online and save $$$?'\\n\\nsequence\\ntokenizer.texts_to_sequences([text])\\npadded_sequence\\npad_sequences(sequence,\\nmaxlen=max_length)\\nmodel.predict(padded_sequence)[0][0]\\nWhat did the network predict for the first email? What about\\nthe second? Do you agree with the predictions? Remember that\\na number close to 0.0 indicates that the email is not spam,\\nwhile a number close to 1.0 indicates that it is.\\nAutomating Text Vectorization\\nRather than run Tokenizer and pad_sequences manually, you can\\npreface an embedding layer with a TextVectorization layer.\\nHere’s an example:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nimport\\ntensorflow\\nas\\ntf\\n\\nmodel\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 500, 'file_type': 'pdf'}, page_content=\"model.add(Embedding(max_words,\\ninput_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary()\\nNote that the input layer (an instance of InputLayer) is now\\nexplicitly defined, and it’s configured to accept string\\ninput. In addition, before training the model, the\\nTextVectorization layer must be fit to the input data by\\ncalling adapt:\\n\\nmodel.layers[0].adapt(x)\\nNow you no longer need to preprocess the training text, and\\nyou can pass raw text strings to predict:\\n\\ntext\\n'Why pay more for expensive meds when ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'you can order them online and save $$$?'\\nmodel.predict([text])[0][0]\\nTextVectorization doesn’t remove stop words, so if you want\\nthem removed, you can do that separately or use the\\nTextVectorization function’s standardize parameter to\\nidentify a callback function that does it for you.\\nUsing TextVectorization in a Sentiment\\nAnalysis Model\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 501, 'file_type': 'pdf'}, page_content='To demonstrate how TextVectorization layers simplify text\\nprocessing, let’s use it to build a binary classifier that\\nperforms sentiment analysis. We’ll use the same dataset we\\nused in Chapter\\xa04: the IMDB reviews dataset containing\\n25,000 positive reviews and 25,000 negative reviews.\\nDownload the dataset if you haven’t already and place it in\\nyour Jupyter notebooks’ Data subdirectory. Then create a new\\nnotebook and use the following statements to load and shuffle\\nthe dataset:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv(\\'Data/reviews.csv\\',\\nencoding=\"ISO-8859-1\")\\ndf\\ndf.sample(frac=1,\\nrandom_state=0)\\ndf.head()\\nRemove duplicate rows and check for balance:\\n\\ndf\\ndf.drop_duplicates()\\ndf.groupby(\\'Sentiment\\').describe()\\nNow create the model and include a TextVectorization layer to\\npreprocess input text:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nFlatten,\\nEmbedding\\nimport\\ntensorflow\\nas\\ntf\\n\\nmax_words\\n20000'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 502, 'file_type': 'pdf'}, page_content=\"max_length\\n\\nmodel\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\\nmodel.add(Embedding(max_words,\\ninput_length=max_length))\\nmodel.add(Flatten())\\nmodel.add(Dense(128,\\nactivation='relu'))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary()\\nExtract the reviews from the DataFrame’s Text column and the\\nlabels (0 for negative sentiment, 1 for positive) from the\\nSentiment column, and use the former to fit the\\nTextVectorization layer to the text. Then train the model:\\n\\nx\\ndf['Text']\\ny\\ndf['Sentiment']\\nmodel.layers[0].adapt(x)\\n\\nhist\\nmodel.fit(x,\\ny,\\nvalidation_split=0.5,\\nepochs=5,\\nbatch_size=250)\\nWhen training is complete, plot the training and validation\\naccuracy:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 503, 'file_type': 'pdf'}, page_content=\"acc\\nhist.history['accuracy']\\nval\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe model fits to the training text extremely well, but\\nvalidation accuracy usually peaks between 85% and 90%:\\nUse the model to score a positive comment for sentiment:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 504, 'file_type': 'pdf'}, page_content=\"text\\n'Excellent food and fantastic service!'\\nmodel.predict([text])[0][0]\\nNow do the same for a negative comment:\\n\\ntext\\n'The long lines and poor customer service really turned me off.'\\nmodel.predict([text])[0][0]\\nObserve how much simpler the code is. Operationalizing the\\nmodel is simpler too, because you no longer need a Tokenizer\\nfit to the training data to prepare text submitted to the\\nmodel for predictions. Be aware, however, that a model with a\\nText\\u200bVec\\u2060torization layer can’t be saved in Keras’s H5\\nformat. It can be saved in TensorFlow’s SavedModel format.\\nThe following statement saves the model in the saved_model\\nsubdirectory of the current directory:\\n\\nmodel.save('saved_model')\\nOnce the model is reloaded, you can pass text directly to it\\nfor making predictions.\\nFactoring Word Order into Predictions\\nBoth of the models you just built are bag-of-words models\\nthat ignore word order. Such models are common and are often\\nmore accurate than other types of models. But that’s not\\nalways the case. The relative position of the words in a\\nsentence sometimes has meaning. Credit and card should\\nprobably influence a spam classifier one way if they appear\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 505, 'file_type': 'pdf'}, page_content='far apart in a sentence and another way if they appear\\ntogether.\\nOne way to improve—or at least attempt to improve—on a\\nsimple bag-of-words model is to use n-grams as described in\\nChapter\\xa04. An n-gram is a collection of n words appearing in\\nconsecutive order. Keras’s TextVectorization class features\\nan ngrams parameter that makes applying n-grams easy. The\\nfollowing statement creates a TextVectorization layer that\\nconsiders word pairs as well as individual words:\\n\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ngrams=2))\\nOne limitation of n-grams is that they only consider words\\nthat are directly adjacent to each other. A slightly more\\nrobust way to factor word position into a classification task\\nis to replace dense layers with Conv1D and MaxPooling1D\\nlayers, turning the network into a convolutional neural\\nnetwork. CNNs are most often used to classify images, but\\none-dimensional convolution layers play well with text\\nsequences. Here’s the network presented in the previous\\nexample recast as a CNN:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nConv1D,\\nMaxPooling1D,\\nGlobalMaxPooling1D\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nEmbedding\\nimport\\ntensorflow\\nas\\ntf\\n\\nmodel\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 506, 'file_type': 'pdf'}, page_content=\"model.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\\nmodel.add(Embedding(max_words,\\ninput_length=max_length))\\nmodel.add(Conv1D(32,\\nactivation='relu'))\\nmodel.add(MaxPooling1D(5))\\nmodel.add(Conv1D(32,\\nactivation='relu'))\\nmodel.add(GlobalMaxPooling1D())\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nRather than process individual words, the Conv1D layers in\\nthis example extract features from groups of vectors\\nrepresenting words (seven in the first layer and seven more\\nin the second), just as Conv2D layers extract features from\\nblocks of pixels. The MaxPooling1D layer condenses the output\\nfrom the first Conv1D layer to reveal higher-level structure\\nin input sequences, similar to the way reducing the\\nresolution of an image tends to draw out macro features such\\nas the shape of a person’s body while minimizing or\\nfiltering out altogether lesser features such as the shape of\\na person’s eyes. A simple CNN like this one sometimes\\nclassifies text more accurately than bag-of-words models, and\\nsometimes does not. As is so often the case in machine\\nlearning, the only way to know is to try.\\nRecurrent Neural Networks (RNNs)\\nYet another way to factor word position into a classifier is\\nto include recurrent layers in the network. Recurrent layers\\nwere originally invented to process time-series data—for\\nexample, to look at weather data for the past five days and\\npredict what tomorrow’s high temperature will be. If you\\nsimply took all the weather data for those five days and\\ntreated each day independently, trends evident in the data\\nwould be lost. A recurrent layer, however, might detect those\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 507, 'file_type': 'pdf'}, page_content='trends and factor them into its output. A sequence of vectors\\noutput by an embedding layer qualifies as a time series\\nbecause words in a phrase are ordered consecutively and words\\nused early in a phrase could inform how words that occur\\nlater are interpreted.\\nFigure\\xa013-2 illustrates how a recurrent layer transforms\\nword embeddings into a vector that’s influenced by word\\norder. In this example, sequences are input to an embedding\\nlayer, which transforms each word (token) in the sequence\\ninto a vector of floating-point numbers. These word\\nembeddings are input to a recurrent layer, yielding another\\noutput vector. To compute that vector, cells in the recurrent\\nlayer loop over the embeddings comprising the sequence. The\\ninput to iteration n + 1 of the loop is the current embedding\\nvector and the output from iteration n—the so-called hidden\\nstate. The output from the recurrent layer is the output from\\nthe final iteration of the loop. The result is different than\\nit would have been had each embedding vector been processed\\nindependently because each iteration uses information from\\nthe previous iteration to compute an output. Context from a\\nword early in a sequence can carry over to words that occur\\nlater on.\\nFigure 13-2. Processing word embeddings with a recurrent layer'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 508, 'file_type': 'pdf'}, page_content='It’s not difficult to build a very simple recurrent layer by\\nhand that works reasonably well with short sequences (say,\\nfour or five words), but longer sequences will suffer from a\\nvanishing-gradient effect that means words far apart will\\nexert little influence over one another. One solution is a\\nrecurrent layer composed of Long Short-Term Memory (LSTM)\\ncells, which were introduced in a 1997 paper titled,\\nappropriately enough, “Long Short-Term Memory”.\\nLSTM cells are miniature neural networks in their own right.\\nAs the cells loop over the words in a sequence, they learn\\n(just as the weights connecting neurons in dense layers are\\nlearned) which words are important and lend that information\\nprecedence in subsequent iterations through the loop. A dense\\nlayer doesn’t recognize that there’s a connection between\\nblue and sky in the phrase “I like blue, for on a clear and\\nsunny day, it is the color of the sky.” An LSTM layer does\\nand can factor that into its output. Its power diminishes,\\nhowever, as words grow farther apart.\\nKeras provides a handy implementation of LSTM layers in its\\nLSTM class. It also provides a GRU class implementing gated\\nrecurrent unit (GRU) layers, which are simplified LSTM layers\\nthat train faster and often yield results that equal or\\nexceed those of LSTM layers. Here’s how our spam classifier\\nwould look if it were modified to use LSTM:\\n\\nfrom\\ntensorflow.keras.models\\nimport\\nSequential\\nfrom\\ntensorflow.keras.layers\\nimport\\nTextVectorization,\\nInputLayer\\nfrom\\ntensorflow.keras.layers\\nimport\\nDense,\\nEmbedding,\\nLSTM\\nimport\\ntensorflow\\nas\\ntf\\n\\nmodel\\nSequential()\\nmodel.add(InputLayer(input_shape=(1,),\\ndtype=tf.string))\\nmodel.add(TextVectorization(max_tokens=max_words,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 output_sequence_length=max_length))\\nmodel.add(Embedding(max_words,\\ninput_length=max_length))'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 509, 'file_type': 'pdf'}, page_content=\"model.add(LSTM(32))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nmodel.compile(loss='binary_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nLSTMs are compute intensive, especially when dealing with\\nlong sequences. If you try this code, you’ll find that the\\nmodel takes much longer to train. You’ll also find that\\nvalidation accuracy improves little if at all. This goes back\\nto the fact that bag-of-words models, while simple, also tend\\nto be effective at classifying text.\\nLSTM layers can be stacked, just like dense layers. The trick\\nis to include a return_sequences=True attribute in all LSTM\\nlayers except the last so that the previous layers return all\\nthe vectors generated (all the hidden state) rather than just\\nthe final vector. Google Translate once used two stacks of\\nLSTMs eight layers deep to encode phrases in one language and\\ndecode them into another.\\nUsing Pretrained Models to Classify Text\\nIf training your own sentiment analysis model doesn’t yield\\nthe accuracy you require, you can always turn to a pretrained\\nmodel. Just as there are pretrained computer-vision models\\ntrained with millions of labeled images, there are pretrained\\nsentiment analysis models available that were trained with\\nmillions of labeled text samples. Keras doesn’t provide a\\nconvenient wrapper around these models as it does for\\npretrained CNNs, but they are relatively easy to consume\\nnonetheless.\\nMany such models are available from Hugging Face, an AI-\\nfocused company whose goal is to advance and democratize AI.\\nHugging Face originally concentrated on NLP models but has\\nsince expanded its library to include other types of models,\\nincluding image classification models and object detection\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 510, 'file_type': 'pdf'}, page_content=\"models. Currently, Hugging Face hosts more than 400 sentiment\\nanalysis models trained on different types of input ranging\\nfrom tweets to product reviews in a variety of languages. It\\neven offers models that analyze text for joy, anger,\\nsurprise, and other emotions.\\nAll of these models are free for you to use. Care to give it\\na try? Start by installing Hugging Face’s Transformers\\npackage in your Python environment (for example, pip install\\ntransformers). Then fire up a Jupyter notebook and use the\\nfollowing code to load Hugging Face’s default sentiment\\nanalysis model:\\n\\nfrom\\ntransformers\\nimport\\npipeline\\n\\nmodel\\npipeline('sentiment-analysis')\\nYou’ll incur a short delay while the model is downloaded for\\nthe first time. Once the download completes, score a sentence\\nfor sentiment:\\n\\nmodel('The long lines and poor customer service really turned me off')\\nHere’s the result:\\n\\n[{'label': 'NEGATIVE', 'score': 0.9995430707931519}]\\nTry it with a positive comment:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 511, 'file_type': 'pdf'}, page_content=\"model('Great food and excellent service!')\\nHere’s the result:\\n\\n[{'label': 'POSITIVE', 'score': 0.9998843669891357}]\\nIn the return value, label indicates whether the sentiment is\\npositive or negative, and score reveals the model’s\\nconfidence in the label.\\nIt’s just as easy to analyze a text string for emotion by\\nloading a different pretrained model. To demonstrate, try\\nthis:\\n\\nmodel\\npipeline('text-classification',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 model='bhadresh-savani/distilbert-base-uncased-emotion',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return_all_scores=True)\\n\\nmodel('The long lines and poor customer service really turned me off')\\nHere’s what it returned for me:\\n\\n[[{'label': 'sadness', 'score': 0.10837080329656601},\\n\\xa0 {'label': 'joy', 'score': 0.002373947761952877},\\n\\xa0 {'label': 'love', 'score': 0.0006029471987858415},\\n\\xa0 {'label': 'anger', 'score': 0.8861245512962341},\\n\\xa0 {'label': 'fear', 'score': 0.0019340706057846546},\\n\\xa0 {'label': 'surprise', 'score': 0.0005936296074651182}]]\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 512, 'file_type': 'pdf'}, page_content='It doesn’t get much easier than that! And as you’ll see in\\na moment, pretrained Hugging Face models lend themselves to\\nmuch more than just text classification.\\nNeural Machine Translation\\nIn the universe of natural language processing, text\\nclassification is a relatively simple task. At the opposite\\nend of the spectrum lies neural machine translation (NMT),\\nwhich uses deep learning to translate text from one language\\nto another. NMT has proven superior to the rules-based\\nmachine translation (RBMT) and statistical machine\\ntranslation (SMT) systems that predated the explosion of deep\\nlearning and today is the basis for virtually all state-of-\\nthe-art text translation services.\\nThe gist of text classification is that you transform an\\ninput sequence into a vector characterizing the sequence, and\\nthen you input the vector to a classifier. There are several\\nways to generate that vector. You can reshape the 2D output\\nfrom an embedding layer into a 1D vector, or you can feed\\nthat output into a recurrent layer or convolution layer in\\nhopes of generating a vector that is more context aware.\\nWhichever route you choose, the goal is simple: convert a\\nstring of text into an array of floating-point numbers that\\nuniquely describes it and use a sigmoid or softmax output\\nlayer to classify it.\\nNMT is basically an extension of text classification. You\\nstart by converting a text sequence into a vector. But rather\\nthan classify the vector, you use it to generate a new\\nsequence. One way to do that is with an LSTM encoder-decoder.\\nLSTM Encoder-Decoders\\nUntil a few years ago, most NMT models, including the one\\nunderlying Google Translate, were LSTM-based sequence-to-'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 513, 'file_type': 'pdf'}, page_content='sequence models similar to the one in Figure\\xa013-3. In such\\nmodels, one or more LSTM layers encode a tokenized input\\nsequence representing the phrase to be translated into a\\nvector. A second set of recurrent layers uses that vector as\\ninput and decodes it into a tokenized phrase in another\\nlanguage. The model accepts sequences as input and returns\\nsequences as output, hence the term sequence-to-sequence\\nmodel. A softmax output layer at the end outputs a set of\\nprobabilities for each token in the output sequence. If the\\nmaximum output phrase length that’s supported is 20 tokens,\\nfor example, and the vocabulary of the output language\\ncontains 20,000 words, then the output is 20 sets (one per\\ntoken) of 20,000 probabilities. The word selected for each\\noutput token is the word assigned the highest probability.\\nFigure 13-3. LSTM-based encoder-decoder for neural machine translation\\nLSTM-based sequence-to-sequence models are relatively easy to\\nbuild with Keras and TensorFlow. This book’s GitHub repo\\ncontains a notebook that uses 50,000 samples to train an LSTM\\nmodel to translate English to French. The model is defined\\nthis way:\\n\\nmodel\\nSequential()\\nmodel.add(Embedding(en_vocab_size,\\ninput_length=en_max_len,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 514, 'file_type': 'pdf'}, page_content=\"mask_zero=True))\\nmodel.add(LSTM(256))\\nmodel.add(RepeatVector(fr_max_len))\\nmodel.add(LSTM(256,\\nreturn_sequences=True))\\nmodel.add(Dropout(0.4))\\nmodel.add(TimeDistributed(Dense(fr_vocab_size,\\nactivation='softmax')))\\nmodel.compile(loss='sparse_categorical_crossentropy',\\noptimizer='adam',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])model.summary(line_length=100)\\nThe first layer is an embedding layer that converts word\\ntokens into vectors containing 256 floating-point values. The\\nmask_zero=True parameter indicates that zeros in the input\\nsequences denote padding so that the next layer can ignore\\nthem. (There is no need to translate those tokens, after\\nall.) Next is an LSTM layer that encodes English phrases\\ninput to the model. A second LSTM layer decodes the phrases\\ninto dense vectors representing the French equivalents. In\\nbetween lies a RepeatVector layer that reshapes the output\\nfrom the first LSTM layer for input to the second by\\nrepeating the output a specified number of times. The final\\nlayer is a softmax classification layer that outputs\\nprobabilities for each word in the French vocabulary. The\\nTimeDistributed wrapper ensures that the model outputs a set\\nof probabilities for each token in the output rather than\\njust one set for the entire sequence.\\nAfter 34 epochs of training, the model translates 10 test\\nphrases this way:\\n\\nits fall now => cest maintenant maintenant\\nim losing => je suis en train\\nit was quite funny => cetait fut amusant amusant\\nthats not unusual => ce nest pas inhabituel\\ni think ill do that => je pense que je le\\ntom looks different => tom a lair different\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 515, 'file_type': 'pdf'}, page_content='its worth a try => ca vaut le coup\\nfortune smiled on him => la la lui a souri\\nlets hit the road => taillons la\\ni love winning => jadore gagner\\nThe model isn’t perfect, in part due to the limited size of\\nthe training set. Real NMT models are trained with hundreds\\nof millions or even billions of phrases. But is this one\\ntruly representative of the models used for state-of-the-art\\ntext translation? Figure\\xa013-4 is adapted from an image in a\\n2016 paper written by Google engineers documenting the\\narchitecture of Google Translate. The architecture maps\\nclosely to that of the model just presented. It’s deeper,\\nwith eight LSTM layers in the encoder and eight in the\\ndecoder. It also employs residual connections between layers\\nto support the network’s greater depth.\\nFigure 13-4. Google Translate circa 2016\\nOne difference between our model and the model described in\\nthe paper is the block labeled “Attention” between the'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 516, 'file_type': 'pdf'}, page_content='encoder and decoder. In deep learning, attention is a\\nmechanism for focusing a model’s attention on the parts of a\\nphrase that are most important, recognizing that one word can\\nhave different meanings in different contexts and that the\\nmeaning of a word sometimes depends on what’s around it.\\nAttention was introduced to deep learning in a seminal 2014\\npaper titled “Neural Machine Translation by Jointly Learning\\nto Align and Translate”, but it wasn’t until a few years\\nlater that attention took center stage as a way to replace\\nLSTM layers rather than supplement them. Enter perhaps the\\nmost significant contribution to the field of NMT, and to NLP\\noverall, to date: the transformer model.\\nTransformer Encoder-Decoders\\nA landmark 2017 paper titled “Attention Is All You Need”\\nchanged the way data scientists approach NMT and other neural\\ntext processing tasks. It proposed a better way to perform\\nsequence-to-sequence processing based on transformer models\\nthat eschew recurrent layers and use attention mechanisms to\\nmodel the context in which words are used. Today transformer\\nmodels have almost entirely replaced LSTM-based models.\\nFigure\\xa013-5 is adapted from an image in the aforementioned\\npaper. On the left is the encoder, which takes text sequences\\nas input and generates dense vector representations of those\\nsequences. On the right is the decoder, which transforms\\ndense vector representations of input sequences into output\\nsequences. At a high level, a transformer model uses the same\\nencoder-decoder architecture as an LSTM-based model. The\\ndifference lies in how it does the encoding and decoding.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 517, 'file_type': 'pdf'}, page_content='Positional\\nencoding\\n\\nMulti-head\\nattention\\n\\nOutput probabilities\\n\\nLinear\\n\\nMulti-head\\nattention\\n\\nMasked\\n\\nattention\\n\\nmulti-head\\n\\nNx\\n\\nPositional\\nencoding\\n\\nOutput\\nembedding.\\nInputs Outputs (shifted right)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 518, 'file_type': 'pdf'}, page_content='The chief innovation introduced by the transformer model is\\nthe use of multi-head attention (MHA) layers in place of LSTM\\nlayers. MHA layers embody the concept of self-attention,\\nwhich enables a model to analyze an input sequence and focus\\non the words that are most important as well as the context\\nin which the words are used. In the sentence “We took a walk\\nin the park,” for example, the word park has a different\\nmeaning than it does in “Where did you park the car?” An\\nembedding layer stores one vector representation for park,\\nbut in a transformer model, the MHA layer modifies the vector\\noutput by the embedding layer so that park is represented by\\ntwo different vectors in the two sentences. Not surprisingly,\\nthe values used to make embedding vectors context aware are\\nlearned during training.\\nNOTE\\nHow does self-attention work? The gist is that an MHA layer uses\\ndot products to compute similarity scores for every word pair in a\\nsequence. After normalizing the scores, it uses them to compute\\nweighted versions of each word embedding in the sequence. Then it\\nmodifies them again using weights learned during training.\\nThe “multi” in multi-head attention denotes the fact that an MHA\\nlayer learns several sets of weights rather than just one, not\\nunlike a convolution layer in a CNN. This gives MHA the ability to\\ndiscern context in long sequences where words might have multiple\\nrelationships to one another.\\nMHA also provides additional context regarding words that\\nrefer to other words. An embedding layer represents the\\npronoun it with a single vector, but an MHA layer helps the\\nmodel understand that in the sentence “I love my car because\\nit is fast,” it refers to a car. It also adds weight to the'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 519, 'file_type': 'pdf'}, page_content='word fast because it’s crucial to the meaning of the\\nsentence. Without it, it’s not clear why you love your car.\\nUnlike LSTM layers, MHA layers’ ability to model\\nrelationships among the words in an input sequence is\\nindependent of sequence length. MHA layers also support\\nparallel workloads and therefore train faster on multiple\\nGPUs. They do not, however, encode information regarding the\\npositions of the words in a phrase. To compensate, a\\ntransformer uses simple vector addition to add information\\ndenoting a word’s position in a sequence to each vector\\noutput from the embedding layer. This is referred to as\\npositional encoding or positional embedding. It’s denoted by\\nthe plus signs labeled “Positional encoding” in Figure\\xa013-\\nTransformers aren’t limited to neural machine translation;\\nthey’re used in virtually all aspects of NLP today. The\\nencoder half of a transformer outputs dense vector\\nrepresentations of the sequences input to it. Text\\nclassification can be performed by using a transformer rather\\nthan a standalone embedding layer to encode input sequences.\\nModels architected this way frequently outperform bag-of-\\nwords models, particularly if the ratio of samples to sample\\nlength (the number of training samples divided by the average\\nlength of each sample) exceeds 1,500. This so-called “golden\\nconstant” was discovered by a team of researchers at Google\\nand documented in a tutorial on text classification.\\nBuilding a Transformer-Based NMT Model\\nKeras provides some, but not all, of the building blocks that\\ncomprise an end-to-end transformer. It provides a handy\\nimplementation of self-attention layers in its\\nMultiHeadAttention class, for example, but it doesn’t\\nimplement positional embedding. However, a separate package\\nnamed KerasNLP does. Among others, it includes the following\\nclasses representing layers in a transformer-based network:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 520, 'file_type': 'pdf'}, page_content=\"TransformerEncoder\\nRepresents a transformer encoder\\nTransformerDecoder\\nRepresents a transformer decoder\\nTokenAndPositionEmbedding\\nImplements an embedding layer that supports\\npositional embedding\\nWith these classes, transformer-based NMT models are\\nrelatively easy to build. You can demonstrate by building an\\nEnglish-to-French translator. Start by installing KerasNLP if\\nit isn’t already installed. Then download en-fr.txt, a data\\nfile that contains 50,000 English phrases and their French\\nequivalents, and drop it into the Data subdirectory where\\nyour Jupyter notebooks are hosted. The file en-fr.txt is a\\nsubset of a larger file containing more than 190,000 phrases\\nand their corresponding translations compiled as part of the\\nTatoeba project. The file is tab delimited. Each line\\ncontains an English phrase, the equivalent French phrase, and\\nan attribution identifying where the translation came from.\\nWe don’t need the attributions, so load the dataset into a\\nDataFrame, remove the attribution column, and shuffle and\\nreindex the rows:\\n\\nimport\\npandas\\nas\\npd\\n\\ndf\\npd.read_csv('Data/en-fr.txt',\\nnames=['en',\\n'fr',\\n'attr'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 usecols=['en',\\n'fr'],\\nsep='\\\\t')\\ndf\\ndf.sample(frac=1,\\nrandom_state=42)\\ndf\\ndf.reset_index(drop=True)\\ndf.head()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 521, 'file_type': 'pdf'}, page_content=\"Here’s the output:\\nThe dataset needs to be cleaned before it’s used to train a\\nmodel. Use the following statements to remove numbers and\\npunctuation symbols, convert words with Unicode characters\\nsuch as où into their ASCII equivalents (ou), convert\\ncharacters to lowercase, and insert [start] and [end] tokens\\nat the beginning and end of each French phrase:\\n\\nimport\\nre\\nfrom\\nunicodedata\\nimport\\nnormalize\\n\\ndef\\nclean_text(text):\\n\\xa0\\xa0\\xa0 text\\nnormalize('NFD',\\ntext.lower())\\n\\xa0\\xa0\\xa0 text\\nre.sub('[^A-Za-z ]+',\\ntext)\\n\\xa0\\xa0\\xa0 return\\ntext\\n\\ndef\\nclean_and_prepare_text(text):\\n\\xa0\\xa0\\xa0 text\\n'[start] '\\nclean_text(text)\\n' [end]'\\n\\xa0\\xa0\\xa0 return\\ntext\\n\\ndf['en']\\ndf['en'].apply(lambda\\nrow:\\nclean_text(row))\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 522, 'file_type': 'pdf'}, page_content=\"df['fr']\\ndf['fr'].apply(lambda\\nrow:\\nclean_and_prepare_text(row))\\ndf.head()\\nThe output looks a little cleaner afterward:\\nThe next step is to scan the dataset and determine the\\nmaximum length of the English phrases and of the French\\nphrases. These lengths will determine the lengths of the\\nsequences input to and output from the model:\\n\\nen\\ndf['en']\\nfr\\ndf['fr']\\n\\nen_max_len\\nmax(len(line.split())\\nfor\\nline\\nin\\nen)\\nfr_max_len\\nmax(len(line.split())\\nfor\\nline\\nin\\nfr)\\nsequence_len\\nmax(en_max_len,\\nfr_max_len)\\n\\nprint(f'Max phrase length (English): {en_max_len}')\\nprint(f'Max phrase length (French): {fr_max_len}')\\nprint(f'Sequence length: {sequence_len}')\\nIn this example, the longest English phrase contains seven\\nwords, while the longest French phrase contains 16 (including\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 523, 'file_type': 'pdf'}, page_content='the [start] and [end] tokens). The model will be able to\\ntranslate English phrases up to seven words in length into\\nFrench phrases up to 14 words in length.\\nNow fit one Tokenizer to the English phrases and another\\nTokenizer to their French equivalents, and generate padded\\nsequences from all the phrases. Note the filters parameter\\npassed to the French tokenizer. It configures the tokenizer\\nto remove all the punctuation characters it normally removes\\nexcept for the square brackets used to delimit [start] and\\n[end] tokens:\\n\\nfrom\\ntensorflow.keras.preprocessing.text\\nimport\\nTokenizer\\nfrom\\ntensorflow.keras.preprocessing.sequence\\nimport\\npad_sequences\\n\\nen_tokenizer\\nTokenizer()\\nen_tokenizer.fit_on_texts(en)\\nen_sequences\\nen_tokenizer.texts_to_sequences(en)\\nen_x\\npad_sequences(en_sequences,\\nmaxlen=sequence_len,\\npadding=\\'post\\')\\n\\nfr_tokenizer\\nTokenizer(filters=\\'!\"#$%&()*+,-./:;<=>?@\\\\\\\\^_`{|}~\\\\t\\\\n\\')\\nfr_tokenizer.fit_on_texts(fr)\\nfr_sequences\\nfr_tokenizer.texts_to_sequences(fr)\\nfr_y\\npad_sequences(fr_sequences,\\nmaxlen=sequence_len\\npadding=\\'post\\')\\nNext, compute the vocabulary size for each language from the\\nTokenizer instances:\\n\\nen_vocab_size\\nlen(en_tokenizer.word_index)\\nfr_vocab_size\\nlen(fr_tokenizer.word_index)\\n\\nprint(f\\'Vocabulary size (English): {en_vocab_size}\\')\\nprint(f\\'Vocabulary size (French): {fr_vocab_size}\\')'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 524, 'file_type': 'pdf'}, page_content=\"The output reveals that the English vocabulary contains 6,033\\nwords, while the French vocabulary contains 12,197. These\\nvalues will be used to size the model’s two embedding\\nlayers. The latter will also be used to size the output\\nlayer.\\nFinally, create the features and the labels the model will be\\ntrained with. The features are the padded English sequences\\nand the padded French sequences minus the [end] tokens. The\\nlabels are the padded French sequences minus the [start]\\ntokens. Package the features in a dictionary so that they can\\nbe input to a model that accepts multiple inputs:\\n\\ninputs\\n'encoder_input':\\nen_x,\\n'decoder_input':\\nfr_y[:,\\noutputs\\nfr_y[:,\\nNow let’s define a model. This time, we’ll use Keras’s\\nfunctional API rather than its sequential API. It’s\\nnecessary because this model has two inputs: one that accepts\\na tokenized English phrase and another that accepts a\\ntoke\\u2060nized French phrase. We’ll also seed the random-number\\ngenerators used by Keras and TensorFlow to get repeatable\\nresults, at least on CPU. This is a departure from all the\\nother examples in this book, but it ensures that when you\\ntrain the model, you get the same results that I did. Here’s\\nthe code:\\n\\nimport\\nnumpy\\nas\\nnp\\nimport\\ntensorflow\\nas\\ntf\\nfrom\\ntensorflow.keras\\nimport\\nModel\\nfrom\\ntensorflow.keras.layers\\nimport\\nInput,\\nDense,\\nDropout\\nfrom\\nkeras_nlp.layers\\nimport\\nTokenAndPositionEmbedding,\\nTransformerEncoder\\nfrom\\nkeras_nlp.layers\\nimport\\nTransformerDecoder\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 525, 'file_type': 'pdf'}, page_content=\"np.random.seed(42)\\ntf.random.set_seed(42)\\n\\nnum_heads\\nembed_dim\\n\\nencoder_input\\nInput(shape=(None,),\\ndtype='int64',\\nname='encoder_input')\\nx\\nTokenAndPositionEmbedding(en_vocab_size,\\nsequence_len,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 embed_dim)(encoder_input)\\nencoder_output\\nTransformerEncoder(embed_dim,\\nnum_heads)(x)\\nencoded_seq_input\\nInput(shape=(None,\\nembed_dim))\\n\\ndecoder_input\\nInput(shape=(None,),\\ndtype='int64',\\nname='decoder_input')\\nx\\nTokenAndPositionEmbedding(fr_vocab_size,\\nsequence_len,\\nembed_dim,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 mask_zero=True)(decoder_input)\\nx\\nTransformerDecoder(embed_dim,\\nnum_heads)(x,\\nencoded_seq_input)\\nx\\nDropout(0.4)(x)\\n\\ndecoder_output\\nDense(fr_vocab_size,\\nactivation='softmax')(x)\\ndecoder\\nModel([decoder_input,\\nencoded_seq_input],\\ndecoder_output)\\ndecoder_output\\ndecoder([decoder_input,\\nencoder_output])\\n\\nmodel\\nModel([encoder_input,\\ndecoder_input],\\ndecoder_output)\\nmodel.compile(optimizer='adam',\\nloss='sparse_categorical_crossentropy',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metrics=['accuracy'])\\nmodel.summary(line_length=100)\\nTHE KERAS FUNCTIONAL API\\nThe functional API is a richer version of Keras’s\\nsequential API. Among other things, it lets you create\\nmodels with multiple inputs or outputs and models with\\nshared layers like the ones in Faster R-CNN’s region\\nproposal network. Here’s a simple binary classifier\\ndefined with the sequential API:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 526, 'file_type': 'pdf'}, page_content=\"model\\nSequential()\\nmodel.add(Dense(128,\\nactivation='relu',\\ninput_dim=3))\\nmodel.add(Dense(1,\\nactivation='sigmoid'))\\nHere’s the same network created with the functional API:\\n\\ninput\\nInput(shape=(3,))\\nhidden\\nDense(128,\\nactivation='relu')(input)\\noutput\\nDense(1,\\nactivation='sigmoid')(hidden)\\nmodel\\nModel(inputs=input,\\noutputs=output)\\nWhen you create the model, you specify the inputs and\\noutputs, and since the inputs and outputs parameters\\naccept Python lists, it’s a simple matter to create a\\nmodel with multiple inputs and outputs. For a concise\\nintroduction to the functional API and examples\\ndemonstrating advanced uses, including multiple inputs\\nand outputs and shared layers, see “How to Use the Keras\\nFunctional API for Deep Learning” by Jason Brownlee.\\nThe model is designed to operate iteratively. To translate\\ntext, you first pass an English phrase to the English input\\nand the word “[start]” to the French input. Then you append\\nthe next French word the model predicts to the previous\\nFrench input and call the model again, and you repeat this\\nprocess until the entire phrase has been translated—that is,\\nuntil the next word predicted by the model is “[end].”\\nFigure\\xa013-6 diagrams the model’s architecture. The model\\nincludes two embedding layers: one for English sequences and\\none for French sequences. Both convert word tokens into\\nvectors of 256 floating point values each, and both are\\ninstances of KerasNLP’s TokenAndPositionEmbedding class,\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 527, 'file_type': 'pdf'}, page_content=\"which adds positional information to word embeddings. Output\\nfrom the English embedding layer passes through an encoder\\n(an instance of TransformerEncoder) before being input along\\nwith the output from the French embedding layer to the\\ndecoder, which is an instance of TransformerDecoder. The\\ndecoder outputs a vector representing the next step in the\\ntranslation, and a softmax output layer converts that vector\\ninto a set of probabilities—one for each word in the French\\nvocabulary—identifying the next token. During training, the\\nmask_zero=True parameter passed to the French embedding layer\\nlimits the model to making predictions based on the tokens\\npreceding the one that’s being predicted. In other words,\\ngiven a set of French tokens numbered 0 through n, the model\\nis trained to predict what token n + 1 will be without\\npeeking at n + 1 in the training text.\\nFigure 13-6. Transformer-based NMT architecture\\nNow call fit to train the model, and use an EarlyStopping\\ncallback to end training if the validation accuracy fails to\\nimprove for three consecutive epochs:\\n\\nfrom\\ntensorflow.keras.callbacks\\nimport\\nEarlyStopping\\n\\ncallback\\nEarlyStopping(monitor='val_accuracy',\\npatience=3,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 restore_best_weights=True)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 528, 'file_type': 'pdf'}, page_content=\"hist\\nmodel.fit(inputs,\\noutputs,\\nepochs=50,\\nvalidation_split=0.2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 callbacks=[callback])\\nTraining typically requires two to three minutes per epoch on\\nCPU. If you don’t have a GPU and training is too slow, I\\nrecommend running the code in Google Colab. (Be sure to go to\\n“Notebook settings” in the Edit menu and select GPU as the\\nhardware accelerator.) When training is complete, plot the\\nper-epoch training and validation accuracy and observe how\\nthe latter steadily increases until it levels off:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\\n\\nplt.plot(epochs,\\nacc,\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nThe output should look like this:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 529, 'file_type': 'pdf'}, page_content='The plot reveals that at the end of 14 epochs, the model was\\nabout 75% accurate in translating English samples in the\\nvalidation data to French. This isn’t a very robust measure\\nof accuracy because it literally compares each word in the\\npredicted text to each word in the target text and ignores\\nthe fact that a missing or misplaced article such as le\\n(French for the) doesn’t necessarily imply a poor\\ntranslation. The accuracy of NMT models is typically measured\\nwith bilingual evaluation understudy (BLEU) scores. BLEU\\nscores are rather easily computed after the training is\\ncomplete using packages such as NLTK, but during training,\\nvalidation accuracy is a reasonable metric for judging when\\nto halt training.\\nCan the model really translate English to French? Use the\\nfollowing code to define a function that accepts an English\\nphrase and returns a French phrase. Then call it on 10 of the\\nphrases used to validate the model during training and see'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 530, 'file_type': 'pdf'}, page_content=\"for yourself. Note that one call to translate_text\\nprecipitates multiple calls to the model. To translate\\n“hello world,” for example, translate_text calls the model\\nwith the inputs “hello world” and “[start].” Assuming the\\nmodel predicts that salut is the next word, translate_text\\ninvokes it again with the inputs “hello world” and\\n“[start] salut.” It repeats this cycle until the next word\\npredicted by the model is “[end]” denoting the end of the\\ntranslation.\\n\\ndef\\ntranslate_text(text,\\nmodel,\\nen_tokenizer,\\nfr_tokenizer,\\nfr_index_lookup,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 sequence_len):\\n\\xa0\\xa0\\xa0 input_sequence\\nen_tokenizer.texts_to_sequences([text])\\n\\xa0\\xa0\\xa0 padded_input_sequence\\npad_sequences(input_sequence,\\nmaxlen=sequence_len,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 padding='post')\\n\\xa0\\xa0\\xa0 decoded_text\\n'[start]'\\n\\n\\xa0\\xa0\\xa0 for\\ni\\nin\\nrange(sequence_len):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_sequence\\nfr_tokenizer.texts_to_sequences([decoded_text])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 padded_target_sequence\\npad_sequences(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 target_sequence,\\nmaxlen=sequence_len,\\npadding='post')[:,\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 prediction\\nmodel([padded_input_sequence,\\npadded_target_sequence])\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 idx\\nnp.argmax(prediction[0,\\ni,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 token\\nfr_index_lookup[idx]\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 decoded_text\\ntoken\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\ntoken\\n'[end]':\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 break\\n\\n\\xa0\\xa0\\xa0 return\\ndecoded_text[8:-6]\\n# Remove [start] and [end] tokens\\n\\nfr_vocab\\nfr_tokenizer.word_index\\nfr_index_lookup\\ndict(zip(range(len(fr_vocab)),\\nfr_vocab))\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 531, 'file_type': 'pdf'}, page_content=\"texts\\nen[40000:40010].values\\n\\nfor\\ntext\\nin\\ntexts:\\n\\xa0\\xa0\\xa0 translated\\ntranslate_text(text,\\nmodel,\\nen_tokenizer,\\nfr_tokenizer,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 fr_index_lookup,\\nsequence_len)\\n\\xa0\\xa0\\xa0 print(f'{text} => {translated}')\\nHere’s the output:\\n\\nits fall now => cest desormais tombe\\nim losing => je suis en train de perdre\\nit was quite funny => ce fut assez amusant\\nthats not unusual => ce nest pas inhabituel\\ni think ill do that => je pense que je ferai ca\\ntom looks different => tom a lair different\\nits worth a try => ca vaut le coup dessayer\\nfortune smiled on him => la chance lui souri\\nlets hit the road => cassonsnous\\ni love winning => jadore gagner\\nIf you don’t speak French, use Google Translate to translate\\nsome of the French phrases to English. According to Google,\\nfor example, “la chance lui souri” translates to “Luck\\nsmiled on him,” while “ce nest pas inhabituel” translates\\nto “it’s not unusual.” The model isn’t perfect, but it’s\\nnot bad, either. The vocabulary you used is small, so you\\ncan’t input just any old phrase and expect the model to\\ntranslate it. But simple phrases that use words in the\\ntraining text translate reasonably well.\\nFinish up by using the translate_text function to see how the\\nmodel translates “Hello world” into French:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 532, 'file_type': 'pdf'}, page_content=\"translate_text('Hello world',\\nmodel,\\nen_tokenizer,\\nfr_tokenizer,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 fr_index_lookup,\\nsequence_len)\\nI haven’t had French lessons since high school, but even I\\nknow that “Salut le monde” is a reasonable translation of\\n“Hello world.”\\nUsing Pretrained Models to Translate Text\\nEngineers at Microsoft, Google, and Facebook have the\\nresources to collect millions of text translation samples and\\nthe hardware to train sophisticated transformer models on\\nthem, but you and I do not. The good news is that this\\nneedn’t stop us from writing software that translates text\\nfrom one language to another. Hugging Face has published\\nseveral pretrained transformer models that do a fine job of\\ntext translation. Leveraging those models in Python is\\nsimplicity itself.\\nHere’s an example that translates English to French:\\n\\nfrom\\ntransformers\\nimport\\npipeline\\n\\ntranslator\\npipeline('translation_en_to_fr')\\ntranslation\\ntranslator('Programming is fun!')\\n[0]['translation_text']\\nprint(translation)\\nThe same syntax can be used to translate English to German\\nand English to Romanian too. Simply replace\\ntranslation_en_to_fr with translation_en_to_de or\\ntranslation_en_to_ro when creating the pipeline.\\nFor other languages, you use a slightly more verbose syntax\\nto load a transformer and a corresponding tokenizer. The\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 533, 'file_type': 'pdf'}, page_content=\"following example translates Dutch to English:\\n\\nfrom\\ntransformers\\nimport\\nAutoTokenizer,\\nAutoModelForSeq2SeqLM\\n\\n# Initialize the tokenizer\\ntokenizer\\nAutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-nl-en')\\n\\n# Initialize the model\\nmodel\\nAutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-nl-en')\\n\\n# Tokenize the input text\\ntext\\n'Hallo vrienden, hoe gaat het vandaag?'\\ntokenized_text\\ntokenizer.prepare_seq2seq_batch([text],\\nreturn_tensors='pt')\\n\\n# Perform translation and decode the output\\ntranslation\\nmodel.generate(**tokenized_text)\\ntranslated_text\\ntokenizer.batch_decode(\\n\\xa0\\xa0\\xa0 translation,\\nskip_special_tokens=True)[0]\\nprint(translated_text)\\nYou’ll find an exhaustive list of Hugging Face translators\\nand tokenizers on the organization’s website. There are\\nhundreds of them covering dozens of languages.\\nBidirectional Encoder Representations from\\nTransformers (BERT)\\nThe introduction of transformers in 2017 laid the groundwork\\nfor another landmark innovation in the NLP space:\\nBidirectional Encoder Representations from Transformers, or\\nBERT for short. Introduced by Google researchers in a 2018\\npaper titled “BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding”, BERT advanced the\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 534, 'file_type': 'pdf'}, page_content='state of the art by providing pretrained transformers that\\ncan be fine-tuned for a variety of NLP tasks.\\nBERT was instilled with language understanding by training it\\nwith more than 2.5 billion words from Wikipedia articles and\\n800 million words from Google Books. Training required four\\ndays on 64 tensor processing units (TPUs). Fine-tuning is\\naccomplished by further training the pretrained model with\\ntask-specific samples and a reduced learning rate\\n(Figure\\xa013-7). It’s a relatively simple matter, for\\nexample, to fine-tune BERT to perform sentiment analysis and\\noutscore bag-of-words models for accuracy. BERT’s value lies\\nin the fact that it possesses an innate understanding of the\\nlanguages it was trained with and can be refined to perform\\ndomain-specific tasks.\\nAside from the fact that it was trained with a huge volume of\\nsamples, the key to BERT’s ability to understand human\\nlanguages is an innovation known as masked language modeling,\\nor MLM for short. The big idea behind MLM is that a model has\\na better chance of predicting what word should fill in the\\nblank in the phrase “Every good ____ does fine” than it has\\nat predicting the next word in the phrase “Every good\\n____.” The answer could be boy, as in “Every good boy does\\nfine,” or it could be turn, as in “Every good turn deserves\\nanother.” Unidirectional models look at the text to the left\\nor the text to the right and attempt to predict what the next\\nword should be. MLM, on the other hand, uses text on the left\\nand right to inform its decisions. That’s why BERT is a\\n“bidirectional” transformer.\\nWhen BERT models are pretrained, a specified percentage of\\nthe word tokens in each sequence—usually 15%—are randomly\\nremoved or “masked” so that the model can learn to predict\\nthem from the words around them. In addition, BERT models are\\nusually pretrained to do next-sentence prediction, which\\nmakes them more adept at certain NLP tasks such as answering\\nquestions.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 535, 'file_type': 'pdf'}, page_content='BERT has been called the “Swiss Army knife” of NLP. Google\\nuses it to improve search results and predict text as you\\ntype into a Gmail or Google Doc. Dozens of variations have\\nbeen published, including DistilBERT, which retains 97% of\\nthe accuracy of the original model while weighing in 40%\\nsmaller and running 60% faster. Also available are variations\\nof BERT already fine-tuned for specific tasks such as\\nquestion answering. Such models can be further refined using\\ndomain-specific datasets, or they can be used as is.\\nFigure 13-7. Bidirectional Encoder Representations from Transformers (BERT)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 536, 'file_type': 'pdf'}, page_content='Building a BERT-Based Question Answering\\nSystem\\nHugging Face’s transformers package contains several\\npretrained BERT models already fine-tuned for specific tasks.\\nOne example is the minilm-uncased-squad2 model, which was\\ntrained with Stanford’s SQuAD 2.0 dataset to answer\\nquestions by extracting text from documents. To get a feel\\nfor what models like this one can accomplish, let’s use it\\nto build a simple question-answering system.\\nFirst, some context. When you ask Google a question like the\\none in Figure\\xa013-8, it queries a database containing\\nbillions of web pages to identify ones that might contain an\\nanswer. Then it uses a BERT-based NLP model to extract\\nanswers from the pages it identified and rank them based on\\nconfidence levels.\\nFigure 13-8. Google question answering\\nLet’s load a pretrained BERT model already fine-tuned for\\nquestion answering and use it to extract answers from\\npassages of text in this book. The model we’ll use is a\\nversion of the MiniLM model introduced in a 2020 paper titled\\n“MiniLM: Deep Self-Attention Distillation for Task-Agnostic'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 537, 'file_type': 'pdf'}, page_content='Compression of Pre-trained Transformers”. This version was\\nfine-tuned on SQuAD 2.0, which contains more than 100,000\\nquestions generated by humans paired with answers culled from\\nWikipedia articles, plus 50,000 questions which lack answers.\\nThe MiniLM architecture enables reading comprehension gained\\nfrom one dataset to be applied to other datasets with little\\nor no retraining.\\nNOTE\\nMy eighth-grade history teacher, Mr. Aird, roomed with Charles\\nLindbergh one summer in the early 1920s. Apparently the world’s\\nmost famous aviator was something of a daredevil in college, and\\nI’ll never forget something Mr. Aird said about him. “In 1927,\\nwhen I learned that Charles had flown solo across the Atlantic, I\\nwasn’t surprised. I have never met a person with less regard for\\nhis own life than Charles Lindbergh.” 😊\\nBegin by creating a new Jupyter notebook and using the\\nfollowing statements to load a pretrained MiniLM model from\\nthe Hugging Face hub and a tokenizer to tokenize text input\\nto the model. (BERT uses a special tokenization format called\\nWordPiece that is slightly different from the one Keras’s\\nTokenizer class and TextVectorization layer use. Fortunately,\\nHugging Face has a solution for that too.) Then compose a\\npipeline from them. The first time you run this code, you’ll\\nexperience a momentary delay while the pretrained weights are\\ndownloaded. After that, the weights will be cached and\\nloading will be fast:\\n\\nfrom\\ntransformers\\nimport\\nAutoTokenizer,\\nTFAutoModelForQuestionAnswering,\\npipeline'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 538, 'file_type': 'pdf'}, page_content=\"id\\n'deepset/minilm-uncased-squad2'\\ntokenizer\\nAutoTokenizer.from_pretrained(id)\\nmodel\\nTFAutoModelForQuestionAnswering.from_pretrained(id,\\nfrom_pt=True)\\npipe\\npipeline('question-answering',\\nmodel=model,\\ntokenizer=tokenizer)\\nHugging Face stores weights for this particular model in\\nPyTorch format. The from_pt=True parameter converts the\\nweights to TensorFlow format. It’s not trivial to convert\\nneural network weights from one format to another, but the\\nHugging Face library reduces it to a simple function\\nparameter.\\nNow use the pipeline to answer a question by extracting text\\nfrom a paragraph:\\n\\nquestion\\n'What does NLP stand for?'\\n\\ncontext\\n'Natural Language Processing, or NLP, encompasses a variety of ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'activities, including text classification, keyword and topic ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'extraction, text summarization, and language translation. The ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'accuracy of NLP models has improved in recent years for a variety '\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'of reasons, not the least of which are newer and better ways of ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'converting words and sentences into dense vector representations ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'that incorporate context, and a relatively new neural network ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'architecture called the transformer that can zero in on the most ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'meaningful words and even differentiate between multiple meanings '\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'of the same word.'\\n\\npipe(question=question,\\ncontext=context)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 539, 'file_type': 'pdf'}, page_content=\"Is the answer accurate? A human could easily read the\\nparagraph and come up with the same answer, but the fact that\\na deep-learning model can do it indicates that the model\\ndisplays some level of reading comprehension. Observe that\\nthe output contains the answer to the question as well as a\\nconfidence score and the starting and ending indices of the\\nanswer in the paragraph:\\n\\n{'score': 0.9793193340301514,\\n'start': 0,\\n'end': 27,\\n'answer': 'Natural Language Processing'}\\nNow try it again with a different question and context:\\n\\nquestion\\n'When was TensorFlow released?'\\n\\ncontext\\n'Machine learning isn\\\\'t hard when you have a properly ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'engineered dataset to work with. The reason it\\\\'s not ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'hard is libraries such as Scikit-Learn and ML.NET, which ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'reduce complex learning algorithms to a few lines of code. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Deep learning isn\\\\'t difficult, either, thanks to libraries ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'such as the Microsoft Cognitive Toolkit (CNTK), Theano, and ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'PyTorch. But the library that most of the world has settled ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'on for building neural networks is TensorFlow, an open source ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'framework created by Google that was released under the ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Apache License 2.0 in 2015.'\\n\\npipe(question=question,\\ncontext=context)['answer']\\nThis time, the output is the answer provided by the model\\nrather than the dictionary containing the answer. Once again,\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 540, 'file_type': 'pdf'}, page_content=\"is the answer reasonable?\\nRepeat this process with another question and context from\\nwhich to extract an answer:\\n\\nquestion\\n'Is Keras part of TensorFlow?'\\n\\ncontext\\n'The learning curve for TensorFlow is rather steep. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Another library, named Keras, provides a simplified ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Python interface to TensorFlow and has emerged as the ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Scikit of deep learning. Keras is all about neural networks. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'It began life as a standalone project in 2015 but was ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'integrated into TensorFlow in 2019. Any code that you write ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'using TensorFlow\\\\'s built-in Keras module ultimately executes ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'in (and is optimized for) TensorFlow. Even Google recommends ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'using the Keras API.'\\n\\npipe(question=question,\\ncontext=context)['answer']\\nPerform one final test using the same context as before but a\\ndifferent question:\\n\\nquestion\\n'Is it better to use Keras or TensorFlow to build neural networks?'\\npipe(question=question,\\ncontext=context)['answer']\\nThe questions posed here were hand-selected to highlight the\\nmodel’s capabilities. It’s not difficult to come up with\\nquestions that the model can’t answer. Nevertheless, you\\nhave proven the principle that a pretrained BERT model fine-\\ntuned on SQuAD 2.0 can answer straightforward questions from\\npassages of text presented to it.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 541, 'file_type': 'pdf'}, page_content='Sophisticated question-answering systems employ a retriever-\\nreader architecture in which the retriever searches a data\\nstore for relevant documents—ones that might contain an\\nanswer to a question—and the reader extracts answers from\\nthe documents. The reader is often a BERT instance similar to\\nthe one shown earlier. The retriever may be one from the open\\nsource Haystack library published by Deepset, a German\\ncompany focused on NLP solutions. Haystack retrievers\\ninterface with a wide range of document stores including\\nElasticsearch stores, which are highly scala\\u2060ble. If you’d\\nlike to learn more or build a retriever-reader system of your\\nown, I recommend reading Chapter 7 of Natural Language\\nProcessing with Transformers by Lewis Tunstall, Leandro von\\nWerra, and Thomas Wolf (O’Reilly).\\nFine-Tuning BERT to Perform Sentiment\\nAnalysis\\nState-of-the-art sentiment analysis can be accomplished by\\nfine-tuning pretrained BERT models on sentiment analysis\\ndatasets. Let’s fine-tune BERT and see if we can create a\\nsentiment analysis model that’s more accurate than the bag-\\nof-words model presented earlier in this chapter. If your\\ncomputer isn’t equipped with a GPU, I highly recommend\\nrunning this example in Google Colab. Even on a GPU, it can\\ntake an hour or so to run.\\nIf you run this code locally, make sure Hugging Face’s\\nDatasets package is installed. Then create a new Jupyter\\nnotebook. If you use Colab instead, create a new notebook and\\nrun the following commands in the first cell to install the\\nnecessary packages in the Colab environment:\\n\\n!pip install transformers\\n!pip install datasets'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 542, 'file_type': 'pdf'}, page_content=\"Next, use the following statements to load the IMDB dataset\\nfrom the Datasets package. This is an alternative to loading\\nit from a CSV file. Since we’re using Hugging Face models,\\nwe may as well load the data from Hugging Face too. Plus, if\\nyou’re using Colab, this prevents you from having to upload\\na CSV to the Colab environment. Note that the dataset might\\ntake a few minutes to load the first time:\\n\\nfrom\\ndatasets\\nimport\\nload_dataset\\n\\nimdb\\nload_dataset('imdb')\\nimdb\\nThe value returned by load_dataset is a dictionary containing\\nthree Hugging Face datasets. Here’s the output from the\\nfinal statement:\\n\\nDatasetDict({\\n\\xa0\\xa0\\xa0 train:\\nDataset({\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 features:\\n['text',\\n'label'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_rows:\\n25000\\n\\xa0\\xa0\\xa0 test:\\nDataset({\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 features:\\n['text',\\n'label'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_rows:\\n25000\\n\\xa0\\xa0\\xa0 unsupervised:\\nDataset({\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 features:\\n['text',\\n'label'],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 num_rows:\\n50000\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 543, 'file_type': 'pdf'}, page_content=\"imdb['train'] contains 25,000 samples for training, while\\nimdb['test'] contains 25,000 samples for testing. Movie\\nreviews are stored in the text column of each dataset. Labels\\nare stored in the label column.\\nNext up is tokenizing the input using a BERT WordPiece\\ntokenizer:\\n\\nfrom\\ntransformers\\nimport\\nAutoTokenizer\\n\\ntokenizer\\nAutoTokenizer.from_pretrained('distilbert-base-uncased')\\n\\ndef\\ntokenize(samples):\\n\\xa0\\xa0\\xa0 return\\ntokenizer(samples['text'],\\ntruncation=True)\\n\\ntokenized_imdb\\nimdb.map(tokenize,\\nbatched=True)\\nNow that the reviews are tokenized, they need to be converted\\ninto TensorFlow datasets with Hugging Face’s\\nDataset.to_tf_dataset method. The collating function passed\\nto the method dynamically pads the sequences so that they’re\\nall the same length. You can also ask the tokenizer to do the\\npadding, but padding performed that way is static and\\nrequires more memory:\\n\\nfrom\\ntransformers\\nimport\\nDataCollatorWithPadding\\n\\ndata_collator\\nDataCollatorWithPadding(tokenizer=tokenizer,\\nreturn_tensors='tf')\\n\\ntrain_data\\ntokenized_imdb['train'].to_tf_dataset(\\n\\xa0\\xa0\\xa0 columns=['attention_mask',\\n'input_ids',\\n'label'],\\n\\xa0\\xa0\\xa0 shuffle=True,\\nbatch_size=16,\\ncollate_fn=data_collator\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 544, 'file_type': 'pdf'}, page_content=\"validation_data\\ntokenized_imdb['test'].to_tf_dataset(\\n\\xa0\\xa0\\xa0 columns=['attention_mask',\\n'input_ids',\\n'label'],\\n\\xa0\\xa0\\xa0 shuffle=False,\\nbatch_size=16,\\ncollate_fn=data_collator\\nNow you’re ready to fine-tune. Call fit on the model as\\nusual, but set the optimizer’s learning rate (the multiplier\\nused to adjust weights and biases during backpropagation) to\\n0.00002, which is a fraction of Adam’s default learning rate\\nof 0.001:\\n\\nfrom\\ntensorflow.keras.optimizers\\nimport\\nAdam\\nfrom\\ntransformers\\nimport\\nTFAutoModelForSequenceClassification\\n\\nmodel\\nTFAutoModelForSequenceClassification.from_pretrained(\\n\\xa0\\xa0\\xa0 'distilbert-base-uncased',\\nnum_labels=2)\\nmodel.compile(Adam(learning_rate=2e-5),\\nmetrics=['accuracy'])\\nhist\\nmodel.fit(train_data,\\nvalidation_data=validation_data,\\nepochs=3)\\nPlot the training and validation accuracy to see where the\\nlatter topped out:\\n\\nimport\\nseaborn\\nas\\nsns\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n%matplotlib\\ninline\\nsns.set()\\n\\nacc\\nhist.history['accuracy']\\nval\\nhist.history['val_accuracy']\\nepochs\\nrange(1,\\nlen(acc)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 545, 'file_type': 'pdf'}, page_content=\"plt.plot(epochs,\\nacc,\\nlabel='Training accuracy')\\nplt.plot(epochs,\\nval,\\nlabel='Validation accuracy')\\nplt.title('Training and Validation Accuracy')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\nplt.plot()\\nHere’s how it turned out for me:\\nWith a little luck, validation accuracy topped out at around\\n93%—a few points better than the equivalent bag-of-words\\nmodel. Just imagine what you could do if you trained the\\nmodel with more than 25,000 reviews. One of Hugging Face’s\\npretrained sentiment analysis models—the twitter-roberta-\\nbase model—was trained with 58 million tweets. Not\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 546, 'file_type': 'pdf'}, page_content=\"surprisingly, it does a wonderful job of scoring text for\\nsentiment.\\nFinish up by defining an analyze_text function that returns a\\nsentiment score and using it to score a positive review for\\nsentiment. The model returns an object wrapping a tensor\\ncontaining unnormalized sentiment scores (one for negative\\nand one for positive), but you can use TensorFlow’s softmax\\nfunction to normalize them to values from 0.0 to 1.0:\\n\\nimport\\ntensorflow\\nas\\ntf\\n\\ndef\\nanalyze_text(text,\\ntokenizer,\\nmodel):\\n\\xa0\\xa0\\xa0 tokenized_text\\ntokenizer(text,\\npadding=True,\\ntruncation=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return_tensors='tf')\\n\\xa0\\xa0\\xa0 prediction\\nmodel(tokenized_text)\\n\\xa0\\xa0\\xa0 return\\ntf.nn.softmax(prediction[0]).numpy()[0][1]\\n\\nanalyze_text('Great food and excellent service!',\\ntokenizer,\\nmodel)\\nTry it again with a negative review:\\n\\nanalyze_text('The long lines and poor customer service really turned me off.',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 tokenizer,\\nmodel)\\nFine-tuning isn’t cheap, but it isn’t nearly as expensive\\nas training a sophisticated transformer from scratch. The\\nfact that you could train a sentiment analysis model to be\\nthis accurate in about an hour of GPU time is a tribute to\\nthe power of pretrained BERT models, and to the Google\\nengineers who created them.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 547, 'file_type': 'pdf'}, page_content='Summary\\nNatural language processing, or NLP, is an area of deep\\nlearning that encompasses text classification, question\\nanswering, text translation, and other tasks that require\\ncomputers to process textual data. A key element of every NLP\\nmodel is an embedding layer, which represents words with\\narrays of floating-point numbers that model the relationships\\nbetween words. The vectors for excellent and amazing in\\nembedding space are close together, for example, while the\\nvectors for butterfly and basketball are far apart since the\\nwords have no semantic relationship. Word embeddings are\\nlearned as a model is trained.\\nText input to an embedding layer must first be tokenized and\\nturned into sequences of equal length. Keras’s Tokenizer\\nclass does most of the work. Rather than tokenize and\\nsequence text separately, you can include a TextVectorization\\nlayer in a model to do the tokenization and padding\\nautomatically.\\nOne way to classify text is to use a traditional dense layer\\nto classify the vectors output from an embedding layer. An\\nalternative is to use convolution layers or recurrent layers\\nto tease information regarding word position from the\\nembedding vectors and classify the output from those layers.\\nThe use of deep learning to translate text to other languages\\nis known as neural machine translation, or NMT. Until\\nrecently, state-of-the-art NMT was performed using LSTM-based\\nencoder-decoder models. Today those models have largely given\\nway to transformer models that use neural attention to focus\\non the words in a phrase that are most meaningful and model\\nword context. A transformer knows that the word train has\\ndifferent meanings in “meet me at the train station” and\\n“it’s time to train a model,” and it factors word order\\ninto its calculations.\\nBERT is a sophisticated transformer model installed with\\nlanguage understanding when engineers at Google trained it'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 548, 'file_type': 'pdf'}, page_content='with billions of words. It can be fine-tuned for specific\\ntasks such as question answering and sentiment analysis, and\\nseveral fine-tuned versions have been published for the\\npublic to use. These models can sometimes be used as is.\\nOther times, they can be further refined and adapted to\\ndomain-specific tasks. Because fine-tuning requires orders of\\nmagnitude less data and compute power than training BERT from\\nscratch, pretrained (and pre–fine-tuned) BERT models have\\nproven a boon to NLP.\\nSophisticated NLP models are trained with millions of words\\nor phrases, requiring a substantial investment in data\\ncollection and hardware for training. Companies such as\\nHugging Face publish pretrained models that you can leverage\\nin your code. This is a growing trend in AI: publishing\\nmodels that are already trained to solve common problems.\\nYou’ll still build models to solve problems that are domain\\nspecific, but many tasks—especially those involving NLP—are\\nnot consigned to a particular domain.\\nDownloading pretrained models isn’t the only way to leverage\\nsophisticated AI to solve business problems. Companies such\\nas Microsoft and Amazon train deep-learning models of their\\nown and make them publicly available using REST APIs.\\nMicrosoft calls its suite of APIs Azure Cognitive Services.\\nYou’ll learn all about them in Chapter\\xa014.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 549, 'file_type': 'pdf'}, page_content='Chapter 14. Azure Cognitive\\nServices\\nAs a child growing up in the 1960s, I idolized the Apollo\\nastronauts. Swaggering out to the launch pad and riding\\nflame-breathing rockets into space, they were my superheroes.\\nBut the group I really wanted to emulate—the people I wanted\\nto be—were the engineers in Mission Control. Seated in front\\nof their CRT screens in white shirts and black ties, chatting\\nwith the astronauts and poised to spring into action at the\\nfirst sign of trouble, they were the epitome of cool. They\\nused computers less powerful than today’s smartphones to put\\nmen on the moon—a scientific achievement that is unsurpassed\\nto this day.\\nThanks to deep learning, computers today can perform feats of\\nmagic that the engineers in Mission Control could only have\\ndreamed of. They can recognize objects in images, translate\\ntext and speech to other languages, identify people in video\\nfeeds, turn art into words and words into art, and more. But\\nstate-of-the-art deep-learning models are too complex—and\\ntoo costly—for the average engineer or software developer to\\nbuild. Microsoft reportedly spent hundreds of thousands of\\ndollars training the ResNet model that won the 2015 ImageNet\\nLarge Scale Visual Recognition Challenge. Creating that model\\nrequired a great deal of expertise, massive amounts of GPU\\ntime, and millions of images.\\nA welcome trend in AI today is AI as a service. Microsoft,\\nAmazon, Google, and other tech giants employ professional\\ndata scientists who build sophisticated deep-learning models.\\nThey train them at their own expense and make them available\\nto anyone who wishes to use them by means of REST APIs. If\\nyou can write code to send an HTTP request over the internet,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 550, 'file_type': 'pdf'}, page_content='you can leverage these APIs to infuse AI into your apps\\nwithout taking time off from your day job to earn a PhD in\\ndeep learning.\\nMicrosoft calls its suite of AI services Azure Cognitive\\nServices. Amazon uses the name AWS AI Services. Both offer a\\nrich assortment of APIs served by deep-learning models on the\\nbackend—models that are continually refined so that they\\nbecome smarter over time. Need to caption photos uploaded to\\na website? Microsoft’s Computer Vision service can do that;\\nso can Amazon’s Rekognition service. How about building a\\nscreen reader featuring a lifelike human voice to help the\\nhearing impaired? Amazon Polly can handle that, as can Azure\\nCognitive Services’ Speech service and Google’s text-to-\\nspeech API. These are just a few examples of the actions\\ncognitive services can perform, often with just a few lines\\nof code.\\nYou saw Azure Cognitive Services at work in Chapter\\xa012 when\\nyou used the Custom Vision service to train a custom object\\ndetection model. It’s one of several services that comprise\\nthe Cognitive Services family. This chapter introduces others\\nand demonstrates how to use them and how to build solutions\\nwith them. The focus on Azure Cognitive Services isn’t meant\\nto imply that they’re better than their counterparts from\\nAmazon and Google. I’m simply more familiar with them\\nbecause I’ve worked closely with Microsoft for more than two\\ndecades. Once you learn how to call Azure Cognitive Services\\nAPIs, it’s a simple matter to apply that knowledge to\\ncognitive services from other vendors.\\nReady to make some magic happen? Let’s get started.\\nIntroducing Azure Cognitive Services\\nThe lineup of services that comprise Azure Cognitive Services\\nchanges from time to time as new ones are added and old ones\\nare deprecated or matriculated into other Microsoft product'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 551, 'file_type': 'pdf'}, page_content='lines. Figure\\xa014-1 shows the services that are currently\\noffered and divides them into four categories: vision,\\nlanguage, speech, and decision.\\nFigure 14-1. Azure Cognitive Services\\nVision services bring deep neural networks to bear on\\ncomputer-vision problems. The Custom Vision service lets you\\nbuild custom image classification and object detection\\nmodels. The Face service, which Uber uses to verify the\\nidentities of its drivers, supports state-of-the-art facial\\nrecognition systems. The Computer Vision service exposes a\\nrich API featuring a plethora of ways to analyze images and\\nextract information from them. One application for it is\\ncaptioning photos and generating keywords characterizing\\ntheir content. Figure\\xa014-2 shows a web app I built called\\nIntellipix. Intellipix captions images uploaded by users and\\nstores keywords describing them in a database so that users\\ncan easily pull up all images containing castles, for\\nexample, or photos with water in the foreground or\\nbackground.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 552, 'file_type': 'pdf'}, page_content='Figure 14-2. Intellipix showing a computer-generated image caption\\nThe language category includes services backed by deep-\\nlearning models trained to perform natural language\\nprocessing (NLP). The Language service provides APIs for\\nsentiment analysis, named-entity recognition, question\\nanswering, key-phrase extraction, language understanding, and\\nmore. Among its many uses is building chatbots that respond\\nintelligently to queries regarding a company’s product or\\nservice offerings. There’s also a Translator service that\\ntranslates text between more than 100 languages and dialects.\\nVolkswagen uses it to translate onscreen instructions in cars\\nto match the language in the owner’s manual and to ensure\\nthe quality of the documentation that they produce in more\\nthan 40 languages.\\nNOTE'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 553, 'file_type': 'pdf'}, page_content='The Language service is a unified service that subsumes the\\nfeatures of several older services, including the Text Analytics\\nservice, the Language Understanding service, and QnA Maker.\\nMicrosoft will continue to support the older services for some\\ntime, but new development should target the Language service.\\nThe Speech service provides APIs for converting text to\\nspeech and speech to text. KPMG uses the latter to transcribe\\nrecorded calls and claims that doing so has saved its\\ncustomers millions of dollars in compliance costs. Airbus\\nuses it to build voice-enabled apps for pilot training. The\\nSpeech service also includes a speaker recognition API that\\ncan identify a person’s voice, and a speech translation API\\nthat can translate speech to dozens of other languages in\\nreal time.\\nDecision services include Anomaly Detector, which identifies\\nanomalies in live or recorded data streams and can aggregate\\ninputs from hundreds of disparate sources, such as\\ntemperature sensors and pressure gauges, and model\\nrelationships between them. (See Chapter\\xa06 for an\\nintroduction to anomaly detection.) Airbus uses Anomaly\\nDetector to analyze stresses and strains in aircraft; Siemens\\nuses it to test medical devices for flaws as the final step\\nin production. The Content Moderator service uses AI\\noptionally supplemented by human intervention to flag\\noffensive content in images and videos, profane text, and\\nother inappropriate (and potentially libelous) content. Last\\nbut not least is the Personalizer service, which is perhaps\\nbest described as a recommender system on steroids that\\nprovides personalized content and experiences to end users.\\nKeys and Endpoints'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 554, 'file_type': 'pdf'}, page_content='Azure Cognitive Services expose REST APIs that are called by\\ntransmitting HTTPS requests over the internet. The APIs are\\nlanguage agnostic: they can be called from any programming\\nlanguage that can put a call on the wire. That includes\\nPython, Java, C#, C, C++, JavaScript, Swift, Go, and\\nvirtually all other modern programming languages. Calls can\\nalso be placed with tools such as Postman and the Linux curl\\ncommand.\\nMost Azure Cognitive Services are free up to a point, but\\nthey’re not altogether free. For example, you can submit\\n5,000 text samples per month to the Language service for\\nsentiment analysis without incurring any costs. More than\\nthat, however, and Azure has to know whose Azure subscription\\nto charge. Consequently, before calling an Azure Cognitive\\nService, you need:\\n\\nAn endpoint for the service—the URL that’s the\\ntarget of calls\\nA subscription key for the service, or some other\\nmeans of authenticating calls\\nYou can acquire both from the Azure Portal. As an example,\\nsuppose you wish to perform sentiment analysis on a\\ncollection of tweets. You first open the Azure Portal in your\\nbrowser and log in with your Microsoft account. You then\\ncreate a Cognitive Services Language resource and specify to\\nwhom the Azure subscription costs should be charged, the\\nAzure region in which the service should be located, and a\\npricing tier, as shown in Figure\\xa014-3. A free tier is\\ngenerally available if it doesn’t already exist for the same\\nservice under the same subscription.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 555, 'file_type': 'pdf'}, page_content='Figure 14-3. Creating a Language resource in the Azure Portal\\nOnce the Language resource is created, you open it in the\\nAzure Portal and click Keys and Endpoint in the menu on the\\nleft side of the page. From there, you can retrieve a\\nsubscription key and endpoint for the service (Figure\\xa014-4).\\nThe key is a string of letters and numbers that uniquely\\nidentifies an Azure subscription. Treat it with care because\\nit can cost you money if it gets out. It’s considered a best\\npractice to rotate the key (replace it with a new one)\\nperiodically in case it falls into the wrong hands. That’s\\nwhy the portal includes Regenerate Key buttons. Why does the\\nportal provide two keys? So you have a valid key to use after\\nregenerating the other one. With only one key, you’d have to\\nregenerate it and then race to modify any apps that use it,\\nand there would be a dead period in which calls placed by\\nthose apps would fail.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 556, 'file_type': 'pdf'}, page_content='Figure 14-4. Retrieving a key and an endpoint for an Azure Cognitive\\nServices resource\\nThe subscription key in this example is a single-service\\nsubscription key because it works only with the Language\\nresource that you created. If you create another Cognitive\\nServices resource—for example, one for the Computer Vision\\nservice—you get a separate key for it. So that you can avoid\\nmanaging multiple keys for multiple services, most Azure\\nCognitive Services support multiservice keys that work with a\\nrange of services. To get a multiservice key, simply create a\\nCognitive Services resource rather than a service-specific\\nresource and use the key and endpoint that the portal\\nprovides.\\nSubscription keys aren’t the only way to authenticate calls\\nto Azure Cognitive Services. Most Cognitive Services support\\nAzure Active Directory (AAD) authentication, which enhances\\nsecurity by combining AAD identities with role-based access\\ncontrol (RBAC). AAD authentication is particularly compelling\\nfor apps that rely on Cognitive Services APIs and are\\nthemselves hosted in Azure. For more information, refer to'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 557, 'file_type': 'pdf'}, page_content=\"the article “Authenticate Requests to Azure Cognitive\\nServices”.\\nCalling Azure Cognitive Services APIs\\nOnce you have a subscription key and endpoint, you’re ready\\nto roll. Here’s a snippet of Python code that uses the\\nLanguage service to evaluate “Programming is fun, but the\\nhours are long” for sentiment. KEY is a placeholder for the\\nLanguage resource’s subscription key, while ENDPOINT is a\\nplaceholder for the corresponding endpoint. The import\\nstatement imports the Python Requests package, which\\nsimplifies the HTTP request-response protocol:\\n\\nimport\\nrequests\\n\\ninput\\n'documents':\\n'id':\\n'1000',\\n'text':\\n'Programming is fun, but the hours are long'\\n\\nheaders\\n\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Key':\\nKEY,\\n\\xa0\\xa0\\xa0 'Content-type':\\n'application/json'\\n\\nuri\\nENDPOINT\\n'text/analytics/v3.0/sentiment'\\nresponse\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\nresults\\nresponse.json()\\n\\nfor\\nresult\\nin\\nresults['documents']:\\n\\xa0\\xa0\\xa0 print(result['confidenceScores'])\\nThe result is as follows:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 558, 'file_type': 'pdf'}, page_content=\"{'positive': 0.94, 'neutral': 0.05, 'negative': 0.01}\\nThe service returns three scores: one for positive sentiment,\\none for neutral sentiment, and one for negative sentiment.\\nObserve that you can submit multiple text samples in one call\\nto the API because the documents item in the dictionary\\npassed to the call is a Python list.\\nIt might not be obvious from the code, but input must be JSON\\nencoded and sent in the body of the request. The output comes\\nback as JSON too. The Requests package simplifies JSON\\nencoding and decoding. requests.post encodes the input and\\ntransmits a POST request to the designated endpoint, while\\nresponse.json converts the JSON returned in the response into\\na Python dictionary.\\nTo simplify matters, and to insulate you from changes to the\\nunderlying APIs as they evolve over time, Microsoft offers\\nfree software development kits (SDKs) for most Cognitive\\nServices. The Python package named Azure-ai-textanalytics is\\nthe Python SDK for sentiment analysis and other text\\nanalytics and is formally known as the Azure Cognitive\\nServices Text Analytics client library for Python. It can be\\ninstalled just like any other Python package—for example,\\nwith a pip install command. Here’s the previous sample\\nrewritten to use the SDK:\\n\\nfrom\\nazure.core.credentials\\nimport\\nAzureKeyCredential\\nfrom\\nazure.ai.textanalytics\\nimport\\nTextAnalyticsClient\\n\\nclient\\nTextAnalyticsClient(ENDPOINT,\\nAzureKeyCredential(KEY))\\ninput\\n'id':\\n'1000',\\n'text':\\n'Programming is fun, but the hours are long'\\nresponse\\nclient.analyze_sentiment(input)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 559, 'file_type': 'pdf'}, page_content=\"for\\nresult\\nin\\nresponse:\\n\\xa0\\xa0\\xa0 print(result.confidence_scores)\\nThe output is exactly the same, but the JSON is hidden away,\\nand you don’t have to know what magic string to append to\\nthe endpoint because analyze_sentiment does it for you. That\\nmethod belongs to the SDK’s Text\\u200bAna\\u2060lyticsClient class, and\\nit’s one of several methods available for analyzing text.\\nAnother benefit of using the SDKs is more robust error\\nhandling. Calls can and sometimes do fail, and a well-written\\napp responds gracefully to such failures. When a call to\\nAzure Cognitive Services fails, code in the SDK throws an\\nexception that you can catch in your code. The following\\nexample responds to errors by printing the error message\\ncontained in the exception object:\\n\\nfrom\\nazure.core.credentials\\nimport\\nAzureKeyCredential\\nfrom\\nazure.ai.textanalytics\\nimport\\nTextAnalyticsClient\\nfrom\\nazure.core.exceptions\\nimport\\nAzureError\\n\\ntry:\\n\\xa0\\xa0\\xa0 client\\nTextAnalyticsClient(ENDPOINT,\\nAzureKeyCredential(KEY))\\n\\xa0\\xa0\\xa0 input\\n'id':\\n'1000',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'text':\\n'Programming is fun, but the hours are long'\\n\\xa0\\xa0\\xa0 response\\nclient.analyze_sentiment(input)\\n\\n\\xa0\\xa0\\xa0 for\\nresult\\nin\\nresponse:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(result.confidence_scores)\\n\\nexcept\\nAzureError\\nas\\ne:\\n\\xa0\\xa0\\xa0 print(e.message)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 560, 'file_type': 'pdf'}, page_content='AzureError is defined in Azure-core, which is a library\\nshared by all Azure Cognitive Services Python SDKs. It’s the\\nparent class for other exception classes that correspond to\\nspecific errors. If, for example, you pass a subscription key\\nthat’s invalid (perhaps because the corresponding\\nsubscription is no longer active or payment is past due), a\\nClientAuthenticationError exception occurs. You can include as\\nmany exception handlers as you’d like to respond to specific\\ntypes of errors. The preceding example uses AzureError as a\\ncatchall.\\nCognitive Services SDKs are available for a variety of\\nprogramming languages,\\xa0including Python, Java, and C#.\\nHere’s the same sample written in C# using the Azure\\nCognitive Services Text Analytics client library for .NET,\\nwhich comes in the form of a NuGet package named\\nAzure.AI.TextAnalyt\\u2060ics:\\n\\nusing\\nAzure;\\nusing\\nAzure.AI.TextAnalytics;\\nusing\\nSystem;\\n\\ntry\\n\\xa0\\xa0\\xa0 var\\nclient\\nnew\\nTextAnalyticsClient(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 new\\nUri(ENDPOINT),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 new\\nAzureKeyCredential(KEY)\\n\\n\\xa0\\xa0\\xa0 var\\nresponse\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 client.AnalyzeSentiment(\"Programming is fun, but the hours are long\");\\n\\n\\xa0\\xa0\\xa0 var\\nsentiment\\nresponse.Value.ConfidenceScores.Positive;\\n\\xa0\\xa0\\xa0 Console.WriteLine(sentiment);'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 561, 'file_type': 'pdf'}, page_content='catch\\n(Exception\\nex)\\n\\xa0\\xa0\\xa0 Console.WriteLine(ex.Message);\\nThe output from the code is 0.94, which is the same positive-\\nsentiment score output by the Python examples. Different\\nlanguage, different SDK, but same result, and all with very\\nlittle effort. That’s what Azure Cognitive Services are all\\nabout.\\nAzure Cognitive Services Containers\\nCognitive Services vastly simplify the process of—and lower\\nthe skills barrier for—infusing AI into the apps that you\\nwrite. But they have drawbacks too:\\n\\nBecause Azure Cognitive Services run in the\\ncloud, an app that uses them requires an internet\\nconnection.\\nCalls that travel over the internet incur higher\\nlatencies than calls performed locally.\\nAzure Cognitive Services APIs evolve over time\\nand sometimes introduce breaking changes, which\\nmeans code that worked just fine yesterday could\\nbehave differently or be inoperative tomorrow.\\nThe deep-learning models on the backend are\\ncontinually refined and improved, with the result\\nthat the sentiment score for “programming is\\nfun, but the hours are long” could be 0.94 today\\nand 0.85 tomorrow.\\nChanges to the APIs are rarely abrupt. Microsoft usually\\nwarns its customers months in advance before making breaking'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 562, 'file_type': 'pdf'}, page_content='changes to an API, and in many cases, you can specify the\\nversion of the service or API you wish to use and insulate\\nyourself from future changes. These issues might not be a big\\ndeal to small firms, but for enterprises, which value\\nstability and reliability above all else, they can be deal\\nbreakers.\\nMicrosoft has addressed these issues by making most Azure\\nCognitive Services available in Docker containers. A\\ncontainerized service can run on premises or in the cloud,\\nand it locks in API versions and the models that back them.\\nNo changes occur unless you update a container image or\\nreplace it with a newer version. You can learn more and view\\nthe latest list of services that are available in\\ncontainerized form in “What Are Azure Cognitive Services\\nContainers?”. Containerized services are also a solution to\\nsecurity and privacy policies that require data to stay on\\npremises. Microsoft doesn’t store data passed to Cognitive\\nServices, but the data does leave your company’s domain.\\nContainerized services are not a way to do an end run around\\nthe billing department and use Azure Cognitive Services for\\nfree. From the documentation:\\nThe Cognitive Services containers are required to submit\\nmetering information for billing purposes. Failure to\\nallow-list various network channels that the Cognitive\\nServices containers rely on will prevent the container\\nfrom working.\\nContainers send encrypted usage information back to Microsoft\\nthrough port 443, and Microsoft uses that information to bill\\nan Azure subscription. Consequently, containers have to be\\nconnected to the internet even if they’re hosted on\\npremises.\\nThere is one exception. If you want to use Azure Cognitive\\nServices in apps that can’t be connected to the internet for\\ntechnical or compliance reasons, disconnected containers can\\nrun absent an internet connection, and they don’t transmit'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 563, 'file_type': 'pdf'}, page_content='metering information to Microsoft. Not just anyone can use\\nthem, however. You have to submit an application to Microsoft\\nfor approval, and one of the requirements is that your\\norganization have an Enterprise Agreement (EA) in place with\\nMicrosoft. See “Use Docker Containers in Disconnected\\nEnvironments” for more information and for an up-to-date\\nlist of Azure Cognitive Services that are available for use\\nin disconnected scenarios.\\nThe Computer Vision Service\\nBy now you’re probably ready to stop talking about Azure\\nCognitive Services and write some code. So am I. Let’s start\\nwith perhaps the most feature-rich cognitive service of all.\\nThe Computer Vision service is one of three vision services\\nin Azure Cognitive Services. It exposes a set of APIs that\\nsupport a variety of tasks, including:\\n\\nCaptioning images\\nGenerating tags describing the contents of an\\nimage\\nIdentifying objects (and their bounding boxes) in\\nimages\\nDetecting sensitive or inappropriate content in\\nimages\\nExtracting text from images\\nDetecting faces in images (a subset of the Face\\nservice)\\nGenerating “smart thumbnail” images by using AI\\nto identify the subject of a photo and creating a\\nthumbnail version that’s centered on the subject\\nPerforming spatial analysis on video streams'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 564, 'file_type': 'pdf'}, page_content='Spatial analysis is the latest addition to the Computer\\nVision service. Among other things, it can track people in\\nlive video feeds, measure distances between them in real\\ntime, and determine who’s wearing a mask (and who is not),\\nas shown in Figure\\xa014-5. It’s currently in public preview,\\nand you can read all about it in “What Is Spatial\\nAnalysis?”.\\nFigure 14-5. Using spatial analysis to verify social distancing and masking\\n(images © Microsoft; used with permission)\\nThe best way to get acquainted with the Computer Vision\\nservice is to call a few of its APIs. First install the Azure\\nCognitive Services Computer Vision SDK for Python. (It’s in\\na Python package named Azure-cognitiveservices-vision-\\ncomputervision.) Next, download a ZIP file containing a few\\nsample images and copy the images into the Data subdirectory\\nwhere your Jupyter notebooks are hosted. Use the Azure Portal\\nto create a Computer Vision resource and obtain a key and an\\nendpoint. Then create a new notebook and run the following\\ncode in the first cell after replacing KEY with the key and\\nENDPOINT with the endpoint:\\n\\nfrom\\nazure.cognitiveservices.vision.computervision\\nimport\\nComputerVisionClient\\nfrom\\nmsrest.authentication\\nimport\\nCognitiveServicesCredentials\\n\\nclient\\nComputerVisionClient(ENDPOINT,\\nCognitiveServicesCredentials(KEY))'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 565, 'file_type': 'pdf'}, page_content=\"The ComputerVisionClient class implements several methods\\nthat you can call to invoke Computer Vision APIs. Most of\\nthose methods come in two versions: one that accepts an image\\nURL and one that accepts an actual image. The describe_image\\nmethod, for example, accepts an image URL, while the\\ndescribe_image_in_stream method accepts a stream containing\\nthe image. My examples use the in_stream methods, but you can\\neasily modify them to pass image URLs rather than images.\\nNow load one of the sample images you copied from the ZIP\\nfile and use describe_image_in_stream to caption it:\\n\\n%matplotlib\\ninline\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\n\\nimage\\nplt.imread('Data/dubai.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\n\\nwith\\nopen('Data/dubai.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\nclient.describe_image_in_stream(image)\\n\\n\\xa0\\xa0\\xa0 for\\ncaption\\nin\\nresult.captions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f'{caption.text} ({caption.confidence:.1%})')\\nConfirm that the output is as follows:\\n\\nA man riding a sand dune (53.8%)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 566, 'file_type': 'pdf'}, page_content=\"describe_image_in_stream returns zero or more captions, each\\nwith a score reflecting the Computer Vision service’s\\nconfidence that the caption is accurate. In this example, it\\nreturned just one caption with a confidence of 53.8%.\\nIn addition to captioning images, the Computer Vision service\\ncan generate a list of keywords (“tags”) describing an\\nimage’s content. One use for such tags is to make an image\\ndatabase searchable. Use the tag_image_in_stream method to\\ntag the image captioned in the previous example:\\n\\nwith\\nopen('Data/dubai.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\nclient.tag_image_in_stream(image)\\n\\n\\xa0\\xa0\\xa0 for\\ntag\\nin\\nresult.tags:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f'{tag.name} ({tag.confidence:.1%})')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 567, 'file_type': 'pdf'}, page_content=\"Here are the tags and the confidence levels assigned to them:\\n\\ndune (99.5%)\\nsky (99.2%)\\noutdoor (98.7%)\\nclothing (98.2%)\\ndesert (98.1%)\\nsand (97.9%)\\naeolian landform (96.9%)\\nperson (96.1%)\\nsinging sand (95.8%)\\nerg (94.0%)\\nsahara (93.6%)\\nnature (93.4%)\\nfootwear (90.9%)\\nlandscape (88.0%)\\nsand dune (83.5%)\\nground (77.5%)\\nThe Computer Vision service can also detect objects in\\nimages. The following statements load an image and show all\\nthe objects that were detected along with bounding boxes and\\nconfidence scores:\\n\\nfrom\\nmatplotlib.patches\\nimport\\nRectangle\\n\\ndef\\nannotate_object(name,\\nconfidence,\\nbbox,\\nmin_confidence=0.5):\\n\\xa0\\xa0\\xa0 if\\n(confidence\\nmin_confidence):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 x,\\ny,\\nw,\\nh\\nbbox.x,\\nbbox.y,\\nbbox.w,\\nbbox.h\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n= f'{name} ({confidence:.1%})'\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ax.text(x\\n(w\\ny,\\ntext,\\ncolor='white',\\nbackgroundcolor='red',\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 568, 'file_type': 'pdf'}, page_content=\"ha='center',\\nva='bottom',\\nfontweight='bold',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 bbox=dict(color='red'))\\n\\nimage\\nplt.imread('Data/xian.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 subplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\n\\nwith\\nopen('Data/xian.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\nclient.detect_objects_in_stream(image)\\n\\n\\xa0\\xa0\\xa0 for\\nobject\\nin\\nresult.objects:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 annotate_object(object.object_property,\\nobject.confidence,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 object.rectangle)\\ndetect_objects_in_stream detected the people and the bicycles\\nin the photo, as well as a few other items:\\nCan the Computer Vision service detect faces in photos too?\\nIt certainly can. It can also provide information about age\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 569, 'file_type': 'pdf'}, page_content=\"and gender. The following example annotates the faces in a\\nphoto with labels denoting age and gender. The\\nComputerVisionClient class lacks a dedicated method for\\ndetecting faces, but you can call the general-purpose\\nanalyze_image_in_stream method with a visual\\u200b_features\\nparameter requesting facial info:\\n\\nfrom\\nazure.cognitiveservices.vision.computervision.models \\\\\\nimport\\nVisualFeatureTypes\\n\\ndef\\nannotate_face(face):\\n\\xa0\\xa0\\xa0 x,\\ny\\nface.face_rectangle.left,\\nface.face_rectangle.top\\n\\xa0\\xa0\\xa0 w,\\nh\\nface.face_rectangle.width,\\nface.face_rectangle.height\\n\\xa0\\xa0\\xa0 rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\xa0\\xa0\\xa0 text\\n= f'{face.gender} ({face.age})'\\n\\xa0\\xa0\\xa0 ax.text(x\\n(w\\ny,\\ntext,\\ncolor='white',\\nbackgroundcolor='red',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ha='center',\\nva='bottom',\\nfontweight='bold',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 bbox=dict(color='red'))\\n\\nimage\\nplt.imread('Data/amsterdam.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\n\\nwith\\nopen('Data/amsterdam.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\nclient.analyze_image_in_stream(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 image,\\nvisual_features=[VisualFeatureTypes.faces])\\n\\n\\xa0\\xa0\\xa0 for\\nface\\nin\\nresult.faces:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 annotate_face(face)\\nHere’s the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 570, 'file_type': 'pdf'}, page_content='NOTE\\nThe Face service can detect faces and identify ages, genders,\\nemotions, facial landmarks, and more. It also has methods for\\ncomparing facial images, identifying faces, and building facial\\nrecognition systems. Its API is a superset of the Computer Vision\\nservice’s face API. As a result of Microsoft’s Responsible AI\\ninitiative, however, the Face service is no longer available to\\nthe general public. For more information about the Face service\\nand how to apply for access to it, refer to “What Is the Azure\\nFace Service?”.\\nSuppose you’re the proprietor of a public website that\\naccepts photo uploads and you’d like to reject photos'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 571, 'file_type': 'pdf'}, page_content=\"containing inappropriate content. Called as follows,\\nanalyze_image_in_stream scores a photo for adultness (does\\nthe photo contain nudity?), raciness (does it contain bare\\nskin?), and goriness (does it contain blood and gore?) on a\\nscale of 0.0 to 1.0. Here’s how it responds to a photo of a\\nyoung girl cliff-jumping in her bathing suit in Hawaii:\\n\\nimage\\nplt.imread('Data/maui.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\n\\nwith\\nopen('Data/maui.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\nclient.analyze_image_in_stream(image,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 visual_features=[VisualFeatureTypes.adult])\\n\\n\\xa0\\xa0\\xa0 print(f'Adultness: {result.adult.adult_score}')\\n\\xa0\\xa0\\xa0 print(f'Raciness: {result.adult.racy_score}')\\n\\xa0\\xa0\\xa0 print(f'Goriness: {result.adult.gore_score}')\\n\\n\\xa0\\xa0\\xa0 print(f'Is adult: {result.adult.is_adult_content}')\\n\\xa0\\xa0\\xa0 print(f'Is racy: {result.adult.is_racy_content}')\\n\\xa0\\xa0\\xa0 print(f'Is gory: {result.adult.is_gory_content}')\\nHere’s the output:\\n\\nAdultness: 0.02214685082435608\\nRaciness: 0.4205135107040405\\nGoriness: 0.0016634463099762797\\nIs adult: False\\nIs racy: False\\nIs gory: False\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 572, 'file_type': 'pdf'}, page_content=\"The bikini in the photo yielded a moderate raciness score,\\nbut one that’s below the threshold of 0.5 required for\\nis_racy_content to be True. Based on the other scores it\\nreturned, the Computer Vision service believes that the photo\\nis neither “adult” nor gory.\\nYet another capability that the Computer Vision service lends\\nto application developers is using AI to extract text from\\nphotos. It’s perfect for digitizing printed documents.\\nHere’s an example:\\n\\ndef\\ndraw_box(bbox):\\n\\xa0\\xa0\\xa0 vals\\nbbox.split(',')\\n\\xa0\\xa0\\xa0 x\\nint(vals[0])\\n\\xa0\\xa0\\xa0 y\\nint(vals[1])\\n\\xa0\\xa0\\xa0 w\\nint(vals[2])\\n\\xa0\\xa0\\xa0 h\\nint(vals[3])\\n\\xa0\\xa0\\xa0 rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor='red',\\nfill=False,\\nlw=2)\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 573, 'file_type': 'pdf'}, page_content=\"ax.add_patch(rect)\\n\\nimage\\nplt.imread('Data/1040-es.jpg')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={'xticks':\\n'yticks':\\nax.imshow(image)\\n\\nwith\\nopen('Data/1040-es.jpg',\\nmode='rb')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 result\\nclient.recognize_printed_text_in_stream(image)\\n\\n\\xa0\\xa0\\xa0 for\\nregion\\nin\\nresult.regions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nline\\nin\\nregion.lines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n' '.join([word.text\\nfor\\nword\\nin\\nline.words])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 draw_box(line.bounding_box)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(text)\\nHere’s the output, minus all the lines of text output by the\\nprint statement:\\nThe recognize_printed_text_in_stream method only recognizes\\nprinted text. A related method named read_in_stream\\nrecognizes handwritten text too. Let’s see how it performs\\non the same scanned document:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 574, 'file_type': 'pdf'}, page_content='import\\ntime\\nfrom\\nazure.cognitiveservices.vision.computervision.models \\\\\\nimport\\nOperationStatusCodes\\n\\ndef\\ndraw_box(bbox):\\n\\xa0\\xa0\\xa0 x,\\ny\\xa0 =\\nbbox[0],\\nbbox[1]\\n\\xa0\\xa0\\xa0 w\\nbbox[4]\\nx\\n\\xa0\\xa0\\xa0 h\\nbbox[5]\\ny\\n\\xa0\\xa0\\xa0 rect\\nRectangle((x,\\ny),\\nw,\\nh,\\ncolor=\\'red\\',\\nfill=False,\\nlw=2)\\n\\xa0\\xa0\\xa0 ax.add_patch(rect)\\n\\nimage\\nplt.imread(\\'Data/1040-es.jpg\\')\\nfig,\\nax\\nplt.subplots(figsize=(12,\\nsubplot_kw={\\'xticks\\':\\n\\'yticks\\':\\nax.imshow(image)\\n\\nwith\\nopen(\\'Data/1040-es.jpg\\',\\nmode=\\'rb\\')\\nas\\nimage:\\n\\xa0\\xa0\\xa0 response\\nclient.read_in_stream(image,\\nraw=True)\\n\\n\\xa0\\xa0\\xa0 location\\nresponse.headers[\"Operation-Location\"]\\n\\xa0\\xa0\\xa0 opid\\nlocation[len(location)\\n\\xa0\\xa0\\xa0 results\\nclient.get_read_result(opid)\\n\\n\\xa0\\xa0\\xa0 while\\nresults.status\\nOperationStatusCodes.running:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 results\\nclient.get_read_result(opid)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 time.sleep(1)\\n\\n\\xa0\\xa0\\xa0 if\\nresults.status\\nOperationStatusCodes.succeeded:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nresult\\nin\\nresults.analyze_result.read_results:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nline\\nin\\nresult.lines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 draw_box(line.bounding_box)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(line.text)\\nThis example is a little more involved because read_in_stream\\nreturns before the call has completed. We therefore loop'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 575, 'file_type': 'pdf'}, page_content=\"until the call completes and then retrieve the results. The\\ncall to sleep in each iteration of the while loop allows the\\nuser interface to remain responsive while waiting for the\\ncall to complete. The results are worth the wait:\\nPerhaps you’d prefer to extract only handwritten text from a\\ndocument. You can do that too, because for each line of text\\nit detects, read_in_stream includes a style attribute equal\\nto “handwriting” for handwritten text. To demonstrate,\\nreplace the final two lines in the previous example with\\nthese:\\n\\nif\\n(line.appearance.style.name\\n'handwriting'):\\n\\xa0\\xa0\\xa0 draw_box(line.bounding_box)\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 print(line.text)\\nThis time, only handwritten text is highlighted:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 576, 'file_type': 'pdf'}, page_content='read_in_stream does a commendable job of converting the\\nhandwritten text into Python strings too:\\n\\n$ 100\\nJeff\\nProsise\\n111-22-0000\\n1313 Mockingbird Lane\\nOak Ridge\\nTN\\n37830\\nA final note regarding the Computer Vision service is that it\\ndoesn’t accept images larger than 4 MB. Anything larger\\nproduces a ComputerVisionErrorResponse\\u200bExcep\\u2060tion. You can catch\\nthese exceptions (or AzureError exceptions, which are higher\\nup the food chain) and recover gracefully, or you can check\\nan image’s size before submitting it and downsize it if\\nnecessary.\\nThe Language Service'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 577, 'file_type': 'pdf'}, page_content=\"The Computer Vision service employs deep-learning models\\nsimilar to the ones you learned about in Chapters 10, 11, and\\n12. The Language and Translator services use models like the\\nones in Chapter\\xa013. The latter two services embody NLP:\\nsentiment analysis, neural machine translation, question\\nanswering, and more. The Text\\u200bAnaly\\u2060ticsClient class in the\\nPython text analytics SDK provides a convenient interface to\\nmany of the Language service’s features. You’ve already\\nseen it used for sentiment analysis. Here’s another example\\nthat applies sentiment analysis to multiple text samples with\\none round trip to the Language service:\\n\\nfrom\\nazure.core.credentials\\nimport\\nAzureKeyCredential\\nfrom\\nazure.ai.textanalytics\\nimport\\nTextAnalyticsClient\\n\\nclient\\nTextAnalyticsClient(ENDPOINT,\\nAzureKeyCredential(KEY))\\n\\ninput\\n'id':\\n'1000',\\n'text':\\n'Programming is fun, but the hours are long'\\n'id':\\n'1001',\\n'text':\\n'Great food and excellent service'\\n'id':\\n'1002',\\n'text':\\n'The product worked as advertised but is ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'overpriced'\\n'id':\\n'1003',\\n'text':\\n'Moving to the cloud was the best decision ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'we ever made'\\n'id':\\n'1004',\\n'text':\\n'Programming is so fun I\\\\'d do it for free. ' \\\\\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Don\\\\'t tell my boss!'\\nresponse\\nclient.analyze_sentiment(input)\\n\\nfor\\nresult\\nin\\nresponse:\\n\\xa0\\xa0\\xa0 text\\n''.join([x.text\\nfor\\nx\\nin\\nresult.sentences])\\n\\xa0\\xa0\\xa0 print(f'{text} => {result.confidence_scores.positive}')\\nHere’s the output:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 578, 'file_type': 'pdf'}, page_content=\"Programming is fun, but the hours are long => 0.94\\nGreat food and excellent service => 1.0\\nThe product worked as advertised but is overpriced => 0.0\\nMoving to the cloud was the best decision we ever made => 1.0\\nProgramming is so fun I'd do it for free. Don't tell my boss! => 1.0\\nLet’s say you’re writing an app that collects tweets\\nreferencing your company and analyzes them for sentiment. The\\nidea is that if sentiment turns negative, you can give the\\nmarketing department a heads-up. It’s faster and more\\nefficient to place one call to Azure Cognitive Services and\\nanalyze 100 tweets than to make 100 calls analyzing one tweet\\nat a time.\\nSentiment analysis is one of several operations supported by\\nthe TextAnalytic\\u2060s\\u200bCli\\u2060ent class. Another is named-entity\\nrecognition. Suppose you’re building a system that sorts and\\nprioritizes support tickets received by your company’s help\\ndesk. The recognize_entities method extracts entities such as\\npeople, places, organizations, dates and times, and\\nquantities from input text. It reveals the entity types as\\nwell:\\n\\ndocuments\\n\\xa0\\xa0\\xa0 'My printer isn\\\\'t working. Can someone from IT come to my office ' \\\\\\n\\xa0\\xa0\\xa0 'and have a look?'\\n\\nresults\\nclient.recognize_entities(documents)\\n\\nfor\\nresult\\nin\\nresults:\\n\\xa0\\xa0\\xa0 for\\nentity\\nin\\nresult.entities:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(f'{entity.text} ({entity.category})')\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 579, 'file_type': 'pdf'}, page_content=\"The output from this example is as follows:\\n\\nprinter (Product)\\nIT (Skill)\\noffice (Location)\\nA related method named recognize_pii_entities extracts\\npersonally identifiable information (PII) entities such as\\nbank account info, Social Security numbers, and credit card\\nnumbers from text input to it, while the extract_key_phrases\\nmethod extracts key phrases:\\n\\ndocuments\\n\\xa0\\xa0\\xa0 'Natural Language Processing, or NLP, encompasses a variety of ' \\\\\\n\\xa0\\xa0\\xa0 'activities including text classification, keyword extraction,' \\\\\\n\\xa0\\xa0\\xa0 'named-entity recognition, question answering, and language '\\\\\\n\\xa0\\xa0\\xa0 'translation.'\\n\\nresults\\nclient.extract_key_phrases(documents)\\n\\nfor\\nresult\\nin\\nresults:\\n\\xa0\\xa0\\xa0 for\\nphrase\\nin\\nresult.key_phrases:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 print(phrase)\\nHere’s the output:\\n\\nNatural Language Processing\\nlanguage translation\\ntext classification\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 580, 'file_type': 'pdf'}, page_content='keyword extraction\\nquestion answering\\nNLP\\nvariety\\nactivities\\nrecognition\\nThis example is a simple one given that the text is so brief,\\nbut you could pass in hundreds of large documents and use the\\nresults to get a snapshot of each document’s content, or\\ngroup documents that contain similar keywords.\\nTextAnalyticsClient provides a wrapper around text analytics\\nAPIs. Other Python SDKs unlock additional features of the\\nLanguage service. For example, the question-answering SDK\\nprovides APIs for answering questions from manuals, FAQs,\\nblog posts, and other documents that you provide. For more\\ninformation, and to see this aspect of the Language service\\nin action, refer to the article titled “Azure Cognitive\\nLanguage Services Question Answering Client Library for\\nPython”.\\nThe Translator Service\\nThe Translator service uses state-of-the-art neural machine\\ntranslation to translate text between dozens of languages. It\\ncan also identify written languages. Suppose your objective\\nis to translate into English questions written in other\\nlanguages and submitted through your company’s website.\\nFirst you need to determine whether the source language is\\nEnglish. If it’s not, you want to translate it so that you\\ncan respond to the customer’s request.\\nThe following code analyzes a text sample and shows the\\nlanguage it’s written in. Microsoft doesn’t currently offer\\na Python SDK for the Translator service, but you can use\\nPython’s Requests package to simplify calls. To demonstrate,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 581, 'file_type': 'pdf'}, page_content=\"create a Translator resource in the Azure Portal and grab the\\nsubscription key (the one labeled “Text Translation”),\\nendpoint, and region. Then run the following code, replacing\\nKEY and ENDPOINT with the key and endpoint and REGION with\\nthe Azure region you selected (for example, southcentralus):\\n\\nimport\\nrequests\\n\\ninput\\n'text':\\n'Quand votre nouveau livre sera-t-il disponible?'\\n\\nheaders\\n\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Key':\\nKEY,\\n\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Region':\\nREGION,\\n\\xa0\\xa0\\xa0 'Content-type':\\n'application/json'\\n\\nuri\\nENDPOINT\\n'detect?api-version=3.0&to=en'\\nresponse\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\nresults\\nresponse.json()\\n\\nprint(results[0]['language'])\\nThe output is fr for French. Now that you’ve determined the\\nsource language isn’t English, you can translate it this\\nway:\\n\\nuri\\nENDPOINT\\n'translate?api-version=3.0&from=fr&to=en'\\nresponse\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\nresults\\nresponse.json()\\n\\nprint(results[0]['translations'][0]['text'])\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 582, 'file_type': 'pdf'}, page_content=\"The translated text is:\\n\\nWhen will your new book be available?\\nYou can omit from=fr from the URL and allow the Translator\\nservice to detect the language for you. You can also detect\\nthe language and translate the text with one call. If you\\ncall the translate endpoint without a from parameter, the\\nreturn value includes a detectedLanguage item that identifies\\nthe language in the source text:\\n\\n[{'detectedLanguage': {'language': 'fr', 'score': 1.0}, 'translations':\\n[{'text':\\n'When will your new book be available?', 'to': 'en'}]}]\\nIf passed a list containing multiple text samples, the\\nTranslator service will translate all of them in one call. It\\nalso supports transliteration: translating text to other\\nalphabets. To see for yourself, use Google Translate to\\ntranslate “When will your new book be available” to Thai or\\nHindi. Then paste the Thai or Hindi text over the French text\\nin the previous example and run it again. Be sure to also\\nchange from=fr to from=th or from=hi or simply remove the from\\nparameter altogether.\\nTIP\\nWhen you create a Translator resource, you have a choice of\\ncreating a global resource or one tied to a specific Azure region.\\nThe preceding examples assume that you created it as a regional\\nresource. If you created a global Translator resource instead, you\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 583, 'file_type': 'pdf'}, page_content='can set the Ocp-Apim-Subscription-Region header to global or omit the\\nheader altogether.\\nWhen you obtained an endpoint for the Translator service, did\\nyou notice that the Azure Portal offered two endpoints—one\\nfor “Text Translation” and another for “Document\\nTranslation?” That’s because the Translator service\\nfeatures a second API for translating entire documents,\\nincluding PDFs. There’s even a Python SDK to help out. The\\nonly catch is that documents must first be uploaded to Azure\\nblob storage. For examples showing the document translation\\nAPI in action, see “Azure Document Translation Client\\nLibrary for Python”. The API is asynchronous and can process\\nbatches of documents in one call, so it’s ideal not just for\\ntranslating individual documents, but for translating large\\nvolumes of documents at scale.\\nThe Speech Service\\nOne of the more challenging tasks for deep-learning models is\\nprocessing human speech. Azure Cognitive Services includes a\\nSpeech service that converts text to speech, speech to text,\\nand more. It’s even capable of captioning recorded videos\\nand live video streams and filtering out profanity as it\\ndoes. A Python SDK simplifies the code you write and makes it\\nremarkably easy to incorporate speech into your apps.\\nTo demonstrate, install the package named Azure-\\ncognitiveservices-speech containing the Python Speech SDK.\\nUse the Azure Portal to create a Cognitive Services Speech\\nresource and make note of the subscription key and service\\nregion. Then create a Jupyter notebook and run the following\\ncode in the first cell after replacing KEY with the\\nsubscription key and REGION with the region you selected:'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 584, 'file_type': 'pdf'}, page_content=\"from\\nazure.cognitiveservices\\nimport\\nspeech\\n\\nspeech_config\\nspeech.SpeechConfig(KEY,\\nREGION)\\nspeech_config.speech_recognition_language\\n'en-US'\\nNow run the following statements, and when prompted, speak\\ninto your microphone. This sample creates a SpeechRecognizer\\nobject and uses its recognize_once_async method to convert up\\nto 30 seconds of live audio from your PC’s default\\nmicrophone into text. Observe that the text doesn’t appear\\nuntil you’ve finished speaking:\\n\\nrecognizer\\nspeech.SpeechRecognizer(speech_config)\\n\\nprint('Speak into your microphone')\\nresult\\nrecognizer.recognize_once_async().get()\\n\\nif\\nresult.reason\\nspeech.ResultReason.RecognizedSpeech:\\n\\xa0\\xa0\\xa0 print(result.text)\\nIt couldn’t be much simpler than that. How about converting\\ntext to speech? Here’s an example that uses the SDK’s\\nSpeechSynthesizer class to vocalize a sentence. The\\nsynthesized voice belongs to an English speaker named Jenny\\n(en-US-JennyNeural), and it’s one of more than 300 neural\\nvoices you can choose from:\\n\\nspeech_config.speech_synthesis_voice_name\\n'en-US-JennyNeural'\\nsynthesizer\\nspeech.SpeechSynthesizer(speech_config)\\nsynthesizer.speak_text_async('When will your new book be published?').get()\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 585, 'file_type': 'pdf'}, page_content=\"All of the “speakers” are multilingual. If you ask a French\\nspeaker—for example, fr-FR-CelesteNeural—to synthesize an\\nEnglish sentence, the vocalization will feature a French\\naccent.\\nYou can combine a TranslationRecognizer object with a\\nSpeechSynthesizer object to translate speech in real time.\\nThe following example takes spoken English as input and plays\\nit back in French using the voice of a native French speaker:\\n\\nspeech_config.speech_synthesis_voice_name\\n'fr-FR-YvetteNeural'\\nsynthesizer\\nspeech.SpeechSynthesizer(speech_config)\\n\\ntranslation_config\\nspeech.translation.SpeechTranslationConfig(KEY,\\nREGION)\\ntranslation_config.speech_recognition_language\\n'en-US'\\ntranslation_config.add_target_language('fr')\\n\\nrecognizer\\nspeech.translation.TranslationRecognizer(translation_config)\\n\\nprint('Speak into your microphone')\\nresult\\nrecognizer.recognize_once_async().get()\\n\\nif\\nresult.reason\\nspeech.ResultReason.TranslatedSpeech:\\n\\xa0\\xa0\\xa0 text\\nresult.translations['fr']\\n\\xa0\\xa0\\xa0 synthesizer.speak_text_async(text).get()\\nThese samples use your PC’s default microphone for voice\\ninput and default speakers for output. You can specify other\\nsources of input and output by passing an Audio\\u200bCon\\u2060fig object\\nto the methods that create SpeechRecognizer,\\nSpeechSynthesizer, and TranslationRecognizer objects. Among\\nthe options this enables is using a file or stream rather\\nthan a microphone as the source of input.\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 586, 'file_type': 'pdf'}, page_content='Putting It All Together: Contoso Travel\\nImagine you’re a web developer and your client is Contoso\\nTravel. To motivate its customers to stay in touch, the\\ntravel agency wants its website to include a service that\\ntranslates signage. The idea is that a customer exploring a\\nfaraway land can snap a picture of a sign, upload it to the\\nsite, and see a translation in the language of their choice.\\nNo typing, no forms to fill out—just “Here’s a picture,\\ntell me what it says.”\\nA few years ago, such a website would have been unthinkable\\nfor most small businesses. Today it’s within the\\ncapabilities of anyone who can sling a few lines of code.\\nLet’s use Python’s Flask framework to build a website that\\nmakes your client happy. We’ll use Azure Cognitive Services\\nto do the heavy lifting of extracting and translating text,\\nand we’ll end up with the product shown in Figure\\xa014-6.\\nBegin by making sure the required packages are installed,\\nincluding Flask, Requests, and Azure-cognitiveservices-\\nvision-computervision. Then create a project directory in the\\nlocation of your choice on your hard disk. Name it Contoso or\\nanything else you’d like. Next, download a ZIP file\\ncontaining a starter kit for the website and copy its\\ncontents into the project directory. Take a moment to examine\\nthe files that you copied. These files comprise a website\\nwritten in Python and Flask. They include the following:\\napp.py\\nHolds the Python code that drives the site\\ntemplates/index.xhtml\\nContains the site’s home page\\nstatic/main.css\\nContains CSS to dress up the home page\\nstatic/banner.jpg'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 587, 'file_type': 'pdf'}, page_content='Contains the website’s banner\\nstatic/placeholder.jpg\\nContains a placeholder image for photos that have yet\\nto be uploaded\\nFigure 14-6. The Contoso Travel website translating a road sign\\nThe site contains a single page named index.xhtml that’s\\ndisplayed when a user navigates to the site. The code that\\ndisplays it lives in app.py, and it includes logic for\\nuploading photos. You will modify app.py to extract and\\ntranslate text from the photos that users upload.\\nNOTE\\nDoes “Contoso” sound familiar? It’s the company name used in\\ncountless Microsoft samples and tutorials. There’s a story behind'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 588, 'file_type': 'pdf'}, page_content='why it’s so popular. When you write content for Microsoft,\\nyou’re not allowed to make up company names because doing so is\\nlikely to get Microsoft sued. Contoso is one of several fictitious\\ncompany names Microsoft has trademarked over the years. Others\\nthat might be familiar include Northwind Traders, Fabrikam, and\\nAdventureWorks.\\nOpen a command prompt or terminal window and cd to the\\nproject directory. If you’re running Windows, use the\\nfollowing command to create an environment variable named\\nFLASK_ENV that tells Flask to run in development mode:\\n\\nset FLASK_ENV=development\\nIf you’re running Linux or macOS, use this command instead:\\n\\nexport FLASK_ENV=development\\nRunning Flask in development mode is helpful when you’re\\ndeveloping a website because Flask automatically reloads any\\nfiles that change while the site is running. If you let Flask\\ndefault to production mode and change the contents of an HTML\\nfile or other asset, you have to restart Flask for the\\nchanges to appear in your browser.\\nNow use the following command to start Flask:\\n\\nflask run'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 589, 'file_type': 'pdf'}, page_content=\"Open a browser and go to http://localhost:5000. When Contoso\\nTravel’s home page appears, click the Upload Photo button\\nand select a JPG or PNG file on your computer. Confirm that\\nthe photo uploads without errors. At this point, no\\nprocessing is performed on the photo, so the only visual clue\\nthat the upload succeeded is that the photo appears in the\\nweb page.\\nNext, use the Azure Portal to create a Computer Vision\\nresource and a Translator resource if you haven’t already.\\nOpen app.py in your favorite code editor and find the\\nfollowing statements near the top of the file:\\n\\n# Define Cognitive Services variables\\nvision_key\\n'VISION_KEY'\\nvision_endpoint\\n'VISION_ENDPOINT'\\ntranslator_key\\n'TRANSLATOR_KEY'\\ntranslator_endpoint\\n'TRANSLATOR_ENDPOINT'\\ntranslator_region\\n'TRANSLATOR_REGION'\\nReplace VISION_KEY and TRANSLATOR_KEY with the services’\\nsubscription keys, VISION_ENDPOINT and TRANSLATOR_ENDPOINT\\nwith the endpoints, and TRANSLA\\u2060TOR\\u200b_REGION with the region\\nselected for the Translator service. For Translator, use the\\nendpoint labeled “Text Translation.” Be sure to leave the\\ntick marks delimiting the strings intact.\\nNow add the following function for extracting text from a\\nphoto to app.py, placing it right after the comment that\\nreads “Function that extracts text from images” near the\\nbottom of the file:\\n\\ndef\\nextract_text(endpoint,\\nkey,\\nimage):\\n\\xa0\\xa0\\xa0 try:\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 590, 'file_type': 'pdf'}, page_content=\"client\\nComputerVisionClient(endpoint,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 CognitiveServicesCredentials(key))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 result\\nclient.recognize_printed_text_in_stream(image)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lines\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nregion\\nin\\nresult.regions:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nline\\nin\\nregion.lines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 text\\n' '.join([word.text\\nfor\\nword\\nin\\nline.words])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lines.append(text)\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if\\nlen(lines)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lines.append('Photo contains no text to translate')\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\nlines\\n\\n\\xa0\\xa0\\xa0 except\\nComputerVisionErrorResponseException\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n['Error calling the Computer Vision service: '\\ne.message]\\n\\n\\xa0\\xa0\\xa0 except\\nException\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n['Error calling the Computer Vision service']\\nThen add the following function for translating text. Place\\nit after the comment that reads “Function that translates\\ntext into a specified language” near the bottom of app.py:\\n\\ndef\\ntranslate_text(endpoint,\\nregion,\\nkey,\\nlines,\\nlanguage):\\n\\xa0\\xa0\\xa0 try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 headers\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Key':\\nkey,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Ocp-Apim-Subscription-Region':\\nregion,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 'Content-type':\\n'application/json'\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input\"),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 591, 'file_type': 'pdf'}, page_content='for\\nline\\nin\\nlines:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 input.append({\\n\"text\":\\nline\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 uri\\nendpoint\\n\\'translate?api-version=3.0&to=\\'\\nlanguage\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 response\\nrequests.post(uri,\\nheaders=headers,\\njson=input)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 response.raise_for_status()\\n# Raise exception if call failed\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 results\\nresponse.json()\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 translated_lines\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\nresult\\nin\\nresults:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for\\ntranslated_line\\nin\\nresult[\"translations\"]:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 translated_lines.append(translated_line[\"text\"])\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\ntranslated_lines\\n\\n\\xa0\\xa0\\xa0 except\\nrequests.exceptions.HTTPError\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n[\\'Error calling the Translator service: \\'\\ne.strerror]\\n\\n\\xa0\\xa0\\xa0 except\\nException\\nas\\ne:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return\\n[\\'Error calling the Translator service\\']\\nFind the function named index in app.py. This is the function\\ncalled when the home page is requested or a photo is\\nuploaded. Under the comment that reads “Use the Computer\\nVision service to extract text from the image,” add the\\nfollowing line of code to call extract_text with the photo\\nthat the user just uploaded:\\n\\nlines\\nextract_text(vision_endpoint,\\nvision_key,\\nimage)\\nUnder the comment that reads “Use the Translator service to\\ntranslate text extracted from the image,” add a line of code'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 592, 'file_type': 'pdf'}, page_content='to translate the text returned by extract_text to the\\nspecified language:\\n\\ntranslated_lines\\ntranslate_text(translator_endpoint,\\ntranslator_region,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 translator_key,\\nlines,\\nlanguage)\\nAdd the following statements immediately after the comment\\nthat reads “Flash the translated text”:\\n\\nfor\\ntranslated_line\\nin\\ntranslated_lines:\\n\\xa0\\xa0\\xa0 flash(translated_line)\\nThese statements use Flask’s message-flashing support to\\ndisplay the strings in translated_lines in a modal dialog.\\nFinish up by saving your changes to app.py. Return to the\\nbrowser in which the website is running and refresh the page.\\n(If you closed it, open a new browser instance and navigate\\nto http://localhost:5000.) Select a language from the drop-\\ndown list at the top of the page. Then upload a photo\\ncontaining a road sign or any other image with text in it.\\nWas the text extracted and translated? The app isn’t\\nperfect, but it should work as expected most of the time. If\\nyou’re not satisfied, try replacing calls to\\nrecognize_printed_text_in_stream with calls to read_in_stream.\\nThe latter is more aggressive at finding text in photos.\\nWhile you’re at it, you could also use the Translator\\nservice to translate error messages into the language the\\nuser selected or the Speech service to vocalize translated\\ntext. With Azure Cognitive Services lending a hand, the only\\nlimit is your imagination.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 593, 'file_type': 'pdf'}, page_content='Summary\\nAzure Cognitive Services lower the bar for incorporating\\nsophisticated deep-learning models into the apps you write by\\nproviding REST APIs that any app with an internet connection\\ncan call. Containerized versions of most services allow them\\nto be hosted locally or on premises and also provide an\\noption for running in disconnected scenarios. One of the\\nbenefits of containers is locking in a particular version of\\nthe models you’re using or the APIs that access them so that\\nchanges to Azure Cognitive Services won’t affect your apps.\\nThe range of services offered by Azure Cognitive Services\\nincludes vision services for extracting information from\\nimages and video streams, language services for translating\\ntext into other languages and performing other NLP tasks, and\\nspeech services for incorporating speech into your apps. Free\\nSDKs are available for most services. They simplify the code\\nyou write, and they’re available for a variety of popular\\nprogramming languages including Python, Java, and C#.\\nMachine learning and deep learning are making the world a\\nbetter place one model at a time. Soon both will be\\nconsidered part of the essential skill set of every engineer\\nand software developer. Are you up to the task? My hope is\\nthat this book gives you the confidence to say yes and the\\ntools to make it happen. It’s too late to monitor Apollo\\nmissions in Mission Control, but countless other frontiers\\nare waiting to be explored.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 594, 'file_type': 'pdf'}, page_content='Index\\nA\\n\\naccuracy measures\\nfor classification models, Classification Models,\\nAccuracy Measures for Classification Models-Accuracy\\nMeasures for Classification Models\\nconfusion matrix, Accuracy Measures for\\nClassification Models, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nfor neural network predictions, Training a Neural\\nNetwork to Detect Credit Card Fraud-Training a Neural\\nNetwork to Detect Credit Card Fraud, Training a CNN\\nto Recognize Arctic Wildlife\\nfor regression models, Accuracy Measures for\\nRegression Models-Accuracy Measures for Regression\\nModels\\ntraining versus validation accuracy, Dropout\\nvalidation accuracy (see validation accuracy)\\naccuracy score, Accuracy Measures for Classification\\nModels, Accuracy Measures for Classification Models,\\nClassifying Passengers Who Sailed on the Titanic\\nactivation functions, Understanding Neural Networks\\nReLU, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow\\nsigmoid, Building Neural Networks with Keras and\\nTensorFlow, Binary Classification with Neural'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 595, 'file_type': 'pdf'}, page_content='Networks-Making Predictions, Understanding CNNs\\nsoftmax (see softmax activation function)\\ntanh, Building Neural Networks with Keras and\\nTensorFlow\\nAdam optimizer, Building Neural Networks with Keras and\\nTensorFlow\\nadaptive learning rate algorithms, Training Neural\\nNetworks\\nadditive modeling, Gradient-Boosting Machines\\nadditive smoothing, Naive Bayes\\nagglomerative clustering, Segmenting Customers Using More\\nThan Two Dimensions\\nAlexNet, Image Classification with Convolutional Neural\\nNetworks\\nalgorithm, Machine Learning\\nAlphaGo, Machine Learning Versus Artificial Intelligence\\nanalyze_image_in_stream method, The Computer Vision\\nService\\nanalyze_sentiment, Calling Azure Cognitive Services APIs\\nanchors and anchor boxes, R-CNNs\\nannotate_image function, Mask R-CNN\\nanomaly detection, Anomaly Detection-Multivariate Anomaly\\nDetection\\nbearing failure prediction, Using PCA to Predict\\nBearing Failure-Using PCA to Predict Bearing Failure\\ncredit card fraud detection, Using PCA to Detect\\nCredit Card Fraud-Using PCA to Detect Credit Card\\nFraud\\nmultivariate, Multivariate Anomaly Detection'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 596, 'file_type': 'pdf'}, page_content='Anomaly Detector, Introducing Azure Cognitive Services\\nanonymizing data, Anonymizing Data-Anonymizing Data\\nArcFace, ArcFace, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\narctic wildlife recognition, Accuracy Measures for\\nClassification Models, Training a CNN to Recognize Arctic\\nWildlife-Training a CNN to Recognize Arctic Wildlife,\\nUsing Transfer Learning to Identify Arctic Wildlife-Using\\nTransfer Learning to Identify Arctic Wildlife, Applying\\nImage Augmentation to Arctic Wildlife-Applying Image\\nAugmentation to Arctic Wildlife\\narea under the curve (AUC), Accuracy Measures for\\nClassification Models\\nargmax function, Multiclass Classification with Neural\\nNetworks\\nartificial intelligence (AI)\\nhistory of, Machine Learning Versus Artificial\\nIntelligence\\nversus machine learning (ML), Machine Learning Versus\\nArtificial Intelligence-Machine Learning Versus\\nArtificial Intelligence\\nattention mechanisms, Transformer Encoder-Decoders-\\nTransformer Encoder-Decoders\\nAUC (area under the curve), Accuracy Measures for\\nClassification Models\\naudio classification with CNNs, Audio Classification with\\nCNNs-Audio Classification with CNNs\\nAudioConfig object, The Speech Service\\naugmentation layers, Image Augmentation with Augmentation\\nLayers\\naverage pooling layer, Understanding CNNs'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 597, 'file_type': 'pdf'}, page_content='AWS AI Services, Azure Cognitive Services\\nAzure Cognitive Services APIs, Custom Object Detection,\\nAzure Cognitive Services-Summary\\ncalling APIs, Calling Azure Cognitive Services APIs-\\nCalling Azure Cognitive Services APIs\\nComputer Vision service, Image Classification with\\nConvolutional Neural Networks, Introducing Azure\\nCognitive Services, The Computer Vision Service-The\\nComputer Vision Service\\ncontainers, Azure Cognitive Services Containers-Azure\\nCognitive Services Containers\\nContoso Travel exercise, Putting It All Together:\\nContoso Travel-Putting It All Together: Contoso\\nTravel\\nDecision services, Introducing Azure Cognitive\\nServices\\nkeys and endpoints, Keys and Endpoints-Keys and\\nEndpoints\\nLanguage service, Introducing Azure Cognitive\\nServices, The Language Service-The Language Service\\nSDKs available, Calling Azure Cognitive Services\\nAPIs-Calling Azure Cognitive Services APIs\\nSpeech service, Introducing Azure Cognitive Services,\\nThe Speech Service-The Speech Service\\nsubscription setup, Training a Custom Object\\nDetection Model with the Custom Vision Service-\\nTraining a Custom Object Detection Model with the\\nCustom Vision Service\\nTranslator service, The Translator Service-The\\nTranslator Service\\nAzure Portal, Keys and Endpoints'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 598, 'file_type': 'pdf'}, page_content='AzureError, Calling Azure Cognitive Services APIs\\nB\\n\\nbackpropagation, Training Neural Networks-Training Neural\\nNetworks, Building Neural Networks with Keras and\\nTensorFlow, Dropout\\nbag of words, Text Classification\\nbatch normalization, Pretrained CNNs\\nBayes’ theorem, Naive Bayes\\nbearing failure prediction, anomaly detection, Using PCA\\nto Predict Bearing Failure-Using PCA to Predict Bearing\\nFailure\\nBERT (bidirectional encoder representations from\\ntransformers), Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\nbuilding a question answering system, Building a\\nBERT-Based Question Answering System-Building a BERT-\\nBased Question Answering System\\nsentiment analysis, Fine-Tuning BERT to Perform\\nSentiment Analysis-Fine-Tuning BERT to Perform\\nSentiment Analysis\\nbiases, neural networks, Understanding Neural Networks,\\nUnderstanding Neural Networks, Using a Neural Network to\\nPredict Taxi Fares\\nbidirectional encoder representations from transformers\\n(BERT), Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\nbilingual evaluation understudy (BLEU) scores, Building a\\nTransformer-Based NMT Model'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 599, 'file_type': 'pdf'}, page_content='binary classification models, Binary Classification-\\nDetecting Credit Card Fraud\\ncredit card fraud detection, Detecting Credit Card\\nFraud-Detecting Credit Card Fraud\\nlogistic regression (see logistic regression)\\nwith neural networks, Binary Classification with\\nNeural Networks-Training a Neural Network to Detect\\nCredit Card Fraud\\nTitanic passengers, Classifying Passengers Who Sailed\\non the Titanic-Classifying Passengers Who Sailed on\\nthe Titanic\\nbinary classifiers, in Viola-Jones face detection, Face\\nDetection with Viola-Jones\\nbinary trees, Decision Trees\\nbinary_crossentropy function, Binary Classification with\\nNeural Networks\\nBLEU (bilingual evaluation understudy) scores, Building a\\nTransformer-Based NMT Model\\nBoltzmann machines, Machine Learning Versus Artificial\\nIntelligence\\nboosting, Gradient-Boosting Machines-Gradient-Boosting\\nMachines\\nbottleneck layers, Understanding CNNs, Understanding\\nCNNs, Using Keras and TensorFlow to Build CNNs, Transfer\\nLearning-Transfer Learning\\nbottom Sobel kernel, Understanding CNNs\\nbounding boxes, object detection, R-CNNs\\nROI pooling, R-CNNs\\nRPN, Mask R-CNN, Training a Custom Object Detection\\nModel with the Custom Vision Service'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 600, 'file_type': 'pdf'}, page_content='Viola-Jones, Using the OpenCV Implementation of\\nViola-Jones\\nYOLO, YOLO, YOLOv3 and Keras\\nbreast cancer dataset, Data Normalization, Anonymizing\\nData\\nC\\n\\nC parameter, SVMs, How Support Vector Machines Work,\\nHyperparameter Tuning-Hyperparameter Tuning\\nC#\\nbuilding models with ML.NET, Building ML Models in C#\\nwith ML.NET-Saving and Loading ML.NET Models\\nusing ONNX to work with Python, Using ONNX to Bridge\\nthe Language Gap-Using ONNX to Bridge the Language\\nGap\\nCalifornia Housing Prices dataset, Accuracy Measures for\\nRegression Models\\nCallback class, Keras Callbacks\\ncallbacks, Keras, Keras Callbacks-Keras Callbacks\\nCART (Classification and Regression Tree) algorithm,\\nDecision Trees\\ncascade classifiers, Face Detection with Viola-Jones-\\nUsing the OpenCV Implementation of Viola-Jones\\nCascadeClassifier class, Using the OpenCV Implementation\\nof Viola-Jones-Using the OpenCV Implementation of Viola-\\nJones\\ncategorical data, Categorical Data-Categorical Data\\ncategorical values, Categorical Data'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 601, 'file_type': 'pdf'}, page_content='categorical_crossentropy function, Multiclass\\nClassification with Neural Networks\\ncentroid, cluster, Unsupervised Learning with k-Means\\nClustering\\nClassification and Regression Tree (CART) algorithm,\\nDecision Trees\\nclassification boundary, Supervised Learning\\nclassification models, Supervised Learning-Supervised\\nLearning, Classification Models-Summary\\naccuracy measures for, Classification Models,\\nAccuracy Measures for Classification Models-Accuracy\\nMeasures for Classification Models\\naudio classification, Audio Classification with CNNs-\\nAudio Classification with CNNs\\nbinary classifiers, Binary Classification-Detecting\\nCredit Card Fraud, Binary Classification with Neural\\nNetworks-Training a Neural Network to Detect Credit\\nCard Fraud\\nCNNs, Understanding CNNs, Understanding CNNs,\\nTransfer Learning-Transfer Learning\\ndigit recognition model, Building a Digit Recognition\\nModel-Building a Digit Recognition Model\\nGBMs, Gradient-Boosting Machines-Gradient-Boosting\\nMachines\\nimage (see images, classifying and generating)\\nand k-nearest neighbors, k-Nearest Neighbors, Using\\nk-Nearest Neighbors to Classify Flowers-Using k-\\nNearest Neighbors to Classify Flowers\\nlogistic regression (see logistic regression)\\nMLPs for, Understanding CNNs'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 602, 'file_type': 'pdf'}, page_content='multiclass, Supervised Learning, Using k-Nearest\\nNeighbors to Classify Flowers-Using k-Nearest\\nNeighbors to Classify Flowers, Classification Models,\\nMulticlass Classification-Multiclass Classification,\\nMulticlass Classification with Neural Networks-\\nMulticlass Classification with Neural Networks\\nmultilabel, Classification Models\\nSVMs (see support vector machines)\\ntext (see text classification and processing)\\ncleaning of text for vectorization, Preparing Text for\\nClassification\\nclosed-set versus open-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set Classification-\\nHandling Unknown Faces: Closed-Set Versus Open-Set\\nClassification\\nclustering algorithms, Unsupervised Learning with k-Means\\nClustering\\nagglomerative clustering, Segmenting Customers Using\\nMore Than Two Dimensions\\nDBSCAN, Segmenting Customers Using More Than Two\\nDimensions\\nk-means (see k-means clustering)\\nlabel encoding, Segmenting Customers Using More Than\\nTwo Dimensions-Segmenting Customers Using More Than\\nTwo Dimensions, Categorical Data-Categorical Data\\nCNNs (see convolutional neural networks)\\nCOCO dataset, Mask R-CNN, YOLOv3 and Keras\\ncoefficient of determination (R²), Accuracy Measures for\\nRegression Models, Using a Neural Network to Predict Taxi\\nFares'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 603, 'file_type': 'pdf'}, page_content='collaborative recommender system, Recommender Systems\\nComputer Vision service, Image Classification with\\nConvolutional Neural Networks, Introducing Azure\\nCognitive Services, The Computer Vision Service-The\\nComputer Vision Service\\nComputerVisionClient class, The Computer Vision Service\\nconditional probability, Naive Bayes\\nconfidence value, object detection, YOLOv3 and Keras\\nconfusion matrix, Accuracy Measures for Classification\\nModels, Using SVMs for Facial Recognition-Using SVMs for\\nFacial Recognition\\nConfusionMatrixDisplay class, Accuracy Measures for\\nClassification Models, Spam Filtering\\nconfusion_matrix function, Accuracy Measures for\\nClassification Models\\nconsuming Python model from C# client, Consuming a Python\\nModel from a C# Client-Consuming a Python Model from a C#\\nClient\\ncontainer image, Containerizing a Machine Learning Model\\ncontainer registry, Containerizing a Machine Learning\\nModel\\ncontainerizing an ML model, Operationalizing Machine\\nLearning Models, Containerizing a Machine Learning Model-\\nContainerizing a Machine Learning Model, Azure Cognitive\\nServices Containers-Azure Cognitive Services Containers\\nContent Moderator service, Introducing Azure Cognitive\\nServices\\ncontent-based recommender system, Recommender Systems\\ncontinual learning, Saving and Loading Models'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 604, 'file_type': 'pdf'}, page_content='Contoso Travel exercise, Putting It All Together: Contoso\\nTravel-Putting It All Together: Contoso Travel\\nConv1D class, Factoring Word Order into Predictions\\nConv2D layers, Sizing a Neural Network, Using Keras and\\nTensorFlow to Build CNNs-Using Keras and TensorFlow to\\nBuild CNNs\\nconvolutional kernels (kernels), Understanding CNNs-\\nUnderstanding CNNs\\nconvolutional neural networks (CNNs), Understanding\\nNeural Networks, Image Classification with Convolutional\\nNeural Networks-Summary\\narchitectures, Pretrained CNNs\\narctic wildlife recognition, Training a CNN to\\nRecognize Arctic Wildlife-Training a CNN to Recognize\\nArctic Wildlife, Using Transfer Learning to Identify\\nArctic Wildlife-Using Transfer Learning to Identify\\nArctic Wildlife\\naudio classification, Audio Classification with CNNs-\\nAudio Classification with CNNs\\nbuilding with Keras and TensorFlow, Using Keras and\\nTensorFlow to Build CNNs-Using Keras and TensorFlow\\nto Build CNNs\\nconvolution layers, Understanding CNNs-Understanding\\nCNNs\\ndata augmentation, Data Augmentation-Applying Image\\nAugmentation to Arctic Wildlife\\nface detection, Face Detection with Convolutional\\nNeural Networks-Face Detection with Convolutional\\nNeural Networks, Putting It All Together: Detecting\\nand Recognizing Faces in Photos-Putting It All\\nTogether: Detecting and Recognizing Faces in Photos'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 605, 'file_type': 'pdf'}, page_content='facial recognition, Applying Transfer Learning to\\nFacial Recognition-Putting It All Together: Detecting\\nand Recognizing Faces in Photos\\nGANs, Understanding Neural Networks\\nglobal pooling, Global Pooling-Global Pooling\\nobject detection with R-CNNs, R-CNNs-Mask R-CNN\\npretrained models, Pretrained CNNs-Using ResNet50V2\\nto Classify Images\\ncorr method, Using Regression to Predict Taxi Fares-Using\\nRegression to Predict Taxi Fares\\ncosine similarity, Cosine Similarity-Building a Movie\\nRecommendation System, ArcFace\\nCountVectorizer class, Preparing Text for Classification-\\nPreparing Text for Classification, Sentiment Analysis-\\nSentiment Analysis, Spam Filtering, Recommender Systems,\\nConsuming a Python Model from a Python Client, Text\\nPreparation\\ncovariance matrix, Understanding Principal Component\\nAnalysis\\nCover’s theorem, How Support Vector Machines Work\\ncredit card fraud detection\\nanomaly detection, Using PCA to Detect Credit Card\\nFraud-Using PCA to Detect Credit Card Fraud\\nbinary classification, Detecting Credit Card Fraud-\\nDetecting Credit Card Fraud\\nwith neural network, Training a Neural Network to\\nDetect Credit Card Fraud-Training a Neural Network to\\nDetect Credit Card Fraud\\nPCA, Anonymizing Data, Using PCA to Detect Credit\\nCard Fraud-Using PCA to Detect Credit Card Fraud'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 606, 'file_type': 'pdf'}, page_content='cross-validation, Accuracy Measures for Regression\\nModels-Accuracy Measures for Regression Models, Logistic\\nRegression, Classifying Passengers Who Sailed on the\\nTitanic\\ncross_val_score function, Accuracy Measures for\\nRegression Models\\nCSVLogger class, Keras Callbacks\\nCustom Vision service, Training a Custom Object Detection\\nModel with the Custom Vision Service-Training a Custom\\nObject Detection Model with the Custom Vision Service,\\nIntroducing Azure Cognitive Services\\ncustomer segmentation, Applying k-Means Clustering to\\nCustomer Data-Segmenting Customers Using More Than Two\\nDimensions\\ncustomers dataset, Applying k-Means Clustering to\\nCustomer Data\\nD\\n\\nDarknet, YOLOv3 and Keras\\ndata\\noverfitting of, Decision Trees, Hyperparameter\\nTuning, Building Neural Networks with Keras and\\nTensorFlow\\npreprocessing of (see preprocessing data)\\nshuffling of, Building Neural Networks with Keras and\\nTensorFlow, Text Classification\\ntest set, Using k-Nearest Neighbors to Classify\\nFlowers, Accuracy Measures for Regression Models,\\nAccuracy Measures for Regression Models\\ntime series, Recurrent Neural Networks (RNNs)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 607, 'file_type': 'pdf'}, page_content='training set, Using k-Nearest Neighbors to Classify\\nFlowers\\nunderfitting of, Hyperparameter Tuning, Building\\nNeural Networks with Keras and TensorFlow\\nvisualizing high-dimensional, Visualizing High-\\nDimensional Data-Visualizing High-Dimensional Data\\ndata augmentation, Data Augmentation-Applying Image\\nAugmentation to Arctic Wildlife\\ndata cleaning, Using k-Nearest Neighbors to Classify\\nFlowers, Preparing Text for Classification, Building a\\nTransformer-Based NMT Model\\ndata normalization, Data Normalization-Data Normalization\\nDataFrame, Categorical Data\\nDataset.to_tf_dataset method, Fine-Tuning BERT to Perform\\nSentiment Analysis\\nDatasets library, Hugging Face, Fine-Tuning BERT to\\nPerform Sentiment Analysis\\nDBSCAN (density-based spatial clustering of applications\\nwith noise), Segmenting Customers Using More Than Two\\nDimensions\\ndecision boundaries, How Support Vector Machines Work,\\nKernels, Kernel Tricks-Hyperparameter Tuning\\nDecision services, Introducing Azure Cognitive Services\\ndecision tree stumps, Gradient-Boosting Machines\\ndecision trees, Decision Trees-Decision Trees\\nCART training algorithm, Decision Trees\\nfor classification, Logistic Regression\\nGBMs, Gradient-Boosting Machines-Gradient-Boosting\\nMachines'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 608, 'file_type': 'pdf'}, page_content='and normalization, Data Normalization\\nrandom forests, Random Forests-Random Forests,\\nDetecting Credit Card Fraud, Using PCA to Detect\\nCredit Card Fraud\\nfor regression, Logistic Regression\\nDecisionTreeClassifier class, Decision Trees\\nDecisionTreeRegressor class, Decision Trees\\ndecode_predictions function, YOLOv3 and Keras\\ndeep learning, Machine Learning Versus Artificial\\nIntelligence, Deep Learning-Summary\\n(see also neural networks)\\nDeepset, Building a BERT-Based Question Answering System\\nDense layers, Building Neural Networks with Keras and\\nTensorFlow, Sizing a Neural Network\\ndense vector representation, Using Keras and TensorFlow\\nto Build CNNs\\ndensity-based spatial clustering of applications with\\nnoise (DBSCAN), Segmenting Customers Using More Than Two\\nDimensions\\ndependent decision trees, Gradient-Boosting Machines\\ndepthwise separable convolutions, Pretrained CNNs\\ndescribe_image method, The Computer Vision Service\\ndescribe_image_in_stream method, The Computer Vision\\nService\\ndetectMultiScale, Using the OpenCV Implementation of\\nViola-Jones\\ndigit recognition model, Building a Digit Recognition\\nModel-Building a Digit Recognition Model'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 609, 'file_type': 'pdf'}, page_content='dimensionality reduction\\nand bottleneck layers, Understanding CNNs\\nclassification layers, CNNs, Understanding CNNs,\\nUnderstanding CNNs\\nIsomap, Visualizing High-Dimensional Data\\nPCA (see principal component analysis)\\npooling layers, Understanding CNNs\\nt-SNE, Linear Regression, Visualizing High-\\nDimensional Data\\ndisconnected containers, Azure Cognitive Services, Azure\\nCognitive Services Containers\\nDistilBERT model, Bidirectional Encoder Representations\\nfrom Transformers (BERT)\\ndocker build command, Containerizing a Machine Learning\\nModel\\nDocker container, Operationalizing Machine Learning\\nModels, Containerizing a Machine Learning Model\\ndocument translation, The Translator Service\\ndot product, Using Keras and TensorFlow to Build CNNs\\nDropout layers, Sizing a Neural Network, Dropout-Dropout,\\nGlobal Pooling\\nE\\n\\nEarlyStopping class, Keras Callbacks, Building a\\nTransformer-Based NMT Model\\neigenvectors and eigenvalues, Understanding Principal\\nComponent Analysis'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 610, 'file_type': 'pdf'}, page_content='Elasticsearch, Building a BERT-Based Question Answering\\nSystem\\nelbow method to plot inertias, Unsupervised Learning with\\nk-Means Clustering\\nEmbedding class, Word Embeddings\\nembedding layer, Natural Language Processing, Word\\nEmbeddings-Text Classification\\nencoder–decoder models, LSTM Encoder-Decoders-\\nTransformer Encoder-Decoders\\nensemble learning, random forests, Random Forests-Random\\nForests, Detecting Credit Card Fraud, Using PCA to Detect\\nCredit Card Fraud\\nepochs, Building Neural Networks with Keras and\\nTensorFlow\\nestimators, Pipelining, Consuming a Python Model from a\\nPython Client\\nEuclidean distance, k-Nearest Neighbors\\nExcel, adding ML capabilities to, Adding Machine Learning\\nCapabilities to Excel-Adding Machine Learning\\nCapabilities to Excel\\nexpert systems, Machine Learning Versus Artificial\\nIntelligence\\nexplained variance, plotting, Understanding Principal\\nComponent Analysis-Understanding Principal Component\\nAnalysis\\nextracting faces from photos, Extracting Faces from\\nPhotos-Extracting Faces from Photos\\nextract_key_phrases method, The Language Service\\nF'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 611, 'file_type': 'pdf'}, page_content='F1 score, Accuracy Measures for Classification Models\\nface detection, Face Detection and Recognition-Extracting\\nFaces from Photos\\nwith CNNs, Face Detection with Convolutional Neural\\nNetworks-Face Detection with Convolutional Neural\\nNetworks, Putting It All Together: Detecting and\\nRecognizing Faces in Photos-Putting It All Together:\\nDetecting and Recognizing Faces in Photos\\nComputer Vision service, The Computer Vision Service\\nextracting faces from photos, Extracting Faces from\\nPhotos-Extracting Faces from Photos\\nwith Viola-Jones, Face Detection with Viola-Jones-\\nUsing the OpenCV Implementation of Viola-Jones\\nface embedding, ArcFace\\nFace service, Introducing Azure Cognitive Services, The\\nComputer Vision Service\\nface verification, ArcFace\\nFaceNet, Facial Recognition\\nfacial recognition, Facial Recognition-Handling Unknown\\nFaces: Closed-Set Versus Open-Set Classification\\nclosed-set versus open-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set\\nClassification-Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nwith CNNs, Applying Transfer Learning to Facial\\nRecognition-Putting It All Together: Detecting and\\nRecognizing Faces in Photos\\nneural networks, Training a Neural Network to\\nRecognize Faces-Training a Neural Network to\\nRecognize Faces\\nand privacy, Face Detection and Recognition'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 612, 'file_type': 'pdf'}, page_content='with SVMs, Using SVMs for Facial Recognition-Using\\nSVMs for Facial Recognition\\nwith transfer learning, Applying Transfer Learning to\\nFacial Recognition-Boosting Transfer Learning with\\nTask-Specific Weights\\nfalse positive rate (FPR), Accuracy Measures for\\nClassification Models\\nFast R-CNN, R-CNNs\\nFaster R-CNN, R-CNNs\\nfeature columns, What Is Machine Learning?\\nfeature maps, Understanding CNNs\\nfiltering noise in images, Filtering Noise-Filtering\\nNoise\\nfit method\\nin Keras, Building Neural Networks with Keras and\\nTensorFlow, Keras Callbacks, Image Augmentation with\\nImageDataGenerator\\nin Scikit-Learn, Using k-Nearest Neighbors to\\nClassify Flowers, Accuracy Measures for Regression\\nModels\\nfit_on_texts method, Text Preparation\\nFlask framework, Operationalizing Machine Learning\\nModels, Putting It All Together: Contoso Travel\\nFlatten layer, Global Pooling, Text Classification\\nflow method, Image Augmentation with ImageDataGenerator\\nflow_from_directory method, Image Augmentation with\\nImageDataGenerator\\nfolds, Accuracy Measures for Regression Models'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 613, 'file_type': 'pdf'}, page_content='FPR (false positive rate), Accuracy Measures for\\nClassification Models\\nfully connected layers, Understanding Neural Networks\\nfunctional API, Keras, Neural Networks, Building a\\nTransformer-Based NMT Model-Building a Transformer-Based\\nNMT Model\\nG\\n\\ngamma parameter, SVM kernels, Hyperparameter Tuning-\\nHyperparameter Tuning\\nGANs (generative adversarial networks), Understanding\\nNeural Networks\\ngated recurrent unit (GRU) layer, Recurrent Neural\\nNetworks (RNNs)\\nGBDTs (gradient-boosted decision trees), Gradient-\\nBoosting Machines-Gradient-Boosting Machines\\nGBMs (gradient-boosting machines), Gradient-Boosting\\nMachines-Gradient-Boosting Machines\\ngenerative adversarial networks (GANs), Understanding\\nNeural Networks\\nget_weights method, Saving and Loading Models\\nGini impurity, Decision Trees\\nglobal minimum, Training Neural Networks\\nglobal pooling, Global Pooling-Global Pooling\\nGlobalAveragePooling2D layer, Global Pooling, Audio\\nClassification with CNNs\\nGlobalMaxPooling2D layer, Global Pooling\\nGlorotUniform initializer, Building Neural Networks with\\nKeras and TensorFlow'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 614, 'file_type': 'pdf'}, page_content='GloVe word vectors, Word Embeddings\\nGPUs (graphics processing units), Machine Learning Versus\\nArtificial Intelligence, Deep Learning, Training a CNN to\\nRecognize Arctic Wildlife\\ngradient descent, Training Neural Networks\\ngradient-boosted decision trees (GBDTs), Gradient-\\nBoosting Machines-Gradient-Boosting Machines\\ngradient-boosting machines (GBMs), Gradient-Boosting\\nMachines-Gradient-Boosting Machines\\nGradientBoostingClassifier class, Gradient-Boosting\\nMachines-Gradient-Boosting Machines, Detecting Credit\\nCard Fraud\\nGradientBoostingRegressor class, Gradient-Boosting\\nMachines-Gradient-Boosting Machines, Using Regression to\\nPredict Taxi Fares\\ngraphics processing units (GPUs), Machine Learning Versus\\nArtificial Intelligence, Deep Learning, Training a CNN to\\nRecognize Arctic Wildlife\\nGridSearchCV, Hyperparameter Tuning, Pipelining, Using\\nSVMs for Facial Recognition-Using SVMs for Facial\\nRecognition\\nGRU (gated recurrent unit) layer, Recurrent Neural\\nNetworks (RNNs)\\nGRU class, Recurrent Neural Networks (RNNs)\\nH\\n\\nH2O framework, Building ML Models in C# with ML.NET,\\nSentiment Analysis with ML.NET\\nH5 file format, Saving and Loading Models\\nHaar-like features, Face Detection with Viola-Jones'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 615, 'file_type': 'pdf'}, page_content='handwritten text\\nbuilding digit recognition model, Building a Digit\\nRecognition Model-Building a Digit Recognition Model\\nextracting from image, The Computer Vision Service-\\nThe Computer Vision Service\\nHashingVectorizer class, Preparing Text for\\nClassification, Preparing Text for Classification,\\nConsuming a Python Model from a Python Client\\nHaystack library, Building a BERT-Based Question\\nAnswering System\\nhidden layers, Understanding Neural Networks,\\nUnderstanding Neural Networks, Building Neural Networks\\nwith Keras and TensorFlow\\nhidden state, layer, Recurrent Neural Networks (RNNs)\\nhigh recall then precision pattern, Face Detection with\\nViola-Jones\\nhigh-dimensional data visualization, Visualizing High-\\nDimensional Data-Visualizing High-Dimensional Data\\nHugging Face, Using Pretrained Models to Classify Text,\\nUsing Pretrained Models to Translate Text, Building a\\nBERT-Based Question Answering System-Fine-Tuning BERT to\\nPerform Sentiment Analysis\\nhyperparameter tuning, Hyperparameter Tuning-\\nHyperparameter Tuning\\nI\\n\\nIDataView, Building ML Models in C# with ML.NET,\\nSentiment Analysis with ML.NET\\nImageDataGenerator, Image Augmentation with\\nImageDataGenerator-Image Augmentation with'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 616, 'file_type': 'pdf'}, page_content='ImageDataGenerator\\nImageNet dataset, Pretrained CNNs\\nimages, classifying and generating, What Is Machine\\nLearning?-What Is Machine Learning?\\narctic wildlife recognition, Applying Image\\nAugmentation to Arctic Wildlife-Applying Image\\nAugmentation to Arctic Wildlife\\nwith augmentation layers, Image Augmentation with\\nAugmentation Layers\\ncaption generating, Introducing Azure Cognitive\\nServices, The Computer Vision Service-The Computer\\nVision Service\\nCNNs (see convolutional neural networks)\\nextracting text from photos, The Computer Vision\\nService\\nface detection (see face detection)\\nfacial recognition (see facial recognition)\\ngenerating with GANs, Understanding Neural Networks\\nwith k-nearest neighbors, Using k-Nearest Neighbors\\nto Classify Flowers-Using k-Nearest Neighbors to\\nClassify Flowers\\nobject detection (see object detection)\\ntuning hyperparameters, Hyperparameter Tuning-\\nHyperparameter Tuning\\nIMDB movie dataset, Sentiment Analysis, Using\\nTextVectorization in a Sentiment Analysis Model, Fine-\\nTuning BERT to Perform Sentiment Analysis\\nimpurity measures, Decision Trees\\nimputing missing values, Classifying Passengers Who\\nSailed on the Titanic'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 617, 'file_type': 'pdf'}, page_content='Inception, Pretrained CNNs\\nincremental training, Saving and Loading Models\\ninertias, plotting, Unsupervised Learning with k-Means\\nClustering\\nInferenceSession method, ONNX, Using ONNX to Bridge the\\nLanguage Gap\\nInput class, ML.NET, Sentiment Analysis with ML.NET\\ninput layer, neural network, Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow\\ninstance segmentation, Mask R-CNN-Mask R-CNN\\nintegral images, Face Detection with Viola-Jones\\nIntellipix, Introducing Azure Cognitive Services\\nintersection-over-union (IoU) score, R-CNNs\\ninverse_transform method, Preparing Text for\\nClassification, Understanding Principal Component\\nAnalysis\\nIris dataset, Using k-Nearest Neighbors to Classify\\nFlowers, Linear Regression\\nisolation forest, Anomaly Detection\\nIsomap, Visualizing High-Dimensional Data\\nJ\\n\\nJSON encoding or decoding, Calling Azure Cognitive\\nServices APIs\\nJupyter, Running the Book’s Code Samples\\nK'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 618, 'file_type': 'pdf'}, page_content='k-fold cross-validation, Accuracy Measures for Regression\\nModels-Accuracy Measures for Regression Models\\nk-means clustering, Unsupervised Learning with k-Means\\nClustering-Segmenting Customers Using More Than Two\\nDimensions\\ncustomer data, applying to, Applying k-Means\\nClustering to Customer Data-Segmenting Customers\\nUsing More Than Two Dimensions\\nsegmenting customer data, Segmenting Customers Using\\nMore Than Two Dimensions-Segmenting Customers Using\\nMore Than Two Dimensions\\nsupervised learning, Supervised Learning-Using k-\\nNearest Neighbors to Classify Flowers\\nk-nearest neighbors, k-Nearest Neighbors-Using k-Nearest\\nNeighbors to Classify Flowers, Data Normalization\\nKeras API, Training Neural Networks, Neural Networks\\nbuilding neural networks with, Building Neural\\nNetworks with Keras and TensorFlow-Using a Neural\\nNetwork to Predict Taxi Fares\\ncallbacks, Keras Callbacks-Keras Callbacks\\nCNNs, Using Keras and TensorFlow to Build CNNs-Using\\nKeras and TensorFlow to Build CNNs\\nfunctional API, Neural Networks, Building a\\nTransformer-Based NMT Model-Building a Transformer-\\nBased NMT Model\\nLSTM-based models, LSTM Encoder-Decoders-LSTM\\nEncoder-Decoders\\nMobileNetV2, Pretrained CNNs\\npretrained CNN models, Pretrained CNNs-Using\\nResNet50V2 to Classify Images'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 619, 'file_type': 'pdf'}, page_content='saving models, Saving and Loading Models, Using\\nTextVectorization in a Sentiment Analysis Model\\nsequential API, Neural Networks-Building Neural\\nNetworks with Keras and TensorFlow\\ntensorflow.keras.callbacks.CSVLogger, Keras Callbacks\\ntensorflow.keras.callbacks.EarlyStopping, Keras\\nCallbacks, Building a Transformer-Based NMT Model\\ntensorflow.keras.callbacks.LearningRateScheduler,\\nKeras Callbacks\\ntensorflow.keras.callbacks.ModelCheckpoint, Keras\\nCallbacks\\ntensorflow.keras.callbacks.TensorBoard, Keras\\nCallbacks\\ntensorflow.keras.layers.Conv1D, Factoring Word Order\\ninto Predictions\\ntensorflow.keras.layers.Conv2D, Sizing a Neural\\nNetwork, Using Keras and TensorFlow to Build CNNs-\\nUsing Keras and TensorFlow to Build CNNs\\ntensorflow.keras.layers.Dense, Building Neural\\nNetworks with Keras and TensorFlow, Using a Neural\\nNetwork to Predict Taxi Fares\\ntensorflow.keras.layers.Dropout, Sizing a Neural\\nNetwork, Dropout-Dropout, Global Pooling\\ntensorflow.keras.layers.Embedding, Word Embeddings\\ntensorflow.keras.layers.Flatten, Global Pooling, Text\\nClassification\\ntensorflow.keras.layers.GlobalAveragePooling2D,\\nGlobal Pooling, Audio Classification with CNNs\\ntensorflow.keras.layers.GlobalMaxPooling2D, Global\\nPooling-Global Pooling'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 620, 'file_type': 'pdf'}, page_content='tensorflow.keras.layers.GRU, Recurrent Neural\\nNetworks (RNNs)\\ntensorflow.keras.layers.Input, Automating Text\\nVectorization\\ntensorflow.keras.layers.LSTM, Recurrent Neural\\nNetworks (RNNs)\\ntensorflow.keras.layers.MaxPooling1D, Factoring Word\\nOrder into Predictions\\ntensorflow.keras.layers.MaxPooling2D, Using Keras and\\nTensorFlow to Build CNNs-Using Keras and TensorFlow\\nto Build CNNs\\ntensorflow.keras.layers.MultiHeadAttention, Building\\na Transformer-Based NMT Model\\ntensorflow.keras.layers.Rescaling, Image Augmentation\\nwith Augmentation Layers\\ntensorflow.keras.layers.TextVectorization, Text\\nPreparation, Automating Text Vectorization-Using\\nTextVectorization in a Sentiment Analysis Model\\ntensorflow.keras.losses.binary_crossentropy, Binary\\nClassification with Neural Networks\\ntensorflow.keras.losses.categorical_crossentropy,\\nMulticlass Classification with Neural Networks\\ntensorflow.keras.losses.sparse_categorical_crossentro\\npy, Multiclass Classification with Neural Networks,\\nTraining a Neural Network to Recognize Faces\\ntensorflow.keras.models.load_model, Saving and\\nLoading Models\\ntensorflow.keras.models.Sequential, Building Neural\\nNetworks with Keras and TensorFlow, Using a Neural\\nNetwork to Predict Taxi Fares'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 621, 'file_type': 'pdf'}, page_content='tensorflow.keras.preprocessing.image.ImageDataGenerat\\nor, Image Augmentation with ImageDataGenerator-Image\\nAugmentation with ImageDataGenerator\\ntensorflow.keras.preprocessing.sequence.pad_sequences\\n, Text Preparation\\ntensorflow.keras.preprocessing.text.Tokenizer, Text\\nPreparation-Text Preparation, Automating Text\\nVectorization, Building a Transformer-Based NMT Model\\ntensorflow.keras.utils.to_categorical, Multiclass\\nClassification with Neural Networks\\ntransformer-based NMT model, Building a Transformer-\\nBased NMT Model-Building a Transformer-Based NMT\\nModel\\nand YOLO, YOLOv3 and Keras-YOLOv3 and Keras\\nKerasNLP, Building a Transformer-Based NMT Model-Building\\na Transformer-Based NMT Model\\nkernel tricks, Support Vector Machines, Kernel Tricks-\\nKernel Tricks\\nkernels (convolution kernels), Understanding CNNs-\\nUnderstanding CNNs\\nkernels, SVM, Kernels-Kernel Tricks\\nkeyword extraction, Preparing Text for Classification\\nKMeans class, Unsupervised Learning with k-Means\\nClustering\\nKNeighborsClassifier class, Using k-Nearest Neighbors to\\nClassify Flowers-Using k-Nearest Neighbors to Classify\\nFlowers\\nKNeighborsRegressor class, Using k-Nearest Neighbors to\\nClassify Flowers\\nKubernetes, Containerizing a Machine Learning Model'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 622, 'file_type': 'pdf'}, page_content='L\\n\\nL-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–\\nShanno) algorithm, Logistic Regression\\nlabel column, What Is Machine Learning?\\nlabeled data, Supervised Versus Unsupervised Learning,\\nSupervised Learning\\nLabeled Faces in the Wild (LFW) dataset, What Is Machine\\nLearning?, Using SVMs for Facial Recognition,\\nUnderstanding Principal Component Analysis, Training a\\nNeural Network to Recognize Faces, Facial Recognition\\nLabelEncoder class, Categorical Data\\nlabels\\nand classification models, Classification Models\\nencoding in clustering, Segmenting Customers Using\\nMore Than Two Dimensions-Segmenting Customers Using\\nMore Than Two Dimensions, Categorical Data-\\nCategorical Data\\nsentiment analysis dataset as labeled, Sentiment\\nAnalysis\\nunlabeled dataset with PCA, Anomaly Detection, Using\\nPCA to Predict Bearing Failure\\nlanguage models (see natural language processing)\\nLanguage service, Introducing Azure Cognitive Services,\\nThe Language Service-The Language Service\\nLaplace smoothing, Naive Bayes\\nLasso class, Linear Regression\\nlazy learning algorithm, Using k-Nearest Neighbors to\\nClassify Flowers'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 623, 'file_type': 'pdf'}, page_content='leaf node, decision tree, Decision Trees\\nlearning algorithm, Machine Learning\\nlearning rates, Gradient-Boosting Machines, Training\\nNeural Networks\\nLearningRateScheduler class, Keras Callbacks\\nlemmatizing, Preparing Text for Classification\\nLFW (Labeled Faces in the Wild) dataset, What Is Machine\\nLearning?, Using SVMs for Facial Recognition,\\nUnderstanding Principal Component Analysis, Training a\\nNeural Network to Recognize Faces, Facial Recognition\\nLibrosa package, Audio Classification with CNNs\\nLimited-memory Broyden–Fletcher–Goldfarb–Shanno (L-\\nBFGS) algorithm, Logistic Regression\\nlinear kernel, Kernels, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nlinear regression, Linear Regression-Linear Regression,\\nUnderstanding Neural Networks\\nLinearRegression class, Linear Regression, Using\\nRegression to Predict Taxi Fares\\nLinearSVC class, Using SVMs for Facial Recognition\\nLoadFromTextFile method, ML.NET, Sentiment Analysis with\\nML.NET\\nload_iris function, Using k-Nearest Neighbors to Classify\\nFlowers\\nlocal outlier factor (LOF), Anomaly Detection\\nlogistic function, Logistic Regression\\nlogistic regression, Logistic Regression-Logistic\\nRegression'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 624, 'file_type': 'pdf'}, page_content='credit card fraud illustration, Detecting Credit Card\\nFraud-Detecting Credit Card Fraud\\ndigit recognition model, Building a Digit Recognition\\nModel-Building a Digit Recognition Model\\nmultinomial, Multiclass Classification\\nsentiment analysis, Sentiment Analysis-Sentiment\\nAnalysis\\nTitanic passenger survival example, Classifying\\nPassengers Who Sailed on the Titanic-Classifying\\nPassengers Who Sailed on the Titanic\\nLogisticRegression class, Logistic Regression\\nLogisticRegressionCV class, Logistic Regression\\nlogit function, Logistic Regression\\nlong short-term memory (LSTM) cell, Recurrent Neural\\nNetworks (RNNs)\\nloss functions, Training Neural Networks, Multiclass\\nClassification with Neural Networks\\nloss landscape, Training Neural Networks\\nloss parameter, Building Neural Networks with Keras and\\nTensorFlow\\nLSTM (long short-term memory) cell, Recurrent Neural\\nNetworks (RNNs)\\nLSTM class, Recurrent Neural Networks (RNNs)\\nLSTM encoder-decoders, LSTM Encoder-Decoders-LSTM\\nEncoder-Decoders\\nM\\n\\nmachine learning (ML), Machine Learning-Summary'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 625, 'file_type': 'pdf'}, page_content='versus artificial intelligence, Machine Learning\\nVersus Artificial Intelligence-Machine Learning\\nVersus Artificial Intelligence\\noperationalizing (see operationalizing ML models)\\nprogramming resources, Running the Book’s Code\\nSamples\\nspam filter example, What Is Machine Learning?\\nsupervised models (see supervised learning models)\\nand testing data, Using k-Nearest Neighbors to\\nClassify Flowers\\ntypes of learning, Supervised Versus Unsupervised\\nLearning-Using k-Nearest Neighbors to Classify\\nFlowers\\nunsupervised models (see unsupervised learning\\nmodels)\\nMAE (mean absolute error), Building Neural Networks with\\nKeras and TensorFlow, Building Neural Networks with Keras\\nand TensorFlow\\nmake_blobs function, Unsupervised Learning with k-Means\\nClustering\\nmake_pipeline function, Pipelining, Consuming a Python\\nModel from a Python Client\\nManhattan distance, Using k-Nearest Neighbors to Classify\\nFlowers\\nmAP (mean Average Precision), Training a Custom Object\\nDetection Model with the Custom Vision Service\\nMask R-CNN, Mask R-CNN-Mask R-CNN\\nmasked language modeling, Bidirectional Encoder\\nRepresentations from Transformers (BERT)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 626, 'file_type': 'pdf'}, page_content='Matplotlib, Unsupervised Learning with k-Means\\nClustering, Visualizing High-Dimensional Data\\nmax pooling layer, Understanding CNNs, Using Keras and\\nTensorFlow to Build CNNs\\nMaxPooling1D class, Factoring Word Order into Predictions\\nMaxPooling2D class, Using Keras and TensorFlow to Build\\nCNNs\\nmax_df parameter, Preparing Text for Classification\\nMBGD (mini-batch gradient descent), Training Neural\\nNetworks\\nmean absolute error (MAE), Building Neural Networks with\\nKeras and TensorFlow, Building Neural Networks with Keras\\nand TensorFlow\\nmean Average Precision (mAP), Training a Custom Object\\nDetection Model with the Custom Vision Service\\nmean squared error (MSE), Linear Regression, Building\\nNeural Networks with Keras and TensorFlow\\nmetrics parameter, Building Neural Networks with Keras\\nand TensorFlow\\nMHA (multi-head attention), Transformer Encoder-Decoders\\nMicrosoft.ML.OnnxRuntime, Using ONNX to Bridge the\\nLanguage Gap\\nmini-batch gradient descent (MBGD), Training Neural\\nNetworks\\nMiniLM models, Building a BERT-Based Question Answering\\nSystem-Building a BERT-Based Question Answering System\\nMinkowski distance, Using k-Nearest Neighbors to Classify\\nFlowers\\nMinMaxScaler, Data Normalization, Data Normalization'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 627, 'file_type': 'pdf'}, page_content='minNeighbors parameter, Using the OpenCV Implementation\\nof Viola-Jones\\nmin_df parameter, Preparing Text for Classification\\nML (see machine learning)\\nML Operations (MLOps), Versioning Pickle Files\\nML.NET, building models in C#, Building ML Models in C#\\nwith ML.NET-Saving and Loading ML.NET Models\\nMLContext class, Sentiment Analysis with ML.NET\\nMLPClassifier class, Understanding Neural Networks\\nMLPRegressor class, Understanding Neural Networks\\nMLPs (multilayer perceptrons), Understanding Neural\\nNetworks, Understanding CNNs\\nMNIST dataset, Using Keras and TensorFlow to Build CNNs,\\nTraining a CNN to Recognize Arctic Wildlife, Global\\nPooling\\nMobiFace, Facial Recognition\\nMobileNetV2, Pretrained CNNs, Audio Classification with\\nCNNs\\nModelCheckpoint class, Keras Callbacks\\nmovie recommendations, Building a Movie Recommendation\\nSystem-Building a Movie Recommendation System\\nMplot3D, Visualizing High-Dimensional Data\\nMSE (mean squared error), Linear Regression, Building\\nNeural Networks with Keras and TensorFlow\\nMTCNNs (multitask cascaded convolutional neural\\nnetworks), Face Detection with Convolutional Neural\\nNetworks-Face Detection with Convolutional Neural\\nNetworks, Putting It All Together: Detecting and\\nRecognizing Faces in Photos-Putting It All Together:\\nDetecting and Recognizing Faces in Photos'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 628, 'file_type': 'pdf'}, page_content='multi-head attention (MHA), Transformer Encoder-Decoders\\nmulticlass classification models, Supervised Learning,\\nUsing k-Nearest Neighbors to Classify Flowers-Using k-\\nNearest Neighbors to Classify Flowers, Classification\\nModels, Multiclass Classification-Multiclass\\nClassification, Multiclass Classification with Neural\\nNetworks-Multiclass Classification with Neural Networks\\nmulticollinearity, Linear Regression\\nMultiHeadAttention class, Building a Transformer-Based\\nNMT Model\\nmultilabel classification models, Classification Models\\nmultilayer perceptrons (MLPs), Understanding Neural\\nNetworks, Understanding CNNs\\nmultinomial logistic regression, Multiclass\\nClassification\\nMultinomialNB class, Naive Bayes, Spam Filtering\\nmultiple linear regression, Linear Regression\\nmultitask cascaded convolutional neural networks\\n(MTCNNs), Face Detection with Convolutional Neural\\nNetworks-Face Detection with Convolutional Neural\\nNetworks, Putting It All Together: Detecting and\\nRecognizing Faces in Photos-Putting It All Together:\\nDetecting and Recognizing Faces in Photos\\nmultivariate anomaly detection, Multivariate Anomaly\\nDetection\\nN\\n\\nn-grams, Preparing Text for Classification, Factoring\\nWord Order into Predictions\\nNaive Bayes learning algorithm, Naive Bayes-Naive Bayes'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 629, 'file_type': 'pdf'}, page_content='named-entity recognition, The Language Service\\nnatural language processing (NLP), Natural Language\\nProcessing-Summary\\nand AI as a service, Introducing Azure Cognitive\\nServices\\nAzure Language service, Introducing Azure Cognitive\\nServices, The Language Service-The Language Service\\nAzure Speech service, Introducing Azure Cognitive\\nServices, The Speech Service-The Speech Service\\nAzure Translator service, Introducing Azure Cognitive\\nServices, The Translator Service-The Translator\\nService\\nBERT, Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform\\nSentiment Analysis\\ncombining with computer vision, Image Classification\\nwith Convolutional Neural Networks\\nencoder-decoder network for machine translation, LSTM\\nEncoder-Decoders-Transformer Encoder-Decoders\\nextracting text from photos, The Computer Vision\\nService\\nneural machine translation, Neural Machine\\nTranslation-Using Pretrained Models to Translate Text\\nsentiment analysis (see sentiment analysis)\\ntext classification (see text classification and\\nprocessing)\\ntext preparation, Text Preparation-Text Preparation\\nword embeddings, Natural Language Processing, Word\\nEmbeddings-Word Embeddings, Recurrent Neural Networks\\n(RNNs)-Recurrent Neural Networks (RNNs)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 630, 'file_type': 'pdf'}, page_content='Natural Language Toolkit (NLTK), Preparing Text for\\nClassification, Text Preparation\\nneural machine translation (NMT), Neural Machine\\nTranslation-Using Pretrained Models to Translate Text\\nbuilding a transformer-based model, Building a\\nTransformer-Based NMT Model-Building a Transformer-\\nBased NMT Model\\nLSTM encoder-decoders, LSTM Encoder-Decoders-LSTM\\nEncoder-Decoders\\ntransformer encoder-decoders, Transformer Encoder-\\nDecoders-Transformer Encoder-Decoders\\nneural networks, Deep Learning-Understanding Neural\\nNetworks, Neural Networks-Summary\\nbackpropagation, Training Neural Networks-Training\\nNeural Networks, Building Neural Networks with Keras\\nand TensorFlow, Dropout\\nbinary classification with, Binary Classification\\nwith Neural Networks-Training a Neural Network to\\nDetect Credit Card Fraud\\ndropout, Sizing a Neural Network, Dropout-Dropout,\\nGlobal Pooling\\nfacial recognition, Training a Neural Network to\\nRecognize Faces-Training a Neural Network to\\nRecognize Faces\\nKeras callbacks, Keras Callbacks-Keras Callbacks\\nmulticlass classification with, Multiclass\\nClassification with Neural Networks-Multiclass\\nClassification with Neural Networks\\nmultilayer perceptrons, Understanding Neural\\nNetworks, Understanding CNNs'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 631, 'file_type': 'pdf'}, page_content='saving and loading models, Saving and Loading Models-\\nSaving and Loading Models\\nsizing, Sizing a Neural Network\\ntaxi fare prediction, Using a Neural Network to\\nPredict Taxi Fares-Using a Neural Network to Predict\\nTaxi Fares\\ntraining, Training Neural Networks-Training Neural\\nNetworks\\nNimbusML, Building ML Models in C# with ML.NET\\nNLTK (Natural Language Toolkit), Preparing Text for\\nClassification, Text Preparation\\nNMS (non-maximum suppression), R-CNNs\\nNMT (see neural machine translation)\\nnoise, filtering, Filtering Noise-Filtering Noise\\nnon-maximum suppression (NMS), R-CNNs\\nnonparametric models, Linear Regression, Random Forests-\\nGradient-Boosting Machines, Detecting Credit Card Fraud,\\nUsing PCA to Detect Credit Card Fraud\\n(see also decision trees)\\nnormalization, Linear Regression, Data Normalization-Data\\nNormalization, Training a Neural Network to Recognize\\nFaces\\nNumPy, Using Keras and TensorFlow to Build CNNs\\nNumPy arrays, Using ONNX to Bridge the Language Gap\\nNuSVC class, Using SVMs for Facial Recognition\\nNvidia GPU card, Deep Learning, Training a CNN to\\nRecognize Arctic Wildlife\\nO'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 632, 'file_type': 'pdf'}, page_content='object detection, Object Detection-Summary\\nbounding boxes, Using the OpenCV Implementation of\\nViola-Jones, R-CNNs, R-CNNs, Mask R-CNN, YOLO,\\nTraining a Custom Object Detection Model with the\\nCustom Vision Service\\nComputer Vision service, The Computer Vision Service-\\nThe Computer Vision Service\\ncustom, Custom Object Detection-Using the Exported\\nModel\\nfaces (see face detection)\\nR-CNNs, R-CNNs-Mask R-CNN\\nYOLO, YOLO-YOLOv3 and Keras\\nobjectness score, R-CNNs\\nocclusions, Training a Custom Object Detection Model with\\nthe Custom Vision Service\\nOLS (ordinary least squares) regression, Linear\\nRegression\\none-class SVM, Anomaly Detection\\none-hot encoding, Categorical Data\\none-versus-all strategy, Multiclass Classification\\none-versus-one strategy, Multiclass Classification\\none-versus-rest strategy, Multiclass Classification\\nOneHotEncoder class, Categorical Data\\nONNX (Open Neural Network Exchange), Operationalizing\\nMachine Learning Models, Using ONNX to Bridge the\\nLanguage Gap-Using ONNX to Bridge the Language Gap, Mask\\nR-CNN-Mask R-CNN\\nopen-set versus closed-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set Classification-'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 633, 'file_type': 'pdf'}, page_content='Handling Unknown Faces: Closed-Set Versus Open-Set\\nClassification\\nOpenCV library, Using the OpenCV Implementation of Viola-\\nJones-Using the OpenCV Implementation of Viola-Jones\\nopenmax output layer, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\noperationalizing ML models, Operationalizing Machine\\nLearning Models-Summary\\nadding ML capabilities to Excel, Adding Machine\\nLearning Capabilities to Excel-Adding Machine\\nLearning Capabilities to Excel\\nbuilding models in C# with ML.NET, Building ML Models\\nin C# with ML.NET-Saving and Loading ML.NET Models\\nconsuming Python model from C# client, Consuming a\\nPython Model from a C# Client-Consuming a Python\\nModel from a C# Client\\nconsuming Python model from Python client, Consuming\\na Python Model from a Python Client-Consuming a\\nPython Model from a Python Client\\ncontainerizing an ML model, Containerizing a Machine\\nLearning Model-Containerizing a Machine Learning\\nModel\\nONNX to bridge language gap, Operationalizing Machine\\nLearning Models, Using ONNX to Bridge the Language\\nGap-Using ONNX to Bridge the Language Gap\\nversioning pickle files, Versioning Pickle Files\\nOptical Recognition of Handwritten Digits dataset,\\nBuilding a Digit Recognition Model, Visualizing High-\\nDimensional Data\\noptimizers, Training Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 634, 'file_type': 'pdf'}, page_content='ordinary least squares (OLS) regression, Linear\\nRegression\\noutlier detection (see anomaly detection)\\noutliers, Segmenting Customers Using More Than Two\\nDimensions, Linear Regression, Using Regression to\\nPredict Taxi Fares, Anomaly Detection\\nOutput class, ML.NET, Sentiment Analysis with ML.NET\\noutput layer, neural network, Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow, Multiclass\\nClassification with Neural Networks\\nOutput Network (O-Net), Face Detection with Convolutional\\nNeural Networks\\noverfitting of data, Decision Trees, Hyperparameter\\nTuning, Building Neural Networks with Keras and\\nTensorFlow\\nP\\n\\npad_sequences function, Text Preparation, Automating Text\\nVectorization\\npair plots, Linear Regression\\npairplot function, Linear Regression\\nparallelism\\nand random forests, Random Forests\\nregion proposal network, R-CNNs\\nparametric learning algorithm, Linear Regression\\n(see also support vector machines)\\nPCA (see principal component analysis)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 635, 'file_type': 'pdf'}, page_content='PCA transform, Understanding Principal Component Analysis\\nPerceptron class, Understanding Neural Networks\\nPersonalizer service, Introducing Azure Cognitive\\nServices\\npersonally identifiable information (PII), The Language\\nService\\npickle files, versioning, Versioning Pickle Files\\npickle module, Consuming a Python Model from a Python\\nClient\\npipelines\\nSVMs, Pipelining-Pipelining\\ntraining and saving, Consuming a Python Model from a\\nPython Client-Consuming a Python Model from a Python\\nClient\\nplot_confusion_matrix function, Accuracy Measures for\\nClassification Models\\npolynomial kernel, Kernels, Hyperparameter Tuning, Using\\nSVMs for Facial Recognition\\nPolynomialFeatures class, Linear Regression\\npooling layers, Understanding CNNs\\npopularity-based recommender system, Recommender Systems\\npositional encoding (positional embedding), Transformer\\nEncoder-Decoders\\nPower BI, Accuracy Measures for Classification Models\\nprecision and recall, classifier metrics, Accuracy\\nMeasures for Classification Models-Accuracy Measures for\\nClassification Models, Classifying Passengers Who Sailed\\non the Titanic'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 636, 'file_type': 'pdf'}, page_content='predict method, Using k-Nearest Neighbors to Classify\\nFlowers, Logistic Regression, Spam Filtering, Building\\nNeural Networks with Keras and TensorFlow\\npredictions, What Is Machine Learning?\\naccuracy measures, Training a Neural Network to\\nDetect Credit Card Fraud-Training a Neural Network to\\nDetect Credit Card Fraud, Training a CNN to Recognize\\nArctic Wildlife\\nanomaly detection, Using PCA to Predict Bearing\\nFailure-Using PCA to Predict Bearing Failure\\nconfusion matrix, Accuracy Measures for\\nClassification Models, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\ndecision trees, Decision Trees-Decision Trees\\nk-nearest neighbors for image classification, Using\\nk-Nearest Neighbors to Classify Flowers\\nwith neural network, Making Predictions-Making\\nPredictions\\npipelining, Pipelining\\nregression models, Using Regression to Predict Taxi\\nFares-Using Regression to Predict Taxi Fares\\ntaxi fares, Using Regression to Predict Taxi Fares-\\nUsing Regression to Predict Taxi Fares, Using a\\nNeural Network to Predict Taxi Fares-Using a Neural\\nNetwork to Predict Taxi Fares\\nword order in, Factoring Word Order into Predictions-\\nFactoring Word Order into Predictions\\npredictors, Gradient-Boosting Machines\\npredict_proba method, Logistic Regression, Sentiment\\nAnalysis, Spam Filtering\\npreprocess function, Mask R-CNN, Mask R-CNN'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 637, 'file_type': 'pdf'}, page_content='preprocessing data\\nimage preprocessing layers, Image Augmentation with\\nImageDataGenerator-Image Augmentation with\\nImageDataGenerator\\nLabelEncoder, Segmenting Customers Using More Than\\nTwo Dimensions, Categorical Data\\nMask R-CNN, Mask R-CNN\\nMinMaxScaler, Data Normalization\\nStandardScaler, Data Normalization, Data\\nNormalization, Pipelining, Training a Neural Network\\nto Recognize Faces\\ntext sequences, Text Preparation\\npreprocessor parameter, Preparing Text for Classification\\npretraining and pretrained models, Pretrained CNNs-Using\\nResNet50V2 to Classify Images\\nArcFace, ArcFace\\nBERT, Bidirectional Encoder Representations from\\nTransformers (BERT)-Fine-Tuning BERT to Perform\\nSentiment Analysis\\ntext classification with, Using Pretrained Models to\\nClassify Text\\nVGGFace, Boosting Transfer Learning with Task-\\nSpecific Weights-Boosting Transfer Learning with\\nTask-Specific Weights\\nword embeddings, Word Embeddings\\nprimary principal component, Understanding Principal\\nComponent Analysis\\nprincipal component analysis (PCA), Linear Regression,\\nDetecting Credit Card Fraud, Principal Component\\nAnalysis-Summary'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 638, 'file_type': 'pdf'}, page_content='anomaly detection, Anomaly Detection-Multivariate\\nAnomaly Detection\\nanonymizing data, Anonymizing Data-Anonymizing Data\\ncredit card fraud detection, Anonymizing Data, Using\\nPCA to Detect Credit Card Fraud-Using PCA to Detect\\nCredit Card Fraud\\nfiltering noise, Filtering Noise-Filtering Noise\\nhigh-dimensional data visualization, Visualizing\\nHigh-Dimensional Data-Visualizing High-Dimensional\\nData\\nprivacy issue, and facial recognition, Face Detection and\\nRecognition\\nprobabilities, estimating, What Is Machine Learning?\\nProposal Network (P-Net), Face Detection with\\nConvolutional Neural Networks\\nPython, Running the Book’s Code Samples\\n(see also Scikit-Learn)\\nAzure Cognitive Services packages, Calling Azure\\nCognitive Services APIs, The Computer Vision Service\\nconsuming Python model from C# client, Consuming a\\nPython Model from a C# Client-Consuming a Python\\nModel from a C# Client\\nconsuming Python model from Python client, Consuming\\na Python Model from a Python Client-Consuming a\\nPython Model from a Python Client\\nenvironment setup, Running the Book’s Code Samples\\nExcel UDFs converted to, Adding Machine Learning\\nCapabilities to Excel\\noperationalizing machine learning models,\\nOperationalizing Machine Learning Models'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 639, 'file_type': 'pdf'}, page_content='Q\\n\\nquestion-answering modules, Building a BERT-Based\\nQuestion Answering System-Building a BERT-Based Question\\nAnswering System\\nR\\n\\nR-CNNs (region-based CNNs), R-CNNs-Mask R-CNN\\nR² (coefficient of determination), Accuracy Measures for\\nRegression Models, Using a Neural Network to Predict Taxi\\nFares\\nradius neighbors, Using k-Nearest Neighbors to Classify\\nFlowers\\nRadiusNeighborsClassifier class, Using k-Nearest\\nNeighbors to Classify Flowers\\nRadiusNeighborsRegressor class, Using k-Nearest Neighbors\\nto Classify Flowers\\nRainforest Connection, Audio Classification with CNNs\\nrainforest sounds dataset, Audio Classification with CNNs\\nrandom forests, Random Forests-Random Forests, Detecting\\nCredit Card Fraud, Using PCA to Detect Credit Card Fraud\\nRandomForestClassifier class, Random Forests, Detecting\\nCredit Card Fraud, Detecting Credit Card Fraud\\nRandomForestRegressor class, Random Forests, Using\\nRegression to Predict Taxi Fares\\nrandom_state parameter, Unsupervised Learning with k-\\nMeans Clustering, Accuracy Measures for Regression Models\\nRBF kernel, Kernels, Hyperparameter Tuning-Hyperparameter\\nTuning, Using SVMs for Facial Recognition'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 640, 'file_type': 'pdf'}, page_content='RBMT (rules-based machine translation), Neural Machine\\nTranslation\\nread_in_stream method, The Computer Vision Service\\nrecall and precision classifier metrics, Accuracy\\nMeasures for Classification Models-Accuracy Measures for\\nClassification Models, Classifying Passengers Who Sailed\\non the Titanic\\nrecall_score, Accuracy Measures for Classification Models\\nreceiver operating characteristic (ROC) curve, Accuracy\\nMeasures for Classification Models\\nrecognize_once_async method, The Speech Service\\nrecognize_pii_entities method, The Language Service\\nrecognize_printed_text_in_stream method, The Computer\\nVision Service\\nrecommender systems, Recommender Systems-Building a Movie\\nRecommendation System\\nreconstruction error, Anomaly Detection\\nrectified linear units (ReLU), Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow, Sizing a Neural\\nNetwork\\nrecurrent neural networks (RNNs), Understanding Neural\\nNetworks, Recurrent Neural Networks (RNNs)-Recurrent\\nNeural Networks (RNNs)\\nRefine Network (R-Net), Face Detection with Convolutional\\nNeural Networks\\nregion of interest (ROI) alignment, Mask R-CNN\\nregion of interest (ROI) pooling, R-CNNs\\nregion proposal network (RPN), R-CNNs, R-CNNs\\nregion-based CNNs (R-CNNs), R-CNNs-Mask R-CNN'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 641, 'file_type': 'pdf'}, page_content='regression models, Supervised Learning-Supervised\\nLearning, Regression Models-Summary\\naccuracy measures for, Accuracy Measures for\\nRegression Models-Accuracy Measures for Regression\\nModels\\nand coefficient of determination, Accuracy Measures\\nfor Regression Models\\ndecision trees, Decision Trees-Decision Trees\\nGBMs, Gradient-Boosting Machines-Gradient-Boosting\\nMachines\\nand k-nearest neighbors, k-Nearest Neighbors-k-\\nNearest Neighbors\\nlinear regression, Linear Regression-Linear\\nRegression, Using Regression to Predict Taxi Fares\\nrandom forests, Random Forests-Random Forests,\\nDetecting Credit Card Fraud, Using PCA to Detect\\nCredit Card Fraud\\nsoftmax regression, Handling Unknown Faces: Closed-\\nSet Versus Open-Set Classification\\nSVMs, Support Vector Machines\\ntaxi fare prediction, Using Regression to Predict\\nTaxi Fares-Using Regression to Predict Taxi Fares\\nregularization, Linear Regression, How Support Vector\\nMachines Work, Using SVMs for Facial Recognition\\nreinforcement learning, Machine Learning Versus\\nArtificial Intelligence\\nRekognition service, Azure Cognitive Services\\nReLU (rectified linear units), Understanding Neural\\nNetworks, Understanding Neural Networks, Building Neural\\nNetworks with Keras and TensorFlow, Sizing a Neural\\nNetwork'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 642, 'file_type': 'pdf'}, page_content='RepeatVector layer, LSTM Encoder-Decoders\\nRequests package, Calling Azure Cognitive Services APIs\\nRescaling layer, Image Augmentation with Augmentation\\nLayers\\nresidual layers, Pretrained CNNs\\nresiduals, decision trees, Gradient-Boosting Machines\\nResNet-152, Image Classification with Convolutional\\nNeural Networks\\nResNet-50V2, Pretrained CNNs-Using ResNet50V2 to Classify\\nImages, Using Transfer Learning to Identify Arctic\\nWildlife-Using Transfer Learning to Identify Arctic\\nWildlife\\nResponsible AI initiative, Microsoft, The Computer Vision\\nService\\nREST APIs, and AI as a service, Azure Cognitive Services,\\nKeys and Endpoints\\nretriever-reader architecture, Building a BERT-Based\\nQuestion Answering System\\nRidge class, Linear Regression\\nRNNs (recurrent neural networks), Understanding Neural\\nNetworks, Recurrent Neural Networks (RNNs)-Recurrent\\nNeural Networks (RNNs)\\nROC (receiver operating characteristic) curve, Accuracy\\nMeasures for Classification Models\\nRocCurveDisplay class, Accuracy Measures for\\nClassification Models, Spam Filtering\\nroc_auc_score, Accuracy Measures for Classification\\nModels\\nROI (region of interest) alignment, Mask R-CNN\\nROI (region of interest) pooling, R-CNNs'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 643, 'file_type': 'pdf'}, page_content='RPN (region proposal network), R-CNNs, R-CNNs\\nrules-based machine translation (RBMT), Neural Machine\\nTranslation\\nS\\n\\nSavedModel format, Saving and Loading Models, Using\\nTextVectorization in a Sentiment Analysis Model\\nScaper, soundscape synthesis, Audio Classification with\\nCNNs\\nscatter function, Unsupervised Learning with k-Means\\nClustering\\nScikit-Learn, Unsupervised Learning with k-Means\\nClustering\\nhyperparameter optimizers, Hyperparameter Tuning\\nversus ML.Net, Building ML Models in C# with ML.NET,\\nSentiment Analysis with ML.NET\\nmulticlass classification feature, Multiclass\\nClassification-Multiclass Classification\\nSkl2onnx conversion, Using ONNX to Bridge the\\nLanguage Gap\\nsklearn.cluster.KMeans, Unsupervised Learning with k-\\nMeans Clustering\\nsklearn.datasets.fetch_california_housing, Accuracy\\nMeasures for Regression Models\\nsklearn.datasets.fetch_lfw_people, Using SVMs for\\nFacial Recognition, Training a Neural Network to\\nRecognize Faces\\nsklearn.datasets.load_breast_cancer, Anonymizing Data\\nsklearn.datasets.load_digits, Visualizing High-\\nDimensional Data'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 644, 'file_type': 'pdf'}, page_content='sklearn.datasets.load_iris, Using k-Nearest Neighbors\\nto Classify Flowers\\nsklearn.decomposition.PCA, Understanding Principal\\nComponent Analysis, Visualizing High-Dimensional\\nData, Using PCA to Detect Credit Card Fraud, Using\\nPCA to Predict Bearing Failure\\nsklearn.ensemble.GradientBoostingClassifier,\\nGradient-Boosting Machines-Gradient-Boosting\\nMachines, Detecting Credit Card Fraud\\nsklearn.ensemble.GradientBoostingRegressor, Gradient-\\nBoosting Machines-Gradient-Boosting Machines, Using\\nRegression to Predict Taxi Fares\\nsklearn.ensemble.RandomForestClassifier, Random\\nForests, Detecting Credit Card Fraud, Detecting\\nCredit Card Fraud\\nsklearn.ensemble.RandomForestRegressor, Random\\nForests, Using Regression to Predict Taxi Fares\\nsklearn.feature_extraction.text.CountVectorizer,\\nPreparing Text for Classification-Preparing Text for\\nClassification, Spam Filtering, Recommender Systems,\\nConsuming a Python Model from a Python Client, Text\\nPreparation\\nsklearn.feature_extraction.text.HashingVectorizer,\\nPreparing Text for Classification, Preparing Text for\\nClassification, Consuming a Python Model from a\\nPython Client\\nsklearn.feature_extraction.text.TfidfVectorizer,\\nPreparing Text for Classification, Preparing Text for\\nClassification\\nsklearn.linear_model.Lasso, Linear Regression\\nsklearn.linear_model.LinearRegression, Linear\\nRegression, Accuracy Measures for Regression Models,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 645, 'file_type': 'pdf'}, page_content='Using Regression to Predict Taxi Fares\\nsklearn.linear_model.LogisticRegression, Consuming a\\nPython Model from a Python Client\\nsklearn.linear_model.Perceptron, Understanding Neural\\nNetworks\\nsklearn.linear_model.Ridge, Linear Regression\\nsklearn.manifold.TSNE, Visualizing High-Dimensional\\nData\\nsklearn.metrics.ConfusionMatrixDisplay, Accuracy\\nMeasures for Classification Models, Spam Filtering\\nsklearn.metrics.RocCurveDisplay, Spam Filtering\\nsklearn.metrics.roc_auc_score, Accuracy Measures for\\nClassification Models\\nsklearn.model_selection.GridSearchCV, Hyperparameter\\nTuning, Pipelining, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nsklearn.model_selection.train_test_split, Using k-\\nNearest Neighbors to Classify Flowers, Accuracy\\nMeasures for Regression Models-Accuracy Measures for\\nRegression Models\\nsklearn.naive_bayes.MultinomialNB, Spam Filtering\\nsklearn.neighbors.KNeighborsClassifier, Using k-\\nNearest Neighbors to Classify Flowers\\nsklearn.neural_network.MLPClassifier, Understanding\\nNeural Networks\\nsklearn.neural_network.MLPRegressor, Understanding\\nNeural Networks\\nsklearn.pipeline.make_pipeline, Pipelining, Consuming\\na Python Model from a Python Client'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 646, 'file_type': 'pdf'}, page_content='sklearn.preprocessing.LabelEncoder, Segmenting\\nCustomers Using More Than Two Dimensions, Categorical\\nData\\nsklearn.preprocessing.MinMaxScaler, Data\\nNormalization\\nsklearn.preprocessing.OneHotEncoder, Categorical Data\\nsklearn.preprocessing.PolynomialFeatures, Linear\\nRegression\\nsklearn.preprocessing.StandardScaler, Data\\nNormalization, Data Normalization, Pipelining,\\nTraining a Neural Network to Recognize Faces\\nsklearn.svm.SVC, Support Vector Machines, Support\\nVector Machines, Hyperparameter Tuning\\nsklearn.svm.SVR, Support Vector Machines, Support\\nVector Machines\\nsklearn.tree.DecisionTreeClassifier, Decision Trees\\nsklearn.tree.DecisionTreeRegressor, Decision Trees\\nsklearn.utils.shuffle, Accuracy Measures for\\nRegression Models\\nscore method, Using k-Nearest Neighbors to Classify\\nFlowers, Accuracy Measures for Regression Models\\nscree plot, Understanding Principal Component Analysis\\nsegmentation masks, Mask R-CNN-Mask R-CNN\\nselective search, R-CNNs\\nself-attention, Transformer Encoder-Decoders\\nsensitivity metric, Accuracy Measures for Classification\\nModels, Classifying Passengers Who Sailed on the Titanic\\nsentiment analysis, What Is Machine Learning?, Text\\nClassification'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 647, 'file_type': 'pdf'}, page_content='Azure Language service, The Language Service\\nwith BERT, Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\nHugging Face resources, Using Pretrained Models to\\nClassify Text\\nwith ML.NET, Sentiment Analysis with ML.NET\\ntext classification, Sentiment Analysis-Sentiment\\nAnalysis, Using TextVectorization in a Sentiment\\nAnalysis Model-Using TextVectorization in a Sentiment\\nAnalysis Model\\ntraining and saving pipeline, Consuming a Python\\nModel from a Python Client-Consuming a Python Model\\nfrom a Python Client\\nseparable convolutions, Pretrained CNNs\\nsequence-to-sequence model, LSTM Encoder-Decoders\\nsequences, word embeddings, Natural Language Processing,\\nText Preparation\\nsequence_to_texts method, Text Preparation\\nsequential API, Keras, Neural Networks-Building Neural\\nNetworks with Keras and TensorFlow\\nset_weights method, Saving and Loading Models\\nSGD (stochastic gradient descent), Training Neural\\nNetworks\\nshuffling data, Accuracy Measures for Regression Models,\\nBuilding Neural Networks with Keras and TensorFlow, Text\\nClassification\\nsigmoid activation function, Building Neural Networks\\nwith Keras and TensorFlow, Binary Classification with\\nNeural Networks-Making Predictions, Understanding CNNs\\nsigmoid kernel, Kernels'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 648, 'file_type': 'pdf'}, page_content='similarity matrix, recommender system, Cosine Similarity\\nsimple linear regression, Linear Regression\\nsizing a neural network, Sizing a Neural Network\\nSMT (statistical machine translation), Neural Machine\\nTranslation\\nsoft mask, Mask R-CNN\\nsoftmax activation function, Multiclass Classification,\\nBuilding Neural Networks with Keras and TensorFlow\\nbuilding CNNs, Understanding CNNs\\nclosed- versus open-set classification, Handling\\nUnknown Faces: Closed-Set Versus Open-Set\\nClassification-Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nin facial recognition, Training a Neural Network to\\nRecognize Faces\\nLSTM encoder-decoders, LSTM Encoder-Decoders\\nversus sigmoid activation, Multiclass Classification\\nwith Neural Networks\\ntransformer-based LMT model, Building a Transformer-\\nBased NMT Model\\nsoftmax regression, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nspam filtering, What Is Machine Learning?, Supervised\\nVersus Unsupervised Learning, Naive Bayes-Spam Filtering,\\nText Classification-Text Classification\\nsparse_categorical_crossentropy function, Multiclass\\nClassification with Neural Networks, Training a Neural\\nNetwork to Recognize Faces\\nspatial analysis, The Computer Vision Service'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 649, 'file_type': 'pdf'}, page_content='specificity metric, Accuracy Measures for Classification\\nModels, Classifying Passengers Who Sailed on the Titanic\\nspectrogram images, Audio Classification with CNNs-Audio\\nClassification with CNNs\\nSpeech service, Introducing Azure Cognitive Services, The\\nSpeech Service-The Speech Service\\nSpeechSynthesizer class, The Speech Service\\nSQuAD 2.0, Building a BERT-Based Question Answering\\nSystem\\nstandardization, Data Normalization\\nStandardScaler class, Data Normalization, Data\\nNormalization, Pipelining, Training a Neural Network to\\nRecognize Faces\\nstatistical machine translation (SMT), Neural Machine\\nTranslation\\nstemming, Preparing Text for Classification\\nstochastic gradient descent (SGD), Training Neural\\nNetworks\\nstop words, removing, Preparing Text for Classification,\\nSentiment Analysis, Text Preparation, Text Preparation,\\nAutomating Text Vectorization\\nsubsampling, Gradient-Boosting Machines\\nsupervised learning models, Supervised Versus\\nUnsupervised Learning, Supervised Learning-Using k-\\nNearest Neighbors to Classify Flowers\\nbinary classification, Binary Classification-\\nDetecting Credit Card Fraud, Binary Classification\\nwith Neural Networks-Training a Neural Network to\\nDetect Credit Card Fraud\\nk-nearest neighbors, k-Nearest Neighbors-Using k-\\nNearest Neighbors to Classify Flowers'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 650, 'file_type': 'pdf'}, page_content='support vector machines (SVMs), Support Vector Machines,\\nSupport Vector Machines-Summary\\nfacial recognition with, Using SVMs for Facial\\nRecognition-Using SVMs for Facial Recognition\\nhyperparameter tuning, Hyperparameter Tuning-\\nHyperparameter Tuning\\nkernels and kernel tricks, Kernels-Kernel Tricks\\nnormalization of data, Data Normalization-Data\\nNormalization\\npipelining, Pipelining-Pipelining\\nSVC class, Support Vector Machines, Support Vector\\nMachines, Hyperparameter Tuning\\nSVR class, Support Vector Machines, Support Vector\\nMachines\\nT\\n\\nt-distributed stochastic neighbor embedding (t-SNE),\\nLinear Regression, Visualizing High-Dimensional Data\\ntag_image_in_stream method, The Computer Vision Service\\ntanh activation function, Building Neural Networks with\\nKeras and TensorFlow\\ntask-specific weights to boost transfer learning,\\nBoosting Transfer Learning with Task-Specific Weights-\\nBoosting Transfer Learning with Task-Specific Weights\\nTatoeba project, Building a Transformer-Based NMT Model\\ntaxi fare prediction, Using Regression to Predict Taxi\\nFares-Using Regression to Predict Taxi Fares, Using a\\nNeural Network to Predict Taxi Fares-Using a Neural\\nNetwork to Predict Taxi Fares'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 651, 'file_type': 'pdf'}, page_content='tensor arrays, Neural Networks\\ntensor processing units (TPUs), Deep Learning\\nTensorBoard, Keras Callbacks\\ntensordot function, Using Keras and TensorFlow to Build\\nCNNs\\nTensorFlow, Training Neural Networks\\n(see also Keras API)\\nbuilding neural networks with, Building Neural\\nNetworks with Keras and TensorFlow-Using a Neural\\nNetwork to Predict Taxi Fares\\nCNNs, Using Keras and TensorFlow to Build CNNs-Using\\nKeras and TensorFlow to Build CNNs\\nconverting BERT tokenized inputs into, Fine-Tuning\\nBERT to Perform Sentiment Analysis\\nsaving models, Saving and Loading Models, Using\\nTextVectorization in a Sentiment Analysis Model\\nTensorFlow Lite, Audio Classification with CNNs\\nterm frequency-inverse document frequency (TF-IDF),\\nPreparing Text for Classification\\ntest set, Using k-Nearest Neighbors to Classify Flowers,\\nAccuracy Measures for Regression Models, Accuracy\\nMeasures for Regression Models\\ntext classification and processing, What Is Machine\\nLearning?, Text Classification-Summary, Text\\nClassification-Using Pretrained Models to Classify Text\\ncleaning text, Preparing Text for Classification\\ndatasets for working with text, What Is Machine\\nLearning?\\nextracting text from photos, The Computer Vision\\nService'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 652, 'file_type': 'pdf'}, page_content='factoring word order into predictions, Factoring Word\\nOrder into Predictions-Factoring Word Order into\\nPredictions\\nhandwritten text, Building a Digit Recognition Model-\\nBuilding a Digit Recognition Model, The Computer\\nVision Service-The Computer Vision Service\\nNaive Bayes learning algorithm, Naive Bayes-Naive\\nBayes\\nNMT as extension of (see neural machine translation)\\npretrained models for, Using Pretrained Models to\\nClassify Text\\nrecommender systems, Recommender Systems-Building a\\nMovie Recommendation System\\nand RNNs, Recurrent Neural Networks (RNNs)-Recurrent\\nNeural Networks (RNNs)\\nsentiment analysis, Sentiment Analysis-Sentiment\\nAnalysis, Using TextVectorization in a Sentiment\\nAnalysis Model-Using TextVectorization in a Sentiment\\nAnalysis Model\\nspam filtering, Naive Bayes-Spam Filtering\\nvectorization, Preparing Text for Classification-\\nPreparing Text for Classification, Automating Text\\nVectorization-Using TextVectorization in a Sentiment\\nAnalysis Model\\nTextAnalyticsClient class, Calling Azure Cognitive\\nServices APIs, The Language Service\\ntexts_to_sequences method, Text Preparation\\nTextVectorization layer, Text Preparation, Automating\\nText Vectorization-Using TextVectorization in a Sentiment\\nAnalysis Model'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 653, 'file_type': 'pdf'}, page_content='TF-IDF (term frequency-inverse document frequency),\\nPreparing Text for Classification\\nTfidfVectorizer class, Preparing Text for Classification,\\nPreparing Text for Classification\\ntime series data, forecasting, Recurrent Neural Networks\\n(RNNs)\\nTimeDistributed wrapper, LSTM Encoder-Decoders\\nTitanic passenger classification dataset, Classifying\\nPassengers Who Sailed on the Titanic-Classifying\\nPassengers Who Sailed on the Titanic\\nTokenAndPositionEmbedding class, Building a Transformer-\\nBased NMT Model, Building a Transformer-Based NMT Model\\ntokenization, BERT, Building a BERT-Based Question\\nAnswering System-Fine-Tuning BERT to Perform Sentiment\\nAnalysis\\ntokenized words, Preparing Text for Classification\\nTokenizer class, Text Preparation-Text Preparation,\\nAutomating Text Vectorization, Building a Transformer-\\nBased NMT Model\\ntokens, word, Text Preparation\\nTPR (true positive rate), Accuracy Measures for\\nClassification Models\\nTPUs (tensor processing units), Deep Learning\\ntrainable parameters, Using a Neural Network to Predict\\nTaxi Fares\\ntraining set, Using k-Nearest Neighbors to Classify\\nFlowers\\ntraining versus validation accuracy, Dropout\\ntrain_test_split function, Using k-Nearest Neighbors to\\nClassify Flowers, Accuracy Measures for Regression'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 654, 'file_type': 'pdf'}, page_content='Models-Accuracy Measures for Regression Models, Building\\nNeural Networks with Keras and TensorFlow\\ntransfer learning, Building ML Models in C# with ML.NET,\\nTransfer Learning-Using Transfer Learning to Identify\\nArctic Wildlife, Applying Transfer Learning to Facial\\nRecognition-Boosting Transfer Learning with Task-Specific\\nWeights\\ntransformer models, Natural Language Processing,\\nTransformer Encoder-Decoders\\nattention mechanisms, Transformer Encoder-Decoders-\\nTransformer Encoder-Decoders\\nBERT-based question answering, Building a BERT-Based\\nQuestion Answering System-Building a BERT-Based\\nQuestion Answering System\\nbuilding an NMT, Building a Transformer-Based NMT\\nModel-Building a Transformer-Based NMT Model\\nDistilBERT, Bidirectional Encoder Representations\\nfrom Transformers (BERT)\\nencoder-decoders for NMT, Transformer Encoder-\\nDecoders-Transformer Encoder-Decoders\\nHugging Face, Using Pretrained Models to Classify\\nText, Using Pretrained Models to Translate Text,\\nBuilding a BERT-Based Question Answering System-Fine-\\nTuning BERT to Perform Sentiment Analysis\\nTransformerDecoder class, Building a Transformer-Based\\nNMT Model, Building a Transformer-Based NMT Model\\nTransformerEncoder class, Building a Transformer-Based\\nNMT Model, Building a Transformer-Based NMT Model\\ntranslate_text function, Building a Transformer-Based NMT\\nModel\\ntranslation (see neural machine translation)'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 655, 'file_type': 'pdf'}, page_content='TranslationRecognizer object, The Speech Service\\nTranslator service, Introducing Azure Cognitive Services,\\nThe Translator Service-The Translator Service\\ntrue positive rate (TPR), Accuracy Measures for\\nClassification Models\\nTSNE class, Visualizing High-Dimensional Data\\n2D to 3D space, kernel trick, Kernel Tricks-Kernel Tricks\\nU\\n\\nUDFs (user-defined functions), Excel, Adding Machine\\nLearning Capabilities to Excel\\nunderfitting of data, Hyperparameter Tuning, Building\\nNeural Networks with Keras and TensorFlow\\nunit variance, normalizing data to, Data Normalization\\nuniversal approximation theorem, Understanding Neural\\nNetworks\\nunsupervised learning, Supervised Versus Unsupervised\\nLearning\\nanomaly detection, Anomaly Detection\\nclustering (see clustering algorithms)\\nGANs, Understanding Neural Networks\\nUrbanSound8K dataset, Audio Classification with CNNs\\nuser-defined functions (UDFs), Excel, Adding Machine\\nLearning Capabilities to Excel\\nV\\n\\nvalidation accuracy, Building Neural Networks with Keras\\nand TensorFlow, Using a Neural Network to Predict Taxi'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 656, 'file_type': 'pdf'}, page_content='Fares-Using a Neural Network to Predict Taxi Fares\\nin building CNNs, Using Keras and TensorFlow to Build\\nCNNs, Training a CNN to Recognize Arctic Wildlife\\ncallbacks to capture peak, Keras Callbacks-Keras\\nCallbacks\\nconfusion matrix to visualize untrained data,\\nTraining a Neural Network to Detect Credit Card Fraud\\nand dropout technique, Dropout\\nand LSTMs, Recurrent Neural Networks (RNNs)\\nin NMT, Building a Transformer-Based NMT Model\\nvalidation_split function, Building Neural Networks with\\nKeras and TensorFlow, Using a Neural Network to Predict\\nTaxi Fares, Text Classification\\nvanishing gradient problem, Sizing a Neural Network\\nvectorization, text, Preparing Text for Classification-\\nPreparing Text for Classification, Automating Text\\nVectorization-Using TextVectorization in a Sentiment\\nAnalysis Model\\nversioning pickle files, Versioning Pickle Files\\nVGGFace\\ndetecting and recognizing faces in photos, Putting It\\nAll Together: Detecting and Recognizing Faces in\\nPhotos-Putting It All Together: Detecting and\\nRecognizing Faces in Photos\\ntask-specific weights to boost transfer learning,\\nBoosting Transfer Learning with Task-Specific\\nWeights-Boosting Transfer Learning with Task-Specific\\nWeights\\nViola-Jones, face detection with, Face Detection with\\nViola-Jones-Using the OpenCV Implementation of Viola-'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 657, 'file_type': 'pdf'}, page_content='Jones\\nVision services\\nComputer Vision service, Image Classification with\\nConvolutional Neural Networks, The Computer Vision\\nService-The Computer Vision Service\\nCustom Vision service, Training a Custom Object\\nDetection Model with the Custom Vision Service-\\nTraining a Custom Object Detection Model with the\\nCustom Vision Service, Introducing Azure Cognitive\\nServices\\nFace service, Introducing Azure Cognitive Services,\\nThe Computer Vision Service\\nvisualization of data\\nconfusion matrix for untrained data, Training a\\nNeural Network to Detect Credit Card Fraud\\nhigh-dimensional for PCA, Visualizing High-\\nDimensional Data-Visualizing High-Dimensional Data\\nvocabulary, word dataset, Text Preparation\\nW\\n\\nweak learners, Gradient-Boosting Machines\\nweb service, accessing Python through, Operationalizing\\nMachine Learning Models, Consuming a Python Model from a\\nC# Client-Consuming a Python Model from a C# Client,\\nUsing ONNX to Bridge the Language Gap\\nWeibull distribution, Handling Unknown Faces: Closed-Set\\nVersus Open-Set Classification\\nweights, Understanding Neural Networks, Understanding\\nNeural Networks, Building Neural Networks with Keras and\\nTensorFlow, Using a Neural Network to Predict Taxi Fares,'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 658, 'file_type': 'pdf'}, page_content='Boosting Transfer Learning with Task-Specific Weights-\\nBoosting Transfer Learning with Task-Specific Weights\\nword embeddings, Natural Language Processing, Word\\nEmbeddings-Word Embeddings, Recurrent Neural Networks\\n(RNNs)-Recurrent Neural Networks (RNNs)\\nword order, factoring into predictions, Factoring Word\\nOrder into Predictions-Factoring Word Order into\\nPredictions\\nword vectors, Natural Language Processing, Recurrent\\nNeural Networks (RNNs)-Recurrent Neural Networks (RNNs)\\nWordPiece format, Building a BERT-Based Question\\nAnswering System\\nwrapping a model, Operationalizing Machine Learning\\nModels\\nX\\n\\nXception (Extreme Inception), Pretrained CNNs\\nXlwings library, Adding Machine Learning Capabilities to\\nExcel\\nY\\n\\nYou Only Look Once (YOLO), YOLO-YOLOv3 and Keras\\nZ\\n\\nZ-score normalization, Data Normalization\\nZeroes initializer, Building Neural Networks with Keras\\nand TensorFlow'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 659, 'file_type': 'pdf'}, page_content='About the Author\\nJeff Prosise is an engineer whose passion is to introduce\\nother engineers and software developers to the wonders of AI\\nand machine learning. Cofounder of Wintellect, he has written\\nnine books and hundreds of magazine articles, trained\\nthousands of developers at Microsoft, and spoken at some of\\nthe world’s largest software conferences. In another life,\\nJeff worked on high-powered laser systems and fusion-energy\\nresearch at Oak Ridge National Laboratory and Lawrence\\nLivermore National Laboratory. In his spare time, he builds\\nand flies large radio-control jets and goes out of his way to\\nget wet in the world’s best dive spots. Following the\\nacquisition of his company in 2021, Jeff serves as chief\\nlearning officer at Atmosera, where he helps customers infuse\\nAI into their products.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 660, 'file_type': 'pdf'}, page_content='Colophon\\nThe animal on the cover of Applied Machine Learning and AI\\nfor Engineers is a festive parrot (Amazona festiva), also\\nknown as a festive amazon. Festive parrots live in the\\ntropical forests, woodlands, and coastal mangroves of several\\nSouth American countries, including Brazil, Colombia,\\nEcuador, Peru, and Bolivia. They are rarely found far from\\nwater.\\nFestive parrots are brightly—you might even say festively—\\ncolored, medium-sized birds. Their plumage is predominantly a\\nstriking green, turning slightly yellow toward the edges of\\ntheir wings. A motley assortment of colors—including red,\\nblue, and sometimes yellow or orange—adorns their faces.\\nFestive parrots are a highly social species, usually spotted\\nin pairs or small flocks. Large groups of the birds often\\ngather at night for communal roosts or around a localized\\nfood source and are known for being incredibly noisy. They\\nenjoy eating fruits such as mangoes and peach palm, with\\nberries, nuts, seeds, flowers, and leaf buds supplementing\\ntheir diet.\\nWhile still relatively common where their forest habitat\\nremains largely intact, festive parrots have been categorized\\nby IUCN as near threatened due to continued deforestation and\\npredicted declines in habitat. Many of the animals on\\nO’Reilly covers are endangered; all of them are important to\\nthe world.\\nThe cover illustration is by Karen Montgomery, based on an\\nantique line engraving from Wood’s Illustrated Natural\\nHistory. The cover fonts are Gilroy Semibold and Guardian\\nSans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s\\nUbuntu Mono.'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 661, 'file_type': 'pdf'}, page_content='Table of Contents\\nForeword\\nPreface\\nWho Should Read This Book\\nWhy I Wrote This Book\\nRunning the Book’s Code Samples\\nNavigating This Book\\nConventions Used in This Book\\nUsing Code Examples\\nO’Reilly Online Learning\\nHow to Contact Us\\nAcknowledgments\\nI. Machine Learning with Scikit-Learn\\n1. Machine Learning\\nWhat Is Machine Learning?\\nMachine Learning Versus Artificial\\nIntelligence\\nSupervised \\nVersus \\nUnsupervised\\nLearning\\nUnsupervised Learning with k-Means Clustering\\nApplying \\nk-Means \\nClustering \\nto\\nCustomer Data\\nSegmenting Customers Using More Than\\nTwo Dimensions\\nSupervised Learning\\nk-Nearest Neighbors\\nUsing k-Nearest Neighbors to Classify\\nFlowers\\nSummary\\n2. Regression Models\\nLinear Regression\\nDecision Trees\\nRandom Forests\\nGradient-Boosting Machines\\nSupport Vector Machines'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 662, 'file_type': 'pdf'}, page_content='Accuracy Measures for Regression Models\\nUsing Regression to Predict Taxi Fares\\nSummary\\n3. Classification Models\\nLogistic Regression\\nAccuracy Measures for Classification Models\\nCategorical Data\\nBinary Classification\\nClassifying Passengers Who Sailed on\\nthe Titanic\\nDetecting Credit Card Fraud\\nMulticlass Classification\\nBuilding a Digit Recognition Model\\nSummary\\n4. Text Classification\\nPreparing Text for Classification\\nSentiment Analysis\\nNaive Bayes\\nSpam Filtering\\nRecommender Systems\\nCosine Similarity\\nBuilding \\na \\nMovie \\nRecommendation\\nSystem\\nSummary\\n5. Support Vector Machines\\nHow Support Vector Machines Work\\nKernels\\nKernel Tricks\\nHyperparameter Tuning\\nData Normalization\\nPipelining\\nUsing SVMs for Facial Recognition\\nSummary\\n6. Principal Component Analysis\\nUnderstanding Principal Component Analysis\\nFiltering Noise\\nAnonymizing Data'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 663, 'file_type': 'pdf'}, page_content='Visualizing High-Dimensional Data\\nAnomaly Detection\\nUsing PCA to Detect Credit Card Fraud\\nUsing PCA to Predict Bearing Failure\\nMultivariate Anomaly Detection\\nSummary\\n7. Operationalizing Machine Learning Models\\nConsuming a Python Model from a Python Client\\nVersioning Pickle Files\\nConsuming a Python Model from a C# Client\\nContainerizing a Machine Learning Model\\nUsing ONNX to Bridge the Language Gap\\nBuilding ML Models in C# with ML.NET\\nSentiment Analysis with ML.NET\\nSaving and Loading ML.NET Models\\nAdding Machine Learning Capabilities to Excel\\nSummary\\nII. Deep Learning with Keras and TensorFlow\\n8. Deep Learning\\nUnderstanding Neural Networks\\nTraining Neural Networks\\nSummary\\n9. Neural Networks\\nBuilding Neural Networks with Keras and TensorFlow\\nSizing a Neural Network\\nUsing a Neural Network to Predict\\nTaxi Fares\\nBinary Classification with Neural Networks\\nMaking Predictions\\nTraining a Neural Network to Detect\\nCredit Card Fraud\\nMulticlass Classification with Neural Networks\\nTraining a Neural Network to Recognize Faces\\nDropout\\nSaving and Loading Models\\nKeras Callbacks\\nSummary'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 664, 'file_type': 'pdf'}, page_content='10. Image Classification with Convolutional Neural Networks\\nUnderstanding CNNs\\nUsing Keras and TensorFlow to Build\\nCNNs\\nTraining a CNN to Recognize Arctic\\nWildlife\\nPretrained CNNs\\nUsing ResNet50V2 to Classify Images\\nTransfer Learning\\nUsing Transfer Learning to Identify Arctic Wildlife\\nData Augmentation\\nImage \\nAugmentation \\nwith\\nImageDataGenerator\\nImage Augmentation with Augmentation\\nLayers\\nApplying Image Augmentation to Arctic\\nWildlife\\nGlobal Pooling\\nAudio Classification with CNNs\\nSummary\\n11. Face Detection and Recognition\\nFace Detection\\nFace Detection with Viola-Jones\\nUsing the OpenCV Implementation of\\nViola-Jones\\nFace Detection with Convolutional\\nNeural Networks\\nExtracting Faces from Photos\\nFacial Recognition\\nApplying Transfer Learning to Facial\\nRecognition\\nBoosting Transfer Learning with Task-\\nSpecific Weights\\nArcFace\\nPutting It All Together: Detecting and Recognizing\\nFaces in Photos'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 665, 'file_type': 'pdf'}, page_content='Handling Unknown Faces: Closed-Set Versus Open-Set\\nClassification\\nSummary\\n12. Object Detection\\nR-CNNs\\nMask R-CNN\\nYOLO\\nYOLOv3 and Keras\\nCustom Object Detection\\nTraining a Custom Object Detection\\nModel with the Custom Vision Service\\nUsing the Exported Model\\nSummary\\n13. Natural Language Processing\\nText Preparation\\nWord Embeddings\\nText Classification\\nAutomating Text Vectorization\\nUsing \\nTextVectorization \\nin \\na\\nSentiment Analysis Model\\nFactoring Word Order into Predictions\\nRecurrent Neural Networks (RNNs)\\nUsing Pretrained Models to Classify\\nText\\nNeural Machine Translation\\nLSTM Encoder-Decoders\\nTransformer Encoder-Decoders\\nBuilding \\na \\nTransformer-Based \\nNMT\\nModel\\nUsing Pretrained Models to Translate\\nText\\nBidirectional \\nEncoder \\nRepresentations \\nfrom\\nTransformers (BERT)\\nBuilding \\na \\nBERT-Based \\nQuestion\\nAnswering System\\nFine-Tuning BERT to Perform Sentiment\\nAnalysis'),\n",
       " Document(metadata={'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'page_number': 666, 'file_type': 'pdf'}, page_content='Summary\\n14. Azure Cognitive Services\\nIntroducing Azure Cognitive Services\\nKeys and Endpoints\\nCalling Azure Cognitive Services APIs\\nAzure Cognitive Services Containers\\nThe Computer Vision Service\\nThe Language Service\\nThe Translator Service\\nThe Speech Service\\nPutting It All Together: Contoso Travel\\nSummary\\nIndex'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 1, 'file_type': 'pdf'}, page_content='DATA STRUCTURES\\n\\n—— AND —\\nALGORITHMS\\nMADE EASY\\n\\n2S and Algorithmic Puzzles\\n\\nNarasimha Karumanchi, mech, wt Bombay\\nFounder, CareerMonk.com'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 2, 'file_type': 'pdf'}, page_content='Data Structures\\nAnd\\nAlgorithms\\nMade Easy\\n-To All My Readers\\nBy\\nNarasimha Karumanchi'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 3, 'file_type': 'pdf'}, page_content='Copyright© 2017 by CareerMonk.com\\nAll rights reserved.\\nDesigned by Narasimha Karumanchi\\nCopyright© 2017 CareerMonk Publications. All rights reserved.\\nAll rights reserved. No part of this book may be reproduced in any form or by any electronic or mechanical means, including\\ninformation storage and retrieval systems, without written permission from the publisher or author.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 4, 'file_type': 'pdf'}, page_content='Acknowledgements\\nMother and Father, it is impossible to thank you adequately for everything you have done, from\\nloving me unconditionally to raising me in a stable household, where your persistent efforts and\\ntraditional values taught your children to celebrate and embrace life. I could not have asked for\\nbetter parents or role-models. You showed me that anything is possible with faith, hard work and\\ndetermination.\\nThis book would not have been possible without the help of many people. I would like to express\\nmy gratitude to all of the people who provided support, talked things over, read, wrote, offered\\ncomments, allowed me to quote their remarks and assisted in the editing, proofreading and design.\\nIn particular, I would like to thank the following individuals:\\nMohan Mullapudi, IIT Bombay, Architect, dataRPM Pvt. Ltd.\\nNavin Kumar Jaiswal, Senior Consultant, Juniper Networks Inc.\\nA. Vamshi Krishna, IIT Kanpur, Mentor Graphics Inc.\\nCathy Reed, BA, MA, Copy Editor\\n–Narasimha Karumanchi\\nM-Tech, IIT Bombay\\nFounder, CareerMonk.com'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 5, 'file_type': 'pdf'}, page_content='Preface\\nDear Reader,\\nPlease hold on! I know many people typically do not read the Preface of a book. But I strongly\\nrecommend that you read this particular Preface.\\nIt is not the main objective of this book to present you with the theorems and proofs on data\\nstructures and algorithms. I have followed a pattern of improving the problem solutions with\\ndifferent complexities (for each problem, you will find multiple solutions with different, and\\nreduced, complexities). Basically, it’s an enumeration of possible solutions. With this approach,\\neven if you get a new question, it will show you a way to think about the possible solutions. You\\nwill find this book useful for interview preparation, competitive exams preparation, and campus\\ninterview preparations.\\nAs a job seeker, if you read the complete book, I am sure you will be able to challenge the\\ninterviewers. If you read it as an instructor, it will help you to deliver lectures with an approach\\nthat is easy to follow, and as a result your students will appreciate the fact that they have opted for\\nComputer Science / Information Technology as their degree.\\nThis book is also useful for Engineering degree students and Masters degree students during\\ntheir academic preparations. In all the chapters you will see that there is more emphasis on\\nproblems and their analysis rather than on theory. In each chapter, you will first read about the\\nbasic required theory, which is then followed by a section on problem sets. In total, there are\\napproximately 700 algorithmic problems, all with solutions.\\nIf you read the book as a student preparing for competitive exams for Computer Science /\\nInformation Technology, the content covers all the required topics in full detail. While writing\\nthis book, my main focus was to help students who are preparing for these exams.\\nIn all the chapters you will see more emphasis on problems and analysis rather than on theory'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 5, 'file_type': 'pdf'}, page_content='. While writing\\nthis book, my main focus was to help students who are preparing for these exams.\\nIn all the chapters you will see more emphasis on problems and analysis rather than on theory. In\\neach chapter, you will first see the basic required theory followed by various problems.\\nFor many problems, multiple solutions are provided with different levels of complexity. We start\\nwith the brute force solution and slowly move toward the best solution possible for that problem.\\nFor each problem, we endeavor to understand how much time the algorithm takes and how much\\nmemory the algorithm uses.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 6, 'file_type': 'pdf'}, page_content='It is recommended that the reader does at least one complete reading of this book to gain a full\\nunderstanding of all the topics that are covered. Then, in subsequent readings you can skip\\ndirectly to any chapter to refer to a specific topic. Even though many readings have been done for\\nthe purpose of correcting errors, there could still be some minor typos in the book. If any are\\nfound, they will be updated at www.CareerMonk.com. You can monitor this site for any\\ncorrections and also for new problems and solutions. Also, please provide your valuable\\nsuggestions at: Info@CareerMonk.com.\\nI wish you all the best and I am confident that you will find this book useful.\\n–Narasimha Karumanchi\\nM-Tech, I IT Bombay\\nFounder, CareerMonk.com'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 7, 'file_type': 'pdf'}, page_content='Other Books by Narasimha Karumanchi\\nIT Interview Questions\\nData Structures and Algorithms for GATE\\nData Structures and Aigorithms Made Easy in Java\\nCoding Interview Questions\\nPeeling Design Patterns\\nElements of Computer Networking\\nData Structures and Algorithmic Thinking with Python'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 8, 'file_type': 'pdf'}, page_content='Table of Contents\\n1.   Introduction\\n1.1\\u2003Variables\\n1.2\\u2003Data Types\\n1.3\\u2003Data Structures\\n1.4\\u2003Abstract Data Types (ADTs)\\n1.5\\u2003What is an Algorithm?\\n1.6\\u2003Why the Analysis of Algorithms?\\n1.7\\u2003Goal of the Analysis of Algorithms\\n1.8\\u2003What is Running Time Analysis?\\n1.9\\u2003How to Compare Algorithms\\n1.10\\u2002What is Rate of Growth?\\n1.11\\u2002Commonly Used Rates of Growth\\n1.12\\u2002Types of Analysis\\n1.13\\u2002Asymptotic Notation\\n1.14\\u2002Big-O Notation [Upper Bounding Function]\\n1.15\\u2002Omega-Q Notation [Lower Bounding Function]\\n1.16\\u2002Theta-Θ Notation [Order Function]\\n1.17\\u2002Important Notes\\n1.18\\u2002Why is it called Asymptotic Analysis?\\n1.19\\u2002Guidelines for Asymptotic Analysis\\n1.20\\u2002Simplyfying properties of asymptotic notations\\n1.21\\u2002Commonly used Logarithms and Summations\\n1.22\\u2002Master Theorem for Divide and Conquer Recurrences\\n1.23\\u2002Divide and Conquer Master Theorem: Problems & Solutions\\n1.24\\u2002Master Theorem for Subtract and Conquer Recurrences\\n1.25\\u2002Variant of Subtraction and Conquer Master Theorem\\n1.26\\u2002Method of Guessing and Confirming'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 9, 'file_type': 'pdf'}, page_content='1.27\\u2002Amortized Analysis\\n1.28\\u2002Algorithms Analysis: Problems & Solutions\\n2.   Recursion and Backtracking\\n2.1\\u2003Introduction\\n2.2\\u2003What is Recursion?\\n2.3\\u2003Why Recursion?\\n2.4\\u2003Format of a Recursive Function\\n2.5\\u2003Recursion and Memory (Visualization)\\n2.6\\u2003Recursion versus Iteration\\n2.7\\u2003Notes on Recursion\\n2.8\\u2003Example Algorithms of Recursion\\n2.9\\u2003Recursion: Problems & Solutions\\n2.10\\u2002What is Backtracking?\\n2.11\\u2002Example Algorithms of Backtracking\\n2.12\\u2002Backtracking: Problems & Solutions\\n3.   Linked Lists\\n3.1\\u2003What is a Linked List?\\n3.2\\u2003Linked Lists ADT\\n3.3\\u2003Why Linked Lists?\\n3.4\\u2003Arrays Overview\\n3.5\\u2003Comparison of Linked Lists with Arrays & Dynamic Arrays\\n3.6\\u2003Singly Linked Lists\\n3.7\\u2003Doubly Linked Lists\\n3.8\\u2003Circular Linked Lists\\n3.9\\u2003A Memory-efficient Doubly Linked List\\n3.10\\u2002Unrolled Linked Lists\\n3.11\\u2002Skip Lists\\n3.12\\u2002Linked Lists: Problems & Solutions\\n4.   Stacks\\n4.1\\u2003What is a Stack?\\n4.2\\u2003How Stacks are used\\n4.3\\u2003Stack ADT'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 10, 'file_type': 'pdf'}, page_content='4.4\\u2003Applications\\n4.5\\u2003Implementation\\n4.6\\u2003Comparison of Implementations\\n4.7\\u2003Stacks: Problems & Solutions\\n5.   Queues\\n5.1\\u2003What is a Queue?\\n5.2\\u2003How are Queues Used?\\n5.3\\u2003Queue ADT\\n5.4\\u2003Exceptions\\n5.5\\u2003Applications\\n5.6\\u2003Implementation\\n5.7\\u2003Queues: Problems & Solutions\\n6.   Trees\\n6.1\\u2003What is a Tree?\\n6.2\\u2003Glossary\\n6.3\\u2003Binary Trees\\n6.4\\u2003Types of Binary Trees\\n6.5\\u2003Properties of Binary Trees\\n6.6\\u2003Binary Tree Traversals\\n6.7\\u2003Generic Trees (N-ary Trees)\\n6.8\\u2003Threaded Binary Tree Traversals (Stack or Queue-less Traversals)\\n6.9\\u2003Expression Trees\\n6.10\\u2002XOR Trees\\n6.11\\u2002Binary Search Trees (BSTs)\\n6.12\\u2003Balanced Binary Search Trees\\n6.13\\u2002AVL (Adelson-Velskii and Landis) Trees\\n6.14\\u2002Other Variations on Trees\\n7.   Priority Queues and Heaps\\n7.1\\u2003What is a Priority Queue?\\n7.2\\u2003Priority Queue ADT\\n7.3\\u2003Priority Queue Applications\\n7.4\\u2003Priority Queue Implementations'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 11, 'file_type': 'pdf'}, page_content='7.5\\u2003Heaps and Binary Heaps\\n7.6\\u2003Binary Heaps\\n7.7\\u2003Heapsort\\n7.8\\u2003Priority Queues [Heaps]: Problems & Solutions\\n8.   Disjoint Sets ADT\\n8.1\\u2003Introduction\\n8.2\\u2003Equivalence Relations and Equivalence Classes\\n8.3\\u2003Disjoint Sets ADT\\n8.4\\u2003Applications\\n8.5\\u2003Tradeoffs in Implementing Disjoint Sets ADT\\n8.8\\u2003Fast UNION Implementation (Slow FIND)\\n8.9\\u2003Fast UNION Implementations (Quick FIND)\\n8.10\\u2002Summary\\n8.11\\u2002Disjoint Sets: Problems & Solutions\\n9.   Graph Algorithms\\n9.1\\u2003Introduction\\n9.2\\u2003Glossary\\n9.3\\u2003Applications of Graphs\\n9.4\\u2003Graph Representation\\n9.5\\u2003Graph Traversals\\n9.6\\u2003Topological Sort\\n9.7\\u2003Shortest Path Algorithms\\n9.8\\u2003Minimal Spanning Tree\\n9.9\\u2003Graph Algorithms: Problems & Solutions\\n10.  Sorting\\n10.1\\u2003What is Sorting?\\n10.2\\u2003Why is Sorting Necessary?\\n10.3\\u2003Classification of Sorting Algorithms\\n10.4\\u2003Other Classifications\\n10.5\\u2003Bubble Sort\\n10.6\\u2003Selection Sort\\n10.7\\u2003Insertion Sort'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 12, 'file_type': 'pdf'}, page_content='10.8\\u2003Shell Sort\\n10.9\\u2003Merge Sort\\n10.10\\u2002Heap Sort\\n10.11\\u2002Quick Sort\\n10.12\\u2002Tree Sort\\n10.13\\u2002Comparison of Sorting Algorithms\\n10.14\\u2002Linear Sorting Algorithms\\n10.15\\u2002Counting Sort\\n10.16\\u2002Bucket Sort (or Bin Sort)\\n10.17\\u2002Radix Sort\\n10.18\\u2002Topological Sort\\n10.19\\u2002External Sorting\\n10.20\\u2002Sorting: Problems & Solutions\\n11.  Searching\\n11.1\\u2003What is Searching?\\n11.2\\u2003Why do we need Searching?\\n11.3\\u2003Types of Searching\\n11.4\\u2003Unordered Linear Search\\n11.5\\u2003Sorted/Ordered Linear Search\\n11.6\\u2003Binary Search\\n11.7\\u2003Interpolation Search\\n11.8\\u2003Comparing Basic Searching Algorithms\\n11.9\\u2003Symbol Tables and Hashing\\n11.10\\u2002String Searching Algorithms\\n11.11\\u2002Searching: Problems & Solutions\\n12.  Selection Algorithms [Medians]\\n12.1\\u2003What are Selection Algorithms?\\n12.2\\u2003Selection by Sorting\\n12.3\\u2003Partition-based Selection Algorithm\\n12.4\\u2003Linear Selection Algorithm - Median of Medians Algorithm\\n12.5\\u2003Finding the K Smallest Elements in Sorted Order\\n12.6\\u2003Selection Algorithms: Problems & Solutions'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 13, 'file_type': 'pdf'}, page_content='13.  Symbol Tables\\n13.1\\u2003Introduction\\n13.2\\u2003What are Symbol Tables?\\n13.3\\u2003Symbol Table Implementations\\n13.4\\u2003Comparison Table of Symbols for Implementations\\n14.  Hashing\\n14.1\\u2003What is Hashing?\\n14.2\\u2003Why Hashing?\\n14.3\\u2003HashTable ADT\\n14.4\\u2003Understanding Hashing\\n14.5\\u2003Components of Hashing\\n14.6\\u2003Hash Table\\n14.7\\u2003Hash Function\\n14.8\\u2003Load Factor\\n14.9\\u2003Collisions\\n14.10\\u2002Collision Resolution Techniques\\n14.11\\u2002Separate Chaining\\n14.12\\u2002Open Addressing\\n14.13\\u2002Comparison of Collision Resolution Techniques\\n14.14\\u2002How Hashing Gets O(1) Complexity?\\n14.15\\u2002Hashing Techniques\\n14.16\\u2002Problems for which Hash Tables are not suitable\\n14.17\\u2002Bloom Filters\\n14.18\\u2002Hashing: Problems & Solutions\\n15.  String Algorithms\\n15.1\\u2003Introduction\\n15.2\\u2003String Matching Algorithms\\n15.3\\u2003Brute Force Method\\n15.4\\u2003Rabin-Karp String Matching Algorithm\\n15.5\\u2003String Matching with Finite Automata\\n15.6\\u2003KMP Algorithm\\n15.7\\u2003Boyer-Moore Algorithm'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 14, 'file_type': 'pdf'}, page_content='15.8\\u2003Data Structures for Storing Strings\\n15.9\\u2003Hash Tables for Strings\\n15.10\\u2002Binary Search Trees for Strings\\n15.11\\u2002Tries\\n15.12\\u2002Ternary Search Trees\\n15.13\\u2002Comparing BSTs, Tries and TSTs\\n15.14\\u2002Suffix Trees\\n15.15\\u2002String Algorithms: Problems & Solutions\\n16.  Algorithms Design Techniques\\n16.1\\u2003Introduction\\n16.2\\u2003Classification\\n16.3\\u2003Classification by Implementation Method\\n16.4\\u2003Classification by Design Method\\n16.5\\u2003Other Classifications\\n17.  Greedy Algorithms\\n17.1\\u2003Introduction\\n17.2\\u2003Greedy Strategy\\n17.3\\u2003Elements of Greedy Algorithms\\n17.4\\u2003Does Greedy Always Work?\\n17.5\\u2003Advantages and Disadvantages of Greedy Method\\n17.6\\u2003Greedy Applications\\n17.7\\u2003Understanding Greedy Technique\\n17.8\\u2003Greedy Algorithms: Problems & Solutions\\n18.  Divide and Conquer Algorithms\\n18.1\\u2003Introduction\\n18.2\\u2003What is the Divide and Conquer Strategy?\\n18.3\\u2003Does Divide and Conquer Always Work?\\n18.4\\u2003Divide and Conquer Visualization\\n18.5\\u2003Understanding Divide and Conquer\\n18.6\\u2003Advantages of Divide and Conquer\\n18.7\\u2003Disadvantages of Divide and Conquer\\n18.8\\u2003Master Theorem'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 15, 'file_type': 'pdf'}, page_content='18.9\\u2003Divide and Conquer Applications\\n18.10\\u2002Divide and Conquer: Problems & Solutions\\n19.  Dynamic Programming\\n19.1\\u2003Introduction\\n19.2\\u2003What is Dynamic Programming Strategy?\\n19.3\\u2003Properties of Dynamic Programming Strategy\\n19.4\\u2003Can Dynamic Programming Solve All Problems?\\n19.5\\u2003Dynamic Programming Approaches\\n19.6\\u2003Examples of Dynamic Programming Algorithms\\n19.7\\u2003Understanding Dynamic Programming\\n19.8\\u2003Longest Common Subsequence\\n19.9\\u2003Dynamic Programming: Problems & Solutions\\n20.  Complexity Classes\\n20.1\\u2003Introduction\\n20.2\\u2003Polynomial/Exponential Time\\n20.3\\u2003What is a Decision Problem?\\n20.4\\u2003Decision Procedure\\n20.5\\u2003What is a Complexity Class?\\n20.6\\u2003Types of Complexity Classes\\n20.7\\u2003Reductions\\n20.8\\u2003Complexity Classes: Problems & Solutions\\n21.  Miscellaneous Concepts\\n21.1\\u2003Introduction\\n21.2\\u2003Hacks on Bit-wise Programming\\n21.3\\u2003Other Programming Questions\\nReferences'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 16, 'file_type': 'pdf'}, page_content='The objective of this chapter is to explain the importance of the analysis of algorithms, their\\nnotations, relationships and solving as many problems as possible. Let us first focus on\\nunderstanding the basic elements of algorithms, the importance of algorithm analysis, and then\\nslowly move toward the other topics as mentioned above. After completing this chapter, you\\nshould be able to find the complexity of any given algorithm (especially recursive functions).\\n1.1 Variables\\nBefore going to the definition of variables, let us relate them to old mathematical equations. All of\\nus have solved many mathematical equations since childhood. As an example, consider the below\\nequation:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 17, 'file_type': 'pdf'}, page_content='We don’t have to worry about the use of this equation. The important thing that we need to\\nunderstand is that the equation has names (x and y), which hold values (data). That means the\\nnames (x and y) are placeholders for representing data. Similarly, in computer science\\nprogramming we need something for holding data, and variables is the way to do that.\\n1.2 Data Types\\nIn the above-mentioned equation, the variables x and y can take any values such as integral\\nnumbers (10, 20), real numbers (0.23, 5.5), or just 0 and 1. To solve the equation, we need to\\nrelate them to the kind of values they can take, and data type is the name used in computer science\\nprogramming for this purpose. A data type in a programming language is a set of data with\\npredefined values. Examples of data types are: integer, floating point, unit number, character,\\nstring, etc.\\nComputer memory is all filled with zeros and ones. If we have a problem and we want to code it,\\nit’s very difficult to provide the solution in terms of zeros and ones. To help users, programming\\nlanguages and compilers provide us with data types. For example, integer takes 2 bytes (actual\\nvalue depends on compiler), float takes 4 bytes, etc. This says that in memory we are combining\\n2 bytes (16 bits) and calling it an integer. Similarly, combining 4 bytes (32 bits) and calling it a\\nfloat. A data type reduces the coding effort. At the top level, there are two types of data types:\\nSystem-defined data types (also called Primitive data types)\\nUser-defined data types\\nSystem-defined data types (Primitive data types)\\nData types that are defined by system are called primitive data types. The primitive data types\\nprovided by many programming languages are: int, float, char, double, bool, etc. The number of\\nbits allocated for each primitive data type depends on the programming languages, the compiler\\nand the operating system. For the same primitive data type, different languages may use different\\nsizes'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 17, 'file_type': 'pdf'}, page_content='. For the same primitive data type, different languages may use different\\nsizes. Depending on the size of the data types, the total available values (domain) will also\\nchange.\\nFor example, “int” may take 2 bytes or 4 bytes. If it takes 2 bytes (16 bits), then the total possible\\nvalues are minus 32,768 to plus 32,767 (-215 to 215-1). If it takes 4 bytes (32 bits), then the\\npossible values are between -2,147,483,648 and +2,147,483,647 (-231 to 231-1). The same is the\\ncase with other data types.\\nUser defined data types\\nIf the system-defined data types are not enough, then most programming languages allow the users'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 18, 'file_type': 'pdf'}, page_content='to define their own data types, called user – defined data types. Good examples of user defined\\ndata types are: structures in C/C + + and classes in Java. For example, in the snippet below, we\\nare combining many system-defined data types and calling the user defined data type by the name\\n“newType”. This gives more flexibility and comfort in dealing with computer memory.\\n1.3 Data Structures\\nBased on the discussion above, once we have data in variables, we need some mechanism for\\nmanipulating that data to solve problems. Data structure is a particular way of storing and\\norganizing data in a computer so that it can be used efficiently. A data structure is a special\\nformat for organizing and storing data. General data structure types include arrays, files, linked\\nlists, stacks, queues, trees, graphs and so on.\\nDepending on the organization of the elements, data structures are classified into two types:\\nLinear data structures: Elements are accessed in a sequential order but it is not\\ncompulsory to store all elements sequentially. Examples: Linked Lists, Stacks and\\nQueues.\\nNon – linear data structures: Elements of this data structure are stored/accessed in a\\nnon-linear order. Examples: Trees and graphs.\\n1.4 Abstract Data Types (ADTs)\\nBefore defining abstract data types, let us consider the different view of system-defined data\\ntypes. We all know that, by default, all primitive data types (int, float, etc.) support basic\\noperations such as addition and subtraction. The system provides the implementations for the\\nprimitive data types. For user-defined data types we also need to define operations. The\\nimplementation for these operations can be done when we want to actually use them. That means,\\nin general, user defined data types are defined along with their operations.\\nTo simplify the process of solving problems, we combine the data structures with their operations\\nand we call this Abstract Data Types (ADTs). An ADT consists of two parts:\\nDeclaration of data'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 19, 'file_type': 'pdf'}, page_content='Declaration of operations\\nCommonly used ADTs include: Linked Lists, Stacks, Queues, Priority Queues, Binary Trees,\\nDictionaries, Disjoint Sets (Union and Find), Hash Tables, Graphs, and many others. For\\nexample, stack uses LIFO (Last-In-First-Out) mechanism while storing the data in data structures.\\nThe last element inserted into the stack is the first element that gets deleted. Common operations\\nof it are: creating the stack, pushing an element onto the stack, popping an element from stack,\\nfinding the current top of the stack, finding number of elements in the stack, etc.\\nWhile defining the ADTs do not worry about the implementation details. They come into the\\npicture only when we want to use them. Different kinds of ADTs are suited to different kinds of\\napplications, and some are highly specialized to specific tasks. By the end of this book, we will\\ngo through many of them and you will be in a position to relate the data structures to the kind of\\nproblems they solve.\\n1.5 What is an Algorithm?\\nLet us consider the problem of preparing an omelette. To prepare an omelette, we follow the\\nsteps given below:\\nGet the frying pan.\\nGet the oil.\\na.\\nDo we have oil?\\n  i. If yes, put it in the pan.\\nii. If no, do we want to buy oil?\\n1. If yes, then go out and buy.\\n2. If no, we can terminate.\\nTurn on the stove, etc...\\nWhat we are doing is, for a given problem (preparing an omelette), we are providing a step-by-\\nstep procedure for solving it. The formal definition of an algorithm can be stated as:\\nAn algorithm is the step-by-step unambiguous instructions to solve a given problem.\\nIn the traditional study of algorithms, there are two main criteria for judging the merits of\\nalgorithms: correctness (does the algorithm give solution to the problem in a finite number of\\nsteps?) and efficiency (how much resources (in terms of memory and time) does it take to execute\\nthe).\\nNote: We do not have to prove each step of the algorithm.\\n1.6 Why the Analysis of Algorithms?'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 20, 'file_type': 'pdf'}, page_content='To go from city “A” to city “B”, there can be many ways of accomplishing this: by flight, by bus,\\nby train and also by bicycle. Depending on the availability and convenience, we choose the one\\nthat suits us. Similarly, in computer science, multiple algorithms are available for solving the\\nsame problem (for example, a sorting problem has many algorithms, like insertion sort, selection\\nsort, quick sort and many more). Algorithm analysis helps us to determine which algorithm is\\nmost efficient in terms of time and space consumed.\\n1.7 Goal of the Analysis of Algorithms\\nThe goal of the analysis of algorithms is to compare algorithms (or solutions) mainly in terms of\\nrunning time but also in terms of other factors (e.g., memory, developer effort, etc.)\\n1.8 What is Running Time Analysis?\\nIt is the process of determining how processing time increases as the size of the problem (input\\nsize) increases. Input size is the number of elements in the input, and depending on the problem\\ntype, the input may be of different types. The following are the common types of inputs.\\nSize of an array\\nPolynomial degree\\nNumber of elements in a matrix\\nNumber of bits in the binary representation of the input\\nVertices and edges in a graph.\\n1.9 How to Compare Algorithms\\nTo compare algorithms, let us define a few objective measures:\\nExecution times? Not a good measure as execution times are specific to a particular computer.\\nNumber of statements executed? Not a good measure, since the number of statements varies\\nwith the programming language as well as the style of the individual programmer.\\nIdeal solution? Let us assume that we express the running time of a given algorithm as a function\\nof the input size n (i.e., f(n)) and compare these different functions corresponding to running\\ntimes. This kind of comparison is independent of machine time, programming style, etc.\\n1.10 What is Rate of Growth?\\nThe rate at which the running time increases as a function of input is called rate of growth. Let us'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 21, 'file_type': 'pdf'}, page_content='assume that you go to a shop to buy a car and a bicycle. If your friend sees you there and asks\\nwhat you are buying, then in general you say buying a car. This is because the cost of the car is\\nhigh compared to the cost of the bicycle (approximating the cost of the bicycle to the cost of the\\ncar).\\nFor the above-mentioned example, we can represent the cost of the car and the cost of the bicycle\\nin terms of function, and for a given function ignore the low order terms that are relatively\\ninsignificant (for large value of input size, n). As an example, in the case below, n4, 2n2, 100n\\nand 500 are the individual costs of some function and approximate to n4 since n4 is the highest\\nrate of growth.\\n1.11 Commonly Used Rates of Growth\\nThe diagram below shows the relationship between different rates of growth.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 22, 'file_type': 'pdf'}, page_content='Below is the list of growth rates you will come across in the following chapters.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 23, 'file_type': 'pdf'}, page_content='1.12 Types of Analysis\\nTo analyze the given algorithm, we need to know with which inputs the algorithm takes less time\\n(performing wel1) and with which inputs the algorithm takes a long time. We have already seen\\nthat an algorithm can be represented in the form of an expression. That means we represent the\\nalgorithm with multiple expressions: one for the case where it takes less time and another for the\\ncase where it takes more time.\\nIn general, the first case is called the best case and the second case is called the worst case for\\nthe algorithm. To analyze an algorithm we need some kind of syntax, and that forms the base for\\nasymptotic analysis/notation. There are three types of analysis:\\nWorst case\\nDefines the input for which the algorithm takes a long time (slowest\\ntime to complete).\\nInput is the one for which the algorithm runs the slowest.\\nBest case\\nDefines the input for which the algorithm takes the least time (fastest\\ntime to complete).\\nInput is the one for which the algorithm runs the fastest.\\nAverage case\\nProvides a prediction about the running time of the algorithm.\\nRun the algorithm many times, using many different inputs that come\\nfrom some distribution that generates these inputs, compute the total\\nrunning time (by adding the individual times), and divide by the\\nnumber of trials.\\nAssumes that the input is random.\\nLower Bound <= Average Time <= Upper Bound'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 24, 'file_type': 'pdf'}, page_content='For a given algorithm, we can represent the best, worst and average cases in the form of\\nexpressions. As an example, let f(n) be the function which represents the given algorithm.\\nSimilarly for the average case. The expression defines the inputs with which the algorithm takes\\nthe average running time (or memory).\\n1.13 Asymptotic Notation\\nHaving the expressions for the best, average and worst cases, for all three cases we need to\\nidentify the upper and lower bounds. To represent these upper and lower bounds, we need some\\nkind of syntax, and that is the subject of the following discussion. Let us assume that the given\\nalgorithm is represented in the form of function f(n).\\n1.14 Big-O Notation [Upper Bounding Function]\\nThis notation gives the tight upper bound of the given function. Generally, it is represented as f(n)\\n= O(g(n)). That means, at larger values of n, the upper bound of f(n) is g(n). For example, if f(n)\\n= n4 + 100n2 + 10n + 50 is the given algorithm, then n4 is g(n). That means g(n) gives the\\nmaximum rate of growth for f(n) at larger values of n.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 25, 'file_type': 'pdf'}, page_content='Let us see the O–notation with a little more detail. O–notation defined as O(g(n)) = {f(n): there\\nexist positive constants c and n0 such that 0 ≤ f(n) ≤ cg(n) for all n > n0}. g(n) is an asymptotic\\ntight upper bound for f(n). Our objective is to give the smallest rate of growth g(n) which is\\ngreater than or equal to the given algorithms’ rate of growth /(n).\\nGenerally we discard lower values of n. That means the rate of growth at lower values of n is not\\nimportant. In the figure, n0 is the point from which we need to consider the rate of growth for a\\ngiven algorithm. Below n0, the rate of growth could be different. n0 is called threshold for the\\ngiven function.\\nBig-O Visualization\\nO(g(n)) is the set of functions with smaller or the same order of growth as g(n). For example;\\nO(n2) includes O(1), O(n), O(nlogn), etc.\\nNote: Analyze the algorithms at larger values of n only. What this means is, below n0 we do not\\ncare about the rate of growth.\\nBig-O Examples\\nExample-1 Find upper bound for f(n) = 3n + 8\\nSolution: 3n + 8 ≤ 4n, for all n ≥ 8\\n∴ 3n + 8 = O(n) with c = 4 and n0 = 8'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 26, 'file_type': 'pdf'}, page_content='Example-2 Find upper bound for f(n) = n2 + 1\\nSolution: n2 + 1 ≤ 2n2, for all n ≥ 1\\n∴ n2 + 1 = O(n2) with c = 2 and n0 = 1\\nExample-3 Find upper bound for f(n) = n4 + 100n2 + 50\\nSolution: n4 + 100n2 + 50 ≤ 2n4, for all n ≥ 11\\n∴ n4 + 100n2 + 50 = O(n4 ) with c = 2 and n0 = 11\\nExample-4 Find upper bound for f(n) = 2n3 – 2n2\\nSolution: 2n3 – 2n2 ≤ 2n3, for all n > 1\\n∴ 2n3 – 2n2 = O(n3 ) with c = 2 and n0 = 1\\nExample-5 Find upper bound for f(n) = n\\nSolution: n ≤ n, for all n ≥ 1\\n∴ n = O(n) with c = 1 and n0 = 1\\nExample-6 Find upper bound for f(n) = 410\\nSolution: 410 ≤ 410, for all n > 1\\n∴ 410 = O(1) with c = 1 and n0 = 1\\nNo Uniqueness?\\nThere is no unique set of values for n0 and c in proving the asymptotic bounds. Let us consider,\\n100n + 5 = O(n). For this function there are multiple n0 and c values possible.\\nSolution1: 100n + 5 ≤ 100n + n = 101n ≤ 101n, for all n ≥ 5, n0 = 5 and c = 101 is a solution.\\nSolution2: 100n + 5 ≤ 100n + 5n = 105n ≤ 105n, for all n > 1, n0 = 1 and c = 105 is also a\\nsolution.\\n1.15 Omega-Q Notation [Lower Bounding Function]\\nSimilar to the O discussion, this notation gives the tighter lower bound of the given algorithm and\\nwe represent it as f(n) = Ω(g(n)). That means, at larger values of n, the tighter lower bound of\\nf(n) is g(n). For example, if f(n) = 100n2 + 10n + 50, g(n) is Ω(n2).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 27, 'file_type': 'pdf'}, page_content='The Ω notation can be defined as Ω(g(n)) = {f(n): there exist positive constants c and n0 such that\\n0 ≤ cg(n) ≤ f(n) for all n ≥ n0}. g(n) is an asymptotic tight lower bound for f(n). Our objective is\\nto give the largest rate of growth g(n) which is less than or equal to the given algorithm’s rate of\\ngrowth f(n).\\nΩ Examples\\nExample-1 Find lower bound for f(n) = 5n2.\\nSolution: ∃ c, n0 Such that: 0 ≤ cn2≤ 5n2 ⇒ cn2 ≤ 5n2 ⇒ c = 5 and n0 = 1\\n∴ 5n2 = Ω(n2) with c = 5 and n0 = 1\\nExample-2 Prove f(n) = 100n + 5 ≠ Ω(n2).\\nSolution: ∃ c, n0 Such that: 0 ≤ cn2 ≤ 100n + 5\\n100n + 5 ≤ 100n + 5n(∀n ≥ 1) = 105n\\ncn2 ≤ 105n ⇒ n(cn - 105) ≤ 0\\nSince n is positive ⇒cn - 105 ≤0 ⇒ n ≤105/c\\n⇒ Contradiction: n cannot be smaller than a constant\\nExample-3 2n = Q(n), n3 = Q(n3), = O(logn).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 28, 'file_type': 'pdf'}, page_content='1.16 Theta-Θ Notation [Order Function]\\nThis notation decides whether the upper and lower bounds of a given function (algorithm) are the\\nsame. The average running time of an algorithm is always between the lower bound and the upper\\nbound. If the upper bound (O) and lower bound (Ω) give the same result, then the Θ notation will\\nalso have the same rate of growth.\\nAs an example, let us assume that f(n) = 10n + n is the expression. Then, its tight upper bound\\ng(n) is O(n). The rate of growth in the best case is g(n) = O(n).\\nIn this case, the rates of growth in the best case and worst case are the same. As a result, the\\naverage case will also be the same. For a given function (algorithm), if the rates of growth\\n(bounds) for O and Ω are not the same, then the rate of growth for the Θ case may not be the same.\\nIn this case, we need to consider all possible time complexities and take the average of those (for\\nexample, for a quick sort average case, refer to the Sorting chapter).\\nNow consider the definition of Θ notation. It is defined as Θ(g(n)) = {f(n): there exist positive\\nconstants c1,c2 and n0 such that 0 ≤ c1g(n) ≤ f(n) ≤ c2g(n) for all n ≥ n0}. g(n) is an asymptotic\\ntight bound for f(n). Θ(g(n)) is the set of functions with the same order of growth as g(n).\\nΘ Examples'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 29, 'file_type': 'pdf'}, page_content='Example 1 Find Θ bound for \\nSolution: \\n for all, n ≥ 2\\n with c1 = 1/5,c2 = 1 and n0 = 2\\nExample 2 Prove n ≠ Θ(n2)\\nSolution: c1 n2 ≤ n ≤ c2n2 ⇒ only holds for: n ≤ 1/c1\\n∴ n ≠ Θ(n2)\\nExample 3 Prove 6n3 ≠ Θ(n2)\\nSolution: c1 n2≤ 6n3 ≤ c2 n2 ⇒ only holds for: n ≤ c2 /6\\n∴ 6n3 ≠ Θ(n2)\\nExample 4 Prove n ≠ Θ(logn)\\nSolution: c1logn ≤ n ≤ c2logn ⇒ c2 ≥ \\n, ∀ n ≥ n0 – Impossible\\n1.17 Important Notes\\nFor analysis (best case, worst case and average), we try to give the upper bound (O) and lower\\nbound (Ω) and average running time (Θ). From the above examples, it should also be clear that,\\nfor a given function (algorithm), getting the upper bound (O) and lower bound (Ω) and average\\nrunning time (Θ) may not always be possible. For example, if we are discussing the best case of\\nan algorithm, we try to give the upper bound (O) and lower bound (Ω) and average running time\\nIn the remaining chapters, we generally focus on the upper bound (O) because knowing the lower\\nbound (Ω) of an algorithm is of no practical importance, and we use the Θ notation if the upper\\nbound (O) and lower bound (Ω) are the same.\\n1.18 Why is it called Asymptotic Analysis?\\nFrom the discussion above (for all three notations: worst case, best case, and average case), we\\ncan easily understand that, in every case for a given function f(n) we are trying to find another\\nfunction g(n) which approximates f(n) at higher values of n. That means g(n) is also a curve\\nwhich approximates f(n) at higher values of n.\\nIn mathematics we call such a curve an asymptotic curve. In other terms, g(n) is the asymptotic'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 30, 'file_type': 'pdf'}, page_content='curve for f(n). For this reason, we call algorithm analysis asymptotic analysis.\\n1.19 Guidelines for Asymptotic Analysis\\nThere are some general rules to help us determine the running time of an algorithm.\\nLoops: The running time of a loop is, at most, the running time of the statements\\ninside the loop (including tests) multiplied by the number of iterations.\\nTotal time = a constant c × n = c n = O(n).\\nNested loops: Analyze from the inside out. Total running time is the product of the\\nsizes of all the loops.\\nTotal time = c × n × n = cn2 = O(n2).\\nConsecutive statements: Add the time complexities of each statement.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 31, 'file_type': 'pdf'}, page_content='Total time = c0 + c1n + c2n2 = O(n2).\\nIf-then-else statements: Worst-case running time: the test, plus either the then part\\nor the else part (whichever is the larger).\\nTotal time = c0 + c1 + (c2 + c3) * n = O(n).\\nLogarithmic complexity: An algorithm is O(logn) if it takes a constant time to cut\\nthe problem size by a fraction (usually by ½). As an example let us consider the\\nfollowing program:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 32, 'file_type': 'pdf'}, page_content='If we observe carefully, the value of i is doubling every time. Initially i = 1, in next step i\\n= 2, and in subsequent steps i = 4,8 and so on. Let us assume that the loop is executing\\nsome k times. At kth step 2k = n, and at (k + 1)th step we come out of the loop. Taking\\nlogarithm on both sides, gives\\nTotal time = O(logn).\\nNote: Similarly, for the case below, the worst case rate of growth is O(logn). The same\\ndiscussion holds good for the decreasing sequence as well.\\nAnother example: binary search (finding a word in a dictionary of n pages)\\nLook at the center point in the dictionary\\nIs the word towards the left or right of center?\\nRepeat the process with the left or right part of the dictionary until the word is found.\\n1.20 Simplyfying properties of asymptotic notations\\nTransitivity: f(n) = Θ(g(n)) and g(n) = Θ(h(n)) ⇒ f(n) = Θ(h(n)). Valid for O and Ω\\nas well.\\nReflexivity: f(n) = Θ(f(n)). Valid for O and Ω.\\nSymmetry: f(n) = Θ(g(n)) if and only if g(n) = Θ(f(n)).\\nTranspose symmetry: f(n) = O(g(n)) if and only if g(n) = Ω(f(n)).\\nIf f(n) is in O(kg(n)) for any constant k > 0, then f(n) is in O(g(n)).\\nIf f1(n) is in O(g1(n)) and f2(n) is in O(g2(n)), then (f1 + f2)(n) is in O(max(g1(n)),\\n(g1(n))).\\nIf f1(n) is in O(g1(n)) and f2(n) is in O(g2(n)) then f1(n) f2(n) is in O(g1(n) g1(n)).\\n1.21 Commonly used Logarithms and Summations\\nLogarithms'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 33, 'file_type': 'pdf'}, page_content='Arithmetic series\\nGeometric series\\nHarmonic series\\nOther important formulae\\n1.22 Master Theorem for Divide and Conquer Recurrences\\nAll divide and conquer algorithms (also discussed in detail in the Divide and Conquer chapter)\\ndivide the problem into sub-problems, each of which is part of the original problem, and then\\nperform some additional work to compute the final answer. As an example, a merge sort\\nalgorithm [for details, refer to Sorting chapter] operates on two sub-problems, each of which is\\nhalf the size of the original, and then performs O(n) additional work for merging. This gives the'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 34, 'file_type': 'pdf'}, page_content='running time equation:\\nThe following theorem can be used to determine the running time of divide and conquer\\nalgorithms. For a given program (algorithm), first we try to find the recurrence relation for the\\nproblem. If the recurrence is of the below form then we can directly give the answer without fully\\nsolving it. If the recurrence is of the form \\n, where a ≥ 1,b >\\n1,k ≥ 0 and p is a real number, then:\\nIf a > bk, then \\nIf a= bk\\na.\\nIf p > –1, then \\nb.\\nIf p = –1, then \\nc.\\nIf p < –1, then \\nIf a < bk\\na.\\nIf p ≥ 0, then T(n) = Θ(nklogpn)\\nb.\\nIf p < 0, then T(n) = O(nk)\\n1.23 Divide and Conquer Master Theorem: Problems & Solutions\\nFor each of the following recurrences, give an expression for the runtime T(n) if the recurrence\\ncan be solved with the Master Theorem. Otherwise, indicate that the Master Theorem does not\\napply.\\nProblem-1\\u2003\\u2003T(n) = 3T (n/2) + n2\\nSolution: T(n) = 3T (n/2) + n2 => T (n) =Θ(n2) (Master Theorem Case 3.a)\\nProblem-2\\u2003\\u2003T(n) = 4T (n/2) + n2\\nSolution: T(n) = 4T (n/2) + n2 => T (n) = Θ(n2logn) (Master Theorem Case 2.a)\\nProblem-3\\u2003\\u2003T(n) = T(n/2) + n2\\nSolution: T(n) = T(n/2) + n2 => Θ(n2) (Master Theorem Case 3.a)\\nProblem-4\\u2003\\u2003T(n) = 2nT(n/2) + nn\\nSolution: T(n) = 2nT(n/2) + nn => Does not apply (a is not constant)\\nProblem-5\\u2003\\u2003T(n) = 16T(n/4) + n\\nSolution: T(n) = 16T (n/4) + n => T(n) = Θ(n2) (Master Theorem Case 1)\\nProblem-6\\u2003\\u2003T(n) = 2T(n/2) + nlogn'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 35, 'file_type': 'pdf'}, page_content='Solution: T(n) = 2T(n/2) + nlogn => T(n) = Θ(nlog2n) (Master Theorem Case 2.a)\\nProblem-7\\u2003\\u2003T(n) = 2T(n/2) + n/logn\\nSolution: T(n) = 2T(n/2)+ n/logn =>T(n) = Θ(nloglogn) (Master Theorem Case 2. b)\\nProblem-8\\u2003\\u2003T(n) = 2T (n/4) + n051\\nSolution: T(n) = 2T(n/4) + n051 => T (n) = Θ(n0.51) (Master Theorem Case 3.b)\\nProblem-9\\u2003\\u2003T(n) = 0.5T(n/2) + 1/n\\nSolution: T(n) = 0.5T(n/2) + 1/n => Does not apply (a < 1)\\nProblem-10\\u2003\\u2003T (n) = 6T(n/3)+ n2 logn\\nSolution: T(n) = 6T(n/3) + n2logn => T(n) = Θ(n2logn) (Master Theorem Case 3.a)\\nProblem-11\\u2003\\u2003T(n) = 64T(n/8) – n2logn\\nSolution: T(n) = 64T(n/8) – n2logn => Does not apply (function is not positive)\\nProblem-12\\u2003\\u2003T(n) = 7T(n/3) + n2\\nSolution: T(n) = 7T(n/3) + n2 => T(n) = Θ(n2) (Master Theorem Case 3.as)\\nProblem-13\\u2003\\u2003T(n) = 4T(n/2) + logn\\nSolution: T(n) = 4T(n/2) + logn => T(n) = Θ(n2) (Master Theorem Case 1)\\nProblem-14\\u2003\\u2003T(n) = 16T (n/4) + n!\\nSolution: T(n) = 16T (n/4) + n! => T(n) = Θ(n!) (Master Theorem Case 3.a)\\nProblem-15\\u2003\\u2003T(n) = \\nT(n/2) + logn\\nSolution: T(n) = \\nT(n/2) + logn => T(n) = Θ(\\n) (Master Theorem Case 1)\\nProblem-16\\u2003\\u2003T(n) = 3T(n/2) + n\\nSolution: T(n) = 3T(n/2) + n =>T(n) = Θ(nlog3) (Master Theorem Case 1)\\nProblem-17\\u2003\\u2003T(n) = 3T(n/3) + \\nSolution: T(n) = 3T(n/3) + \\n => T(n) = Θ(n) (Master Theorem Case 1)\\nProblem-18\\u2003\\u2003T(n) = 4T(n/2) + cn\\nSolution: T(n) = 4T(n/2) + cn => T(n) = Θ(n2) (Master Theorem Case 1)\\nProblem-19\\u2003\\u2003T(n) = 3T(n/4) + nlogn\\nSolution: T(n) = 3T(n/4) + nlogn => T(n) = Θ(nlogn) (Master Theorem Case 3.a)\\nProblem-20\\u2003\\u2003T (n) = 3T(n/3) + n/2\\nSolution: T(n) = 3T(n/3)+ n/2 => T (n) = Θ(nlogn) (Master Theorem Case 2.a)\\n1.24 Master Theorem for Subtract and Conquer Recurrences'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 36, 'file_type': 'pdf'}, page_content='Let T(n) be a function defined on positive n, and having the property\\nfor some constants c,a > 0,b ≥ 0,k ≥ 0, and function f(n). If f(n) is in O(nk), then\\n1.25 Variant of Subtraction and Conquer Master Theorem\\nThe solution to the equation T(n) = T(α n) + T((1 – α)n) + βn, where 0 < α < 1 and β > 0 are\\nconstants, is O(nlogn).\\n1.26 Method of Guessing and Confirming\\nNow, let us discuss a method which can be used to solve any recurrence. The basic idea behind\\nthis method is:\\nguess the answer; and then prove it correct by induction.\\nIn other words, it addresses the question: What if the given recurrence doesn’t seem to match with\\nany of these (master theorem) methods? If we guess a solution and then try to verify our guess\\ninductively, usually either the proof will succeed (in which case we are done), or the proof will\\nfail (in which case the failure will help us refine our guess).\\nAs an example, consider the recurrence \\n. This doesn’t fit into the form\\nrequired by the Master Theorems. Carefully observing the recurrence gives us the impression that\\nit is similar to the divide and conquer method (dividing the problem into \\n subproblems each\\nwith size \\n). As we can see, the size of the subproblems at the first level of recursion is n. So,\\nlet us guess that T(n) = O(nlogn), and then try to prove that our guess is correct.\\nLet’s start by trying to prove an upper bound T(n) < cnlogn:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 37, 'file_type': 'pdf'}, page_content='The last inequality assumes only that 1 ≤ c. .logn. This is correct if n is sufficiently large and for\\nany constant c, no matter how small. From the above proof, we can see that our guess is correct\\nfor the upper bound. Now, let us prove the lower bound for this recurrence.\\nThe last inequality assumes only that 1 ≥ k. .logn. This is incorrect if n is sufficiently large and\\nfor any constant k. From the above proof, we can see that our guess is incorrect for the lower\\nbound.\\nFrom the above discussion, we understood that Θ(nlogn) is too big. How about Θ(n)? The lower\\nbound is easy to prove directly:\\nNow, let us prove the upper bound for this Θ(n).\\nFrom the above induction, we understood that Θ(n) is too small and Θ(nlogn) is too big. So, we\\nneed something bigger than n and smaller than nlogn. How about \\nProving the upper bound for'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 38, 'file_type': 'pdf'}, page_content='Proving the lower bound for \\nThe last step doesn’t work. So, Θ(\\n) doesn’t work. What else is between n and nlogn?\\nHow about nloglogn? Proving upper bound for nloglogn:\\nProving lower bound for nloglogn:\\nFrom the above proofs, we can see that T(n) ≤ cnloglogn, if c ≥ 1 and T(n) ≥ knloglogn, if k ≤ 1.\\nTechnically, we’re still missing the base cases in both proofs, but we can be fairly confident at\\nthis point that T(n) = Θ(nloglogn).\\n1.27 Amortized Analysis'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 39, 'file_type': 'pdf'}, page_content='Amortized analysis refers to determining the time-averaged running time for a sequence of\\noperations. It is different from average case analysis, because amortized analysis does not make\\nany assumption about the distribution of the data values, whereas average case analysis assumes\\nthe data are not “bad” (e.g., some sorting algorithms do well on average over all input orderings\\nbut very badly on certain input orderings). That is, amortized analysis is a worst-case analysis,\\nbut for a sequence of operations rather than for individual operations.\\nThe motivation for amortized analysis is to better understand the running time of certain\\ntechniques, where standard worst case analysis provides an overly pessimistic bound. Amortized\\nanalysis generally applies to a method that consists of a sequence of operations, where the vast\\nmajority of the operations are cheap, but some of the operations are expensive. If we can show\\nthat the expensive operations are particularly rare we can change them to the cheap operations,\\nand only bound the cheap operations.\\nThe general approach is to assign an artificial cost to each operation in the sequence, such that the\\ntotal of the artificial costs for the sequence of operations bounds the total of the real costs for the\\nsequence. This artificial cost is called the amortized cost of an operation. To analyze the running\\ntime, the amortized cost thus is a correct way of understanding the overall running time – but note\\nthat particular operations can still take longer so it is not a way of bounding the running time of\\nany individual operation in the sequence.\\nWhen one event in a sequence affects the cost of later events:\\nOne particular task may be expensive.\\nBut it may leave data structure in a state that the next few operations become easier.\\nExample: Let us consider an array of elements from which we want to find the kth smallest\\nelement. We can solve this problem using sorting'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 39, 'file_type': 'pdf'}, page_content='. We can solve this problem using sorting. After sorting the given array, we just need to\\nreturn the kth element from it. The cost of performing the sort (assuming comparison based sorting\\nalgorithm) is O(nlogn). If we perform n such selections then the average cost of each selection is\\nO(nlogn/n) = O(logn). This clearly indicates that sorting once is reducing the complexity of\\nsubsequent operations.\\n1.28 Algorithms Analysis: Problems & Solutions\\nNote: From the following problems, try to understand the cases which have different\\ncomplexities (O(n), O(logn), O(loglogn) etc.).\\nProblem-21\\u2003\\u2003Find the complexity of the below recurrence:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 40, 'file_type': 'pdf'}, page_content='Solution: Let us try solving this function with substitution.\\nT(n) = 3T(n – 1)\\nT(n) = 3(3T(n – 2)) = 32T(n – 2)\\nT(n) = 32(3T(n – 3))\\nT(n) = 3nT(n – n) = 3nT(0) = 3n\\nThis clearly shows that the complexity of this function is O(3n).\\nNote: We can use the Subtraction and Conquer master theorem for this problem.\\nProblem-22\\u2003\\u2003Find the complexity of the below recurrence:\\nSolution: Let us try solving this function with substitution.\\nT(n) = 2T(n – 1) – 1\\nT(n) = 2(2T(n – 2) – 1) – 1 = 22T(n – 2) – 2 – 1\\nT(n) = 22(2T(n – 3) – 2 – 1) – 1 = 23T(n – 4) – 22 – 21 – 20\\nT(n) = 2nT(n – n) – 2n–1 – 2n–2 – 2n–3 .... 22 – 21 – 20\\nT(n) =2n – 2n–1 – 2n–2 – 2n – 3 .... 22 – 21 – 20\\nT(n) =2n – (2n – 1) [note: 2n–1 + 2n–2 + ··· + 20 = 2n]\\nT(n) = 1\\n∴ Time Complexity is O(1). Note that while the recurrence relation looks exponential, the\\nsolution to the recurrence relation here gives a different result.\\nProblem-23\\u2003\\u2003What is the running time of the following function?'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 41, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the below function:\\nWe can define the ‘s’ terms according to the relation si = si–1 + i. The value oft’ increases by 1\\nfor each iteration. The value contained in ‘s’ at the ith iteration is the sum of the first ‘(‘positive\\nintegers. If k is the total number of iterations taken by the program, then the while loop terminates\\nif:\\nProblem-24\\u2003\\u2003Find the complexity of the function given below.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 42, 'file_type': 'pdf'}, page_content='Solution:\\nIn the above-mentioned function the loop will end, if i2 > n ⇒ T(n) = O(\\n). This is similar to\\nProblem-23.\\nProblem-25\\u2003\\u2003What is the complexity of the program given below:\\nSolution: Consider the comments in the following function.\\nThe complexity of the above function is O(n2logn).\\nProblem-26\\u2003\\u2003What is the complexity of the program given below:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 43, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the following function.\\nThe complexity of the above function is O(nlog2n).\\nProblem-27\\u2003\\u2003Find the complexity of the program below.\\nSolution: Consider the comments in the function below.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 44, 'file_type': 'pdf'}, page_content='The complexity of the above function is O(n). Even though the inner loop is bounded by n, due to\\nthe break statement it is executing only once.\\nProblem-28\\u2003\\u2003Write a recursive function for the running time T(n) of the function given below.\\nProve using the iterative method that T(n) = Θ(n3).\\nSolution: Consider the comments in the function below:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 45, 'file_type': 'pdf'}, page_content='The recurrence for this code is clearly T(n) = T(n – 3) + cn2 for some constant c > 0 since each\\ncall prints out n2 asterisks and calls itself recursively on n – 3. Using the iterative method we get:\\nT(n) = T(n – 3) + cn2. Using the Subtraction and Conquer master theorem, we get T(n) = Θ(n3).\\nProblem-29\\u2003\\u2003Determine Θ bounds for the recurrence relation: \\nSolution: Using Divide and Conquer master theorem, we get O(nlog2n).\\nProblem-30\\u2003\\u2003Determine \\nbounds \\nfor \\nthe \\nrecurrence: \\nSolution: \\nSubstituting \\nin \\nthe \\nrecurrence \\nequation, \\nwe \\nget: \\n, where k is a constant. This clearly\\nsays Θ(n).\\nProblem-31\\u2003\\u2003Determine Θ bounds for the recurrence relation: T(n) = T(⌈n/2⌉) + 7.\\nSolution: Using Master Theorem we get: Θ(logn).\\nProblem-32\\u2003\\u2003Prove that the running time of the code below is Ω(logn).\\nSolution: The while loop will terminate once the value of ‘k’ is greater than or equal to the value\\nof ‘n’. In each iteration the value of ‘k’ is multiplied by 3. If i is the number of iterations, then ‘k’\\nhas the value of 3i after i iterations. The loop is terminated upon reaching i iterations when 3i ≥ n'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 46, 'file_type': 'pdf'}, page_content='↔ i ≥ log3 n, which shows that i = Ω(logn).\\nProblem-33\\u2003\\u2003Solve the following recurrence.\\nSolution: By iteration:\\nNote: We can use the Subtraction and Conquer master theorem for this problem.\\nProblem-34\\u2003\\u2003Consider the following program:\\nSolution: The recurrence relation for the running time of this program is: T(n) = T(n – 1) + T(n –\\n2) + c. Note T(n) has two recurrence calls indicating a binary tree. Each step recursively calls the\\nprogram for n reduced by 1 and 2, so the depth of the recurrence tree is O(n). The number of\\nleaves at depth n is 2n since this is a full binary tree, and each leaf takes at least O(1)\\ncomputations for the constant factor. Running time is clearly exponential in n and it is O(2n).\\nProblem-35\\u2003\\u2003Running time of following program?'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 47, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the function below:\\nIn the above code, inner loop executes n/i times for each value of i. Its running time is \\nProblem-36\\u2003\\u2003What is the complexity of \\nSolution: Using the logarithmic property, logxy = logx + logy, we can see that this problem is\\nequivalent to\\nThis shows that the time complexity = O(nlogn).\\nProblem-37\\u2003\\u2003What is the running time of the following recursive function (specified as a\\nfunction of the input value n)? First write the recurrence formula and then find its\\ncomplexity.\\nSolution: Consider the comments in the below function:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 48, 'file_type': 'pdf'}, page_content='We can assume that for asymptotical analysis k = ⌈k⌉ for every integer k ≥ 1. The recurrence for\\nthis code is \\n. Using master theorem, we get T(n) = Θ(n).\\nProblem-38\\u2003\\u2003What is the running time of the following recursive function (specified as a\\nfunction of the input value n)? First write a recurrence formula, and show its solution using\\ninduction.\\nSolution: Consider the comments in the function below:\\nThe if statement requires constant time [O(1)]. With the for loop, we neglect the loop overhead\\nand only count three times that the function is called recursively. This implies a time complexity\\nrecurrence:\\nUsing the Subtraction and Conquer master theorem, we get T(n) = Θ(3n).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 49, 'file_type': 'pdf'}, page_content='Problem-39\\u2003\\u2003Write a recursion formula for the running time T(n) of the function whose code\\nis below.\\nSolution: Consider the comments in the function below:\\nThe recurrence for this piece of code is T(n) = T(.8n) + O(n) = T(4/5n) + O(n) =4/5 T(n) + O(n).\\nApplying master theorem, we get T(n) = O(n).\\nProblem-40\\u2003\\u2003Find the complexity of the recurrence: T(n) = 2T(\\n) + logn\\nSolution: The given recurrence is not in the master theorem format. Let us try to convert this to the\\nmaster theorem format by assuming n = 2m. Applying the logarithm on both sides gives, logn =\\nmlogl ⇒ m = logn. Now, the given function becomes:\\nTo \\nmake \\nit \\nsimple \\nwe \\nassume \\nApplying the master theorem format would result in S(m) = O(mlogm).\\nIf we substitute m = logn back, T(n) = S(logn) = O((logn) loglogn).\\nProblem-41\\u2003\\u2003Find the complexity of the recurrence: T(n) = T(\\n) + 1\\nSolution: Applying the logic of Problem-40 gives \\n. Applying the master'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 50, 'file_type': 'pdf'}, page_content='theorem would result in S(m) = O(logm). Substituting m = logn, gives T(n) = S(logn) =\\nO(loglogn).\\nProblem-42\\u2003\\u2003Find the complexity of the recurrence: T(n) = 2T(\\n) + 1\\nSolution: Applying the logic of Problem-40 gives: \\n. Using the master\\ntheorem results S(m) = \\n. Substituting m = logn gives T(n) =O(logn).\\nProblem-43\\u2003\\u2003Find the complexity of the below function.\\nSolution: Consider the comments in the function below:\\nFor the above code, the recurrence function can be given as: T(n) = T(\\n) + 1. This is same as\\nthat of Problem-41.\\nProblem-44\\u2003\\u2003Analyze the running time of the following recursive pseudo-code as a function of\\nn.\\nSolution: Consider the comments in below pseudo-code and call running time of function(n) as\\nT(n).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 51, 'file_type': 'pdf'}, page_content='T(n) can be defined as follows:\\nUsing the master theorem gives: \\nProblem-45\\u2003\\u2003Find the complexity of the below pseudocode:\\nSolution: Consider the comments in the pseudocode below:\\nThe recurrence for this function is T(n) = T(n/2) + n. Using master theorem, we get T(n) = O(n).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 52, 'file_type': 'pdf'}, page_content='Problem-46\\u2003\\u2003Running time of the following program?\\nSolution: Consider the comments in the below function:\\nComplexity of above program is: O(nlogn).\\nProblem-47\\u2003\\u2003Running time of the following program?\\nSolution: Consider the comments in the below function:\\nThe time complexity of this program is: O(n2).\\nProblem-48\\u2003\\u2003Find the complexity of the below function:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 53, 'file_type': 'pdf'}, page_content='Solution: Consider the comments in the below function:\\nThe recurrence for this function is: \\n. Using master theorem, we get T(n) =\\nO(n).\\nProblem-49\\u2003\\u2003Find the complexity of the below function:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 54, 'file_type': 'pdf'}, page_content='Solution:\\nTime Complexity: O(logn * logn) = O(log2n).\\nProblem-50\\u2003\\u2003∑i≤k≤n O(n), where O(n) stands for order n is:\\n(A)\\nO(n)\\n(B)\\nO(n2)\\n(C)\\nO(n3)\\n(D)\\nO(3n2)\\n(E)\\nO(1.5n2)\\nSolution: (B). ∑i≤k≤n O(n) = O(n) ∑i≤k≤n 1 = O(n2).\\nProblem-51\\u2003\\u2003Which of the following three claims are correct?\\nI\\u2003(n + k)m = Θ(nm), where k and m are constants\\nII\\u20032n+1 = O(2n)\\nIII\\u200322n+1 = O(2n)\\n(A)\\nI and II\\n(B)\\nI and III\\n(C)\\nII and III\\n(D)\\nI, II and III\\nSolution: (A). (I) (n + k)m =nh + c1*nk–1 + ... km = Θ(nh) and (II) 2n+1 = 2*2n = O(2n)\\nProblem-52\\u2003\\u2003Consider the following functions:\\nf(n) = 2n\\ng(n) = n!\\nh(n) = nlogn\\nWhich of the following statements about the asymptotic behavior of f(n), g(n), and h(n) is\\ntrue?\\n(A)\\nf(n) = O(g(n)); g(n) = O(h(n))\\n(B)\\nf(n) = Ω (g(n)); g(n) = O(h(n))'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 55, 'file_type': 'pdf'}, page_content='(C)\\ng(n) = O(f(n)); h(n) = O(f(n))\\n(D)\\nh(n) = O(f(n)); g(n) = Ω (f(n))\\nSolution: (D). According to the rate of growth: h(n) < f(n) < g(n) (g(n) is asymptotically greater\\nthan f(n), and f(n) is asymptotically greater than h(n)). We can easily see the above order by\\ntaking logarithms of the given 3 functions: lognlogn < n < log(n!). Note that, log(n!) = O(nlogn).\\nProblem-53\\u2003\\u2003Consider the following segment of C-code:\\nThe number of comparisons made in the execution of the loop for any n > 0 is:\\n(A)\\n(B)\\nn\\n(C)\\n(D)\\nSolution: (a). Let us assume that the loop executes k times. After kth step the value of j is 2k.\\nTaking logarithms on both sides gives \\n. Since we are doing one more comparison for\\nexiting from the loop, the answer is \\nProblem-54\\u2003\\u2003Consider the following C code segment. Let T(n) denote the number of times the\\nfor loop is executed by the program on input n. Which of the following is true?\\n(A)\\nT(n) = O(\\n) and T(n) = Ω(\\n(B)\\nT(n) = O(\\n) and T(n) = Ω(1)\\n(C)\\nT(n) = O(n) and T(n) = Ω(\\n(D)\\nNone of the above\\nSolution: (B). Big O notation describes the tight upper bound and Big Omega notation describes\\nthe tight lower bound for an algorithm. The for loop in the question is run maximum \\n times and'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 56, 'file_type': 'pdf'}, page_content='minimum 1 time. Therefore, T(n) = O(\\n) and T(n) = Ω(1).\\nProblem-55\\u2003\\u2003In the following C function, let n ≥ m. How many recursive calls are made by\\nthis function?\\n(A)\\n(B)\\nΩ(n)\\n(C)\\n(D)\\nΘ(n)\\nSolution: No option is correct. Big O notation describes the tight upper bound and Big Omega\\nnotation describes the tight lower bound for an algorithm. For m = 2 and for all n = 2i, the running\\ntime is O(1) which contradicts every option.\\nProblem-56\\u2003\\u2003Suppose T(n) = 2T(n/2) + n, T(O)=T(1)=1. Which one of the following is false?\\n(A)\\nT(n) = O(n2)\\n(B)\\nT(n) = Θ(nlogn)\\n(C)\\nT(n) = Q(n2)\\n(D)\\nT(n) = O(nlogn)\\nSolution: (C). Big O notation describes the tight upper bound and Big Omega notation describes\\nthe tight lower bound for an algorithm. Based on master theorem, we get T(n) = Θ(nlogn). This\\nindicates that tight lower bound and tight upper bound are the same. That means, O(nlogn) and\\nΩ(nlogn) are correct for given recurrence. So option (C) is wrong.\\nProblem-57\\u2003\\u2003Find the complexity of the below function:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 57, 'file_type': 'pdf'}, page_content='Solution:\\nTime Complexity: O(n5).\\nProblem-58\\u2003\\u2003To calculate 9n, give an algorithm and discuss its complexity.\\nSolution: Start with 1 and multiply by 9 until reaching 9n.\\nTime Complexity: There are n – 1 multiplications and each takes constant time giving a Θ(n)\\nalgorithm.\\nProblem-59\\u2003\\u2003For Problem-58, can we improve the time complexity?\\nSolution: Refer to the Divide and Conquer chapter.\\nProblem-60\\u2003\\u2003Find the time complexity of recurrence \\nSolution: Let us solve this problem by method of guessing. The total size on each level of the\\nrecurrance tree is less than n, so we guess that f(n) = n will dominate. Assume for all i < n that\\nc1n ≤ T(i) < c2n. Then,'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 58, 'file_type': 'pdf'}, page_content='If c1 ≥ 8k and c2 ≤ 8k, then c1n = T(n) = c2n. So, T(n) = Θ(n). In general, if you have multiple\\nrecursive calls, the sum of the arguments to those calls is less than n (in this case \\nand f(n) is reasonably large, a good guess is T(n) = Θ(f(n)).\\nProblem-61\\u2003\\u2003Solve the following recurrence relation using the recursion tree method: \\nSolution: How much work do we do in each level of the recursion tree?\\nIn level 0, we take n2 time. At level 1, the two subproblems take time:\\nAt level 2 the four subproblems are of size \\n and \\n respectively. These two\\nsubproblems take time:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 59, 'file_type': 'pdf'}, page_content='Similarly the amount of work at level k is at most \\nLet \\n, the total runtime is then:\\nThat is, the first level provides a constant fraction of the total runtime.\\nProblem-62\\u2003\\u2003Rank the following functions by order of growth: (n + 1)!, n!, 4n, n × 3n, 3n + n2\\n+ 20n, \\n, n2 + 200, 20n + 500, 2lgn, n2/3, 1.\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 60, 'file_type': 'pdf'}, page_content='Problem-63\\u2003\\u2003Find the complexity of the below function:\\nSolution: Consider the worst-case.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 61, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2).\\nProblem-64\\u2003\\u2003Can we say \\nSolution: Yes: because \\nProblem-65\\u2003\\u2003Can we say 23n = O(2n)?\\nSolution: No: because 23n = (23)n = 8n not less than 2n.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 62, 'file_type': 'pdf'}, page_content='2.1 Introduction\\nIn this chapter, we will look at one of the important topics, “recursion”, which will be used in\\nalmost every chapter, and also its relative “backtracking”.\\n2.2 What is Recursion?\\nAny function which calls itself is called recursive. A recursive method solves a problem by\\ncalling a copy of itself to work on a smaller problem. This is called the recursion step. The\\nrecursion step can result in many more such recursive calls.\\nIt is important to ensure that the recursion terminates. Each time the function calls itself with a\\nslightly simpler version of the original problem. The sequence of smaller problems must\\neventually converge on the base case.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 63, 'file_type': 'pdf'}, page_content='2.3 Why Recursion?\\nRecursion is a useful technique borrowed from mathematics. Recursive code is generally shorter\\nand easier to write than iterative code. Generally, loops are turned into recursive functions when\\nthey are compiled or interpreted.\\nRecursion is most useful for tasks that can be defined in terms of similar subtasks. For example,\\nsort, search, and traversal problems often have simple recursive solutions.\\n2.4 Format of a Recursive Function\\nA recursive function performs a task in part by calling itself to perform the subtasks. At some\\npoint, the function encounters a subtask that it can perform without calling itself. This case, where\\nthe function does not recur, is called the base case. The former, where the function calls itself to\\nperform a subtask, is referred to as the ecursive case. We can write all recursive functions using\\nthe format:\\nAs an example consider the factorial function: n! is the product of all integers between n and 1.\\nThe definition of recursive factorial looks like:\\nThis definition can easily be converted to recursive implementation. Here the problem is\\ndetermining the value of n!, and the subproblem is determining the value of (n – l)!. In the\\nrecursive case, when n is greater than 1, the function calls itself to determine the value of (n – l)!\\nand multiplies that with n.\\nIn the base case, when n is 0 or 1, the function simply returns 1. This looks like the following:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 64, 'file_type': 'pdf'}, page_content='2.5 Recursion and Memory (Visualization)\\nEach recursive call makes a new copy of that method (actually only the variables) in memory.\\nOnce a method ends (that is, returns some data), the copy of that returning method is removed\\nfrom memory. The recursive solutions look simple but visualization and tracing takes time. For\\nbetter understanding, let us consider the following example.\\nFor this example, if we call the print function with n=4, visually our memory assignments may\\nlook like:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 65, 'file_type': 'pdf'}, page_content='Now, let us consider our factorial function. The visualization of factorial function with n=4 will\\nlook like:\\n2.6 Recursion versus Iteration\\nWhile discussing recursion, the basic question that comes to mind is: which way is better? –\\niteration or recursion? The answer to this question depends on what we are trying to do. A\\nrecursive approach mirrors the problem that we are trying to solve. A recursive approach makes\\nit simpler to solve a problem that may not have the most obvious of answers. But, recursion adds'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 66, 'file_type': 'pdf'}, page_content='overhead for each recursive call (needs space on the stack frame).\\nRecursion\\nTerminates when a base case is reached.\\nEach recursive call requires extra space on the stack frame (memory).\\nIf we get infinite recursion, the program may run out of memory and result in stack\\noverflow.\\nSolutions to some problems are easier to formulate recursively.\\nIteration\\nTerminates when a condition is proven to be false.\\nEach iteration does not require extra space.\\nAn infinite loop could loop forever since there is no extra memory being created.\\nIterative solutions to a problem may not always be as obvious as a recursive\\nsolution.\\n2.7 Notes on Recursion\\nRecursive algorithms have two types of cases, recursive cases and base cases.\\nEvery recursive function case must terminate at a base case.\\nGenerally, iterative solutions are more efficient than recursive solutions [due to the\\noverhead of function calls].\\nA recursive algorithm can be implemented without recursive function calls using a\\nstack, but it’s usually more trouble than its worth. That means any problem that can\\nbe solved recursively can also be solved iteratively.\\nFor some problems, there are no obvious iterative algorithms.\\nSome problems are best suited for recursive solutions while others are not.\\n2.8 Example Algorithms of Recursion\\nFibonacci Series, Factorial Finding\\nMerge Sort, Quick Sort\\nBinary Search\\nTree Traversals and many Tree Problems: InOrder, PreOrder PostOrder\\nGraph Traversals: DFS [Depth First Search] and BFS [Breadth First Search]\\nDynamic Programming Examples\\nDivide and Conquer Algorithms\\nTowers of Hanoi'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 67, 'file_type': 'pdf'}, page_content='Backtracking Algorithms [we will discuss in next section]\\n2.9 Recursion: Problems & Solutions\\nIn this chapter we cover a few problems with recursion and we will discuss the rest in other\\nchapters. By the time you complete reading the entire book, you will encounter many recursion\\nproblems.\\nProblem-1\\u2003\\u2003Discuss Towers of Hanoi puzzle.\\nSolution: The Towers of Hanoi is a mathematical puzzle. It consists of three rods (or pegs or\\ntowers), and a number of disks of different sizes which can slide onto any rod. The puzzle starts\\nwith the disks on one rod in ascending order of size, the smallest at the top, thus making a conical\\nshape. The objective of the puzzle is to move the entire stack to another rod, satisfying the\\nfollowing rules:\\nOnly one disk may be moved at a time.\\nEach move consists of taking the upper disk from one of the rods and sliding it onto\\nanother rod, on top of the other disks that may already be present on that rod.\\nNo disk may be placed on top of a smaller disk.\\nAlgorithm:\\nMove the top n – 1 disks from Source to Auxiliary tower,\\nMove the nth disk from Source to Destination tower,\\nMove the n – 1 disks from Auxiliary tower to Destination tower.\\nTransferring the top n – 1 disks from Source to Auxiliary tower can again be thought\\nof as a fresh problem and can be solved in the same manner. Once we solve Towers\\nof Hanoi with three disks, we can solve it with any number of disks with the above\\nalgorithm.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 68, 'file_type': 'pdf'}, page_content='Problem-2\\u2003\\u2003Given an array, check whether the array is in sorted order with recursion.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n) for recursive stack space.\\n2.10 What is Backtracking?\\nBacktracking is an improvement of the brute force approach. It systematically searches for a\\nsolution to a problem among all available options. In backtracking, we start with one possible\\noption out of many available options and try to solve the problem if we are able to solve the\\nproblem with the selected move then we will print the solution else we will backtrack and select\\nsome other option and try to solve it. If none if the options work out we will claim that there is no\\nsolution for the problem.\\nBacktracking is a form of recursion. The usual scenario is that you are faced with a number of\\noptions, and you must choose one of these. After you make your choice you will get a new set of'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 69, 'file_type': 'pdf'}, page_content='options; just what set of options you get depends on what choice you made. This procedure is\\nrepeated over and over until you reach a final state. If you made a good sequence of choices, your\\nfinal state is a goal state; if you didn’t, it isn’t.\\nBacktracking can be thought of as a selective tree/graph traversal method. The tree is a way of\\nrepresenting some initial starting position (the root node) and a final goal state (one of the\\nleaves). Backtracking allows us to deal with situations in which a raw brute-force approach\\nwould explode into an impossible number of options to consider. Backtracking is a sort of refined\\nbrute force. At each node, we eliminate choices that are obviously not possible and proceed to\\nrecursively check only those that have potential.\\nWhat’s interesting about backtracking is that we back up only as far as needed to reach a previous\\ndecision point with an as-yet-unexplored alternative. In general, that will be at the most recent\\ndecision point. Eventually, more and more of these decision points will have been fully explored,\\nand we will have to backtrack further and further. If we backtrack all the way to our initial state\\nand have explored all alternatives from there, we can conclude the particular problem is\\nunsolvable'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 69, 'file_type': 'pdf'}, page_content='. If we backtrack all the way to our initial state\\nand have explored all alternatives from there, we can conclude the particular problem is\\nunsolvable. In such a case, we will have done all the work of the exhaustive recursion and known\\nthat there is no viable solution possible.\\nSometimes the best algorithm for a problem is to try all possibilities.\\nThis is always slow, but there are standard tools that can be used to help.\\nTools: algorithms for generating basic objects, such as binary strings [2n\\npossibilities for n-bit string], permutations [n!], combinations [n!/r!(n – r)!],\\ngeneral strings [k –ary strings of length n has kn possibilities], etc...\\nBacktracking speeds the exhaustive search by pruning.\\n2.11 Example Algorithms of Backtracking\\nBinary Strings: generating all binary strings\\nGenerating k – ary Strings\\nN-Queens Problem\\nThe Knapsack Problem\\nGeneralized Strings\\nHamiltonian Cycles [refer to Graphs chapter]\\nGraph Coloring Problem\\n2.12 Backtracking: Problems & Solutions\\nProblem-3\\u2003\\u2003Generate all the strings of n bits. Assume A[0..n – 1] is an array of size n.\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 70, 'file_type': 'pdf'}, page_content='Let T(n) be the running time of binary(n). Assume function printf takes time O(1).\\nUsing Subtraction and Conquer Master theorem we get: T(n) = O(2n). This means the algorithm\\nfor generating bit-strings is optimal.\\nProblem-4\\u2003\\u2003Generate all the strings of length n drawn from 0... k – 1.\\nSolution: Let us assume we keep current k-ary string in an array A[0.. n – 1]. Call function k-\\nstring(n, k):\\nLet T(n) be the running time of k – string(n). Then,'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 71, 'file_type': 'pdf'}, page_content='Using Subtraction and Conquer Master theorem we get: T(n) = O(kn).\\nNote: For more problems, refer to String Algorithms chapter.\\nProblem-5\\u2003\\u2003Finding the length of connected cells of 1s (regions) in an matrix of Os and\\n1s: Given a matrix, each of which may be 1 or 0. The filled cells that are connected form a\\nregion. Two cells are said to be connected if they are adjacent to each other horizontally,\\nvertically or diagonally. There may be several regions in the matrix. How do you find the\\nlargest region (in terms of number of cells) in the matrix?\\nSolution: The simplest idea is: for each location traverse in all 8 directions and in each of those\\ndirections keep track of maximum region found.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 72, 'file_type': 'pdf'}, page_content='in getvalint (45 int int jn L, int H)\\niffieO || i>*L || js 0 || j>*H)\\nreturn 0;\\nelse\\n\\nreturn Al\\n\\n‘oid finaxBlockfint (4, nt, int int Lint Hint sie, bool entarr in masz)\\nif{r>=L || c>=H)\\nreturn;\\ncntar (true;\\nsize;\\nif [size > mansze\\nmaxsize = sie;\\n[search in eight directions\\nint direction [21,0 1-1}0- (1, 1}{0,0,0s1 0h 5\\nforint 0; i i+4)\\nint newi=rirectioni0};\\nint newjsctdretioni[;\\nint valegetval (A,newi,new},L,H);\\nif vabO Qs entarr{newilinewi|>*fals})\\nfindMaxBlock(A.newi ne LH, size ntarr,maxsie);\\n\\ncntar¢false;\\n\\nint getMaxOnes(int (4[5}, int rmex, int cola,\\n‘int masize=0;\\nint size=0;\\nbool “entarereatddafmacoa'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 73, 'file_type': 'pdf'}, page_content='Sample Call:\\nProblem-6\\u2003\\u2003Solve the recurrence T(n) = 2T(n – 1) + 2n.\\nSolution: At each level of the recurrence tree, the number of problems is double from the\\nprevious level, while the amount of work being done in each problem is half from the previous\\nlevel. Formally, the ith level has 2i problems, each requiring 2n–i work. Thus the ith level requires\\nexactly 2n work. The depth of this tree is n, because at the ith level, the originating call will be\\nT(n – i). Thus the total complexity for T(n) is T(n2n).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 74, 'file_type': 'pdf'}, page_content='3.1 What is a Linked List?\\nA linked list is a data structure used for storing collections of data. A linked list has the following\\nproperties.\\nSuccessive elements are connected by pointers\\nThe last element points to NULL\\nCan grow or shrink in size during execution of a program\\nCan be made just as long as required (until systems memory exhausts)\\nDoes not waste memory space (but takes some extra memory for pointers). It\\nallocates memory as list grows.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 75, 'file_type': 'pdf'}, page_content='3.2 Linked Lists ADT\\nThe following operations make linked lists an ADT:\\nMain Linked Lists Operations\\nInsert: inserts an element into the list\\nDelete: removes and returns the specified position element from the list\\nAuxiliary Linked Lists Operations\\nDelete List: removes all elements of the list (disposes the list)\\nCount: returns the number of elements in the list\\nFind nth node from the end of the list\\n3.3 Why Linked Lists?\\nThere are many other data structures that do the same thing as linked lists. Before discussing\\nlinked lists it is important to understand the difference between linked lists and arrays. Both\\nlinked lists and arrays are used to store collections of data, and since both are used for the same\\npurpose, we need to differentiate their usage. That means in which cases arrays are suitable and\\nin which cases linked lists are suitable.\\n3.4 Arrays Overview\\nOne memory block is allocated for the entire array to hold the elements of the array. The array\\nelements can be accessed in constant time by using the index of the particular element as the\\nsubscript.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 76, 'file_type': 'pdf'}, page_content='Why Constant Time for Accessing Array Elements?\\nTo access an array element, the address of an element is computed as an offset from the base\\naddress of the array and one multiplication is needed to compute what is supposed to be added to\\nthe base address to get the memory address of the element. First the size of an element of that data\\ntype is calculated and then it is multiplied with the index of the element to get the value to be\\nadded to the base address.\\nThis process takes one multiplication and one addition. Since these two operations take constant\\ntime, we can say the array access can be performed in constant time.\\nAdvantages of Arrays\\nSimple and easy to use\\nFaster access to the elements (constant access)\\nDisadvantages of Arrays\\nPreallocates all needed memory up front and wastes memory space for indices in the\\narray that are empty.\\nFixed size: The size of the array is static (specify the array size before using it).\\nOne block allocation: To allocate the array itself at the beginning, sometimes it may\\nnot be possible to get the memory for the complete array (if the array size is big).\\nComplex position-based insertion: To insert an element at a given position, we may\\nneed to shift the existing elements. This will create a position for us to insert the\\nnew element at the desired position. If the position at which we want to add an\\nelement is at the beginning, then the shifting operation is more expensive.\\nDynamic Arrays\\nDynamic array (also called as growable array, resizable array, dynamic table, or array list) is a\\nrandom access, variable-size list data structure that allows elements to be added or removed.\\nOne simple way of implementing dynamic arrays is to initially start with some fixed size array.\\nAs soon as that array becomes full, create the new array double the size of the original array.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 77, 'file_type': 'pdf'}, page_content='Similarly, reduce the array size to half if the elements in the array are less than half.\\nNote: We will see the implementation for dynamic arrays in the Stacks, Queues and Hashing\\nchapters.\\nAdvantages of Linked Lists\\nLinked lists have both advantages and disadvantages. The advantage of linked lists is that they can\\nbe expanded in constant time. To create an array, we must allocate memory for a certain number\\nof elements. To add more elements to the array when full, we must create a new array and copy\\nthe old array into the new array. This can take a lot of time.\\nWe can prevent this by allocating lots of space initially but then we might allocate more than we\\nneed and waste memory. With a linked list, we can start with space for just one allocated element\\nand add on new elements easily without the need to do any copying and reallocating.\\nIssues with Linked Lists (Disadvantages)\\nThere are a number of issues with linked lists. The main disadvantage of linked lists is access\\ntime to individual elements. Array is random-access, which means it takes O(1) to access any\\nelement in the array. Linked lists take O(n) for access to an element in the list in the worst case.\\nAnother advantage of arrays in access time is spacial locality in memory. Arrays are defined as\\ncontiguous blocks of memory, and so any array element will be physically near its neighbors. This\\ngreatly benefits from modern CPU caching methods.\\nAlthough the dynamic allocation of storage is a great advantage, the overhead with storing and\\nretrieving data can make a big difference. Sometimes linked lists are hard to manipulate. If the\\nlast item is deleted, the last but one must then have its pointer changed to hold a NULL reference.\\nThis requires that the list is traversed to find the last but one link, and its pointer set to a NULL\\nreference.\\nFinally, linked lists waste memory in terms of extra reference points.\\n3.5 Comparison of Linked Lists with Arrays & Dynamic Arrays'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 78, 'file_type': 'pdf'}, page_content='3.6 Singly Linked Lists\\nGenerally “linked list” means a singly linked list. This list consists of a number of nodes in which\\neach node has a next pointer to the following element. The link of the last node in the list is\\nNULL, which indicates the end of the list.\\nFollowing is a type declaration for a linked list of integers:\\nBasic Operations on a List'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 79, 'file_type': 'pdf'}, page_content='Traversing the list\\nInserting an item in the list\\nDeleting an item from the list\\nTraversing the Linked List\\nLet us assume that the head points to the first node of the list. To traverse the list we do the\\nfollowing\\nFollow the pointers.\\nDisplay the contents of the nodes (or count) as they are traversed.\\nStop when the next pointer points to NULL.\\nThe ListLength() function takes a linked list as input and counts the number of nodes in the list.\\nThe function given below can be used for printing the list data with extra print function.\\nTime Complexity: O(n), for scanning the list of size n.\\nSpace Complexity: O(1), for creating a temporary variable.\\nSingly Linked List Insertion\\nInsertion into a singly-linked list has three cases:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 80, 'file_type': 'pdf'}, page_content='Inserting a new node before the head (at the beginning)\\nInserting a new node after the tail (at the end of the list)\\nInserting a new node at the middle of the list (random location)\\nNote: To insert an element in the linked list at some position p, assume that after inserting the\\nelement the position of this new node is p.\\nInserting a Node in Singly Linked List at the Beginning\\nIn this case, a new node is inserted before the current head node. Only one next pointer needs to\\nbe modified (new node’s next pointer) and it can be done in two steps:\\nUpdate the next pointer of new node, to point to the current head.\\nUpdate head pointer to point to the new node.\\nInserting a Node in Singly Linked List at the Ending\\nIn this case, we need to modify two next pointers (last nodes next pointer and new nodes next\\npointer).\\nNew nodes next pointer points to NULL.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 81, 'file_type': 'pdf'}, page_content='Last nodes next pointer points to the new node.\\nInserting a Node in Singly Linked List at the Middle\\nLet us assume that we are given a position where we want to insert the new node. In this case\\nalso, we need to modify two next pointers.\\nIf we want to add an element at position 3 then we stop at position 2. That means we\\ntraverse 2 nodes and insert the new node. For simplicity let us assume that the\\nsecond node is called position node. The new node points to the next node of the\\nposition where we want to add this node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 82, 'file_type': 'pdf'}, page_content='Position node’s next pointer now points to the new node.\\nLet us write the code for all three cases. We must update the first element pointer in the calling\\nfunction, not just in the called function. For this reason we need to send a double pointer. The\\nfollowing code inserts a node in the singly linked list.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 83, 'file_type': 'pdf'}, page_content='Note: We can implement the three variations of the insert operation separately.\\nTime Complexity: O(n), since, in the worst case, we may need to insert the node at the end of the\\nlist.\\nSpace Complexity: O(1), for creating one temporary variable.\\nSingly Linked List Deletion'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 84, 'file_type': 'pdf'}, page_content='Similar to insertion, here we also have three cases.\\nDeleting the first node\\nDeleting the last node\\nDeleting an intermediate node.\\nDeleting the First Node in Singly Linked List\\nFirst node (current head node) is removed from the list. It can be done in two steps:\\nCreate a temporary node which will point to the same node as that of head.\\nNow, move the head nodes pointer to the next node and dispose of the temporary\\nnode.\\nDeleting the Last Node in Singly Linked List\\nIn this case, the last node is removed from the list. This operation is a bit trickier than removing\\nthe first node, because the algorithm should find a node, which is previous to the tail. It can be\\ndone in three steps:\\nTraverse the list and while traversing maintain the previous node address also. By\\nthe time we reach the end of the list, we will have two pointers, one pointing to the\\ntail node and the other pointing to the node before the tail node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 85, 'file_type': 'pdf'}, page_content='Update previous node’s next pointer with NULL.\\nDispose of the tail node.\\nDeleting an Intermediate Node in Singly Linked List\\nIn this case, the node to be removed is always located between two nodes. Head and tail links\\nare not updated in this case. Such a removal can be done in two steps:\\nSimilar to the previous case, maintain the previous node while traversing the list.\\nOnce we find the node to be deleted, change the previous node’s next pointer to the\\nnext pointer of the node to be deleted.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 86, 'file_type': 'pdf'}, page_content='4 ena “\\n\\n4 40 > NULL\\n\\nHead Previous node Node to be deleted\\n\\n+ Dispose of the current node to be deleted.\\n\\n> NULL\\n\\nHead Previous node Node to be deleted'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 87, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). In the worst case, we may need to delete the node at the end of the list.\\nSpace Complexity: O(1), for one temporary variable.\\nDeleting Singly Linked List'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 88, 'file_type': 'pdf'}, page_content='This works by storing the current node in some temporary variable and freeing the current node.\\nAfter freeing the current node, go to the next node with a temporary variable and repeat this\\nprocess for all nodes.\\nTime Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for creating one temporary variable.\\n3.7 Doubly Linked Lists\\nThe advantage of a doubly linked list (also called two – way linked list) is that given a node in\\nthe list, we can navigate in both directions. A node in a singly linked list cannot be removed\\nunless we have the pointer to its predecessor. But in a doubly linked list, we can delete a node\\neven if we don’t have the previous node’s address (since each node has a left pointer pointing to\\nthe previous node and can move backward).\\nThe primary disadvantages of doubly linked lists are:\\nEach node requires an extra pointer, requiring more space.\\nThe insertion or deletion of a node takes a bit longer (more pointer operations).\\nSimilar to a singly linked list, let us implement the operations of a doubly linked list. If you\\nunderstand the singly linked list operations, then doubly linked list operations are obvious.\\nFollowing is a type declaration for a doubly linked list of integers:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 89, 'file_type': 'pdf'}, page_content='Doubly Linked List Insertion\\nInsertion into a doubly-linked list has three cases (same as singly linked list):\\nInserting a new node before the head.\\nInserting a new node after the tail (at the end of the list).\\nInserting a new node at the middle of the list.\\nInserting a Node in Doubly Linked List at the Beginning\\nIn this case, new node is inserted before the head node. Previous and next pointers need to be\\nmodified and it can be done in two steps:\\nUpdate the right pointer of the new node to point to the current head node (dotted\\nlink in below figure) and also make left pointer of new node as NULL.\\nUpdate head node’s left pointer to point to the new node and make new node as\\nhead. Head'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 90, 'file_type': 'pdf'}, page_content='Inserting a Node in Doubly Linked List at the Ending\\nIn this case, traverse the list till the end and insert the new node.\\nNew node right pointer points to NULL and left pointer points to the end of the list.\\nUpdate right pointer of last node to point to new node.\\nInserting a Node in Doubly Linked List at the Middle\\nAs discussed in singly linked lists, traverse the list to the position node and insert the new node.\\nNew node right pointer points to the next node of the position node where we want\\nto insert the new node. Also, new node left pointer points to the position node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 91, 'file_type': 'pdf'}, page_content='Position node right pointer points to the new node and the next node of position node\\nleft pointer points to new node.\\nNow, let us write the code for all of these three cases. We must update the first element pointer in\\nthe calling function, not just in the called function. For this reason we need to send a double\\npointer. The following code inserts a node in the doubly linked list'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 92, 'file_type': 'pdf'}, page_content=\"void DLLinsert(struct DLLNode “head, int data, int position) {\\nintk= 1;\\nstruct DLINode *temp, ‘newNode;\\n‘newNode = (struct DLLNode *) malloc(szeof ( struct DLLNode };\\n\\niffinewNode)( [Always check for memory errors\\nprintf Memory Ero)\\nreturn,\\n\\nnewNode-data * data;\\n\\niflposition == 1} [ | /lnsertng a node atthe beginning\\n\\nnewNode-next = *head;\\nnewNode-prev = NULL;\\nifhead)\\n(head) prev = newNode;\\n“head = newNode;\\nreturn;\\ntemp = *head;\\nwhile ( < position - 1) 8 temp-net!*NULL {\\ntemp = temp—next;\\nkt\\niffkl=position},\\nprint{l’Desired position does not exist\\\\n');\\nnewNode-next=temp—next;\\nnewNode-prev=temp;\\nifftemp-next)\\ntemp-mnextsprevenewNode;\\n\\n‘temp—next=newNode;\\nreturn;\"),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 93, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). In the worst case, we may need to insert the node at the end of the list.\\nSpace Complexity: O(1), for creating one temporary variable.\\nDoubly Linked List Deletion\\nSimilar to singly linked list deletion, here we have three cases:\\nDeleting the first node\\nDeleting the last node\\nDeleting an intermediate node\\nDeleting the First Node in Doubly Linked List\\nIn this case, the first node (current head node) is removed from the list. It can be done in two\\nsteps:\\nCreate a temporary node which will point to the same node as that of head.\\nNow, move the head nodes pointer to the next node and change the heads left pointer\\nto NULL. Then, dispose of the temporary node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 94, 'file_type': 'pdf'}, page_content='Deleting the Last Node in Doubly Linked List\\nThis operation is a bit trickier than removing the first node, because the algorithm should find a\\nnode, which is previous to the tail first. This can be done in three steps:\\nTraverse the list and while traversing maintain the previous node address also. By\\nthe time we reach the end of the list, we will have two pointers, one pointing to the\\ntail and the other pointing to the node before the tail.\\nUpdate the next pointer of previous node to the tail node with NULL.\\nDispose the tail node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 95, 'file_type': 'pdf'}, page_content='Deleting an Intermediate Node in Doubly Linked List\\nIn this case, the node to be removed is always located between two nodes, and the head and tail\\nlinks are not updated. The removal can be done in two steps:\\nSimilar to the previous case, maintain the previous node while also traversing the\\nlist. Upon locating the node to be deleted, change the previous node’s next pointer\\nto the next node of the node to be deleted.\\nDispose of the current node to be deleted.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 96, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for creating one temporary variable.\\n3.8 Circular Linked Lists'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 97, 'file_type': 'pdf'}, page_content='In singly linked lists and doubly linked lists, the end of lists are indicated with NULL value. But\\ncircular linked lists do not have ends. While traversing the circular linked lists we should be\\ncareful; otherwise we will be traversing the list infinitely. In circular linked lists, each node has a\\nsuccessor. Note that unlike singly linked lists, there is no node with NULL pointer in a circularly\\nlinked list. In some situations, circular linked lists are useful.\\nFor example, when several processes are using the same computer resource (CPU) for the same\\namount of time, we have to assure that no process accesses the resource before all other\\nprocesses do (round robin algorithm). The following is a type declaration for a circular linked\\nlist of integers:\\nIn a circular linked list, we access the elements using the head node (similar to head node in\\nsingly linked list and doubly linked lists).\\nCounting Nodes in a Circular Linked List\\nThe circular list is accessible through the node marked head. To count the nodes, the list has to be\\ntraversed from the node marked head, with the help of a dummy node current, and stop the\\ncounting when current reaches the starting node head.\\nIf the list is empty, head will be NULL, and in that case set count = 0. Otherwise, set the current\\npointer to the first node, and keep on counting till the current pointer reaches the starting node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 98, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for creating one temporary variable.\\nPrinting the Contents of a Circular Linked List\\nWe assume here that the list is being accessed by its head node. Since all the nodes are arranged\\nin a circular fashion, the tail node of the list will be the node previous to the head node. Let us\\nassume we want to print the contents of the nodes starting with the head node. Print its contents,\\nmove to the next node and continue printing till we reach the head node again.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 99, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for temporary variable.\\nInserting a Node at the End of a Circular Linked List\\nLet us add a node containing data, at the end of a list (circular list) headed by head. The new\\nnode will be placed just after the tail node (which is the last node of the list), which means it will\\nhave to be inserted in between the tail node and the first node.\\nCreate a new node and initially keep its next pointer pointing to itself.\\nUpdate the next pointer of the new node with the head node and also traverse the list\\nto the tail. That means in a circular list we should stop at the node whose next node\\nis head.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 100, 'file_type': 'pdf'}, page_content='Update the next pointer of the previous node to point to the new node and we get the\\nlist as shown below.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 101, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for temporary variable.\\nInserting a Node at the Front of a Circular Linked List\\nThe only difference between inserting a node at the beginning and at the end is that, after inserting\\nthe new node, we just need to update the pointer. The steps for doing this are given below:\\nCreate a new node and initially keep its next pointer pointing to itself.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 102, 'file_type': 'pdf'}, page_content='Update the next pointer of the new node with the head node and also traverse the list\\nuntil the tail. That means in a circular list we should stop at the node which is its\\nprevious node in the list.\\nUpdate the previous head node in the list to point to the new node.\\nMake the new node as the head.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 103, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for temporary variable.\\nDeleting the Last Node in a Circular Linked List\\nThe list has to be traversed to reach the last but one node. This has to be named as the tail node,\\nand its next field has to point to the first node. Consider the following list.\\nTo delete the last node 40, the list has to be traversed till you reach 7. The next field of 7 has to'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 104, 'file_type': 'pdf'}, page_content='be changed to point to 60, and this node must be renamed pTail.\\nTraverse the list and find the tail node and its previous node.\\nUpdate the next pointer of tail node’s previous node to point to head.\\nDispose of the tail node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 105, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n. Space Complexity: O(1), for a\\ntemporary variable.\\nDeleting the First Node in a Circular List\\nThe first node can be deleted by simply replacing the next field of the tail node with the next field\\nof the first node.\\nFind the tail node of the linked list by traversing the list. Tail node is the previous\\nnode to the head node which we want to delete.\\nCreate a temporary node which will point to the head. Also, update the tail nodes\\nnext pointer to point to next node of head (as shown below).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 106, 'file_type': 'pdf'}, page_content='Now, move the head pointer to next node. Create a temporary node which will point\\nto head. Also, update the tail nodes next pointer to point to next node of head (as\\nshown below).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 107, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), for scanning the complete list of size n.\\nSpace Complexity: O(1), for a temporary variable.\\nApplications of Circular List\\nCircular linked lists are used in managing the computing resources of a computer. We can use\\ncircular lists for implementing stacks and queues.\\n3.9 A Memory-efficient Doubly Linked List\\nIn conventional implementation, we need to keep a forward pointer to the next item on the list and\\na backward pointer to the previous item. That means elements in doubly linked list\\nimplementations consist of data, a pointer to the next node and a pointer to the previous node in\\nthe list as shown below.\\nConventional Node Definition'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 108, 'file_type': 'pdf'}, page_content='Recently a journal (Sinha) presented an alternative implementation of the doubly linked list ADT,\\nwith insertion, traversal and deletion operations. This implementation is based on pointer\\ndifference. Each node uses only one pointer field to traverse the list back and forth.\\nNew Node Definition\\nThe ptrdiff pointer field contains the difference between the pointer to the next node and the\\npointer to the previous node. The pointer difference is calculated by using exclusive-or (⊕)\\noperation.\\nptrdiff = pointer to previous node ⊕ pointer to next node.\\nThe ptrdiff of the start node (head node) is the ⊕ of NULL and next node (next node to head).\\nSimilarly, the ptrdiff of end node is the ⊕ of previous node (previous to end node) and NULL. As\\nan example, consider the following linked list.\\nIn the example above,\\nThe next pointer of A is: NULL ⊕ B\\nThe next pointer of B is: A ⊕ C\\nThe next pointer of C is: B ⊕ D\\nThe next pointer of D is: C ⊕ NULL'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 109, 'file_type': 'pdf'}, page_content='Why does it work?\\nTo find the answer to this question let us consider the properties of ⊕:\\nX ⊕ X=0\\nX ⊕ 0 = X\\nX ⊕ Y = Y ⊕ X (symmetric)\\n(X ⊕ Y) ⊕ Z = X ⊕ (Y ⊕ Z) (transitive)\\nFor the example above, let us assume that we are at C node and want to move to B. We know that\\nC’s ptrdiff is defined as B ⊕ D. If we want to move to B, performing ⊕ on C’s ptrdiff with D\\nwould give B. This is due to the fact that\\n(B ⊕ D) ⊕ D = B(since, D ⊕ D= 0)\\nSimilarly, if we want to move to D, then we have to apply ⊕ to C’s ptrdiff with B to give D.\\n(B ⊕ D) ⊕ B = D (since, B © B=0)\\nFrom the above discussion we can see that just by using a single pointer, we can move back and\\nforth. A memory-efficient implementation of a doubly linked list is possible with minimal\\ncompromising of timing efficiency.\\n3.10 Unrolled Linked Lists\\nOne of the biggest advantages of linked lists over arrays is that inserting an element at any\\nlocation takes only O(1) time. However, it takes O(n) to search for an element in a linked list.\\nThere is a simple variation of the singly linked list called unrolled linked lists.\\nAn unrolled linked list stores multiple elements in each node (let us call it a block for our\\nconvenience). In each block, a circular linked list is used to connect all nodes.\\nAssume that there will be no more than n elements in the unrolled linked list at any time. To\\nsimplify this problem, all blocks, except the last one, should contain exactly \\n elements. Thus,'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 110, 'file_type': 'pdf'}, page_content='there will be no more than \\n blocks at any time.\\nSearching for an element in Unrolled Linked Lists\\nIn unrolled linked lists, we can find the kth element in O(\\nTraverse the list of blocks to the one that contains the kth node, i.e., the \\nblock. It takes O(\\n) since we may find it by going through no more than \\nblocks.\\nFind the (k mod \\n)th node in the circular linked list of this block. It also takes O(\\n) since there are no more than \\n nodes in a single block.\\nInserting an element in Unrolled Linked Lists'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 111, 'file_type': 'pdf'}, page_content='When inserting a node, we have to re-arrange the nodes in the unrolled linked list to maintain the\\nproperties previously mentioned, that each block contains \\n nodes. Suppose that we insert a\\nnode x after the ith node, and x should be placed in the jth block. Nodes in the jth block and in the\\nblocks after the jth block have to be shifted toward the tail of the list so that each of them still\\nhave \\n nodes. In addition, a new block needs to be added to the tail if the last block of the list\\nis out of space, i.e., it has more than \\n nodes.\\nPerforming Shift Operation\\nNote that each shift operation, which includes removing a node from the tail of the circular linked\\nlist in a block and inserting a node to the head of the circular linked list in the block after, takes\\nonly O(1). The total time complexity of an insertion operation for unrolled linked lists is therefore\\nO(\\n); there are at most O(\\n) blocks and therefore at most O(\\n) shift operations.\\nA temporary pointer is needed to store the tail of A.\\nIn block A, move the next pointer of the head node to point to the second-to-last\\nnode, so that the tail node of A can be removed.\\nLet the next pointer of the node, which will be shifted (the tail node of A), point\\nto the tail node of B.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 112, 'file_type': 'pdf'}, page_content='Let the next pointer of the head node of B point to the node temp points to.\\nFinally, set the head pointer of B to point to the node temp points to. Now the\\nnode temp points to becomes the new head node of B.\\ntemp pointer can be thrown away. We have completed the shift operation to\\nmove the original tail node of A to become the new head node of B.\\nPerformance'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 113, 'file_type': 'pdf'}, page_content='With unrolled linked lists, there are a couple of advantages, one in speed and one in space. First,\\nif the number of elements in each block is appropriately sized (e.g., at most the size of one cache\\nline), we get noticeably better cache performance from the improved memory locality. Second,\\nsince we have O(n/m) links, where n is the number of elements in the unrolled linked list and m is\\nthe number of elements we can store in any block, we can also save an appreciable amount of\\nspace, which is particularly noticeable if each element is small.\\nComparing Linked Lists and Unrolled Linked Lists\\nTo compare the overhead for an unrolled list, elements in doubly linked list implementations\\nconsist of data, a pointer to the next node, and a pointer to the previous node in the list, as shown\\nbelow.\\nAssuming we have 4 byte pointers, each node is going to take 8 bytes. But the allocation overhead\\nfor the node could be anywhere between 8 and 16 bytes. Let’s go with the best case and assume it\\nwill be 8 bytes. So, if we want to store IK items in this list, we are going to have 16KB of\\noverhead.\\nNow, let’s think about an unrolled linked list node (let us call it LinkedBlock). It will look\\nsomething like this:\\nTherefore, allocating a single node (12 bytes + 8 bytes of overhead) with an array of 100\\nelements (400 bytes + 8 bytes of overhead) will now cost 428 bytes, or 4.28 bytes per element.\\nThinking about our IK items from above, it would take about 4.2KB of overhead, which is close\\nto 4x better than our original list. Even if the list becomes severely fragmented and the item arrays\\nare only 1/2 full on average, this is still an improvement. Also, note that we can tune the array\\nsize to whatever gets us the best overhead for our application.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 114, 'file_type': 'pdf'}, page_content='Implementation'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 115, 'file_type': 'pdf'}, page_content='struct LinkedBlockt\\n‘struct LinkedBlock *next;\\n{nt nodeCoust\\n\\nis\\n\\nstruct LinkedBlock* blockHead;\\n\\nJ /eveate an empty block\\n\\nruct LinkedBlock* nevLinleedBlock)}|\\nstruct LinkedBlock\" blocks(struct LinkedBlock\")mallocsizeofistruct LinkedBlock));\\n‘lock--next=NULL;\\nblockhead =NUL\\n\\n[/ereate a node\\nstruct ListNode\" newListNodelint value),\\n‘struct ListNede\" temp>(struct ListNode*)malloc{sizeofstruct ListNode)};\\ntemp-next>NULL:\\ntemp—value-value;\\nreturn temps\\nvoid searchElementiint k,struct LinkedBlock *LinkedBlock,struct ListNode **ListNode\\n[find the block\\nint jofkblockSize-1)/blockSize; //k-th node is inthe jth block\\nstruct Linkedilock\" prblockHead:\\nwhile\\naa\\n*LinkedBloce\\n[fod the nose\\n‘struct ListNede* qrp-—head:\\neeloblock Size;\\niffk==0} k-blockSize:\\n\\n{/staxt shift operation from block\\n‘oid shiftstruct LinkedBlock *A),\\n‘struct LinkedBlock B;\\nStruct ListNede* temp:\\n‘while(A-snodeCount > blockSize), //if this block still have to shift\\nMA-next~-NULL //reach the end. A little different\\nAnsnext=newLinkedBleck):\\nBeA-next,\\ntemp-A--head—nest;\\nAvshead-next-A-shead-next-né'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 117, 'file_type': 'pdf'}, page_content='B-enodeCount++;\\nAB;\\nvoid addElement(int k.int 3),\\n‘struct ListNode *p,\\nstruct LinkedBlock\\nif fblockHead) / intial, first node and block\\nblockHead-newLinkedBlock’);\\nblockHead-shead=newListNode(x);\\nDlockHead-shead-next-blockHead-shead;\\nblockdfead—-nodeCount* +5,\\nJetset\\niffke=O}, //special case fork\\nprblockHead--head:\\n\\n-pnext;\\npoonext=newListNode x}\\nPoonext-snexteg:\\nDlockHtead—head-p—next;\\nDlockHead-nodeCount*:\\n\\nap:\\nwhile(q-next!=p) q-q-~next\\nqunext-newLiatNode(x)\\nquonext-onext-p:\\nFonodeCount\\n\\nint searchElementfint kj,\\nstruct ListNode *p:\\nstruct LinkedBlock *q\\nsearchElement(k,84,8p):\\nreturn p-value:\\n\\nint testUnRolledLinkedList{\\nint ttrelockt)'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 118, 'file_type': 'pdf'}, page_content='3.11 Skip Lists\\nBinary trees can be used for representing abstract data types such as dictionaries and ordered\\nlists. They work well when the elements are inserted in a random order. Some sequences of\\noperations, such as inserting the elements in order, produce degenerate data structures that give\\nvery poor performance. If it were possible to randomly permute the list of items to be inserted,\\ntrees would work well with high probability for any input sequence. In most cases queries must\\nbe answered on-line, so randomly permuting the input is impractical. Balanced tree algorithms re-\\narrange the tree as operations are performed to maintain certain balance conditions and assure\\ngood performance.\\nSkip lists are a probabilistic alternative to balanced trees. Skip list is a data structure that can be\\nused as an alternative to balanced binary trees (refer to Trees chapter). As compared to a binary\\ntree, skip lists allow quick search, insertion and deletion of elements. This is achieved by using\\nprobabilistic balancing rather than strictly enforce balancing. It is basically a linked list with\\nadditional pointers such that intermediate nodes can be skipped. It uses a random number\\ngenerator to make some decisions.\\nIn an ordinary sorted linked list, search, insert, and delete are in O(n) because the list must be\\nscanned node-by-node from the head to find the relevant node. If somehow we could scan down\\nthe list in bigger steps (skip down, as it were), we would reduce the cost of scanning. This is the\\nfundamental idea behind Skip Lists.\\nSkip Lists with One Level\\nSkip Lists with Two Levels\\nSkip Lists with Three Levels'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 119, 'file_type': 'pdf'}, page_content='Performance\\nIn a simple linked list that consists of n elements, to perform a search n comparisons are required\\nin the worst case. If a second pointer pointing two nodes ahead is added to every node, the\\nnumber of comparisons goes down to n/2 + 1 in the worst case.\\nAdding one more pointer to every fourth node and making them point to the fourth node ahead\\nreduces the number of comparisons to ⌈n/2⌉ + 2. If this strategy is continued so that every node\\nwith i pointers points to 2 * i – 1 nodes ahead, O(logn) performance is obtained and the number\\nof pointers has only doubled (n + n/2 + n/4 + n/8 + n/16 + .... = 2n).\\nThe find, insert, and remove operations on ordinary binary search trees are efficient, O(logn),\\nwhen the input data is random; but less efficient, O(n), when the input data is ordered. Skip List\\nperformance for these same operations and for any data set is about as good as that of randomly-\\nbuilt binary search trees - namely O(logn).\\nComparing Skip Lists and Unrolled Linked Lists\\nIn simple terms, Skip Lists are sorted linked lists with two differences:\\nThe nodes in an ordinary list have one next reference. The nodes in a Skip List have\\nmany next references (also called forward references).\\nThe number of forward references for a given node is determined probabilistically.\\nWe speak of a Skip List node having levels, one level per forward reference. The number of\\nlevels in a node is called the size of the node. In an ordinary sorted list, insert, remove, and find\\noperations require sequential traversal of the list. This results in O(n) performance per operation.\\nSkip Lists allow intermediate nodes in the list to be skipped during a traversal - resulting in an\\nexpected performance of O(logn) per operation.\\nImplementation'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 120, 'file_type': 'pdf'}, page_content='‘struct ListNode *insertElementint data) (\\ninti, newLevel;\\nstruct ListNode *update/MAXSKIPLEVEL* 1];\\nstruct ListNode “temp:\\ntemp = list header:\\nfor (= list istLevel i >= 0; i\\n‘while temp-nexti] Ilia. header && temp-nextf}-sdata < data)\\ntemp = temp--nextil;\\nupdate( = temp;\\n\\n{or (newLevel = 0; rand) < RAND_MAX/2 && newLevel « MAXSKIPLEVEL; newLevelt +);\\nif newLevel > list listLeve) {\\nfor (i= ist lstLevel + 1:4 <= newLevel\\n‘updatel = ist header,\\nlit ntLevel = nena\\n// make new node\\nif (temp = malloc(sizeof{Node) +\\nrnewLevelsizeof{Node *)) == 0) |\\nprintf insufficient memory (insertElement)\\\\n\")\\nexit);\\ntemp—data = data;\\nfor i= 0: i<= newLevel: i+*){ // update next links\\n‘temp--nexti] = updateli--nextis\\njoe toe be!\\n, ttsrntempk\\n// delete node containing data\\nvoid deleteElementint data) (\\nint fs\\nstruct ListNode *update[MAXSKIPLEVEL* 1}, *temp;\\ntemp = list header:\\nfor (= list listLevel i >= 0:5)\\n‘while (temp-onext[ = lit header && temp-onextf|-edata < data)\\ntemp = temp—nexti;\\n\\nee\\ntemp = temp—next{0};\\n\\nif (temp == lis-header | | temp-edata == data) return;\\n{ /adjust next pointers\\n\\nfor i= 0: 1 <= list listLevel: i++) {\\n\\nif (upaateli|~nexti = temp) break:\\n1 Neel « toe'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 122, 'file_type': 'pdf'}, page_content='free (temp);\\n\\nadjust header level\\n\\nhile (list.listLevel > 0) && list header—>next[istlistLevel == list.header})\\nlist listLevel--;\\n\\n// find node containing data\\n‘struct ListNode “indElementint data) {\\ninti;\\nstruct ListNode *temp = list header;\\nfor (i= list.listLevel i >= 0; i-)\\nwhile (temp--nexti I= ls header\\n‘&& temp-nexti|data < data)\\ntemp = temp-next{l;\\n\\ntemp = temp-next};\\nif temp I ist header && temp\\nreturn(0};\\n1 [initialize skip ist\\n‘void inthis) (\\ninti\\nif istheader = mallo(sizeofstruct ListNode) + MAXSKIPLEVEL*sizeofistruc ListNode*) == 0)\\nprintf (Memory Error\\\\n\\'}\\nexit)\\n\\n== data) return (temp);\\n\\n‘MAXSKIPLEVEL; i++)\\nlistheader-snexth]» list header;\\nlist.istLevel = 0;\\n/* command-line: skipList maxnum skipList 2000: process 2000 sequential records */\\nint mainjint arge, char *argy){\\n\\nsmalloc(maxnum * sizeof(a}) =~ 0) {\\n{print (stderr, “insuficient memory (a)\\\\n\"}\\nexit);\\n\\nfor = 0; §< maxnum; i+) ai = rand;\\nprint! (Random, Yi items\\\\n’, maxnum);\\n\\nfor (i= 0;i< maxnum; i+) {\\ninsertElement(lji;\\n\\nfor (i= maxnum-t; i>= 0; +) {\\nfindElement(ai);'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 123, 'file_type': 'pdf'}, page_content='3.12 Linked Lists: Problems & Solutions\\nProblem-1\\u2003\\u2003Implement Stack using Linked List.\\nSolution: Refer to Stacks chapter.\\nProblem-2\\u2003\\u2003Find nth node from the end of a Linked List.\\nSolution: Brute-Force Method: Start with the first node and count the number of nodes present\\nafter that node. If the number of nodes is < n – 1 then return saying “fewer number of nodes in the\\nlist”. If the number of nodes is > n – 1 then go to next node. Continue this until the numbers of\\nnodes after current node are n – 1.\\nTime Complexity: O(n2), for scanning the remaining list (from current node) for each node.\\nSpace Complexity: O(1).\\nProblem-3\\u2003\\u2003Can we improve the complexity of Problem-2?\\nSolution: Yes, using hash table. As an example consider the following list.\\nIn this approach, create a hash table whose entries are < position of node, node address >. That\\nmeans, key is the position of the node in the list and value is the address of that node.\\nPosition in List\\nAddress of Node\\nAddress of 5 node\\nAddress of 1 node\\nAddress of 17 node\\nAddress of 4 node\\nBy the time we traverse the complete list (for creating the hash table), we can find the list length.\\nLet us say the list length is M. To find nth from the end of linked list, we can convert this to M- n\\n+ 1th from the beginning. Since we already know the length of the list, it is just a matter of'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 124, 'file_type': 'pdf'}, page_content='returning M- n + 1th key value from the hash table.\\nTime Complexity: Time for creating the hash table, T(m) = O(m).\\nSpace Complexity: Since we need to create a hash table of size m, O(m).\\nProblem-4\\u2003\\u2003Can we use the Problem-3 approach for solving Problem-2 without creating the\\nhash table?\\nSolution: Yes. If we observe the Problem-3 solution, what we are actually doing is finding the\\nsize of the linked list. That means we are using the hash table to find the size of the linked list. We\\ncan find the length of the linked list just by starting at the head node and traversing the list.\\nSo, we can find the length of the list without creating the hash table. After finding the length,\\ncompute M – n + 1 and with one more scan we can get the M – n+ 1th node from the beginning.\\nThis solution needs two scans: one for finding the length of the list and the other for finding M –\\nn+ 1th node from the beginning.\\nTime Complexity: Time for finding the length + Time for finding the M – n + 1th node from the\\nbeginning. Therefore, T(n) = O(n) + O(n) ≈ O(n). Space Complexity: O(1). Hence, no need to\\ncreate the hash table.\\nProblem-5\\u2003\\u2003Can we solve Problem-2 in one scan?\\nSolution: Yes. Efficient Approach: Use two pointers pNthNode and pTemp. Initially, both point\\nto head node of the list. pNthNode starts moving only after pTemp has made n moves.\\nFrom there both move forward until pTemp reaches the end of the list. As a result pNthNode\\npoints to nth node from the end of the linked list.\\nNote: At any point of time both move one node at a time.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 125, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-6\\u2003\\u2003Check whether the given linked list is either NULL-terminated or ends in a cycle\\n(cyclic).\\nSolution: Brute-Force Approach. As an example, consider the following linked list which has a\\nloop in it. The difference between this list and the regular list is that, in this list, there are two\\nnodes whose next pointers are the same. In regular singly linked lists (without a loop) each node’s\\nnext pointer is unique.\\nThat means the repetition of next pointers indicates the existence of a loop.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 126, 'file_type': 'pdf'}, page_content='One simple and brute force way of solving this is, start with the first node and see whether there\\nis any node whose next pointer is the current node’s address. If there is a node with the same\\naddress then that indicates that some other node is pointing to the current node and we can say a\\nloop exists. Continue this process for all the nodes of the linked list.\\nDoes this method work? As per the algorithm, we are checking for the next pointer addresses,\\nbut how do we find the end of the linked list (otherwise we will end up in an infinite loop)?\\nNote: If we start with a node in a loop, this method may work depending on the size of the loop.\\nProblem-7\\u2003\\u2003Can we use the hashing technique for solving Problem-6?\\nSolution: Yes. Using Hash Tables we can solve this problem.\\nAlgorithm:\\nTraverse the linked list nodes one by one.\\nCheck if the address of the node is available in the hash table or not.\\nIf it is already available in the hash table, that indicates that we are visiting the node\\nthat was already visited. This is possible only if the given linked list has a loop in\\nit.\\nIf the address of the node is not available in the hash table, insert that node’s address\\ninto the hash table.\\nContinue this process until we reach the end of the linked list or we find the loop.\\nTime Complexity; O(n) for scanning the linked list. Note that we are doing a scan of only the\\ninput.\\nSpace Complexity; O(n) for hash table.\\nProblem-8\\u2003\\u2003Can we solve Problem-6 using the sorting technique?\\nSolution: No. Consider the following algorithm which is based on sorting. Then we see why this'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 127, 'file_type': 'pdf'}, page_content='algorithm fails.\\nAlgorithm:\\nTraverse the linked list nodes one by one and take all the next pointer values into an\\narray.\\nSort the array that has the next node pointers.\\nIf there is a loop in the linked list, definitely two next node pointers will be pointing\\nto the same node.\\nAfter sorting if there is a loop in the list, the nodes whose next pointers are the same\\nwill end up adjacent in the sorted list.\\nIf any such pair exists in the sorted list then we say the linked list has a loop in it.\\nTime Complexity; O(nlogn) for sorting the next pointers array.\\nSpace Complexity; O(n) for the next pointers array.\\nProblem with the above algorithm: The above algorithm works only if we can find the length of\\nthe list. But if the list has a loop then we may end up in an infinite loop. Due to this reason the\\nalgorithm fails.\\nProblem-9\\u2003\\u2003Can we solve the Problem-6 in O(n)?\\nSolution: Yes. Efficient Approach (Memoryless Approach): This problem was solved by\\nFloyd. The solution is named the Floyd cycle finding algorithm. It uses two pointers moving at\\ndifferent speeds to walk the linked list. Once they enter the loop they are expected to meet, which\\ndenotes that there is a loop.\\nThis works because the only way a faster moving pointer would point to the same location as a\\nslower moving pointer is if somehow the entire list or a part of it is circular. Think of a tortoise\\nand a hare running on a track. The faster running hare will catch up with the tortoise if they are\\nrunning in a loop. As an example, consider the following example and trace out the Floyd\\nalgorithm. From the diagrams below we can see that after the final step they are meeting at some\\npoint in the loop which may not be the starting point of the loop.\\nNote: slowPtr (tortoise) moves one pointer at a time and fastPtr (hare) moves two pointers at a\\ntime.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 128, 'file_type': 'pdf'}, page_content='slowPtr\\n\\nfastPtr\\n\\nslowPtr  fastPtr\\n\\nfastPtr\\nslowPtr\\nslowPtr\\nfastPtr\\nslowPtr\\nfastPt\\nfastPtr slowPt\\nslowPtr\\n\\nfastPtr'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 129, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-10\\u2003\\u2003are given a pointer to the first element of a linked list L. There are two\\npossibilities for L: it either ends (snake) or its last element points back to one of the\\nearlier elements in the list (snail). Give an algorithm that tests whether a given list L is a\\nsnake or a snail.\\nSolution: It is the same as Problem-6.\\nProblem-11\\u2003\\u2003Check whether the given linked list is NULL-terminated or not. If there is a\\ncycle find the start node of the loop.\\nSolution: The solution is an extension to the solution in Problem-9. After finding the loop in the\\nlinked list, we initialize the slowPtr to the head of the linked list. From that point onwards both\\nslowPtr and fastPtr move only one node at a time. The point at which they meet is the start of the\\nloop. Generally we use this method for removing the loops.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 130, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-12\\u2003\\u2003From the previous discussion and problems we understand that the meeting of\\ntortoise and hare concludes the existence of the loop, but how does moving the tortoise to\\nthe beginning of the linked list while keeping the hare at the meeting place, followed by\\nmoving both one step at a time, make them meet at the starting point of the cycle?\\nSolution: This problem is at the heart of number theory. In the Floyd cycle finding algorithm,\\nnotice that the tortoise and the hare will meet when they are n × L, where L is the loop length.\\nFurthermore, the tortoise is at the midpoint between the hare and the beginning of the sequence\\nbecause of the way they move. Therefore the tortoise is n × L away from the beginning of the\\nsequence as well. If we move both one step at a time, from the position of the tortoise and from\\nthe start of the sequence, we know that they will meet as soon as both are in the loop, since they\\nare n × L, a multiple of the loop length, apart. One of them is already in the loop, so we just move\\nthe other one in single step until it enters the loop, keeping the other n × L away from it at all\\ntimes.\\nProblem-13\\u2003\\u2003In the Floyd cycle finding algorithm, does it work if we use steps 2 and 3\\ninstead of 1 and 2?'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 131, 'file_type': 'pdf'}, page_content='Solution: Yes, but the complexity might be high. Trace out an example.\\nProblem-14\\u2003\\u2003Check whether the given linked list is NULL-terminated. If there is a cycle, find\\nthe length of the loop.\\nSolution: This solution is also an extension of the basic cycle detection problem. After finding the\\nloop in the linked list, keep the slowPtr as it is. The fastPtr keeps on moving until it again comes\\nback to slowPtr. While moving fastPtr, use a counter variable which increments at the rate of 1.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-15\\u2003\\u2003Insert a node in a sorted linked list.\\nSolution: Traverse the list and find a position for the element and insert it.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 132, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-16\\u2003\\u2003Reverse a singly linked list.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(1).\\nRecursive version: We will find it easier to start from the bottom up, by asking and answering\\ntiny questions (this is the approach in The Little Lisper):\\nWhat is the reverse of NULL (the empty list)? NULL.\\nWhat is the reverse of a one element list? The element itself.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 133, 'file_type': 'pdf'}, page_content='What is the reverse of an n element list? The reverse of the second element followed\\nby the first element.\\nTime Complexity: O(n). Space Complexity: O(n),for recursive stack.\\nProblem-17\\u2003\\u2003Suppose there are two singly linked lists both of which intersect at some point\\nand become a single linked list. The head or start pointers of both the lists are known, but\\nthe intersecting node is not known. Also, the number of nodes in each of the lists before\\nthey intersect is unknown and may be different in each list. List1 may have n nodes before\\nit reaches the intersection point, and List2 might have m nodes before it reaches the\\nintersection point where m and n may be m = n,m < n or m > n. Give an algorithm for\\nfinding the merging point.\\nSolution: Brute-Force Approach: One easy solution is to compare every node pointer in the first\\nlist with every other node pointer in the second list by which the matching node pointers will lead\\nus to the intersecting node. But, the time complexity in this case will be O(mn) which will be\\nhigh.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 134, 'file_type': 'pdf'}, page_content='Time Complexity: O(mn). Space Complexity: O(1).\\nProblem-18\\u2003\\u2003Can we solve Problem-17 using the sorting technique?\\nSolution: No. Consider the following algorithm which is based on sorting and see why this\\nalgorithm fails.\\nAlgorithm:\\nTake first list node pointers and keep them in some array and sort them.\\nTake second list node pointers and keep them in some array and sort them.\\nAfter sorting, use two indexes: one for the first sorted array and the other for the\\nsecond sorted array.\\nStart comparing values at the indexes and increment the index according to\\nwhichever has the lower value (increment only if the values are not equal).\\nAt any point, if we are able to find two indexes whose values are the same, then that\\nindicates that those two nodes are pointing to the same node and we return that\\nnode.\\nTime Complexity: Time for sorting lists + Time for scanning (for comparing)\\n= O(mlogm) +O(nlogn) +O(m + n) We need to consider the one that gives the\\nmaximum value.\\nSpace Complexity: O(1).\\nAny problem with the above algorithm? Yes. In the algorithm, we are storing all the node\\npointers of both the lists and sorting. But we are forgetting the fact that there can be many repeated\\nelements'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 134, 'file_type': 'pdf'}, page_content='. In the algorithm, we are storing all the node\\npointers of both the lists and sorting. But we are forgetting the fact that there can be many repeated\\nelements. This is because after the merging point, all node pointers are the same for both the lists.\\nThe algorithm works fine only in one case and it is when both lists have the ending node at their\\nmerge point.\\nProblem-19\\u2003\\u2003Can we solve Problem-17 using hash tables?\\nSolution: Yes.\\nAlgorithm:\\nSelect a list which has less number of nodes (If we do not know the lengths\\nbeforehand then select one list randomly).\\nNow, traverse the other list and for each node pointer of this list check whether the\\nsame node pointer exists in the hash table.\\nIf there is a merge point for the given lists then we will definitely encounter the node\\npointer in the hash table.\\nTime Complexity: Time for creating the hash table + Time for scanning the second list = O(m) +\\nO(n) (or O(n) + O(m), depending on which list we select for creating the hash table. But in both'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 135, 'file_type': 'pdf'}, page_content='cases the time complexity is the same. Space Complexity: O(n) or O(m).\\nProblem-20\\u2003\\u2003Can we use stacks for solving the Problem-17?\\nSolution: Yes.\\nAlgorithm:\\nCreate two stacks: one for the first list and one for the second list.\\nTraverse the first list and push all the node addresses onto the first stack.\\nTraverse the second list and push all the node addresses onto the second stack.\\nNow both stacks contain the node address of the corresponding lists.\\nNow compare the top node address of both stacks.\\nIf they are the same, take the top elements from both the stacks and keep them in\\nsome temporary variable (since both node addresses are node, it is enough if we\\nuse one temporary variable).\\nContinue this process until the top node addresses of the stacks are not the same.\\nThis point is the one where the lists merge into a single list.\\nReturn the value of the temporary variable.\\nTime Complexity: O(m + n), for scanning both the lists.\\nSpace Complexity: O(m + n), for creating two stacks for both the lists.\\nProblem-21\\u2003\\u2003Is there any other way of solving Problem-17?\\nSolution: Yes. Using “finding the first repeating number” approach in an array (for algorithm\\nrefer to Searching chapter).\\nAlgorithm:\\nCreate an array A and keep all the next pointers of both the lists in the array.\\nIn the array find the first repeating element [Refer to Searching chapter for\\nalgorithm].\\nThe first repeating number indicates the merging point of both the lists.\\nTime Complexity: O(m + n). Space Complexity: O(m + n).\\nProblem-22\\u2003\\u2003Can we still think of finding an alternative solution for Problem-17?\\nSolution: Yes. By combining sorting and search techniques we can reduce the complexity.\\nAlgorithm:\\nCreate an array A and keep all the next pointers of the first list in the array.\\nSort these array elements.\\nThen, for each of the second list elements, search in the sorted array (let us assume'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 136, 'file_type': 'pdf'}, page_content='that we are using binary search which gives O(logn)).\\nSince we are scanning the second list one by one, the first repeating element that\\nappears in the array is nothing but the merging point.\\nTime Complexity: Time for sorting + Time for searching = O(Max(mlogm, nlogn)).\\nSpace Complexity: O(Max(m, n)).\\nProblem-23\\u2003\\u2003Can we improve the complexity for Problem-17?\\nSolution: Yes.\\nEfficient Approach:\\nFind lengths (L1 and L2) of both lists - O(n) + O(m) = O(max(m, n)).\\nTake the difference d of the lengths -- O(1).\\nMake d steps in longer list -- O(d).\\nStep in both lists in parallel until links to next node match -- O(min(m, n)).\\nTotal time complexity = O(max(m, n)).\\nSpace Complexity = O(1).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 137, 'file_type': 'pdf'}, page_content='Problem-24\\u2003\\u2003How will you find the middle of the linked list?\\nSolution: Brute-Force Approach: For each of the node, count how many nodes are there in the\\nlist, and see whether it is the middle node of the list.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-25\\u2003\\u2003Can we improve the complexity of Problem-24?'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 138, 'file_type': 'pdf'}, page_content='Solution: Yes.\\nAlgorithm:\\nTraverse the list and find the length of the list.\\nAfter finding the length, again scan the list and locate n/2 node from the beginning.\\nTime Complexity: Time for finding the length of the list + Time for locating middle node = O(n) +\\nO(n) ≈ O(n).\\nSpace Complexity: O(1).\\nProblem-26\\u2003\\u2003Can we use the hash table for solving Problem-24?\\nSolution: Yes. The reasoning is the same as that of Problem-3.\\nTime Complexity: Time for creating the hash table. Therefore, T(n) = O(n).\\nSpace Complexity: O(n). Since we need to create a hash table of size n.\\nProblem-27\\u2003\\u2003Can we solve Problem-24 just in one scan?\\nSolution: Efficient Approach: Use two pointers. Move one pointer at twice the speed of the\\nsecond. When the first pointer reaches the end of the list, the second pointer will be pointing to\\nthe middle node.\\nNote: If the list has an even number of nodes, the middle node will be of ⌊n/2⌋.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 139, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-28\\u2003\\u2003How will you display a Linked List from the end?\\nSolution: Traverse recursively till the end of the linked list. While coming back, start printing the\\nelements.\\nTime Complexity: O(n). Space Complexity: O(n)→ for Stack.\\nProblem-29\\u2003\\u2003Check whether the given Linked List length is even or odd?\\nSolution: Use a 2x pointer. Take a pointer that moves at 2x [two nodes at a time]. At the end, if\\nthe length is even, then the pointer will be NULL; otherwise it will point to the last node.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 140, 'file_type': 'pdf'}, page_content='Time Complexity: O(⌊n/2⌋) ≈ O(n). Space Complexity: O(1).\\nProblem-30\\u2003\\u2003If the head of a Linked List is pointing to kth element, then how will you get the\\nelements before kth element?\\nSolution: Use Memory Efficient Linked Lists [XOR Linked Lists].\\nProblem-31\\u2003\\u2003Given two sorted Linked Lists, how to merge them into the third list in sorted\\norder?\\nSolution: Assume the sizes of lists are m and n.\\nRecursive:\\nTime Complexity: O(n + m), where n and m are lengths of two lists.\\nIterative:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 141, 'file_type': 'pdf'}, page_content='Time Complexity: O(n + m), where n and m are lengths of two lists.\\nProblem-32\\u2003\\u2003Reverse the linked list in pairs. If you have a linked list that holds 1 → 2 → 3\\n→ 4 → X, then after the function has been called the linked list would hold 2 → 1 → 4 →\\n3 → X.\\nSolution:\\nRecursive:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 142, 'file_type': 'pdf'}, page_content='Iterative:\\nTime Complexity: O(n). Space Complexity: O(1).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 143, 'file_type': 'pdf'}, page_content='Problem-33\\u2003\\u2003Given a binary tree convert it to doubly linked list.\\nSolution: Refer Trees chapter.\\nProblem-34\\u2003\\u2003How do we sort the Linked Lists?\\nSolution: Refer Sorting chapter.\\nProblem-35\\u2003\\u2003Split a Circular Linked List into two equal parts. If the number of nodes in the\\nlist are odd then make first list one node extra than second list.\\nSolution:\\nAlgorithm:\\nStore the mid and last pointers of the circular linked list using Floyd cycle finding\\nalgorithm.\\nMake the second half circular.\\nMake the first half circular.\\nSet head pointers of the two linked lists.\\nAs an example, consider the following circular list.\\nAfter the split, the above list will look like:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 144, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-36\\u2003\\u2003If we want to concatenate two linked lists which of the following gives O(1)\\ncomplexity?\\nSingly linked lists\\nDoubly linked lists\\nCircular doubly linked lists\\nSolution: Circular Doubly Linked Lists. This is because for singly and doubly linked lists, we'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 145, 'file_type': 'pdf'}, page_content='need to traverse the first list till the end and append the second list. But in the case of circular\\ndoubly linked lists we don’t have to traverse the lists.\\nProblem-37\\u2003\\u2003How will you check if the linked list is palindrome or not?\\nSolution:\\nAlgorithm:\\nGet the middle of the linked list.\\nReverse the second half of the linked list.\\nCompare the first half and second half.\\nConstruct the original linked list by reversing the second half again and\\nattaching it back to the first half.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-38\\u2003\\u2003For a given K value (K > 0) reverse blocks of K nodes in a list.\\nExample: Input: 1 2 3 4 5 6 7 8 9 10. Output for different K values:\\nFor K = 2: 2 1 4 3 6 5 8 7 10 9\\nFor K = 3: 3 2 1 6 5 4 9 8 7 10\\nFor K = 4: 4 3 2 1 8 7 6 5 9 10\\nSolution:\\nAlgorithm: This is an extension of swapping nodes in a linked list.\\nCheck if remaining list has K nodes.\\na.\\nIf yes get the pointer of K + 1th node.\\nb.\\nElse return.\\nReverse first K nodes.\\nSet next of last node (after reversal) to K + 1th node.\\nMove to K + 1th node.\\nGo to step 1.\\nK – 1th node of first K nodes becomes the new head if available. Otherwise, we can\\nreturn the head.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 146, 'file_type': 'pdf'}, page_content='struct ListNode * GetkPhusOneThNodefintK, struct ListNode “head {\\nstruct ListNode*Kth;\\ninti=0;\\nithead)\\nreturn head;\\nfor (i= 0, Kth = head; Kth & (i<j; t+, Kth» Kth-sneat)\\nili== K 88 Kth I= NULL)\\nreturn Kt;\\nretum head-neat;\\n\\n{nt HasKnodes(struct ListNode *head, int K){\\ninti=0;\\n\\nead & (i < K); i++, head = head—next);\\n\\nstruct ListNode *ReverseBlockOIK-nodestnlinkedList{structListNode *head, int K){\\nstruet ListNode ‘cur = head, temp, ‘next, newHead;\\ninti\\n‘ffK==0 || Ke=1)\\nreturn head;\\niflHasKnodes(cur, K-))\\nnewHead = GetKPlusOneThNode(K-1, cu)\\nelse\\nnewHead » head;\\n\\nwhile(cur && HasKnodes(cur, K) {\\ntemp = GetkPlusOneThNode(K, cur);\\nwhile <K)\\nnext » cur-snext;\\nccur-snext=temp;\\ntemp = cur;\\nccur = next;\\nitt;\\n\\ni\\nreturn newHead;'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 147, 'file_type': 'pdf'}, page_content='Problem-39\\u2003\\u2003Is it possible to get O(1) access time for Linked Lists?\\nSolution: Yes. Create a linked list and at the same time keep it in a hash table. For n elements we\\nhave to keep all the elements in a hash table which gives a preprocessing time of O(n).To read\\nany element we require only constant time O(1) and to read n elements we require n * 1 unit of\\ntime = n units. Hence by using amortized analysis we can say that element access can be\\nperformed within O(1) time.\\nTime Complexity – O(1) [Amortized]. Space Complexity - O(n) for Hash Table.\\nProblem-40\\u2003\\u2003Josephus Circle: N people have decided to elect a leader by arranging\\nthemselves in a circle and eliminating every Mth person around the circle, closing ranks as\\neach person drops out. Find which person will be the last one remaining (with rank 1).\\nSolution: Assume the input is a circular linked list with N nodes and each node has a number\\n(range 1 to N) associated with it. The head node has number 1 as data.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 148, 'file_type': 'pdf'}, page_content='Problem-41\\u2003\\u2003Given a linked list consists of data, a next pointer and also a random pointer\\nwhich points to a random node of the list. Give an algorithm for cloning the list.\\nSolution: We can use a hash table to associate newly created nodes with the instances of node in\\nthe given list.\\nAlgorithm:\\nScan the original list and for each node X, create a new node Y with data of X, then\\nstore the pair (X, Y) in hash table using X as a key. Note that during this scan set Y\\n→ next and Y → random to NULL and we will fix them in the next scan.\\nNow for each node X in the original list we have a copy Y stored in our hash table.\\nWe scan the original list again and set the pointers building the new list.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 149, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-42\\u2003\\u2003Can we solve Problem-41 without any extra space?\\nSolution: Yes.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 150, 'file_type': 'pdf'}, page_content='Time Complexity: O(3n) ≈ O(n). Space Complexity: O(1).\\nProblem-43\\u2003\\u2003We are given a pointer to a node (not the tail node) in a singly linked list. Delete\\nthat node from the linked list.\\nSolution: To delete a node, we have to adjust the next pointer of the previous node to point to the'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 151, 'file_type': 'pdf'}, page_content='next node instead of the current one. Since we don’t have a pointer to the previous node, we can’t\\nredirect its next pointer. So what do we do? We can easily get away by moving the data from the\\nnext node into the current node and then deleting the next node.\\nTime Complexity: O(1). Space Complexity: O(1).\\nProblem-44\\u2003\\u2003Given a linked list with even and odd numbers, create an algorithm for making\\nchanges to the list in such a way that all even numbers appear at the beginning.\\nSolution: To solve this problem, we can use the splitting logic. While traversing the list, split the\\nlinked list into two: one contains all even nodes and the other contains all odd nodes. Now, to get\\nthe final list, we can simply append the odd node linked list after the even node linked list.\\nTo split the linked list, traverse the original linked list and move all odd nodes to a separate\\nlinked list of all odd nodes. At the end of the loop, the original list will have all the even nodes\\nand the odd node list will have all the odd nodes. To keep the ordering of all nodes the same, we\\nmust insert all the odd nodes at the end of the odd node list.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-45\\u2003\\u2003In a linked list with n nodes, the time taken to insert an element after an element\\npointed by some pointer is\\n(A)\\nO(1)\\n(B)\\nO(logn)\\n(C)\\nO(n)\\n(D)\\nO(nlogn)\\nSolution: A.\\nProblem-46\\u2003\\u2003Find modular node: Given a singly linked list, write a function to find the last\\nelement from the beginning whose n%k == 0, where n is the number of elements in the list\\nand k is an integer constant. For example, if n = 19 and k = 3 then we should return 18th\\nnode.\\nSolution: For this problem the value of n is not known in advance.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 152, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-47\\u2003\\u2003Find modular node from the end: Given a singly linked list, write a function to\\nfind the first from the end whose n%k == 0, where n is the number of elements in the list\\nand k is an integer constant. If n = 19 and k = 3 then we should return 16th node.\\nSolution: For this problem the value of n is not known in advance and it is the same as finding the\\nkth element from the end of the the linked list.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 153, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-48\\u2003\\u2003Find fractional node: Given a singly linked list, write a function to find the \\n element, where n is the number of elements in the list.\\nSolution: For this problem the value of n is not known in advance.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 154, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-49\\u2003\\u2003Find \\n node: Given a singly linked list, write a function to find the \\nelement, where n is the number of elements in the list. Assume the value of n is not known\\nin advance.\\nSolution: For this problem the value of n is not known in advance.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 155, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-50\\u2003\\u2003Given two lists List 1 = {A1, A2, . . . , An) and List2 = {B1, B2, . . . , Bm} with\\ndata (both lists) in ascending order. Merge them into the third list in ascending order so\\nthat the merged list will be:\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 156, 'file_type': 'pdf'}, page_content='Time Complexity: The while loop takes O(min(n,m)) time as it will run for min(n,m) times. The\\nother steps run in O(1). Therefore the total time complexity is O(min(n,m)). Space Complexity:\\nO(1).\\nProblem-51\\u2003\\u2003Median in an infinite series of integers\\nSolution: Median is the middle number in a sorted list of numbers (if we have an odd number of\\nelements). If we have an even number of elements, the median is the average of two middle\\nnumbers in a sorted list of numbers. We can solve this problem with linked lists (with both sorted\\nand unsorted linked lists).\\nFirst, let us try with an unsorted linked list. In an unsorted linked list, we can insert the element\\neither at the head or at the tail. The disadvantage with this approach is that finding the median\\ntakes O(n). Also, the insertion operation takes O(1).\\nNow, let us try with a sorted linked list. We can find the median in O(1) time if we keep track of'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 157, 'file_type': 'pdf'}, page_content='the middle elements. Insertion to a particular location is also O(1) in any linked list. But, finding\\nthe right location to insert is not O(logn) as in a sorted array, it is instead O(n) because we can’t\\nperform binary search in a linked list even if it is sorted. So, using a sorted linked list isn’t worth\\nthe effort as insertion is O(n) and finding median is O(1), the same as the sorted array. In the\\nsorted array the insertion is linear due to shifting, but here it’s linear because we can’t do a binary\\nsearch in a linked list.\\nNote: For an efficient algorithm refer to the Priority Queues and Heaps chapter.\\nProblem-52\\u2003\\u2003Given a linked list, how do you modify it such that all the even numbers appear\\nbefore all the odd numbers in the modified linked list?\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 158, 'file_type': 'pdf'}, page_content='struct ListNode “exchangeEvenOdLis{structListNode *head),\\n| initializing the odd and even lst headers\\nstruct ListNode *odList = NULL, ‘evenList =NULL;\\n\\n| creating tail variables for both the list\\nstruct ListNode *oddListEnd = NULL, ‘evenListEnd = NULL;\\nstruct ListNode ‘itr=head;\\n\\nillhead == NULL),\\nreturn;\\nelse\\nwhile itr = NULL\\nilitr-sdata % 2 == 0)\\nil evnlist*= NULL}\\n|/ first even node\\nevenlist = evenlistEnd = itr;\\nelse,\\n// inserting the noe atthe end of inked list\\nevenbist2nd-—mnext = itr;\\nevenlistBnd = itr;\\nelse\\nill oddList == NULL}\\n|/ first odd node\\noniList = oddListnd = itr;\\nelse\\n// inserting the node atthe end of inked list\\nodlListEnd-snext = it;\\nodListEnd = itr,\\nitr = itrnext;\\nevenlistEnd-mext = odds;\\nreturn head;'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 159, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-53\\u2003\\u2003Given two linked lists, each list node with one integer digit, add these two\\nlinked lists. The result should be stored in the third linked list. Also note that the head node\\ncontains the most significant digit of the number.\\nSolution: Since the integer addition starts from the least significant digit, we first need to visit the\\nlast node of both lists and add them up, create a new node to store the result, take care of the carry\\nif any, and link the resulting node to the node which will be added to the second least significant\\nnode and continue.\\nFirst of all, we need to take into account the difference in the number of digits in the two numbers.\\nSo before starting recursion, we need to do some calculation and move the longer list pointer to\\nthe appropriate place so that we need the last node of both lists at the same time. The other thing\\nwe need to take care of is carry. If two digits add up to more than 10, we need to forward the\\ncarry to the next node and add it. If the most significant digit addition results in a carry, we need\\nto create an extra node to store the carry.\\nThe function below is actually a wrapper function which does all the housekeeping like\\ncalculating lengths of lists, calling recursive implementation, creating an extra node for the carry\\nin the most significant digit, and adding any remaining nodes left in the longer list.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 160, 'file_type': 'pdf'}, page_content='void addListNumbersWrapper(stract ListNode ‘ist, struct ListNode “st2 int “cary, struct ListNode “result\\n‘int istLength = 0, list2Length = 0, dif “0;\\nstruct ListNode ‘current = list;\\nwhile(current)\\n‘current = current-snext:\\nlist Length\\n\\n‘current = lis;\\n‘while(current)\\n\\nfhellengthess\\n‘fist Length < list2Length)t\\n‘current = list];\\nfiat =\\n\\nD\\n‘iff = abslist!Length-tist2Length);\\ncurrent « list;\\nwhile\\n\\n‘current = current-snext:\\n‘addListNumbers{current,list2, carry, result)\\n‘iff = abelist Length list2Length),\\n\\ncary, result, dif)\\n\\n‘oid addLitNumbers(struct ListNode “list, struct ListNode “list2, int ‘carry, struct ListNode “result\\n\\n‘ituisty\\naddLiatNumberafiat—snext,list2—mnext, carry, result)\\n\\n1 /End of both ists, add then\\nStruct ListNode * temp = struct ListNode *tmallocsizeoistruct ListNode )\\n\\nSm = list —data + hnt2-sdata + earyls\\n1/ Store carry\\ncarry = sum/10;\\nsum = suenitO;\\ntemap-—data = wm;\\ntemp_onext = (rent\\n\\n“result = temp:\\nseturn:\\n\\n‘void addRemainingNumbers(struct ListNode * list, int ‘carry, struct ListNode “result int dif\\nint sum =O;\\n\\n‘fist || att == 0)\\n‘addRemainingNumbers(list—next, carry result, dif\\n‘xruct ListNode * temp = (struct ListNode *}mallociszeofistruct ListNode ):\\n\\nrum = list >data * Peary)\\n\\nary = sum/ 10;\\nsum ~ sum10;'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 161, 'file_type': 'pdf'}, page_content='Time Complexity: O(max(List1 length,List2 length)).\\nSpace Complexity: O(min(List1 length, List1 length)) for recursive stack.\\nNote: It can also be solved using stacks.\\nProblem-54\\u2003\\u2003Which sorting algorithm is easily adaptable to singly linked lists?\\nSolution: Simple Insertion sort is easily adabtable to singly linked lists. To insert an element, the\\nlinked list is traversed until the proper position is found, or until the end of the list is reached. It\\nis inserted into the list by merely adjusting the pointers without shifting any elements, unlike in the\\narray. This reduces the time required for insertion but not the time required for searching for the\\nproper position.\\nProblem-55\\u2003\\u2003Given a list, List1 = {A1, A2, . . . An–1; An) with data, reorder it to {A1,\\nAn,A2,An–1} without using any extra space.\\nSolution: Find the middle of the linked list. We can do it by slow and fast pointer approach. After\\nfinding the middle node, we reverse the right halfl then we do a in place merge of the two halves\\nof the linked list.\\nProblem-56\\u2003\\u2003Given two sorted linked lists, given an algorithm for the printing common\\nelements of them.\\nSolution: The solution is based on merge sort logic. Assume the given two linked lists are: list1\\nand list2. Since the elements are in sorted order, we run a loop till we reach the end of either of\\nthe list. We compare the values of list1 and list2. If the values are equal, we add it to the common\\nlist. We move list1/list2/both nodes ahead to the next pointer if the values pointed by list1 was\\nless / more / equal to the value pointed by list2.\\nTime complexity O(m + n), where m is the lengh of list1 and n is the length of list2. Space\\nComplexity: O(1).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 163, 'file_type': 'pdf'}, page_content='4.1 What is a Stack?\\nA stack is a simple data structure used for storing data (similar to Linked Lists). In a stack, the\\norder in which the data arrives is important. A pile of plates in a cafeteria is a good example of a\\nstack. The plates are added to the stack as they are cleaned and they are placed on the top. When a\\nplate, is required it is taken from the top of the stack. The first plate placed on the stack is the last\\none to be used.\\nDefinition: A stack is an ordered list in which insertion and deletion are done at one end, called\\ntop. The last element inserted is the first one to be deleted. Hence, it is called the Last in First out\\n(LIFO) or First in Last out (FILO) list.\\nSpecial names are given to the two changes that can be made to a stack. When an element is\\ninserted in a stack, the concept is called push, and when an element is removed from the stack, the\\nconcept is called pop. Trying to pop out an empty stack is called underflow and trying to push an\\nelement in a full stack is called overflow. Generally, we treat them as exceptions. As an example,'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 164, 'file_type': 'pdf'}, page_content='consider the snapshots of the stack.\\n4.2 How Stacks are used\\nConsider a working day in the office. Let us assume a developer is working on a long-term\\nproject. The manager then gives the developer a new task which is more important. The\\ndeveloper puts the long-term project aside and begins work on the new task. The phone rings, and\\nthis is the highest priority as it must be answered immediately. The developer pushes the present\\ntask into the pending tray and answers the phone.\\nWhen the call is complete the task that was abandoned to answer the phone is retrieved from the\\npending tray and work progresses. To take another call, it may have to be handled in the same\\nmanner, but eventually the new task will be finished, and the developer can draw the long-term\\nproject from the pending tray and continue with that.\\n4.3 Stack ADT\\nThe following operations make a stack an ADT. For simplicity, assume the data is an integer type.\\nMain stack operations\\nPush (int data): Inserts data onto stack.\\nint Pop(): Removes and returns the last inserted element from the stack.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 165, 'file_type': 'pdf'}, page_content='Auxiliary stack operations\\nint Top(): Returns the last inserted element without removing it.\\nint Size(): Returns the number of elements stored in the stack.\\nint IsEmptyStack(): Indicates whether any elements are stored in the stack or not.\\nint IsFullStack(): Indicates whether the stack is full or not.\\nExceptions\\nAttempting the execution of an operation may sometimes cause an error condition, called an\\nexception. Exceptions are said to be “thrown” by an operation that cannot be executed. In the\\nStack ADT, operations pop and top cannot be performed if the stack is empty. Attempting the\\nexecution of pop (top) on an empty stack throws an exception. Trying to push an element in a full\\nstack throws an exception.\\n4.4 Applications\\nFollowing are some of the applications in which stacks play an important role.\\nDirect applications\\nBalancing of symbols\\nInfix-to-postfix conversion\\nEvaluation of postfix expression\\nImplementing function calls (including recursion)\\nFinding of spans (finding spans in stock markets, refer to Problems section)\\nPage-visited history in a Web browser [Back Buttons]\\nUndo sequence in a text editor\\nMatching Tags in HTML and XML\\nIndirect applications\\nAuxiliary data structure for other algorithms (Example: Tree traversal algorithms)\\nComponent of other data structures (Example: Simulating queues, refer Queues\\nchapter)\\n4.5 Implementation\\nThere are many ways of implementing stack ADT; below are the commonly used methods.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 166, 'file_type': 'pdf'}, page_content='Simple array based implementation\\nDynamic array based implementation\\nLinked lists implementation\\nSimple Array Implementation\\nThis implementation of stack ADT uses an array. In the array, we add elements from left to right\\nand use a variable to keep track of the index of the top element.\\nThe array storing the stack elements may become full. A push operation will then throw a full\\nstack exception. Similarly, if we try deleting an element from an empty stack it will throw stack\\nempty exception.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 167, 'file_type': 'pdf'}, page_content='define MAXSIZE 10\\nstruct ArrayStack {\\nint top:\\nint capacity;\\nint ‘array;\\nE\\nstruct ArrayStack *CreateStaci |\\nstruct ArrayStack *S = mallocsizeoistruct ArrayStack}\\nsifts)\\nreturn NULL;\\n‘S-apacity = MAXSIZE;\\nSoop *\\n‘S-array* mallo(S-apacity * sizofint);\\nif) Saray)\\nreturn NULL;\\nretum S;\\n\\nint IsEmptyStack(struct ArrayStack *S){\\nreturn (Stop ==-1}; — // ithe condition is true then 1 is returned else 0 is returned\\n\\nint IsFullStack(struct ArrayStack *S){\\n[ifthe condition is true then 1 is retumed else 0 is returned\\nreturn (Stop == S-rcapacity - 1);\\nvoid Push(struct ArrayStack *S, int data\\n[* Stop == capacity-1 indicates thatthe stack is fll/\\niflsFullStack(S)\\nprint “Stack Overiow”};\\nse /*increasing the top’ by 1 and storing the value at top’ position*/\\n$+ arrayi++S-top}= dat;\\nint Pop(structArrayStack *S)\\n/* Stop =~ | indicates empty stack\\niflsEmptyStack(S)\\nprint Stack is Empty’);\\nseturn INT_MIN;;\\nelse /* Removing element from ‘op of the array and reducing ‘op’ by 1*/\\nretum (S- array[S-top-;\\nvoid DeleteStack{struct DynArrayStack *S}\\nsf) {\\nif(Sarray)\\nfree(S—array);\\nfree(S);'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 168, 'file_type': 'pdf'}, page_content='Performance & Limitations\\nPerformance\\nLet n be the number of elements in the stack. The complexities of stack operations with this\\nrepresentation can be given as:\\nSpace Complexity (for n push operations)\\nO(n)\\nTime Complexity of Push()\\nO(1)\\nTime Complexity of Pop()\\nO(1)\\nTime Complexity of Size()\\nO(1)\\nTime Complexity of IsEmptyStack()\\nO(1)\\nTime Complexity of IsFullStackf)\\nO(1)\\nTime Complexity of DeleteStackQ\\nO(1)\\nLimitations\\nThe maximum size of the stack must first be defined and it cannot be changed. Trying to push a\\nnew element into a full stack causes an implementation-specific exception.\\nDynamic Array Implementation\\nFirst, let’s consider how we implemented a simple array based stack. We took one index variable\\ntop which points to the index of the most recently inserted element in the stack. To insert (or push)\\nan element, we increment top index and then place the new element at that index.\\nSimilarly, to delete (or pop) an element we take the element at top index and then decrement the\\ntop index. We represent an empty queue with top value equal to –1. The issue that still needs to\\nbe resolved is what we do when all the slots in the fixed size array stack are occupied?\\nFirst try: What if we increment the size of the array by 1 every time the stack is full?\\nPush(); increase size of S[] by 1\\nPop(): decrease size of S[] by 1\\nProblems with this approach?'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 169, 'file_type': 'pdf'}, page_content='This way of incrementing the array size is too expensive. Let us see the reason for this. For\\nexample, at n = 1, to push an element create a new array of size 2 and copy all the old array\\nelements to the new array, and at the end add the new element. At n = 2, to push an element create\\na new array of size 3 and copy all the old array elements to the new array, and at the end add the\\nnew element.\\nSimilarly, at n = n – 1, if we want to push an element create a new array of size n and copy all the\\nold array elements to the new array and at the end add the new element. After n push operations\\nthe total time T(n) (number of copy operations) is proportional to 1 + 2 + ... + n ≈ O(n2).\\nAlternative Approach: Repeated Doubling\\nLet us improve the complexity by using the array doubling technique. If the array is full, create a\\nnew array of twice the size, and copy the items. With this approach, pushing n items takes time\\nproportional to n (not n2).\\nFor simplicity, let us assume that initially we started with n = 1 and moved up to n = 32. That\\nmeans, we do the doubling at 1,2,4,8,16. The other way of analyzing the same approach is: at n =\\n1, if we want to add (push) an element, double the current size of the array and copy all the\\nelements of the old array to the new array.\\nAt n = 1, we do 1 copy operation, at n = 2, we do 2 copy operations, and at n = 4, we do 4 copy\\noperations and so on. By the time we reach n = 32, the total number of copy operations is 1+2 + 4\\n+ 8+16 = 31 which is approximately equal to 2n value (32). If we observe carefully, we are\\ndoing the doubling operation logn times. Now, let us generalize the discussion. For n push\\noperations we double the array size logn times. That means, we will have logn terms in the\\nexpression below. The total time T(n) of a series of n push operations is proportional to\\nT(n) is O(n) and the amortized time of a push operation is O(1) .'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 170, 'file_type': 'pdf'}, page_content=\"struct DymArrayStack {\\nint top;\\nint capacity;\\nint array;\\nih\\nstruct DynArrayStack ‘CreateStack)t\\nstruct DynArrayStack*S = mallo(sizeofstruct DynArrayStack);\\nifts)\\nreturn NULL;\\nSweapacity\\nStop\\nScary\\nAft array)\\nreturn NULL;\\nreturn S;\\nint IFullStack(struct DynArrayStack ‘),\\nreturn (Stop == S-capacity- 1;\\n‘oid DoubleStack(struct DynArrayStack\\nScapacty = 2;\\nScarray + realoeSaray, Scapacty * sizeof);\\n‘oid Push(struct DynArrayStack ', nt x\\n1 [No oer in this implementation\\niflsFullStack'S)\\nDoubleStak(S);\\nS~array|++8-—top] =x;\\nint IsEmptyStack(struct DynArayStack 8)\\nreturn Stop ==;\\nint Top(struct DymArrayStack ‘5\\niffsEmptyStack(S))\\nreturn INT MIN;\\nreturn Sarray{S-top;\\n\\n{nt Pop(struct DynArrayStack *S),\\n\\nlo(Scapacity*sizeofint); // allocate an array of size 1 initaly\"),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 171, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the stack. The complexities for operations with this\\nrepresentation can be given as:\\nSpace Complexity (for n push operations)\\nO(n)\\nTime Complexity of CreateStack()\\nO(1)\\nTime Complexity of PushQ\\nO(1) (Average)\\nTime Complexity of PopQ\\nO(1)\\nTime Complexity of Top()\\nO(1)\\nTime Complexity of IsEmpryStackf)\\nO(1))\\nTime Complexity of IsFullStackf)\\nO(1)\\nTime Complexity of DeleteStackQ\\nO(1)\\nNote: Too many doublings may cause memory overflow exception.\\nLinked List Implementation\\nThe other way of implementing stacks is by using Linked lists. Push operation is implemented by\\ninserting element at the beginning of the list. Pop operation is implemented by deleting the node\\nfrom the beginning (the header/top node).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 172, 'file_type': 'pdf'}, page_content='struct LstNode|\\nint data;\\nstruct ListNode ‘next;\\n\\nstruct Stack *CreateStakf},\\nretum NULL;\\n\\n‘voi Push(struct Stack top, int data\\nstruct Stack “temp;\\ntemp = mallcsizefitruct Stack)\\nitem\\n\\nreturn NULL\\n\\ntemp-data =\\ntemp-nest*\\n“top = temp;\\n\\nint IsEmpty Stack(struct Stack “op\\nretum top == NULL;\\n\\nint Pop(struct Stack “top,\\nint data\\nstruct Stack “emp;\\niflsEmptyStack(op)\\n\\nint Top(struct Stack * top),\\nilsEmpryStac(op))\\n\\nretumn INT MIN;\\nreturn top-next-data;\\nvoid DeleteStack(struct Stack “*top){\\nstruct Stack ‘temp, \"p;\\np= Mop\\nhile p-next\\ntemp = pert;\\npenext = temp~net;\\nfreetemp);\\nfret'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 173, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the stack. The complexities for operations with this\\nrepresentation can be given as:\\nSpace Complexity (for n push operations)\\nO(n)\\nTime Complexity of CreateStack()\\nO(1)\\nTime Complexity of Push()\\nO(1) (Average)\\nTime Complexity of Pop()\\nO(1)\\nTime Complexity of Top()\\nO(1)\\nTime Complexity of IsEmptyStack()\\nO(1)\\nTime Complexity of DeleteStack()\\nO(n)\\n4.6 Comparison of Implementations\\nComparing Incremental Strategy and Doubling Strategy\\nWe compare the incremental strategy and doubling strategy by analyzing the total time T(n)\\nneeded to perform a series of n push operations. We start with an empty stack represented by an\\narray of size 1.\\nWe call amortized time of a push operation is the average time taken by a push over the series of\\noperations, that is, T(n)/n.\\nIncremental Strategy\\nThe amortized time (average time per operation) of a push operation is O(n) [O(n2)/n].\\nDoubling Strategy\\nIn this method, the amortized time of a push operation is O(1) [O(n)/n].\\nNote: For analysis, refer to the Implementation section.\\nComparing Array Implementation and Linked List Implementation'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 174, 'file_type': 'pdf'}, page_content='Array Implementation\\nOperations take constant time.\\nExpensive doubling operation every once in a while.\\nAny sequence of n operations (starting from empty stack) – “amortized” bound takes\\ntime proportional to n.\\nLinked List Implementation\\nGrows and shrinks gracefully.\\nEvery operation takes constant time O(1).\\nEvery operation uses extra space and time to deal with references.\\n4.7 Stacks: Problems & Solutions\\nProblem-1\\u2003\\u2003Discuss how stacks can be used for checking balancing of symbols.\\nSolution: Stacks can be used to check whether the given expression has balanced symbols. This\\nalgorithm is very useful in compilers. Each time the parser reads one character at a time. If the\\ncharacter is an opening delimiter such as (, {, or [- then it is written to the stack. When a closing\\ndelimiter is encountered like ), }, or ]-the stack is popped.\\nThe opening and closing delimiters are then compared. If they match, the parsing of the string\\ncontinues. If they do not match, the parser indicates that there is an error on the line. A linear-time\\nO(n) algorithm based on stack can be given as:\\nAlgorithm:\\na)\\nCreate a stack.\\nb)\\nwhile (end of input is not reached) {\\nIf the character read is not a symbol to be balanced, ignore it.\\nIf the character is an opening symbol like (, [, {, push it onto the stack\\nIf it is a closing symbol like ),],}, then if the stack is empty report an\\nerror. Otherwise pop the stack.\\nIf the symbol popped is not the corresponding opening symbol, report an\\nerror.\\nc)\\nAt end of input, if the stack is not empty report an error\\nExamples:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 175, 'file_type': 'pdf'}, page_content='For tracing the algorithm let us assume that the input is: () (() [()])'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 176, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Since we are scanning the input only once. Space Complexity: O(n) [for\\nstack].\\nProblem-2\\u2003\\u2003Discuss infix to postfix conversion algorithm using stack.\\nSolution: Before discussing the algorithm, first let us see the definitions of infix, prefix and\\npostfix expressions.\\nInfix: An infix expression is a single letter, or an operator, proceeded by one infix string and\\nfollowed by another Infix string.\\nPrefix: A prefix expression is a single letter, or an operator, followed by two prefix strings.\\nEvery prefix string longer than a single variable contains an operator, first operand and second\\noperand.\\nPostfix: A postfix expression (also called Reverse Polish Notation) is a single letter or an\\noperator, preceded by two postfix strings. Every postfix string longer than a single variable\\ncontains first and second operands followed by an operator.\\nPrefix and postfix notions are methods of writing mathematical expressions without parenthesis.\\nTime to evaluate a postfix and prefix expression is O(n), where n is the number of elements in the\\narray.\\nNow, let us focus on the algorithm. In infix expressions, the operator precedence is implicit'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 177, 'file_type': 'pdf'}, page_content='unless we use parentheses. Therefore, for the infix to postfix conversion algorithm we have to\\ndefine the operator precedence (or priority) inside the algorithm.\\nThe table shows the precedence and their associativity (order of evaluation) among operators.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 178, 'file_type': 'pdf'}, page_content='Important Properties\\nLet us consider the infix expression 2 + 3*4 and its postfix equivalent 234*+. Notice\\nthat between infix and postfix the order of the numbers (or operands) is unchanged.\\nIt is 2 3 4 in both cases. But the order of the operators * and + is affected in the two\\nexpressions.\\nOnly one stack is enough to convert an infix expression to postfix expression. The\\nstack that we use in the algorithm will be used to change the order of operators from\\ninfix to postfix. The stack we use will only contain operators and the open\\nparentheses symbol ‘(‘.\\nPostfix expressions do not contain parentheses. We shall not output the parentheses in the postfix\\noutput.\\nAlgorithm:\\na)\\nCreate a stack\\nb)\\nfor each character t in the input stream}\\nc)\\npop and output tokens until the stack is empty\\nFor better understanding let us trace out an example: A * B- (C + D) + E'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 179, 'file_type': 'pdf'}, page_content='Problem-3\\u2003\\u2003Discuss postfix evaluation using stacks?\\nSolution:\\nAlgorithm:\\nScan the Postfix string from left to right.\\nInitialize an empty stack.\\nRepeat steps 4 and 5 till all the characters are scanned.\\nIf the scanned character is an operand, push it onto the stack.\\nIf the scanned character is an operator, and if the operator is a unary operator, then\\npop an element from the stack. If the operator is a binary operator, then pop two\\nelements from the stack. After popping the elements, apply the operator to those\\npopped elements. Let the result of this operation be retVal onto the stack.\\nAfter all characters are scanned, we will have only one element in the stack.\\nReturn top of the stack as result.\\nExample: Let us see how the above-mentioned algorithm works using an example. Assume that\\nthe postfix string is 123*+5-.\\nInitially the stack is empty. Now, the first three characters scanned are 1, 2 and 3, which are\\noperands. They will be pushed into the stack in that order.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 180, 'file_type': 'pdf'}, page_content='The next character scanned is “*”, which is an operator. Thus, we pop the top two elements from\\nthe stack and perform the “*” operation with the two operands. The second operand will be the\\nfirst element that is popped.\\nThe value of the expression (2*3) that has been evaluated (6) is pushed into the stack.\\nThe next character scanned is “+”, which is an operator. Thus, we pop the top two elements from'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 181, 'file_type': 'pdf'}, page_content='the stack and perform the “+” operation with the two operands. The second operand will be the\\nfirst element that is popped.\\nThe value of the expression (1+6) that has been evaluated (7) is pushed into the stack.\\nThe next character scanned is “5”, which is added to the stack.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 182, 'file_type': 'pdf'}, page_content='The next character scanned is “-”, which is an operator. Thus, we pop the top two elements from\\nthe stack and perform the “-” operation with the two operands. The second operand will be the\\nfirst element that is popped.\\nThe value of the expression(7-5) that has been evaluated(23) is pushed into the stack.\\nNow, since all the characters are scanned, the remaining element in the stack (there will be only\\none element in the stack) will be returned. End result:\\nPostfix String : 123*+5-\\nResult : 2\\nProblem-4\\u2003\\u2003Can we evaluate the infix expression with stacks in one pass?\\nSolution: Using 2 stacks we can evaluate an infix expression in 1 pass without converting to\\npostfix.\\nAlgorithm:\\nCreate an empty operator stack\\nCreate an empty operand stack'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 183, 'file_type': 'pdf'}, page_content='For each token in the input string\\na.\\nGet the next token in the infix string\\nb.\\nIf next token is an operand, place it on the operand stack\\nc.\\nIf next token is an operator\\n  i. Evaluate the operator (next op)\\nWhile operator stack is not empty, pop operator and operands (left and right),\\nevaluate left operator right and push result onto operand stack\\nPop result from operator stack\\nProblem-5\\u2003\\u2003How to design a stack such that GetMinimum( ) should be O(1)?\\nSolution: Take an auxiliary stack that maintains the minimum of all values in the stack. Also,\\nassume that each element of the stack is less than its below elements. For simplicity let us call the\\nauxiliary stack min stack.\\nWhen we pop the main stack, pop the min stack too. When we push the main stack, push either the\\nnew element or the current minimum, whichever is lower. At any point, if we want to get the\\nminimum, then we just need to return the top element from the min stack. Let us take an example\\nand trace it out. Initially let us assume that we have pushed 2, 6, 4, 1 and 5. Based on the above-\\nmentioned algorithm the min stack will look like:\\nMain stack\\nMin stack\\n5 → top\\n1 → top\\nAfter popping twice we get:\\nMain stack\\nMin stack\\n4 -→ top\\n2 → top\\nBased on the discussion above, now let us code the push, pop and GetMinimum() operations.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 184, 'file_type': 'pdf'}, page_content='Time complexity: O(1). Space complexity: O(n) [for Min stack]. This algorithm has much better\\nspace usage if we rarely get a “new minimum or equal”.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 185, 'file_type': 'pdf'}, page_content='Problem-6\\u2003\\u2003For Problem-5 is it possible to improve the space complexity?\\nSolution: Yes. The main problem of the previous approach is, for each push operation we are\\npushing the element on to min stack also (either the new element or existing minimum element).\\nThat means, we are pushing the duplicate minimum elements on to the stack.\\nNow, let us change the algorithm to improve the space complexity. We still have the min stack, but\\nwe only pop from it when the value we pop from the main stack is equal to the one on the min\\nstack. We only push to the min stack when the value being pushed onto the main stack is less than\\nor equal to the current min value. In this modified algorithm also, if we want to get the minimum\\nthen we just need to return the top element from the min stack. For example, taking the original\\nversion and pushing 1 again, we’d get:\\nMain stack\\nMin stack\\n1 → top\\n\\n1 → top\\nPopping from the above pops from both stacks because 1 == 1, leaving:\\nMain stack\\nMin stack\\n5 → top\\n\\n1 → top\\nPopping again only pops from the main stack, because 5 > 1:\\nMain stack\\nMin stack\\n1 → top'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 186, 'file_type': 'pdf'}, page_content='1 → top\\nPopping again pops both stacks because 1 == 1:\\nMain stack\\nMin stack\\n4 → top\\n\\n2 → top\\nNote: The difference is only in push & pop operations.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 187, 'file_type': 'pdf'}, page_content='Time complexity: O(1). Space complexity: O(n) [for Min stack]. But this algorithm has much\\nbetter space usage if we rarely get a “new minimum or equal”.\\nProblem-7\\u2003\\u2003For a given array with n symbols how many stack permutations are possible?'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 188, 'file_type': 'pdf'}, page_content='Solution: The number of stack permutations with n symbols is represented by Catalan number and\\nwe will discuss this in the Dynamic Programming chapter.\\nProblem-8\\u2003\\u2003Given an array of characters formed with a’s and b’s. The string is marked with\\nspecial character X which represents the middle of the list (for example:\\nababa...ababXbabab baaa). Check whether the string is palindrome.\\nSolution: This is one of the simplest algorithms. What we do is, start two indexes, one at the\\nbeginning of the string and the other at the end of the string. Each time compare whether the values\\nat both the indexes are the same or not. If the values are not the same then we say that the given\\nstring is not a palindrome.\\nIf the values are the same then increment the left index and decrement the right index. Continue\\nthis process until both the indexes meet at the middle (at X) or if the string is not palindrome.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-9\\u2003\\u2003For Problem-8, if the input is in singly linked list then how do we check whether\\nthe list elements form a palindrome (That means, moving backward is not possible).\\nSolution: Refer Linked Lists chapter.\\nProblem-10\\u2003\\u2003Can we solve Problem-8 using stacks?\\nSolution: Yes.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 189, 'file_type': 'pdf'}, page_content='Algorithm:\\nTraverse the list till we encounter X as input element.\\nDuring the traversal push all the elements (until X) on to the stack.\\nFor the second half of the list, compare each element’s content with top of the stack.\\nIf they are the same then pop the stack and go to the next element in the input list.\\nIf they are not the same then the given string is not a palindrome.\\nContinue this process until the stack is empty or the string is not a palindrome.\\nTime Complexity: O(n). Space Complexity: O(n/2) ≈ O(n).\\nProblem-11\\u2003\\u2003Given a stack, how to reverse the elements of the stack using only stack\\noperations (push & pop)?\\nSolution:\\nAlgorithm:\\nFirst pop all the elements of the stack till it becomes empty.\\nFor each upward step in recursion, insert the element at the bottom of the stack.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 190, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). Space Complexity: O(n), for recursive stack.\\nProblem-12\\u2003\\u2003Show how to implement one queue efficiently using two stacks. Analyze the\\nrunning time of the queue operations.\\nSolution: Refer Queues chapter.\\nProblem-13\\u2003\\u2003Show how to implement one stack efficiently using two queues. Analyze the\\nrunning time of the stack operations.\\nSolution: Refer Queues chapter.\\nProblem-14\\u2003\\u2003How do we implement two stacks using only one array? Our stack routines\\nshould not indicate an exception unless every slot in the array is used?\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 191, 'file_type': 'pdf'}, page_content='Algorithm:\\nStart two indexes one at the left end and the other at the right end.\\nThe left index simulates the first stack and the right index simulates the second stack.\\nIf we want to push an element into the first stack then put the element at the left\\nindex.\\nSimilarly, if we want to push an element into the second stack then put the element at\\nthe right index.\\nThe first stack grows towards the right, and the second stack grows towards the left.\\nTime Complexity of push and pop for both stacks is O(1). Space Complexity is O(1).\\nProblem-15\\u2003\\u20033 stacks in one array: How to implement 3 stacks in one array?\\nSolution: For this problem, there could be other ways of solving it. Given below is one\\npossibility and it works as long as there is an empty space in the array.\\nTo implement 3 stacks we keep the following information.\\nThe index of the first stack (Topi): this indicates the size of the first stack.\\nThe index of the second stack (Top2): this indicates the size of the second stack.\\nStarting index of the third stack (base address of third stack).\\nTop index of the third stack.\\nNow, let us define the push and pop operations for this implementation.\\nPushing:\\nFor pushing on to the first stack, we need to see if adding a new element causes it to\\nbump into the third stack. If so, try to shift the third stack upwards. Insert the new'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 192, 'file_type': 'pdf'}, page_content='element at (start1 + Top1).\\nFor pushing to the second stack, we need to see if adding a new element causes it to\\nbump into the third stack. If so, try to shift the third stack downward. Insert the new\\nelement at (start2 - Top2).\\nWhen pushing to the third stack, see if it bumps into the second stack. If so, try to\\nshift the third stack downward and try pushing again. Insert the new element at\\n(start3 + Top3).\\nTime Complexity: O(n). Since we may need to adjust the third stack. Space Complexity: O(1).\\nPopping: For popping, we don’t need to shift, just decrement the size of the appropriate stack.\\nTime Complexity: O(1). Space Complexity: O(1).\\nProblem-16\\u2003\\u2003For Problem-15, is there any other way implementing the middle stack?\\nSolution: Yes. When either the left stack (which grows to the right) or the right stack (which\\ngrows to the left) bumps into the middle stack, we need to shift the entire middle stack to make\\nroom. The same happens if a push on the middle stack causes it to bump into the right stack.\\nTo solve the above-mentioned problem (number of shifts) what we can do is: alternating pushes\\ncan be added at alternating sides of the middle list (For example, even elements are pushed to the\\nleft, odd elements are pushed to the right). This would keep the middle stack balanced in the\\ncenter of the array but it would still need to be shifted when it bumps into the left or right stack,\\nwhether by growing on its own or by the growth of a neighboring stack. We can optimize the\\ninitial locations of the three stacks if they grow/shrink at different rates and if they have different\\naverage sizes. For example, suppose one stack doesn’t change much. If we put it at the left, then\\nthe middle stack will eventually get pushed against it and leave a gap between the middle and\\nright stacks, which grow toward each other. If they collide, then it’s likely we’ve run out of space\\nin the array'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 192, 'file_type': 'pdf'}, page_content='. If they collide, then it’s likely we’ve run out of space\\nin the array. There is no change in the time complexity but the average number of shifts will get\\nreduced.\\nProblem-17\\u2003\\u2003Multiple (m) stacks in one array: Similar to Problem-15, what if we want to\\nimplement m stacks in one array?\\nSolution: Let us assume that array indexes are from 1 to n. Similar to the discussion in Problem-\\n15, to implement m stacks in one array, we divide the array into m parts (as shown below). The\\nsize of each part is'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 193, 'file_type': 'pdf'}, page_content='From the above representation we can see that, first stack is starting at index 1 (starting index is\\nstored in Base[l]), second stack is starting at index \\n (starting index is stored in Base[2]), third\\nstack is starting at index \\n (starting index is stored in Base[3]), and so on. Similar to Base array,\\nlet us assume that Top array stores the top indexes for each of the stack. Consider the following\\nterminology for the discussion.\\nTop[i], for 1 ≤ i ≤ m will point to the topmost element of the stack i.\\nIf Base[i] == Top[i], then we can say the stack i is empty.\\nIf Top[i] == Base[i+1], then we can say the stack i is full.\\nInitially Base[i] = Top[i] = \\n (i – 1), for 1 ≤ i ≤ m.\\nThe ith stack grows from Base[i]+1 to Base[i+1].\\nPushing on to ith stack:\\nFor pushing on to the ith stack, we check whether the top of ith stack is pointing to\\nBase[i+1] (this case defines that ith stack is full). That means, we need to see if\\nadding a new element causes it to bump into the i + 1th stack. If so, try to shift the\\nstacks from i + 1th stack to mth stack toward the right. Insert the new element at\\n(Base[i] + Top[i]).\\nIf right shifting is not possible then try shifting the stacks from 1 to i –1th stack toward\\nthe left.\\nIf both of them are not possible then we can say that all stacks are full.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 194, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Since we may need to adjust the stacks. Space Complexity: O(1).\\nPopping from ith stack: For popping, we don’t need to shift, just decrement the size of the\\nappropriate stack. The only case to check is stack empty case.\\nTime Complexity: O(1). Space Complexity: O(1).\\nProblem-18\\u2003\\u2003Consider an empty stack of integers. Let the numbers 1,2,3,4,5,6 be pushed on to\\nthis stack in the order they appear from left to right. Let 5 indicate a push and X indicate a\\npop operation. Can they be permuted in to the order 325641(output) and order 154623?\\nSolution: SSSXXSSXSXXX outputs 325641. 154623 cannot be output as 2 is pushed much\\nbefore 3 so can appear only after 3 is output.\\nProblem-19\\u2003\\u2003Earlier in this chapter, we discussed that for dynamic array implementation of\\nstacks, the ‘repeated doubling’ approach is used. For the same problem, what is the\\ncomplexity if we create a new array whose size is n + if instead of doubling?\\nSolution: Let us assume that the initial stack size is 0. For simplicity let us assume that K = 10.\\nFor inserting the element we create a new array whose size is 0 + 10 = 10. Similarly, after 10\\nelements we again create a new array whose size is 10 + 10 = 20 and this process continues at\\nvalues: 30,40 ... That means, for a given n value, we are creating the new arrays at: \\n The total number of copy operations is:\\nIf we are performing n push operations, the cost per operation is O(logn).\\nProblem-20\\u2003\\u2003Given a string containing n S’s and n X’s where 5 indicates a push operation and'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 195, 'file_type': 'pdf'}, page_content='X indicates a pop operation, and with the stack initially empty, formulate a rule to check\\nwhether a given string 5 of operations is admissible or not?\\nSolution: Given a string of length 2n, we wish to check whether the given string of operations is\\npermissible or not with respect to its functioning on a stack. The only restricted operation is pop\\nwhose prior requirement is that the stack should not be empty. So while traversing the string from\\nleft to right, prior to any pop the stack shouldn’t be empty, which means the number of S’s is\\nalways greater than or equal to that of X’s. Hence the condition is at any stage of processing of the\\nstring, the number of push operations (S) should be greater than the number of pop operations (X).\\nProblem-21\\u2003\\u2003Suppose there are two singly linked lists which intersect at some point and\\nbecome a single linked list. The head or start pointers of both the lists are known, but the\\nintersecting node is not known. Also, the number of nodes in each of the lists before they\\nintersect are unknown and both lists may have a different number. List1 may have n nodes\\nbefore it reaches the intersection point and List2 may have m nodes before it reaches the\\nintersection point where m and n may be m = n,m < n or m > n. Can we find the merging\\npoint using stacks?\\nSolution: Yes. For algorithm refer to Linked Lists chapter.\\nProblem-22\\u2003\\u2003Finding Spans: Given an array A, the span S[i] of A[i] is the maximum number\\nof consecutive elements A[j] immediately preceding A[i] and such that A[j] < A[i]?\\nOther way of asking: Given an array A of integers, find the maximum of j – i subjected to\\nthe constraint of A[i] < A[j].\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 196, 'file_type': 'pdf'}, page_content='This is a very common problem in stock markets to find the peaks. Spans are used in financial\\nanalysis (E.g., stock at 52-week high). The span of a stock price on a certain day, i, is the\\nmaximum number of consecutive days (up to the current day) the price of the stock has been less\\nthan or equal to its price on i.\\nAs an example, let us consider the table and the corresponding spans diagram. In the figure the\\narrows indicate the length of the spans. Now, let us concentrate on the algorithm for finding the\\nspans. One simple way is, each day, check how many contiguous days have a stock price that is'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 197, 'file_type': 'pdf'}, page_content='less than the current price.\\nTime Complexity: O(n2). Space Complexity: O(1).\\nProblem-23\\u2003\\u2003Can we improve the complexity of Problem-22?\\nSolution: From the example above, we can see that span S[i] on day i can be easily calculated if\\nwe know the closest day preceding i, such that the price is greater on that day than the price on\\nday i. Let us call such a day as P. If such a day exists then the span is now defined as S[i] = i – P.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 198, 'file_type': 'pdf'}, page_content='Time Complexity: Each index of the array is pushed into the stack exactly once and also popped\\nfrom the stack at most once. The statements in the while loop are executed at most n times. Even\\nthough the algorithm has nested loops, the complexity is O(n) as the inner loop is executing only n\\ntimes during the course of the algorithm (trace out an example and see how many times the inner\\nloop becomes successful). Space Complexity: O(n) [for stack].\\nProblem-24\\u2003\\u2003Largest rectangle under histogram: A histogram is a polygon composed of a\\nsequence of rectangles aligned at a common base line. For simplicity, assume that the\\nrectangles have equal widths but may have different heights. For example, the figure on the\\nleft shows a histogram that consists of rectangles with the heights 3,2,5,6,1,4,4, measured\\nin units where 1 is the width of the rectangles. Here our problem is: given an array with\\nheights of rectangles (assuming width is 1), we need to find the largest rectangle possible.\\nFor the given example, the largest rectangle is the shared part.\\nSolution: A straightforward answer is to go to each bar in the histogram and find the maximum\\npossible area in the histogram for it. Finally, find the maximum of these values. This will require\\nO(n2).\\nProblem-25\\u2003\\u2003For Problem-24, can we improve the time complexity?\\nSolution: Linear search using a stack of incomplete sub problems: There are many ways of\\nsolving this problem. Judge has given a nice algorithm for this problem which is based on stack.\\nProcess the elements in left-to-right order and maintain a stack of information about started but yet\\nunfinished sub histograms.\\nIf the stack is empty, open a new sub problem by pushing the element onto the stack. Otherwise\\ncompare it to the element on top of the stack. If the new one is greater we again push it. If the new\\none is equal we skip it. In all these cases, we continue with the next new element'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 198, 'file_type': 'pdf'}, page_content='. Otherwise\\ncompare it to the element on top of the stack. If the new one is greater we again push it. If the new\\none is equal we skip it. In all these cases, we continue with the next new element. If the new one\\nis less, we finish the topmost sub problem by updating the maximum area with respect to the\\nelement at the top of the stack. Then, we discard the element at the top, and repeat the procedure\\nkeeping the current new element.\\nThis way, all sub problems are finished when the stack becomes empty, or its top element is less\\nthan or equal to the new element, leading to the actions described above. If all elements have\\nbeen processed, and the stack is not yet empty, we finish the remaining sub problems by updating\\nthe maximum area with respect to the elements at the top.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 199, 'file_type': 'pdf'}, page_content='At the first impression, this solution seems to be having O(n2) complexity. But if we look\\ncarefully, every element is pushed and popped at most once, and in every step of the function at\\nleast one element is pushed or popped. Since the amount of work for the decisions and the update\\nis constant, the complexity of the algorithm is O(n) by amortized analysis. Space Complexity:\\nO(n) [for stack].\\nProblem-26\\u2003\\u2003On a given machine, how do you check whether the stack grows up or down?\\nSolution: Try noting down the address of a local variable. Call another function with a local\\nvariable declared in it and check the address of that local variable and compare.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 200, 'file_type': 'pdf'}, page_content='Time Complexity: O(1). Space Complexity: O(1).\\nProblem-27\\u2003\\u2003Given a stack of integers, how do you check whether each successive pair of\\nnumbers in the stack is consecutive or not. The pairs can be increasing or decreasing, and\\nif the stack has an odd number of elements, the element at the top is left out of a pair. For\\nexample, if the stack of elements are [4, 5, -2, -3, 11, 10, 5, 6, 20], then the output should\\nbe true because each of the pairs (4, 5), (-2, -3), (11, 10), and (5, 6) consists of\\nconsecutive numbers.\\nSolution: Refer to Queues chapter.\\nProblem-28\\u2003\\u2003Recursively remove all adjacent duplicates: Given a string of characters,\\nrecursively remove adjacent duplicate characters from string. The output string should not\\nhave any adjacent duplicates.\\nInput: careermonk\\nOutput: camonk\\nInput: mississippi\\nOutput: m\\nSolution: This solution runs with the concept of in-place stack. When element on stack doesn’t\\nmatch the current character, we add it to stack. When it matches to stack top, we skip characters\\nuntil the element matches the top of stack and remove the element from stack.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 201, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1) as the stack simulation is done inplace.\\nProblem-29\\u2003\\u2003Given an array of elements, replace every element with nearest greater element\\non the right of that element.\\nSolution: One simple approach would involve scanning the array elements and for each of the\\nelements, scan the remaining elements and find the nearest greater element.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 202, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). Space Complexity: O(1).\\nProblem-30\\u2003\\u2003For Problem-29, can we improve the complexity?\\nSolution: The approach is pretty much similar to Problem-22. Create a stack and push the first\\nelement. For the rest of the elements, mark the current element as nextNearestGreater. If stack is\\nnot empty, then pop an element from stack and compare it with nextNearestGreater. If\\nnextNearestGreater is greater than the popped element, then nextNearestGreater is the next\\ngreater element for the popped element. Keep popping from the stack while the popped element is\\nsmaller than nextNearestGreater. nextNearestGreater becomes the next greater element for all\\nsuch popped elements. If nextNearestGreater is smaller than the popped element, then push the\\npopped element back.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 203, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-31\\u2003\\u2003How to implement a stack which will support following operations in O(1) time\\ncomplexity?\\nPush which adds an element to the top of stack.\\nPop which removes an element from top of stack.\\nFind Middle which will return middle element of the stack.\\nDelete Middle which will delete the middle element.\\nSolution: We can use a LinkedList data structure with an extra pointer to the middle element.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 204, 'file_type': 'pdf'}, page_content='Also, we need another variable to store whether the LinkedList has an even or odd number of\\nelements.\\nPush: Add the element to the head of the LinkedList. Update the pointer to the\\nmiddle element according to variable.\\nPop: Remove the head of the LinkedList. Update the pointer to the middle element\\naccording to variable.\\nFind Middle: Find Middle which will return middle element of the stack.\\nDelete Middle: Delete Middle which will delete the middle element use the logic of\\nProblem-43 from Linked Lists chapter.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 205, 'file_type': 'pdf'}, page_content='5.1 What is a Queue?\\nA queue is a data structure used for storing data (similar to Linked Lists and Stacks). In queue, the\\norder in which data arrives is important. In general, a queue is a line of people or things waiting\\nto be served in sequential order starting at the beginning of the line or sequence.\\nDefinition: A queue is an ordered list in which insertions are done at one end (rear) and\\ndeletions are done at other end (front). The first element to be inserted is the first one to be\\ndeleted. Hence, it is called First in First out (FIFO) or Last in Last out (LILO) list.\\nSimilar to Stacks, special names are given to the two changes that can be made to a queue. When\\nan element is inserted in a queue, the concept is called EnQueue, and when an element is\\nremoved from the queue, the concept is called DeQueue.\\nDeQueueing an empty queue is called underflow and EnQueuing an element in a full queue is\\ncalled overflow. Generally, we treat them as exceptions. As an example, consider the snapshot of'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 206, 'file_type': 'pdf'}, page_content='the queue.\\n5.2 How are Queues Used?\\nThe concept of a queue can be explained by observing a line at a reservation counter. When we\\nenter the line we stand at the end of the line and the person who is at the front of the line is the one\\nwho will be served next. He will exit the queue and be served.\\nAs this happens, the next person will come at the head of the line, will exit the queue and will be\\nserved. As each person at the head of the line keeps exiting the queue, we move towards the head\\nof the line. Finally we will reach the head of the line and we will exit the queue and be served.\\nThis behavior is very useful in cases where there is a need to maintain the order of arrival.\\n5.3 Queue ADT\\nThe following operations make a queue an ADT. Insertions and deletions in the queue must\\nfollow the FIFO scheme. For simplicity we assume the elements are integers.\\nMain Queue Operations\\nEnQueue(int data): Inserts an element at the end of the queue\\nint DeQueue(): Removes and returns the element at the front of the queue\\nAuxiliary Queue Operations\\nint Front(): Returns the element at the front without removing it\\nint QueueSize(): Returns the number of elements stored in the queue\\nint IsEmptyQueueQ: Indicates whether no elements are stored in the queue or not\\n5.4 Exceptions'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 207, 'file_type': 'pdf'}, page_content='Similar to other ADTs, executing DeQueue on an empty queue throws an “Empty Queue\\nException” and executing EnQueue on a full queue throws “Full Queue Exception”.\\n5.5 Applications\\nFollowing are some of the applications that use queues.\\nDirect Applications\\nOperating systems schedule jobs (with equal priority) in the order of arrival (e.g., a\\nprint queue).\\nSimulation of real-world queues such as lines at a ticket counter or any other first-\\ncome first-served scenario requires a queue.\\nMultiprogramming.\\nAsynchronous data transfer (file IO, pipes, sockets).\\nWaiting times of customers at call center.\\nDetermining number of cashiers to have at a supermarket.\\nIndirect Applications\\nAuxiliary data structure for algorithms\\nComponent of other data structures\\n5.6 Implementation\\nThere are many ways (similar to Stacks) of implementing queue operations and some of the\\ncommonly used methods are listed below.\\nSimple circular array based implementation\\nDynamic circular array based implementation\\nLinked list implementation\\nWhy Circular Arrays?\\nFirst, let us see whether we can use simple arrays for implementing queues as we have done for\\nstacks. We know that, in queues, the insertions are performed at one end and deletions are\\nperformed at the other end. After performing some insertions and deletions the process becomes\\neasy to understand.\\nIn the example shown below, it can be seen clearly that the initial slots of the array are getting\\nwasted. So, simple array implementation for queue is not efficient. To solve this problem we\\nassume the arrays as circular arrays. That means, we treat the last element and the first array'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 208, 'file_type': 'pdf'}, page_content='elements as contiguous. With this representation, if there are any free slots at the beginning, the\\nrear pointer can easily go to its next free slot.\\nNote: The simple circular array and dynamic circular array implementations are very similar to\\nstack array implementations. Refer to Stacks chapter for analysis of these implementations.\\nSimple Circular Array Implementation\\nThis simple implementation of Queue ADT uses an array. In the array, we add elements circularly\\nand use two variables to keep track of the start element and end element. Generally, front is used\\nto indicate the start element and rear is used to indicate the end element in the queue. The array\\nstoring the queue elements may become full. An EnQueue operation will then throw a full queue\\nexception. Similarly, if we try deleting an element from an empty queue it will throw empty\\nqueue exception.\\nNote: Initially, both front and rear points to -1 which indicates that the queue is empty.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 209, 'file_type': 'pdf'}, page_content='struct ArrayQueuie *Queveint size) {\\nstruct Ayu = maleic Arey Queteh\\n‘return NULL;\\nQueapacity = size\\nQufront = Qu—rear = -1;\\nQ-array~ malloc(Q—capacity * sizeoftint)}:\\niftQ—array)\\nreturn NULL;\\nreturn Q:\\nint IsEmpty Queue(struct ArrayQueue *Q) (\\ntrue then 1 is returned elae 0 is returned\\n\\nint IsFullQueue(struct ArrayQueue *Q) {\\n[ifthe condition is true then 1 is returned else 0 is retumed\\n1 aa OO-ame 51% O-npncty = O-tend\\nint QueveSize() {\\n1 tum @eapciy = @-tont + Q-rear © 1% Q-eapaciy\\nvoid EnQueuefstruct ArrayQueue *Q, int data) {\\nif _sFullQueweQ))\\npine\" Queue Overflow”)\\ntse |\\nQ-rear = (Q-rear+1) % Queapacity:\\nQ- arraylQ—rearl- data;\\nf{Q-tront == -1)\\n‘Q-tront = Q-rear:\\n{nt DeQueue(struct ArrayQueue *Q){\\n{nt data 0;//or element which does not exist in Queue\\nifflsEmptvOuewe(O |\\nrin Queue is Empty’)\\nreturn 0;\\nelse {\\n‘data = Q-arraylQ-tront);\\n‘f\\\\Q—front == Q-rear)\\n‘Q-front = Q-rear = -1;\\nelse: Q-front = (Q-front+i) % Q-reapacity;\\nreturn data;\\n‘eld DeleteQueoetuct ArayQuei \"0 (\\nHt\\n‘i(Q-array)\\nfree(Q—array;\\n, har'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 210, 'file_type': 'pdf'}, page_content='Performance and Limitations\\nPerformance: Let n be the number of elements in the queue:\\nSpace Complexity (for n EnQueue operations)\\nO(n)\\nTime Complexity of EnQueue()\\nO(1)\\nTime Complexity of DeQueue()\\nO(1)\\nTime Complexity of IsEmptyQueue()\\nO(1)\\nTime Complexity of IsFullQueue()\\nO(1)\\nTime Complexity of QueueSize()\\nO(1)\\nTime Complexity of DeleteQueue()\\nO(1)\\nLimitations: The maximum size of the queue must be defined as prior and cannot be changed.\\nTrying to EnQueue a new element into a full queue causes an implementation-specific exception.\\nDynamic Circular Array Implementation'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 212, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the queue.\\nSpace Complexity (for n EnQueue operations)\\nO(n)\\nTime Complexity of EnQueue()\\nO(1) (Average)\\nTime Complexity of DeQueue()\\nO(1)\\nTime Complexity of QueueSize()\\nO(1)\\nTime Complexity of IsEmptyQueue()\\nO(1)\\nTime Complexity of IsFullQueue()\\nO(1)\\nTime Complexity of QueueSize()\\nO(1)\\nTime Complexity of DeleteQueue()\\nO(1)\\nLinked List Implementation\\nAnother way of implementing queues is by using Linked lists. EnQueue operation is implemented\\nby inserting an element at the end of the list. DeQueue operation is implemented by deleting an\\nelement from the beginning of the list.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 213, 'file_type': 'pdf'}, page_content='struct Queue *CreateQueuel) {\\nstruct Quete *Q;\\nstruct ListNode \"temp;\\nQ= malloisizeofistruct Queue);\\nintQ)\\nreturn NULL;\\ntemp ~ malloc(szeofstruct ListNode)};\\nQ-front = Q-rear = NULL;\\nreturn Q;\\nint IsEmpty Queue(struct Queue *Q) {\\n// ifthe condition is true then 1 is returned else 0 is returned\\nreturn (Q~front == NULL};\\nvoid EnQueue(struct Queue *Q, int data) {\\nstruct ListNode *newNode;\\nnewNode = malloc(sizeof(struct ListNode!);\\niffinewNode)\\nreturn NULL:\\nnewNode—data = data;\\nnewNode-rnext = NULL;\\nif(Q—rear) Q—rear—next = newNode;\\n(Qrrear = newNode;\\n\\n‘if(Q—front == NULL)\\nQofront = Qorear;\\nint DeQueue(struct Queue *Q) [\\nint data=0; — //or element which does not exist in Queue\\nstruct ListNode “temp;\\nisEmpty Quewe(Q) |\\nprin\" Queue is empty’);\\nreturn 0;\\nelse (\\ntemp = Q—front;\\ndata - Q-ront-rdata;\\nQufront== Q-front—-next;\\nfree(temp);\\nreturn data;\\nt\\nvoid DeleteQueue(struct Queue *Q) {\\nstruct ListNode \"temp;\\nwhile(Q) {\\ntemp = Q:\\nQ=Q-next\\nfree(temp];\\ni\\nfree(Q);'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 214, 'file_type': 'pdf'}, page_content='Performance\\nLet n be the number of elements in the queue, then\\nSpace Complexity (for n EnQueue operations)\\nO(n)\\nTime Complexity of EnQueue()\\nO(1) (Average)\\nTime Complexity of DeQueue()\\nO(1)\\nTime Complexity of IsEmptyQueue()\\nO(1)\\nTime Complexity of DeleteQueue()\\nO(1)\\nComparison of Implementations\\nNote: Comparison is very similar to stack implementations and Stacks chapter.\\n5.7 Queues: Problems & Solutions\\nProblem-1\\u2003\\u2003Give an algorithm for reversing a queue Q. To access the queue, we are only\\nallowed to use the methods of queue ADT.\\nSolution:\\nTime Complexity: O(n).\\nProblem-2\\u2003\\u2003How can you implement a queue using two stacks?\\nSolution: Let SI and S2 be the two stacks to be used in the implementation of queue. All we have\\nto do is to define the EnQueue and DeQueue operations for the queue.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 215, 'file_type': 'pdf'}, page_content='EnQueue Algorithm\\nJust push on to stack S1\\nTime Complexity: O(1).\\nDeQueue Algorithm\\nIf stack S2 is not empty then pop from S2 and return that element.\\nIf stack is empty, then transfer all elements from SI to S2 and pop the top element\\nfrom S2 and return that popped element [we can optimize the code a little by\\ntransferring only n – 1 elements from SI to S2 and pop the nth element from SI and\\nreturn that popped element].\\nIf stack S1 is also empty then throw error.\\nTime Complexity: From the algorithm, if the stack S2 is not empty then the complexity is O(1). If\\nthe stack S2 is empty, then we need to transfer the elements from SI to S2. But if we carefully\\nobserve, the number of transferred elements and the number of popped elements from S2 are\\nequal. Due to this the average complexity of pop operation in this case is O(1).The amortized\\ncomplexity of pop operation is O(1).\\nProblem-3\\u2003\\u2003Show how you can efficiently implement one stack using two queues. Analyze the'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 216, 'file_type': 'pdf'}, page_content='running time of the stack operations.\\nSolution: Yes, it is possible to implement the Stack ADT using 2 implementations of the Queue\\nADT. One of the queues will be used to store the elements and the other to hold them temporarily\\nduring the pop and top methods. The push method would enqueue the given element onto the\\nstorage queue. The top method would transfer all but the last element from the storage queue onto\\nthe temporary queue, save the front element of the storage queue to be returned, transfer the last\\nelement to the temporary queue, then transfer all elements back to the storage queue. The pop\\nmethod would do the same as top, except instead of transferring the last element onto the\\ntemporary queue after saving it for return, that last element would be discarded. Let Q1 and Q2 be\\nthe two queues to be used in the implementation of stack. All we have to do is to define the push\\nand pop operations for the stack.\\nIn the algorithms below, we make sure that one queue is always empty.\\nPush Operation Algorithm: Insert the element in whichever queue is not empty.\\nCheck whether queue Q1 is empty or not. If Q1 is empty then Enqueue the element\\ninto Q2.\\nOtherwise EnQueue the element into Q1.\\nTime Complexity: O(1).\\nPop Operation Algorithm: Transfer n – 1 elements to the other queue and delete last from queue\\nfor performing pop operation.\\nIf queue Q1 is not empty then transfer n – 1 elements from Q1 to Q2 and then,\\nDeQueue the last element of Q1 and return it.\\nIf queue Q2 is not empty then transfer n – 1 elements from Q2 to Q1 and then,\\nDeQueue the last element of Q2 and return it.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 217, 'file_type': 'pdf'}, page_content='Time Complexity: Running time of pop operation is O(n) as each time pop is called, we are\\ntransferring all the elements from one queue to the other.\\nProblem-4\\u2003\\u2003Maximum sum in sliding window: Given array A[] with sliding window of size\\nw which is moving from the very left of the array to the very right. Assume that we can\\nonly see the w numbers in the window. Each time the sliding window moves rightwards by\\none position. For example: The array is [1 3 -1 -3 5 3 6 7], and w is 3.\\nWindow position\\nMax\\n[1 3 -1] -3 5 3 6 7\\n1 [3 -1 -3] 5 3 6 7\\n1 3 [-1 -3 5] 3 6 7\\n1 3 -1 [-3 5 3] 6 7\\n1 3 -1 -3 [5 3 6] 7\\n1 3 -1 -3 5 [3 6 7]'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 218, 'file_type': 'pdf'}, page_content='Input: A long array A[], and a window width w. Output: An array B[], B[i] is the\\nmaximum value from A[i] to A[i+w-1]. Requirement: Find a good optimal way to get\\nB[i]\\nSolution: This problem can be solved with doubly ended queue (which supports insertion and\\ndeletion at both ends). Refer Priority Queues chapter for algorithms.\\nProblem-5\\u2003\\u2003Given a queue Q containing n elements, transfer these items on to a stack S\\n(initially empty) so that front element of Q appears at the top of the stack and the order of\\nall other items is preserved. Using enqueue and dequeue operations for the queue, and push\\nand pop operations for the stack, outline an efficient O(n) algorithm to accomplish the\\nabove task, using only a constant amount of additional storage.\\nSolution: Assume the elements of queue Q are a1:a2 ...an. Dequeuing all elements and pushing\\nthem onto the stack will result in a stack with an at the top and a1 at the bottom. This is done in\\nO(n) time as dequeue and each push require constant time per operation. The queue is now empty.\\nBy popping all elements and pushing them on the queue we will get a1 at the top of the stack. This\\nis done again in O(n) time.\\nAs in big-oh arithmetic we can ignore constant factors. The process is carried out in O(n) time.\\nThe amount of additional storage needed here has to be big enough to temporarily hold one item.\\nProblem-6\\u2003\\u2003A queue is set up in a circular array A[O..n - 1] with front and rear defined as\\nusual. Assume that n – 1 locations in the array are available for storing the elements (with\\nthe other element being used to detect full/empty condition). Give a formula for the number\\nof elements in the queue in terms of rear, front, and n.\\nSolution: Consider the following figure to get a clear idea of the queue.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 219, 'file_type': 'pdf'}, page_content='Rear of the queue is somewhere clockwise from the front.\\nTo enqueue an element, we move rear one position clockwise and write the element\\nin that position.\\nTo dequeue, we simply move front one position clockwise.\\nQueue migrates in a clockwise direction as we enqueue and dequeue.\\nEmptiness and fullness to be checked carefully.\\nAnalyze the possible situations (make some drawings to see where front and rear\\nare when the queue is empty, and partially and totally filled). We will get this:\\nProblem-7\\u2003\\u2003What is the most appropriate data structure to print elements of queue in reverse\\norder?\\nSolution: Stack.\\nProblem-8\\u2003\\u2003Implement doubly ended queues. A double-ended queue is an abstract data\\nstructure that implements a queue for which elements can only be added to or removed\\nfrom the front (head) or back (tail). It is also often called a head-tail linked list.\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 220, 'file_type': 'pdf'}, page_content='‘wid pushBackDEQistruct LstNode \"head, int data\\nstruct ListNde *newNode = (struct ListNode!) malloc(sizeofistrutListNode));\\nnewNode-data = data;\\nifaead «= NULL),\\n‘head = newNode;\\n(thead)next = #he\\n(thead)~prev = *head\\nelse{\\nnewNode-prev = /head-sprev;\\nnewNode-next » *head;\\n(thead)prev-mext = newNode;\\n(thead)prev = newNode;\\n‘wid pushFrontDEQ(struct ListNode “head, int data\\npushBackDEQihead data;\\n“head = ‘head)-prey;\\nint popBackDEQistrut LstNode *head)t\\nint data;\\nif *head)-prev == ‘head j\\ndata = \"head)-sdata;\\nfree(*head);\\n*head = NULL;\\nelse{\\nstruct LstNode *newTal = head)-spret-prev;\\ndata = ‘head)-prev-sdata;\\nnewTail-next = *head;\\nfre((head)~prev);\\n(head) ~prev » newTal;\\nreturn data;\\nint popFrontstructListNode head\\nint data;\\n*head = ‘head}~next;\\ndata = popBackDEQ(head);\\nreturn data;'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 221, 'file_type': 'pdf'}, page_content='Problem-9\\u2003\\u2003Given a stack of integers, how do you check whether each successive pair of\\nnumbers in the stack is consecutive or not. The pairs can be increasing or decreasing, and\\nif the stack has an odd number of elements, the element at the top is left out of a pair. For\\nexample, if the stack of elements are [4, 5, -2, -3, 11, 10, 5, 6, 20], then the output should\\nbe true because each of the pairs (4, 5), (-2, -3), (11, 10), and (5, 6) consists of\\nconsecutive numbers.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-10\\u2003\\u2003Given a queue of integers, rearrange the elements by interleaving the first half of\\nthe list with the second half of the list. For example, suppose a queue stores the following\\nsequence of values: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. Consider the two halves of\\nthis list: first half: [11, 12, 13, 14, 15] second half: [16, 17, 18, 19, 20]. These are'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 222, 'file_type': 'pdf'}, page_content='combined in an alternating fashion to form a sequence of interleave pairs: the first values\\nfrom each half (11 and 16), then the second values from each half (12 and 17), then the\\nthird values from each half (13 and 18), and so on. In each pair, the value from the first\\nhalf appears before the value from the second half. Thus, after the call, the queue stores the\\nfollowing values: [11, 16, 12, 17, 13, 18, 14, 19, 15, 20].\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-11\\u2003\\u2003Given an integer k and a queue of integers, how do you reverse the order of the\\nfirst k elements of the queue, leaving the other elements in the same relative order? For\\nexample, if k=4 and queue has the elements [10, 20, 30, 40, 50, 60, 70, 80, 90]; the output\\nshould be [40, 30, 20, 10, 50, 60, 70, 80, 90].\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 223, 'file_type': 'pdf'}, page_content='‘Time Complexity: O(n). Space Complexity: O(n).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 224, 'file_type': 'pdf'}, page_content='6.1 What is a Tree?\\nA tree is a data structure similar to a linked list but instead of each node pointing simply to the\\nnext node in a linear fashion, each node points to a number of nodes. Tree is an example of a non-\\nlinear data structure. A tree structure is a way of representing the hierarchical nature of a structure\\nin a graphical form.\\nIn trees ADT (Abstract Data Type), the order of the elements is not important. If we need ordering\\ninformation, linear data structures like linked lists, stacks, queues, etc. can be used.\\n6.2 Glossary'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 225, 'file_type': 'pdf'}, page_content='The root of a tree is the node with no parents. There can be at most one root node in\\na tree (node A in the above example).\\nAn edge refers to the link from parent to child (all links in the figure).\\nA node with no children is called leaf node (E,J,K,H and I).\\nChildren of same parent are called siblings (B,C,D are siblings of A, and E,F are the\\nsiblings of B).\\nA node p is an ancestor of node q if there exists a path from root to q and p appears\\non the path. The node q is called a descendant of p. For example, A,C and G are the\\nancestors of if.\\nThe set of all nodes at a given depth is called the level of the tree (B, C and D are\\nthe same level). The root node is at level zero.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 226, 'file_type': 'pdf'}, page_content='The depth of a node is the length of the path from the root to the node (depth of G is\\n2, A – C – G).\\nThe height of a node is the length of the path from that node to the deepest node. The\\nheight of a tree is the length of the path from the root to the deepest node in the tree.\\nA (rooted) tree with only one node (the root) has a height of zero. In the previous\\nexample, the height of B is 2 (B – F – J).\\nHeight of the tree is the maximum height among all the nodes in the tree and depth of\\nthe tree is the maximum depth among all the nodes in the tree. For a given tree,\\ndepth and height returns the same value. But for individual nodes we may get\\ndifferent results.\\nThe size of a node is the number of descendants it has including itself (the size of the\\nsubtree C is 3).\\nIf every node in a tree has only one child (except leaf nodes) then we call such trees\\nskew trees. If every node has only left child then we call them left skew trees.\\nSimilarly, if every node has only right child then we call them right skew trees.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 227, 'file_type': 'pdf'}, page_content='6.3 Binary Trees\\nA tree is called binary tree if each node has zero child, one child or two children. Empty tree is\\nalso a valid binary tree. We can visualize a binary tree as consisting of a root and two disjoint\\nbinary trees, called the left and right subtrees of the root.\\nGeneric Binary Tree'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 228, 'file_type': 'pdf'}, page_content='6.4 Types of Binary Trees\\nStrict Binary Tree: A binary tree is called strict binary tree if each node has exactly two\\nchildren or no children.\\nFull Binary Tree: A binary tree is called full binary tree if each node has exactly two children\\nand all leaf nodes are at the same level.\\nComplete Binary Tree: Before defining the complete binary tree, let us assume that the height of\\nthe binary tree is h. In complete binary trees, if we give numbering for the nodes by starting at the\\nroot (let us say the root node has 1) then we get a complete sequence from 1 to the number of\\nnodes in the tree. While traversing we should give numbering for NULL pointers also. A binary\\ntree is called complete binary tree if all leaf nodes are at height h or h – 1 and also without any\\nmissing number in the sequence.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 229, 'file_type': 'pdf'}, page_content='6.5 Properties of Binary Trees\\nFor the following properties, let us assume that the height of the tree is h. Also, assume that root\\nnode is at height zero.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 230, 'file_type': 'pdf'}, page_content='From the diagram we can infer the following properties:\\nThe number of nodes n in a full binary tree is 2h+1 – 1. Since, there are h levels we\\nneed to add all nodes at each level [20 + 21+ 22 + ··· + 2h = 2h+1 – 1].\\nThe number of nodes n in a complete binary tree is between 2h (minimum) and 2h+1\\n– 1 (maximum). For more information on this, refer to Priority Queues chapter.\\nThe number of leaf nodes in a full binary tree is 2h.\\nThe number of NULL links (wasted pointers) in a complete binary tree of n nodes is\\nn + 1.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 231, 'file_type': 'pdf'}, page_content='Structure of Binary Trees\\nNow let us define structure of the binary tree. For simplicity, assume that the data of the nodes are\\nintegers. One way to represent a node (which contains data) is to have two links which point to\\nleft and right children along with data fields as shown below:\\nNote: In trees, the default flow is from parent to children and it is not mandatory to show directed\\nbranches. For our discussion, we assume both the representations shown below are the same.\\nOperations on Binary Trees\\nBasic Operations\\nInserting an element into a tree\\nDeleting an element from a tree\\nSearching for an element\\nTraversing the tree\\nAuxiliary Operations\\nFinding the size of the tree\\nFinding the height of the tree\\nFinding the level which has maximum sum\\nFinding the least common ancestor (LCA) for a given pair of nodes, and many more.\\nApplications of Binary Trees'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 232, 'file_type': 'pdf'}, page_content='Following are the some of the applications where binary trees play an important role:\\nExpression trees are used in compilers.\\nHuffman coding trees that are used in data compression algorithms.\\nBinary Search Tree (BST), which supports search, insertion and deletion on a\\ncollection of items in O(logn) (average).\\nPriority Queue (PQ), which supports search and deletion of minimum (or maximum)\\non a collection of items in logarithmic time (in worst case).\\n6.6 Binary Tree Traversals\\nIn order to process trees, we need a mechanism for traversing them, and that forms the subject of\\nthis section. The process of visiting all nodes of a tree is called tree traversal. Each node is\\nprocessed only once but it may be visited more than once. As we have already seen in linear data\\nstructures (like linked lists, stacks, queues, etc.), the elements are visited in sequential order. But,\\nin tree structures there are many different ways.\\nTree traversal is like searching the tree, except that in traversal the goal is to move through the\\ntree in a particular order. In addition, all nodes are processed in the traversal but searching\\nstops when the required node is found.\\nTraversal Possibilities\\nStarting at the root of a binary tree, there are three main steps that can be performed and the order\\nin which they are performed defines the traversal type. These steps are: performing an action on\\nthe current node (referred to as “visiting” the node and denoted with “D”), traversing to the left\\nchild node (denoted with “L”), and traversing to the right child node (denoted with “R”). This\\nprocess can be easily described through recursion'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 232, 'file_type': 'pdf'}, page_content='. This\\nprocess can be easily described through recursion. Based on the above definition there are 6\\npossibilities:\\nLDR: Process left subtree, process the current node data and then process right\\nsubtree\\nLRD: Process left subtree, process right subtree and then process the current node\\ndata\\nDLR: Process the current node data, process left subtree and then process right\\nsubtree\\nDRL: Process the current node data, process right subtree and then process left\\nsubtree\\nRDL: Process right subtree, process the current node data and then process left\\nsubtree\\nRLD: Process right subtree, process left subtree and then process the current node\\ndata'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 233, 'file_type': 'pdf'}, page_content='Classifying the Traversals\\nThe sequence in which these entities (nodes) are processed defines a particular traversal method.\\nThe classification is based on the order in which current node is processed. That means, if we are\\nclassifying based on current node (D) and if D comes in the middle then it does not matter\\nwhether L is on left side of D or R is on left side of D.\\nSimilarly, it does not matter whether L is on right side of D or R is on right side of D. Due to this,\\nthe total 6 possibilities are reduced to 3 and these are:\\nPreorder (DLR) Traversal\\nInorder (LDR) Traversal\\nPostorder (LRD) Traversal\\nThere is another traversal method which does not depend on the above orders and it is:\\nLevel Order Traversal: This method is inspired from Breadth First Traversal (BFS\\nof Graph algorithms).\\nLet us use the diagram below for the remaining discussion.\\nPreOrder Traversal\\nIn preorder traversal, each node is processed before (pre) either of its subtrees. This is the\\nsimplest traversal to understand. However, even though each node is processed before the\\nsubtrees, it still requires that some information must be maintained while moving down the tree.\\nIn the example above, 1 is processed first, then the left subtree, and this is followed by the right\\nsubtree.\\nTherefore, processing must return to the right subtree after finishing the processing of the left\\nsubtree. To move to the right subtree after processing the left subtree, we must maintain the root'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 234, 'file_type': 'pdf'}, page_content='information. The obvious ADT for such information is a stack. Because of its LIFO structure, it is\\npossible to get the information about the right subtrees back in the reverse order.\\nPreorder traversal is defined as follows:\\nVisit the root.\\nTraverse the left subtree in Preorder.\\nTraverse the right subtree in Preorder.\\nThe nodes of tree would be visited in the order: 1 2 4 5 3 6 7\\nTime Complexity: O(n). Space Complexity: O(n).\\nNon-Recursive Preorder Traversal\\nIn the recursive version, a stack is required as we need to remember the current node so that after\\ncompleting the left subtree we can go to the right subtree. To simulate the same, first we process\\nthe current node and before going to the left subtree, we store the current node on stack. After\\ncompleting the left subtree processing, pop the element and go to its right subtree. Continue this\\nprocess until stack is nonempty.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 235, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nInOrder Traversal\\nIn Inorder Traversal the root is visited between the subtrees. Inorder traversal is defined as\\nfollows:\\nTraverse the left subtree in Inorder.\\nVisit the root.\\nTraverse the right subtree in Inorder.\\nThe nodes of tree would be visited in the order: 4 2 5 1 6 3 7'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 236, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nNon-Recursive Inorder Traversal\\nThe Non-recursive version of Inorder traversal is similar to Preorder. The only change is, instead\\nof processing the node before going to left subtree, process it after popping (which is indicated\\nafter completion of left subtree processing).\\nTime Complexity: O(n). Space Complexity: O(n).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 237, 'file_type': 'pdf'}, page_content='PostOrder Traversal\\nIn postorder traversal, the root is visited after both subtrees. Postorder traversal is defined as\\nfollows:\\nTraverse the left subtree in Postorder.\\nTraverse the right subtree in Postorder.\\nVisit the root.\\nThe nodes of the tree would be visited in the order: 4 5 2 6 7 3 1\\nTime Complexity: O(n). Space Complexity: O(n).\\nNon-Recursive Postorder Traversal\\nIn preorder and inorder traversals, after popping the stack element we do not need to visit the\\nsame vertex again. But in postorder traversal, each node is visited twice. That means, after\\nprocessing the left subtree we will visit the current node and after processing the right subtree we\\nwill visit the same current node. But we should be processing the node during the second visit.\\nHere the problem is how to differentiate whether we are returning from the left subtree or the\\nright subtree.\\nWe use a previous variable to keep track of the earlier traversed node. Let’s assume current is the\\ncurrent node that is on top of the stack. When previous is current’s parent, we are traversing\\ndown the tree. In this case, we try to traverse to current’s left child if available (i.e., push left\\nchild to the stack). If it is not available, we look at current’s right child. If both left and right child\\ndo not exist (ie, current is a leaf node), we print current’s value and pop it off the stack.\\nIf prev is current’s left child, we are traversing up the tree from the left. We look at current’s right\\nchild. If it is available, then traverse down the right child (i.e., push right child to the stack);\\notherwise print current’s value and pop it off the stack. If previous is current’s right child, we are\\ntraversing up the tree from the right. In this case, we print current’s value and pop it off the stack.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 238, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nLevel Order Traversal\\nLevel order traversal is defined as follows:\\nVisit the root.\\nWhile traversing level (, keep all the elements at level ( + 1 in queue.\\nGo to the next level and visit all the nodes at that level.\\nRepeat this until all levels are completed.\\nThe nodes of the tree are visited in the order: 1 2 3 4 5 6 7'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 239, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n). Since, in the worst case, all the nodes on the\\nentire last level could be in the queue simultaneously.\\nBinary Trees: Problems & Solutions\\nProblem-1\\u2003\\u2003Give an algorithm for finding maximum element in binary tree.\\nSolution: One simple way of solving this problem is: find the maximum element in left subtree,\\nfind the maximum element in right sub tree, compare them with root data and select the one which\\nis giving the maximum value. This approach can be easily implemented with recursion.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 240, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-2\\u2003\\u2003Give an algorithm for finding the maximum element in binary tree without\\nrecursion.\\nSolution: Using level order traversal: just observe the element’s data while deleting.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 241, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-3\\u2003\\u2003Give an algorithm for searching an element in binary tree.\\nSolution: Given a binary tree, return true if a node with data is found in the tree. Recurse down\\nthe tree, choose the left or right branch by comparing data with each node’s data.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 242, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-4\\u2003\\u2003Give an algorithm for searching an element in binary tree without recursion.\\nSolution: We can use level order traversal for solving this problem. The only change required in\\nlevel order traversal is, instead of printing the data, we just need to check whether the root data is\\nequal to the element we want to search.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 243, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-5\\u2003\\u2003Give an algorithm for inserting an element into binary tree.\\nSolution: Since the given tree is a binary tree, we can insert the element wherever we want. To\\ninsert an element, we can use the level order traversal and insert the element wherever we find\\nthe node whose left or right child is NULL.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 244, 'file_type': 'pdf'}, page_content='‘oid InsertInBinaryTree(struct BinaryTreeNode *root, int data\\nstruct Queue *Q;\\nstruct BinaryTreeNode *temp;\\nstruct BinaryTreeNode ‘newNode;\\nnewode = (struct BinaryTreeNode #] mallocsizeofstruc BinaryTreeNode);\\nnewNode—left = newNode-right = NULL;\\niffnewNode (\\nprint{(\"Memory Error’); return;\\n\\nifftrot){\\nroot = newNode;\\nreturn;\\n\\nQ = CreateQueuel);\\nEnQueue(Q,ro0t);\\n\\nwhile(tsEmpty Queue(Q)) {\\ntemp » DeQueue{Q);\\niftemp-left\\nEnQueue(Q, temple\\nese (\\ntemp-left=newNode;\\nDeleteQueue(Q);\\nreturn;\\niftemp-right)\\nEnQueuel0, temp-right);\\n\\ntemp-right=newNode;\\nDeleteQueue(Q);\\nreturn;\\n\\nDeleteQueue(Q);'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 245, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-6\\u2003\\u2003Give an algorithm for finding the size of binary tree.\\nSolution: Calculate the size of left and right subtrees recursively, add 1 (current node) and return\\nto its parent.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-7\\u2003\\u2003Can we solve Problem-6 without recursion?\\nSolution: Yes, using level order traversal.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 246, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-8\\u2003\\u2003Give an algorithm for printing the level order data in reverse order. For example,\\nthe output for the below tree should be: 4 5 6 7 2 3 1\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 247, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-9\\u2003\\u2003Give an algorithm for deleting the tree.\\nSolution:\\nTo delete a tree, we must traverse all the nodes of the tree and delete them one by one. So which\\ntraversal should we use: Inorder, Preorder, Postorder or Level order Traversal?\\nBefore deleting the parent node we should delete its children nodes first. We can use postorder\\ntraversal as it does the work without storing anything. We can delete tree with other traversals\\nalso with extra space complexity. For the following, tree nodes are deleted in order – 4,5,2,3,1.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-10\\u2003\\u2003Give an algorithm for finding the height (or depth) of the binary tree.\\nSolution: Recursively calculate height of left and right subtrees of a node and assign height to the\\nnode as max of the heights of two children plus 1. This is similar to PreOrder tree traversal (and\\nDFS of Graph algorithms).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 248, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-11\\u2003\\u2003Can we solve Problem-10 without recursion?\\nSolution: Yes, using level order traversal. This is similar to BFS of Graph algorithms. End of\\nlevel is identified with NULL.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 249, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-12\\u2003\\u2003Give an algorithm for finding the deepest node of the binary tree.\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 250, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-13\\u2003\\u2003Give an algorithm for deleting an element (assuming data is given) from binary\\ntree.\\nSolution: The deletion of a node in binary tree can be implemented as\\nStarting at root, find the node which we want to delete.\\nFind the deepest node in the tree.\\nReplace the deepest node’s data with node to be deleted.\\nThen delete the deepest node.\\nProblem-14\\u2003\\u2003Give an algorithm for finding the number of leaves in the binary tree without\\nusing recursion.\\nSolution: The set of nodes whose both left and right children are NULL are called leaf nodes.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 251, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-15\\u2003\\u2003Give an algorithm for finding the number of full nodes in the binary tree without\\nusing recursion.\\nSolution: The set of all nodes with both left and right children are called full nodes.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 252, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-16\\u2003\\u2003Give an algorithm for finding the number of half nodes (nodes with only one\\nchild) in the binary tree without using recursion.\\nSolution: The set of all nodes with either left or right child (but not both) are called half nodes.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 253, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-17\\u2003\\u2003Given two binary trees, return true if they are structurally identical.\\nSolution:\\nAlgorithm:\\nIf both trees are NULL then return true.\\nIf both trees are not NULL, then compare data and recursively check left and right\\nsubtree structures.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 254, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for recursive stack.\\nProblem-18\\u2003\\u2003Give an algorithm for finding the diameter of the binary tree. The diameter of a\\ntree (sometimes called the width) is the number of nodes on the longest path between two\\nleaves in the tree.\\nSolution: To find the diameter of a tree, first calculate the diameter of left subtree and right\\nsubtrees recursively. Among these two values, we need to send maximum value along with\\ncurrent level (+1).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 255, 'file_type': 'pdf'}, page_content='There is another solution and the complexity is O(n). The main idea of this approach is that the\\nnode stores its left child’s and right child’s maximum diameter if the node’s child is the “root”,\\ntherefore, there is no need to recursively call the height method. The drawback is we need to add\\ntwo extra variables in the node structure.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 256, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-19\\u2003\\u2003Give an algorithm for finding the level that has the maximum sum in the binary\\ntree.\\nSolution: The logic is very much similar to finding the number of levels. The only change is, we'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 257, 'file_type': 'pdf'}, page_content='need to keep track of the sums as well.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 258, 'file_type': 'pdf'}, page_content=\"int FindLevelwithMaxSumstruct BinaryTreeNode ‘root\\nstruct BinaryTreeNode “temp;\\nint level=0, maxLevel=0;\\nstruct Queue *Q;\\n{nt currentSum = 0, maxSum = 0;\\niflroot}\\nreturn 0;\\nQ-CreateQueue')\\nEnQueue(Q,root};\\nEnQueue(Q,NULL); | Bnd of irs level.\\nwhile({IsEmptyQueue(Q)) |\\ntemp =DeQueue(Q);\\n/| Ifthe curtent levels completed then compare sums\\n{ftemp == NULL {\\niffeurrentSum> maxSum){\\n‘maxSum = currentSum;\\nrmaxLevel = level;\\ncurrentSum = 0;\\n[place the indicator for end of next level at the end of queue\\nifsEmptyQueue(Q))\\nEnQueue(Q,NULL\\nlevel;\\nse {\\ncurrentSum += temp-data;\\ntemplet\\nEnQueue(temp, temp—left);\\niflroot-right)\\nEnQueue(temp, temp—right);\\n\\nreturn maxLevel;\"),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 259, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-20\\u2003\\u2003Given a binary tree, print out all its root-to-leaf paths.\\nSolution: Refer to comments in functions.\\nTime Complexity: O(n). Space Complexity: O(n), for recursive stack.\\nProblem-21\\u2003\\u2003Give an algorithm for checking the existence of path with given sum. That\\nmeans, given a sum, check whether there exists a path from root to any of the nodes.\\nSolution: For this problem, the strategy is: subtract the node value from the sum before calling its\\nchildren recursively, and check to see if the sum is 0 when we run out of tree.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 260, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-22\\u2003\\u2003Give an algorithm for finding the sum of all elements in binary tree.\\nSolution: Recursively, call left subtree sum, right subtree sum and add their values to current\\nnodes data.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 261, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-23\\u2003\\u2003Can we solve Problem-22 without recursion?\\nSolution: We can use level order traversal with simple change. Every time after deleting an\\nelement from queue, add the nodes data value to sum variable.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-24\\u2003\\u2003Give an algorithm for converting a tree to its mirror. Mirror of a tree is another\\ntree with left and right children of all non-leaf nodes interchanged. The trees below are\\nmirrors to each other.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 262, 'file_type': 'pdf'}, page_content='Solution:\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-25\\u2003\\u2003Given two trees, give an algorithm for checking whether they are mirrors of\\neach other.\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 263, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-26\\u2003\\u2003Give an algorithm for finding LCA (Least Common Ancestor) of two nodes in a\\nBinary Tree.\\nSolution:\\nTime Complexity: O(n). Space Complexity: O(n) for recursion.\\nProblem-27\\u2003\\u2003Give an algorithm for constructing binary tree from given Inorder and Preorder\\ntraversals.\\nSolution: Let us consider the traversals below:\\nInorder sequence: D B E A F C\\nPreorder sequence: A B D E C F'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 264, 'file_type': 'pdf'}, page_content='In a Preorder sequence, leftmost element denotes the root of the tree. So we know ‘A’ is the root\\nfor given sequences. By searching ‘A’ in Inorder sequence we can find out all elements on the left\\nside of ‘A’, which come under the left subtree, and elements on the right side of ‘A’, which come\\nunder the right subtree. So we get the structure as seen below.\\nWe recursively follow the above steps and get the following tree.\\nAlgorithm: BuildTree()\\nSelect an element from Preorder. Increment a Preorder index variable\\n(preOrderIndex in code below) to pick next element in next recursive call.\\nCreate a new tree node (newNode) from heap with the data as selected element.\\nFind the selected element’s index in Inorder. Let the index be inOrderIndex.\\nCall BuildBinaryTree for elements before inOrderIndex and make the built tree as left\\nsubtree of newNode.\\nCall BuildBinaryTree for elements after inOrderIndex and make the built tree as right\\nsubtree of newNode.\\nreturn newNode.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 265, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-28\\u2003\\u2003If we are given two traversal sequences, can we construct the binary tree\\nuniquely?\\nSolution: It depends on what traversals are given. If one of the traversal methods is Inorder then\\nthe tree can be constructed uniquely, otherwise not.\\nTherefore, the following combinations can uniquely identify a tree:\\nInorder and Preorder\\nInorder and Postorder\\nInorder and Level-order'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 266, 'file_type': 'pdf'}, page_content='The following combinations do not uniquely identify a tree.\\nPostorder and Preorder\\nPreorder and Level-order\\nPostorder and Level-order\\nFor example, Preorder, Level-order and Postorder traversals are the same for the above trees:\\nSo, even if three of them (PreOrder, Level-Order and PostOrder) are given, the tree cannot be\\nconstructed uniquely.\\nProblem-29\\u2003\\u2003Give an algorithm for printing all the ancestors of a node in a Binary tree. For\\nthe tree below, for 7 the ancestors are 1 3 7.\\nSolution: Apart from the Depth First Search of this tree, we can use the following recursive way\\nto print the ancestors.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 267, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n) for recursion.\\nProblem-30\\u2003\\u2003Zigzag Tree Traversal: Give an algorithm to traverse a binary tree in Zigzag\\norder. For example, the output for the tree below should be: 1 3 2 4 5 6 7\\nSolution: This problem can be solved easily using two stacks. Assume the two stacks are:\\ncurrentLevel and nextLevel. We would also need a variable to keep track of the current level\\norder (whether it is left to right or right to left).\\nWe pop from currentLevel stack and print the node’s value. Whenever the current level order is\\nfrom left to right, push the node’s left child, then its right child, to stack nextLevel. Since a stack\\nis a Last In First Out (LIFO) structure, the next time that nodes are popped off nextLevel, it will\\nbe in the reverse order.\\nOn the other hand, when the current level order is from right to left, we would push the node’s\\nright child first, then its left child. Finally, don’t forget to swap those two stacks at the end of each\\nlevel (i. e., when currentLevel is empty).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 268, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: Space for two stacks = O(n) + O(n) = O(n).\\nProblem-31\\u2003\\u2003Give an algorithm for finding the vertical sum of a binary tree. For example, The\\ntree has 5 vertical lines\\nVertical-1: nodes-4 => vertical sum is 4\\nVertical-2: nodes-2 => vertical sum is 2\\nVertical-3: nodes-1,5,6 => vertical sum is 1 + 5 + 6 = 12\\nVertical-4: nodes-3 => vertical sum is 3\\nVertical-5: nodes-7 => vertical sum is 7\\nWe need to output: 4 2 12 3 7'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 269, 'file_type': 'pdf'}, page_content='Solution: \\nWe \\ncan \\ndo \\nan \\ninorder \\ntraversal \\nand \\nhash \\nthe \\ncolumn. \\nWe \\ncall\\nVerticalSumlnBinaryTreefroot, 0) which means the root is at column 0. While doing the traversal,\\nhash the column and increase its value by root → data.\\nProblem-32\\u2003\\u2003How many different binary trees are possible with n nodes?\\nSolution: For example, consider a tree with 3 nodes (n = 3). It will have the maximum\\ncombination of 5 different (i.e., 23 -3 = 5) trees.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 270, 'file_type': 'pdf'}, page_content='In general, if there are n nodes, there exist 2n –n different trees.\\nProblem-33\\u2003\\u2003Given a tree with a special property where leaves are represented with ‘L’ and\\ninternal node with ‘I’. Also, assume that each node has either 0 or 2 children. Given\\npreorder traversal of this tree, construct the tree.\\nExample: Given preorder string => ILILL\\nSolution: First, we should see how preorder traversal is arranged. Pre-order traversal means\\nfirst put root node, then pre-order traversal of left subtree and then pre-order traversal of right\\nsubtree. In a normal scenario, it’s not possible to detect where left subtree ends and right subtree\\nstarts using only pre-order traversal. Since every node has either 2 children or no child, we can\\nsurely say that if a node exists then its sibling also exists. So every time when we are computing a\\nsubtree, we need to compute its sibling subtree as well.\\nSecondly, whenever we get ‘L’ in the input string, that is a leaf and we can stop for a particular\\nsubtree at that point. After this ‘L’ node (left child of its parent ‘L’), its sibling starts. If ‘L’ node is\\nright child of its parent, then we need to go up in the hierarchy to find the next subtree to compute.\\nKeeping the above invariant in mind, we can easily determine when a subtree ends and the next\\none starts. It means that we can give any start node to our method and it can easily complete the\\nsubtree it generates going outside of its nodes. We just need to take care of passing the correct\\nstart nodes to different sub-trees.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 271, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nProblem-34\\u2003\\u2003Given a binary tree with three pointers (left, right and nextSibling), give an\\nalgorithm for filling the nextSibling pointers assuming they are NULL initially.\\nSolution: We can use simple queue (similar to the solution of Problem-11). Let us assume that the\\nstructure of binary tree is:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 272, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n).\\nProblem-35\\u2003\\u2003Is there any other way of solving Problem-34?\\nSolution: The trick is to re-use the populated nextSibling pointers. As mentioned earlier, we just'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 273, 'file_type': 'pdf'}, page_content='need one more step for it to work. Before we pass the left and right to the recursion function\\nitself, we connect the right child’s nextSibling to the current node’s nextSibling left child. In order\\nfor this to work, the current node nextSibling pointer must be populated, which is true in this\\ncase.\\nTime Complexity: O(n).\\n6.7 Generic Trees (N-ary Trees)\\nIn the previous section we discussed binary trees where each node can have a maximum of two\\nchildren and these are represented easily with two pointers. But suppose if we have a tree with\\nmany children at every node and also if we do not know how many children a node can have, how\\ndo we represent them?\\nFor example, consider the tree shown below.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 274, 'file_type': 'pdf'}, page_content='How do we represent the tree?\\nIn the above tree, there are nodes with 6 children, with 3 children, with 2 children, with 1 child,\\nand with zero children (leaves). To present this tree we have to consider the worst case (6\\nchildren) and allocate that many child pointers for each node. Based on this, the node\\nrepresentation can be given as:\\nSince we are not using all the pointers in all the cases, there is a lot of memory wastage. Another\\nproblem is that we do not know the number of children for each node in advance. In order to'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 275, 'file_type': 'pdf'}, page_content='solve this problem we need a representation that minimizes the wastage and also accepts nodes\\nwith any number of children.\\nRepresentation of Generic Trees\\nSince our objective is to reach all nodes of the tree, a possible solution to this is as follows:\\nAt each node link children of same parent (siblings) from left to right.\\nRemove the links from parent to all children except the first child.\\nWhat these above statements say is if we have a link between children then we do not need extra\\nlinks from parent to all children. This is because we can traverse all the elements by starting at\\nthe first child of the parent. So if we have a link between parent and first child and also links\\nbetween all children of same parent then it solves our problem.\\nThis representation is sometimes called first child/next sibling representation. First child/next\\nsibling representation of the generic tree is shown above. The actual representation for this tree\\nis:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 276, 'file_type': 'pdf'}, page_content='Based on this discussion, the tree node declaration for general tree can be given as:\\nNote: Since we are able to convert any generic tree to binary representation; in practice we use\\nbinary trees. We can treat all generic trees with a first child/next sibling representation as binary\\ntrees.\\nGeneric Trees: Problems & Solutions\\nProblem-36\\u2003\\u2003Given a tree, give an algorithm for finding the sum of all the elements of the tree.\\nSolution: The solution is similar to what we have done for simple binary trees. That means,\\ntraverse the complete list and keep on adding the values. We can either use level order traversal'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 277, 'file_type': 'pdf'}, page_content='or simple recursion.\\nTime Complexity: O(n). Space Complexity: O(1) (if we do not consider stack space), otherwise\\nO(n).\\nNote: All problems which we have discussed for binary trees are applicable for generic trees\\nalso. Instead of left and right pointers we just need to use firstChild and nextSibling.\\nProblem-37\\u2003\\u2003For a 4-ary tree (each node can contain maximum of 4 children), what is the\\nmaximum possible height with 100 nodes? Assume height of a single node is 0.\\nSolution: In 4-ary tree each node can contain 0 to 4 children, and to get maximum height, we need\\nto keep only one child for each parent. With 100 nodes, the maximum possible height we can get\\nis 99.\\nIf we have a restriction that at least one node has 4 children, then we keep one node with 4\\nchildren and the remaining nodes with 1 child. In this case, the maximum possible height is 96.\\nSimilarly, with n nodes the maximum possible height is n – 4.\\nProblem-38\\u2003\\u2003For a 4-ary tree (each node can contain maximum of 4 children), what is the\\nminimum possible height with n nodes?\\nSolution: Similar to the above discussion, if we want to get minimum height, then we need to fill\\nall nodes with maximum children (in this case 4). Now let’s see the following table, which\\nindicates the maximum number of nodes for a given height.\\nFor a given height h the maximum possible nodes are: \\n. To get minimum height, take\\nlogarithm on both sides:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 278, 'file_type': 'pdf'}, page_content='Problem-39\\u2003\\u2003Given a parent array P, where P[i] indicates the parent of ith node in the tree\\n(assume parent of root node is indicated with –1). Give an algorithm for finding the height\\nor depth of the tree.\\nSolution:\\nFor example: if the P is\\nIts corresponding tree is:\\nFrom the problem definition, the given array represents the parent array. That means, we need to\\nconsider the tree for that array and find the depth of the tree. The depth of this given tree is 4. If\\nwe carefully observe, we just need to start at every node and keep going to its parent until we\\nreach –1 and also keep track of the maximum depth among all nodes.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 279, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). For skew trees we will be re-calculating the same values. Space\\nComplexity: O(1).\\nNote: We can optimize the code by storing the previous calculated nodes’ depth in some hash\\ntable or other array. This reduces the time complexity but uses extra space.\\nProblem-40\\u2003\\u2003Given a node in the generic tree, give an algorithm for counting the number of\\nsiblings for that node.\\nSolution: Since tree is represented with the first child/next sibling method, the tree structure can\\nbe given as:\\nFor a given node in the tree, we just need to traverse all its next siblings.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 280, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nProblem-41\\u2003\\u2003Given a node in the generic tree, give an algorithm for counting the number of\\nchildren for that node.\\nSolution: Since the tree is represented as first child/next sibling method, the tree structure can be\\ngiven as:\\nFor a given node in the tree, we just need to point to its first child and keep traversing all its next\\nsiblings.\\nTime Complexity: O(n). Space Complexity: O(1).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 281, 'file_type': 'pdf'}, page_content='Problem-42\\u2003\\u2003Given two trees how do we check whether the trees are isomorphic to each\\nother or not?\\nSolution:\\nTwo binary trees root1 and root2 are isomorphic if they have the same structure. The values of\\nthe nodes does not affect whether two trees are isomorphic or not. In the diagram below, the tree\\nin the middle is not isomorphic to the other trees, but the tree on the right is isomorphic to the tree\\non the left.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-43\\u2003\\u2003Given two trees how do we check whether they are quasi-isomorphic to each\\nother or not?\\nSolution:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 282, 'file_type': 'pdf'}, page_content='Two trees root1 and root2 are quasi-isomorphic if root1 can be transformed into root2 by\\nswapping the left and right children of some of the nodes of root1. Data in the nodes are not\\nimportant in determining quasi-isomorphism; only the shape is important. The trees below are\\nquasi-isomorphic because if the children of the nodes on the left are swapped, the tree on the right\\nis obtained.\\nTime Complexity: O(n). Space Complexity: O(n).\\nProblem-44\\u2003\\u2003A full k –ary tree is a tree where each node has either 0 or k children. Given an\\narray which contains the preorder traversal of full k –ary tree, give an algorithm for\\nconstructing the full k –ary tree.\\nSolution: In k –ary tree, for a node at ith position its children will be at k * i + 1 to k * i + k. For\\nexample, the example below is for full 3-ary tree.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 283, 'file_type': 'pdf'}, page_content='As we have seen, in preorder traversal first left subtree is processed then followed by root node\\nand right subtree. Because of this, to construct a full k-ary, we just need to keep on creating the\\nnodes without bothering about the previous constructed nodes. We can use this trick to build the\\ntree recursively by using one global index. The declaration for k-ary tree can be given as:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 284, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), where n is the size of the pre-order array. This is because we are moving\\nsequentially and not visiting the already constructed nodes.\\n6.8 Threaded Binary Tree Traversals (Stack or Queue-less Traversals)\\nIn earlier sections we have seen that, preorder, inorder and postorder binary tree traversals used\\nstacks and level order traversals used queues as an auxiliary data structure. In this section we\\nwill discuss new traversal algorithms which do not need both stacks and queues. Such traversal'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 285, 'file_type': 'pdf'}, page_content='algorithms are called threaded binary tree traversals or stack/queue – less traversals.\\nIssues with Regular Binary Tree Traversals\\nThe storage space required for the stack and queue is large.\\nThe majority of pointers in any binary tree are NULL. For example, a binary tree\\nwith n nodes has n + 1 NULL pointers and these were wasted.\\nIt is difficult to find successor node (preorder, inorder and postorder successors) for\\na given node.\\nMotivation for Threaded Binary Trees\\nTo solve these problems, one idea is to store some useful information in NULL pointers. If we\\nobserve the previous traversals carefully, stack/ queue is required because we have to record the\\ncurrent position in order to move to the right subtree after processing the left subtree. If we store\\nthe useful information in NULL pointers, then we don’t have to store such information in stack/\\nqueue.\\nThe binary trees which store such information in NULL pointers are called threaded binary trees.\\nFrom the above discussion, let us assume that we want to store some useful information in NULL'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 286, 'file_type': 'pdf'}, page_content='pointers. The next question is what to store?\\nThe common convention is to put predecessor/successor information. That means, if we are\\ndealing with preorder traversals, then for a given node, NULL left pointer will contain preorder\\npredecessor information and NULL right pointer will contain preorder successor information.\\nThese special pointers are called threads.\\nClassifying Threaded Binary Trees\\nThe classification is based on whether we are storing useful information in both NULL pointers or\\nonly in one of them.\\nIf we store predecessor information in NULL left pointers only, then we can call\\nsuch binary trees left threaded binary trees.\\nIf we store successor information in NULL right pointers only, then we can call such\\nbinary trees right threaded binary trees.\\nIf we store predecessor information in NULL left pointers and successor information\\nin NULL right pointers, then we can call such binary trees fully threaded binary\\ntrees or simply threaded binary trees.\\nNote: For the remaining discussion we consider only (fully) threaded binary trees.\\nTypes of Threaded Binary Trees\\nBased on above discussion we get three representations for threaded binary trees.\\nPreorder Threaded Binary Trees: NULL left pointer will contain PreOrder\\npredecessor information and NULL right pointer will contain PreOrder successor\\ninformation.\\nInorder Threaded Binary Trees: NULL left pointer will contain InOrder\\npredecessor information and NULL right pointer will contain InOrder successor\\ninformation.\\nPostorder Threaded Binary Trees: NULL left pointer will contain PostOrder\\npredecessor information and NULL right pointer will contain PostOrder successor\\ninformation.\\nNote: As the representations are similar, for the remaining discussion we will use InOrder\\nthreaded binary trees.\\nThreaded Binary Tree structure\\nAny program examining the tree must be able to differentiate between a regular left/right pointer'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 287, 'file_type': 'pdf'}, page_content='and a thread. To do this, we use two additional fields in each node, giving us, for threaded trees,\\nnodes of the following form:\\nDifference between Binary Tree and Threaded Binary Tree Structures\\nNote: Similarly, we can define preorder/postorder differences as well.\\nAs an example, let us try representing a tree in inorder threaded binary tree form. The tree below\\nshows what an inorder threaded binary tree will look like. The dotted arrows indicate the\\nthreads. If we observe, the left pointer of left most node (2) and right pointer of right most node\\n(31) are hanging.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 288, 'file_type': 'pdf'}, page_content='What should leftmost and rightmost pointers point to?\\nIn the representation of a threaded binary tree, it is convenient to use a special node Dummy\\nwhich is always present even for an empty tree. Note that right tag of Dummy node is 1 and its\\nright child points to itself.\\nWith this convention the above tree can be represented as:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 289, 'file_type': 'pdf'}, page_content='Finding Inorder Successor in Inorder Threaded Binary Tree\\nTo find inorder successor of a given node without using a stack, assume that the node for which\\nwe want to find the inorder successor is P.\\nStrategy: If P has a no right subtree, then return the right child of P. If P has right subtree, then\\nreturn the left of the nearest node whose left subtree contains P.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 290, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nInorder Traversal in Inorder Threaded Binary Tree\\nWe can start with dummy node and call InorderSuccessor() to visit each node until we reach\\ndummy node.\\nAlternative coding:\\nTime Complexity: O(n). Space Complexity: O(1).\\nFinding PreOrder Successor in InOrder Threaded Binary Tree\\nStrategy: If P has a left subtree, then return the left child of P. If P has no left subtree, then return\\nthe right child of the nearest node whose right subtree contains P.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 291, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nPreOrder Traversal of InOrder Threaded Binary Tree\\nAs in inorder traversal, start with dummy node and call PreorderSuccessorf) to visit each node\\nuntil we get dummy node again.\\nAlternative coding:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 292, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nNote: From the above discussion, it should be clear that inorder and preorder successor finding\\nis easy with threaded binary trees. But finding postorder successor is very difficult if we do not\\nuse stack.\\nInsertion of Nodes in InOrder Threaded Binary Trees\\nFor simplicity, let us assume that there are two nodes P and Q and we want to attach Q to right of\\nP. For this we will have two cases.\\nNode P does not have right child: In this case we just need to attach Q to P and\\nchange its left and right pointers.\\nNode P has right child (say, R): In this case we need to traverse R’s left subtree and\\nfind the left most node and then update the left and right pointer of that node (as\\nshown below).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 293, 'file_type': 'pdf'}, page_content='Tsao Ng\\nXo wo\\nby? bY?\\n\\n‘void InsertRightndnorderTBT|structThreadedBinaryTreeNode *, struct ThreadedBinaryTreeNode \"Q\\\\\\nstruct ThreadedBinaryTreeNode Temp;\\nQuright» P-right;\\n(QaRTag= P-RTag,\\nQuiet= P;\\nQui Tag = 0;\\nPeoright = Q;\\nPaRTag =I;\\ni(Q--RTag == 1) | | [Case-2\\nTemp = Q-right;\\nWwhile(Temp-LTag)\\nTemp = Temp—left\\nTemp-left = Q;\\n\\n‘Time Complexity: O(n). Space Complexity: O(1)..'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 294, 'file_type': 'pdf'}, page_content='Threaded Binary Trees: Problems & Solutions\\nProblem-45\\u2003\\u2003For a given binary tree (not threaded) how do we find the preorder successor?\\nSolution: For solving this problem, we need to use an auxiliary stack S. On the first call, the\\nparameter node is a pointer to the head of the tree, and thereafter its value is NULL. Since we are\\nsimply asking for the successor of the node we got the last time we called the function.\\nIt is necessary that the contents of the stack S and the pointer P to the last node “visited” are\\npreserved from one call of the function to the next; they are defined as static variables.\\nProblem-46\\u2003\\u2003For a given binary tree (not threaded) how do we find the inorder successor?\\nSolution: Similar to the above discussion, we can find the inorder successor of a node as:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 295, 'file_type': 'pdf'}, page_content='6.9 Expression Trees\\nA tree representing an expression is called an expression tree. In expression trees, leaf nodes are\\noperands and non-leaf nodes are operators. That means, an expression tree is a binary tree where\\ninternal nodes are operators and leaves are operands. An expression tree consists of binary\\nexpression. But for a u-nary operator, one subtree will be empty. The figure below shows a\\nsimple expression tree for (A + B * C) / D.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 296, 'file_type': 'pdf'}, page_content='Algorithm for Building Expression Tree from Postfix Expression'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 297, 'file_type': 'pdf'}, page_content='Example: Assume that one symbol is read at a time. If the symbol is an operand, we create a tree\\nnode and push a pointer to it onto a stack. If the symbol is an operator, pop pointers to two trees\\nT1 and T2 from the stack (T1 is popped first) and form a new tree whose root is the operator and\\nwhose left and right children point to T2 and T1 respectively. A pointer to this new tree is then\\npushed onto the stack.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 298, 'file_type': 'pdf'}, page_content='As an example, assume the input is A B C * + D /. The first three symbols are operands, so create\\ntree nodes and push pointers to them onto a stack as shown below.\\nNext, an operator ‘*’ is read, so two pointers to trees are popped, a new tree is formed and a\\npointer to it is pushed onto the stack.\\nNext, an operator ‘+’ is read, so two pointers to trees are popped, a new tree is formed and a\\npointer to it is pushed onto the stack.\\nNext, an operand ‘D’ is read, a one-node tree is created and a pointer to the corresponding tree is\\npushed onto the stack.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 299, 'file_type': 'pdf'}, page_content='Finally, the last symbol (‘/’) is read, two trees are merged and a pointer to the final tree is left on\\nthe stack.\\n6.10 XOR Trees\\nThis concept is similar to memory efficient doubly linked lists of Linked Lists chapter. Also, like\\nthreaded binary trees this representation does not need stacks or queues for traversing the trees.\\nThis representation is used for traversing back (to parent) and forth (to children) using ⊕\\noperation. To represent the same in XOR trees, for each node below are the rules used for\\nrepresentation:\\nEach nodes left will have the ⊕ of its parent and its left children.\\nEach nodes right will have the ⊕ of its parent and its right children.\\nThe root nodes parent is NULL and also leaf nodes children are NULL nodes.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 300, 'file_type': 'pdf'}, page_content='Based on the above rules and discussion, the tree can be represented as:\\nThe major objective of this presentation is the ability to move to parent as well to children. Now,'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 301, 'file_type': 'pdf'}, page_content='let us see how to use this representation for traversing the tree. For example, if we are at node B\\nand want to move to its parent node A, then we just need to perform ⊕ on its left content with its\\nleft child address (we can use right child also for going to parent node).\\nSimilarly, if we want to move to its child (say, left child D) then we have to perform ⊕ on its left\\ncontent with its parent node address. One important point that we need to understand about this\\nrepresentation is: When we are at node B, how do we know the address of its children D? Since\\nthe traversal starts at node root node, we can apply ⊕ on root’s left content with NULL. As a\\nresult we get its left child, B. When we are at B, we can apply ⊕ on its left content with A\\naddress.\\n6.11 Binary Search Trees (BSTs)\\nWhy Binary Search Trees?\\nIn previous sections we have discussed different tree representations and in all of them we did\\nnot impose any restriction on the nodes data. As a result, to search for an element we need to\\ncheck both in left subtree and in right subtree. Due to this, the worst case complexity of search\\noperation is O(n).\\nIn this section, we will discuss another variant of binary trees: Binary Search Trees (BSTs). As\\nthe name suggests, the main use of this representation is for searching. In this representation we\\nimpose restriction on the kind of data a node can contain. As a result, it reduces the worst case\\naverage search operation to O(logn).\\nBinary Search Tree Property\\nIn binary search trees, all the left subtree elements should be less than root data and all the right\\nsubtree elements should be greater than root data. This is called binary search tree property. Note\\nthat, this property should be satisfied at every node in the tree.\\nThe left subtree of a node contains only nodes with keys less than the nodes key.\\nThe right subtree of a node contains only nodes with keys greater than the nodes key.\\nBoth the left and right subtrees must also be binary search trees.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 302, 'file_type': 'pdf'}, page_content='Example: The left tree is a binary search tree and the right tree is not a binary search tree (at\\nnode 6 it’s not satisfying the binary search tree property).\\nBinary Search Tree Declaration\\nThere is no difference between regular binary tree declaration and binary search tree declaration.\\nThe difference is only in data but not in structure. But for our convenience we change the structure\\nname as:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 303, 'file_type': 'pdf'}, page_content='Operations on Binary Search Trees\\nMain operations: Following are the main operations that are supported by binary search trees:\\nFind/ Find Minimum / Find Maximum element in binary search trees\\nInserting an element in binary search trees\\nDeleting an element from binary search trees\\nAuxiliary operations: Checking whether the given tree is a binary search tree or not\\nFinding kth-smallest element in tree\\nSorting the elements of binary search tree and many more\\nImportant Notes on Binary Search Trees\\nSince root data is always between left subtree data and right subtree data,\\nperforming inorder traversal on binary search tree produces a sorted list.\\nWhile solving problems on binary search trees, first we process left subtree, then\\nroot data, and finally we process right subtree. This means, depending on the\\nproblem, only the intermediate step (processing root data) changes and we do not\\ntouch the first and third steps.\\nIf we are searching for an element and if the left subtree root data is less than the\\nelement we want to search, then skip it. The same is the case with the right subtree..\\nBecause of this, binary search trees take less time for searching an element than\\nregular binary trees. In other words, the binary search trees consider either left or\\nright subtrees for searching an element but not both.\\nThe basic operations that can be performed on binary search tree (BST) are\\ninsertion of element, deletion of element, and searching for an element. While\\nperforming these operations on BST the height of the tree gets changed each time.\\nHence there exists variations in time complexities of best case, average case, and\\nworst case.\\nThe basic operations on a binary search tree take time proportional to the height of\\nthe tree. For a complete binary tree with node n, such operations runs in O(lgn)\\nworst-case time'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 303, 'file_type': 'pdf'}, page_content='. For a complete binary tree with node n, such operations runs in O(lgn)\\nworst-case time. If the tree is a linear chain of n nodes (skew-tree), however, the\\nsame operations takes O(n) worst-case time.\\nFinding an Element in Binary Search Trees\\nFind operation is straightforward in a BST. Start with the root and keep moving left or right using\\nthe BST property. If the data we are searching is same as nodes data then we return current node.\\nIf the data we are searching is less than nodes data then search left subtree of current node;\\notherwise search right subtree of current node. If the data is not present, we end up in a NULL'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 304, 'file_type': 'pdf'}, page_content='link.\\nTime Complexity: O(n), in worst case (when BST is a skew tree). Space Complexity: O(n), for\\nrecursive stack.\\nNon recursive version of the above algorithm can be given as:\\nTime Complexity: O(n). Space Complexity: O(1).\\nFinding Minimum Element in Binary Search Trees\\nIn BSTs, the minimum element is the left-most node, which does not has left child. In the BST\\nbelow, the minimum element is 4.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 305, 'file_type': 'pdf'}, page_content='Time Complexity: O(n), in worst case (when BST is a left skew tree).\\nSpace Complexity: O(n), for recursive stack.\\nNon recursive version of the above algorithm can be given as:\\nTime Complexity: O(n). Space Complexity: O(1).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 306, 'file_type': 'pdf'}, page_content='Finding Maximum Element in Binary Search Trees\\nIn BSTs, the maximum element is the right-most node, which does not have right child. In the BST\\nbelow, the maximum element is 16.\\nTime Complexity: O(n), in worst case (when BST is a right skew tree).\\nSpace Complexity: O(n), for recursive stack.\\nNon recursive version of the above algorithm can be given as:'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 307, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(1).\\nWhere is Inorder Predecessor and Successor?\\nWhere is the inorder predecessor and successor of node X in a binary search tree assuming all\\nkeys are distinct?\\nIf X has two children then its inorder predecessor is the maximum value in its left subtree and its\\ninorder successor the minimum value in its right subtree.\\nIf it does not have a left child, then a node’s inorder predecessor is its first left ancestor.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 308, 'file_type': 'pdf'}, page_content='Inserting an Element from Binary Search Tree\\nTo insert data into binary search tree, first we need to find the location for that element. We can\\nfind the location of insertion by following the same mechanism as that of find operation. While\\nfinding the location, if the data is already there then we can simply neglect and come out.\\nOtherwise, insert data at the last location on the path traversed.\\nAs an example let us consider the following tree. The dotted node indicates the element (5) to be\\ninserted. To insert 5, traverse the tree using find function. At node with key 4, we need to go right,\\nbut there is no subtree, so 5 is not in the tree, and this is the correct location for insertion.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 309, 'file_type': 'pdf'}, page_content='Note: In the above code, after inserting an element in subtrees, the tree is returned to its parent.\\nAs a result, the complete tree will get updated.\\nTime Complexity:O(n).\\nSpace Complexity:O(n), for recursive stack. For iterative version, space complexity is O(1).\\nDeleting an Element from Binary Search Tree\\nThe delete operation is more complicated than other operations. This is because the element to be\\ndeleted may not be the leaf node. In this operation also, first we need to find the location of the\\nelement which we want to delete.\\nOnce we have found the node to be deleted, consider the following cases:\\nIf the element to be deleted is a leaf node: return NULL to its parent. That means\\nmake the corresponding child pointer NULL. In the tree below to delete 5, set NULL'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 310, 'file_type': 'pdf'}, page_content='to its parent node 2.\\nIf the element to be deleted has one child: In this case we just need to send the\\ncurrent node’s child to its parent. In the tree below, to delete 4, 4 left subtree is set\\nto its parent node 2.\\nIf the element to be deleted has both children: The general strategy is to replace the\\nkey of this node with the largest element of the left subtree and recursively delete\\nthat node (which is now empty). The largest node in the left subtree cannot have a\\nright child, so the second delete is an easy one. As an example, let us consider the\\nfollowing tree. In the tree below, to delete 8, it is the right child of the root. The key\\nvalue is 8. It is replaced with the largest key in its left subtree (7), and then that\\nnode is deleted as before (second case).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 311, 'file_type': 'pdf'}, page_content='Note: We can replace with minimum element in right subtree also.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 312, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n) for recursive stack. For iterative version, space\\ncomplexity is O(1).\\nBinary Search Trees: Problems & Solutions\\nNote: For ordering related problems with binary search trees and balanced binary search trees,'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 313, 'file_type': 'pdf'}, page_content='Inorder traversal has advantages over others as it gives the sorted order.\\nProblem-47\\u2003\\u2003Given pointers to two nodes in a binary search tree, find the lowest common\\nancestor (LCA). Assume that both values already exist in the tree.\\nSolution:\\nThe main idea of the solution is: while traversing BST from root to bottom, the first node we\\nencounter with value between α and β, i.e., α < node → data < β, is the Least Common\\nAncestor(LCA) of α and β (where α < β). So just traverse the BST in pre-order, and if we find a\\nnode with value in between α and β, then that node is the LCA. If its value is greater than both α\\nand β, then the LCA lies on the left side of the node, and if its value is smaller than both α and β,\\nthen the LCA lies on the right side.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 314, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for skew trees.\\nProblem-48\\u2003\\u2003Give an algorithm for finding the shortest path between two nodes in a BST.\\nSolution: It’s nothing but finding the LCA of two nodes in BST.\\nProblem-49\\u2003\\u2003Give an algorithm for counting the number of BSTs possible with n nodes.\\nSolution: This is a DP problem. Refer to chapter on Dynamic Programming for the algorithm.\\nProblem-50\\u2003\\u2003Give an algorithm to check whether the given binary tree is a BST or not.\\nSolution:\\nConsider the following simple program. For each node, check if the node on its left is smaller and\\ncheck if the node on its right is greater. This approach is wrong as this will return true for binary\\ntree below. Checking only at current node is not enough.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 315, 'file_type': 'pdf'}, page_content='Problem-51\\u2003\\u2003Can we think of getting the correct algorithm?\\nSolution: For each node, check if max value in left subtree is smaller than the current node data\\nand min value in right subtree greater than the node data. It is assumed that we have helper\\nfunctions FindMin() and FindMax() that return the min or max integer value from a non-empty\\ntree.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 316, 'file_type': 'pdf'}, page_content='Time Complexity: O(n2). Space Complexity: O(n).\\nProblem-52\\u2003\\u2003Can we improve the complexity of Problem-51?\\nSolution: Yes. A better solution is to look at each node only once. The trick is to write a utility\\nhelper function IsBSTUtil(struct BinaryTreeNode* root, int min, int max) that traverses down the\\ntree keeping track of the narrowing min and max allowed values as it goes, looking at each node\\nonly once. The initial values for min and max should be INT_MIN and INT_MAX – they narrow\\nfrom there.\\nTime Complexity: O(n). Space Complexity: O(n), for stack space.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 317, 'file_type': 'pdf'}, page_content='Problem-53\\u2003\\u2003Can we further improve the complexity of Problem-51?\\nSolution: Yes, by using inorder traversal. The idea behind this solution is that inorder traversal of\\nBST produces sorted lists. While traversing the BST in inorder, at each node check the condition\\nthat its key value should be greater than the key value of its previous visited node. Also, we need\\nto initialize the prev with possible minimum integer value (say, INT_MIN).\\nTime Complexity: O(n). Space Complexity: O(n), for stack space.\\nProblem-54\\u2003\\u2003Give an algorithm for converting BST to circular DLL with space complexity\\nO(1).\\nSolution: Convert left and right subtrees to DLLs and maintain end of those lists. Then, adjust the\\npointers.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 318, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nProblem-55\\u2003\\u2003For Problem-54, is there any other way of solving it?\\nSolution: Yes. There is an alternative solution based on the divide and conquer method which is\\nquite neat.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 319, 'file_type': 'pdf'}, page_content='Time Complexity: O(n).\\nProblem-56\\u2003\\u2003Given a sorted doubly linked list, give an algorithm for converting it into\\nbalanced binary search tree.\\nSolution: Find the middle node and adjust the pointers.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 320, 'file_type': 'pdf'}, page_content='Time Complexity: 2T(n/2) + O(n) [for finding the middle node] = O(nlogn).\\nNote: For FindMiddleNode function refer Linked Lists chapter.\\nProblem-57\\u2003\\u2003Given a sorted array, give an algorithm for converting the array to BST.\\nSolution: If we have to choose an array element to be the root of a balanced BST, which element\\nshould we pick? The root of a balanced BST should be the middle element from the sorted array.\\nWe would pick the middle element from the sorted array in each iteration. We then create a node\\nin the tree initialized with this element. After the element is chosen, what is left? Could you\\nidentify the sub-problems within the problem?\\nThere are two arrays left – the one on its left and the one on its right. These two arrays are the\\nsub-problems of the original problem, since both of them are sorted. Furthermore, they are\\nsubtrees of the current node’s left and right child.\\nThe code below creates a balanced BST from the sorted array in O(n) time (n is the number of\\nelements in the array). Compare how similar the code is to a binary search algorithm. Both are\\nusing the divide and conquer methodology.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 321, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for stack space.\\nProblem-58\\u2003\\u2003Given a singly linked list where elements are sorted in ascending order, convert\\nit to a height balanced BST.\\nSolution: A naive way is to apply the Problem-56 solution directly. In each recursive call, we\\nwould have to traverse half of the list’s length to find the middle element. The run time complexity\\nis clearly O(nlogn), where n is the total number of elements in the list. This is because each level\\nof recursive call requires a total of n/2 traversal steps in the list, and there are a total of logn\\nnumber of levels (ie, the height of the balanced tree).\\nProblem-59\\u2003\\u2003For Problem-58, can we improve the complexity?\\nSolution: Hint: How about inserting nodes following the list order? If we can achieve this, we no\\nlonger need to find the middle element as we are able to traverse the list while inserting nodes to\\nthe tree.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 322, 'file_type': 'pdf'}, page_content='Best Solution: As usual, the best solution requires us to think from another perspective. In other\\nwords, we no longer create nodes in the tree using the top-down approach. Create nodes bottom-\\nup, and assign them to their parents. The bottom-up approach enables us to access the list in its\\norder while creating nodes [42].\\nIsn’t the bottom-up approach precise? Any time we are stuck with the top-down approach, we can\\ngive bottom-up a try. Although the bottom-up approach is not the most natural way we think, it is\\nhelpful in some cases. However, we should prefer top-down instead of bottom-up in general,\\nsince the latter is more difficult to verify.\\nBelow is the code for converting a singly linked list to a balanced BST. Please note that the\\nalgorithm requires the list length to be passed in as the function parameters. The list length can be\\nfound in O(n) time by traversing the entire list once. The recursive calls traverse the list and\\ncreate tree nodes by the list order, which also takes O(n) time. Therefore, the overall run time\\ncomplexity is still O(n).'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 323, 'file_type': 'pdf'}, page_content='Problem-60\\u2003\\u2003Give an algorithm for finding the kth smallest element in BST.\\nSolution: The idea behind this solution is that, inorder traversal of BST produces sorted lists.\\nWhile traversing the BST in inorder, keep track of the number of elements visited.\\nTime Complexity: O(n). Space Complexity: O(1).\\nProblem-61\\u2003\\u2003Floor and ceiling: If a given key is less than the key at the root of a BST then the\\nfloor of the key (the largest key in the BST less than or equal to the key) must be in the left\\nsubtree. If the key is greater than the key at the root, then the floor of the key could be in the\\nright subtree, but only if there is a key smaller than or equal to the key in the right subtree;\\nif not (or if the key is equal to the the key at the root) then the key at the root is the floor of\\nthe key. Finding the ceiling is similar, with interchanging right and left. For example, if the\\nsorted with input array is {1, 2, 8, 10, 10, 12, 19}, then\\nFor x = 0: floor doesn’t exist in array, ceil = 1, For x = 1: floor = 1, ceil = 1\\nFor x = 5: floor =2, ceil = 8, For x = 20: floor = 19, ceil doesn’t exist in array\\nSolution: The idea behind this solution is that, inorder traversal of BST produces sorted lists.\\nWhile traversing the BST in inorder, keep track of the values being visited. If the roots data is\\ngreater than the given value then return the previous value which we have maintained during\\ntraversal. If the roots data is equal to the given data then return root data.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 324, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for stack space.\\nFor ceiling, we just need to call the right subtree first, followed by left subtree.'),\n",
       " Document(metadata={'source_file': 'Data Structures and Algorithms Made Easy_ Data Structures and Algorithmic Puzzles.pdf', 'page_number': 325, 'file_type': 'pdf'}, page_content='Time Complexity: O(n). Space Complexity: O(n), for stack space.\\nProblem-62\\u2003\\u2003Give an algorithm for finding the union and intersection of BSTs. Assume parent\\npointers are available (say threaded binary trees). Also, assume the lengths of two BSTs\\nare m and n respectively.\\nSolution: If parent pointers are available then the problem is same as merging of two sorted lists.\\nThis is because if we call inorder successor each time we get the next highest element. It’s just a\\nmatter of which InorderSuccessor to call.\\nTime Complexity: O(m + n). Space complexity: O(1).\\nProblem-63\\u2003\\u2003For Problem-62, what if parent pointers are not available?\\nSolution: If parent pointers are not available, the BSTs can be converted to linked lists and then\\nmerged.\\nConvert both the BSTs into sorted doubly linked lists in O(n + m) time. This produces\\n2 sorted lists.\\nMerge the two double linked lists into one and also maintain the count of total\\nelements in O(n + m) time.\\nConvert the sorted doubly linked list into height balanced tree in O(n + m) time.\\nProblem-64\\u2003\\u2003For Problem-62, is there any alternative way of solving the problem?'),\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_text_splitter import TextSplitter\n",
    "# from langchain.text_splitter import TextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def split_docs(documents,chunk_size,chunk_overlap):\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\n",
    "            \"\\n```\",      \n",
    "            \"\\n#include\", \n",
    "            \"\\ndef \",       \n",
    "            \"\\nclass \",     \n",
    "            \"\\n## \",\n",
    "            \"\\n### \",\n",
    "            \"\\n\\n\",\n",
    "            \"\\n- \",\n",
    "            \"\\n* \",\n",
    "            \". \",      \n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    splitted_text = text_splitter.split_documents(documents)\n",
    "    return splitted_text\n",
    "\n",
    "for d in docs:\n",
    "    d.page_content = clean_page_text(d.page_content)\n",
    "chunks = split_docs(docs,2000,200)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7e1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf92939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "\n",
    "    def __init__(self, model_name = \"all-MiniLM-L6-v2\"):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \n",
    "        embeddings = embeddings = self.model.encode(\n",
    "                                    texts,\n",
    "                                    batch_size=64,\n",
    "                                    show_progress_bar=True,\n",
    "                                    normalize_embeddings=True\n",
    "                                    )\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fc88073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 25/25 [00:37<00:00,  1.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-8.05333033e-02,  1.09690423e-04,  2.93825623e-02, ...,\n",
       "         4.13207747e-02,  4.56853807e-02,  3.81768458e-02],\n",
       "       [-6.22593462e-02, -8.87983814e-02,  2.81385034e-02, ...,\n",
       "         7.17375353e-02, -2.90876701e-02,  8.02726310e-04],\n",
       "       [ 9.67695564e-03, -7.73420334e-02,  3.24316509e-02, ...,\n",
       "         7.15736970e-02, -5.13982661e-02, -1.48361865e-02],\n",
       "       ...,\n",
       "       [-1.49592347e-02,  6.91460725e-03,  3.49586420e-02, ...,\n",
       "         1.73235778e-02, -7.55195618e-02,  9.03654844e-03],\n",
       "       [-7.77352080e-02,  3.32668126e-02,  1.26612103e-02, ...,\n",
       "         1.90901458e-02, -8.73328652e-03,  5.48958294e-02],\n",
       "       [-1.11541584e-01,  1.78629197e-02, -1.24846250e-01, ...,\n",
       "         4.16832492e-02, -1.31553458e-02, -3.84143344e-03]],\n",
       "      shape=(1548, 384), dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a55ea513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name = \"pdf_documents\",\n",
    "        persist_directory = r\"C:\\Users\\Pranav Bansal\\Documents\\LLM_POWERED_API_AGENT\\chroma_store\"\n",
    "    ):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        os.makedirs(self.persist_directory, exist_ok=True)\n",
    "        self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            metadata={\"description\": \"Text document embeddings for RAG\"}\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings_list,\n",
    "            metadatas=metadatas,\n",
    "            documents=documents_text\n",
    "        )\n",
    "        \n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore.add_documents(chunks,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08edc21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('. In the real world, engineers\\nsometimes spend the bulk of their time generating these\\ndatasets. One of the more popular repositories for public\\ndatasets is Kaggle.com, which makes lots of useful datasets\\navailable and holds competitions allowing budding ML\\npractitioners to test their skills.\\nMachine Learning Versus Artificial\\nIntelligence\\nThe terms machine learning and artificial intelligence (AI)\\nare used almost interchangeably today, but in fact, each term\\nhas a specific meaning, as shown in Figure\\xa01-5.\\nTechnically speaking, machine learning is a subset of AI,\\nwhich encompasses not only machine learning models but also\\nother types of models such as expert systems (systems that\\nmake decisions based on rules that you define) and\\nreinforcement learning systems, which learn behaviors by\\nrewarding positive outcomes while penalizing negative ones.\\nAn example of a reinforcement learning system is AlphaGo,\\nwhich was the first computer program to beat a professional\\nhuman Go player', {'creationDate': \"D:20221118073855+00'00'\", 'page': 31, 'source': 'C:\\\\Users\\\\Pranav Bansal\\\\Documents\\\\LLM_POWERED_API_AGENT\\\\pdf_files\\\\Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'trapped': '', 'title': 'Applied Machine Learning and AI for Engineers', 'modDate': \"D:20221118153856+08'00'\", 'doc_index': 56, 'moddate': '2022-11-18T15:38:56+08:00', 'author': 'Jeff Prosise', 'file_path': 'C:\\\\Users\\\\Pranav Bansal\\\\Documents\\\\LLM_POWERED_API_AGENT\\\\pdf_files\\\\Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'subject': '', 'total_pages': 666, 'content_length': 990, 'file_name': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'creationdate': '2022-11-18T07:38:55+00:00', 'keywords': '', 'producer': 'calibre (6.8.0) [https://calibre-ebook.com]', 'file_type': 'pdf', 'creator': 'calibre (6.8.0) [https://calibre-ebook.com]', 'format': 'PDF 1.4'}, 0.7038769125938416), ('. In the real world, engineers\\nsometimes spend the bulk of their time generating these\\ndatasets. One of the more popular repositories for public\\ndatasets is Kaggle.com, which makes lots of useful datasets\\navailable and holds competitions allowing budding ML\\npractitioners to test their skills.\\nMachine Learning Versus Artificial\\nIntelligence\\nThe terms machine learning and artificial intelligence (AI)\\nare used almost interchangeably today, but in fact, each term\\nhas a specific meaning, as shown in Figure\\xa01-5.\\nTechnically speaking, machine learning is a subset of AI,\\nwhich encompasses not only machine learning models but also\\nother types of models such as expert systems (systems that\\nmake decisions based on rules that you define) and\\nreinforcement learning systems, which learn behaviors by\\nrewarding positive outcomes while penalizing negative ones.\\nAn example of a reinforcement learning system is AlphaGo,\\nwhich was the first computer program to beat a professional\\nhuman Go player', {'page_number': 32, 'doc_index': 57, 'content_length': 990, 'file_type': 'pdf', 'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf'}, 0.7038769125938416), ('. That\\nintuition comes from experience and from an understanding of\\nhow machine learning fits mathematical models to data. This\\nchapter represents the first step on that journey. It begins\\nwith an overview of machine learning and the most common\\ntypes of machine learning models, and it concludes by\\nintroducing two popular learning algorithms and using them to\\nbuild simple yet fully functional models.\\nWhat Is Machine Learning?\\nAt an existential level, machine learning (ML) is a means for\\nfinding patterns in numbers and exploiting those patterns to\\nmake predictions', {'page_number': 27, 'doc_index': 79, 'content_length': 569, 'file_type': 'pdf', 'source_file': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf'}, 0.7674844264984131), ('Chapter 1. Machine Learning\\nMachine learning expands the boundaries of what’s possible\\nby allowing computers to solve problems that were intractable\\njust a few short years ago. From fraud detection and medical\\ndiagnoses to product recommendations and cars that “see”\\nwhat’s in front of them, machine learning impacts our lives\\nevery day. As you read this, scientists are using machine\\nlearning to unlock the secrets of the human genome. When we\\none day cure cancer, we will thank machine learning for\\nmaking it possible.\\nMachine learning is revolutionary because it provides an\\nalternative to algorithmic problem-solving. Given a recipe,\\nor algorithm, it’s not difficult to write an app that hashes\\na password or computes a monthly mortgage payment. You code\\nup the algorithm, feed it input, and receive output in\\nreturn. It’s another proposition altogether to write code', {'keywords': '', 'creationdate': '2022-11-18T07:38:55+00:00', 'file_type': 'pdf', 'trapped': '', 'file_name': 'Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'file_path': 'C:\\\\Users\\\\Pranav Bansal\\\\Documents\\\\LLM_POWERED_API_AGENT\\\\pdf_files\\\\Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'content_length': 871, 'title': 'Applied Machine Learning and AI for Engineers', 'total_pages': 666, 'modDate': \"D:20221118153856+08'00'\", 'subject': '', 'creationDate': \"D:20221118073855+00'00'\", 'moddate': '2022-11-18T15:38:56+08:00', 'page': 25, 'author': 'Jeff Prosise', 'format': 'PDF 1.4', 'producer': 'calibre (6.8.0) [https://calibre-ebook.com]', 'creator': 'calibre (6.8.0) [https://calibre-ebook.com]', 'source': 'C:\\\\Users\\\\Pranav Bansal\\\\Documents\\\\LLM_POWERED_API_AGENT\\\\pdf_files\\\\Applied-Machine-Learning-and-AI-for-Engineers.pdf', 'doc_index': 46}, 0.7800381183624268)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def retrieve_top_docs(query: str, top_k: int = 4):\n",
    "    q_emb = embedding_manager.generate_embeddings([query])[0].tolist()\n",
    "    results = vectorstore.collection.query(query_embeddings=[q_emb], n_results=top_k)\n",
    "    docs = results['documents'][0]\n",
    "    metas = results['metadatas'][0]\n",
    "    dists = results.get('distances', [[]])[0]\n",
    "    return list(zip(docs, metas, dists))\n",
    "\n",
    "# Example query\n",
    "query = \"What is the intuitive difference between algorithmic problem-solving and machine learning?\"\n",
    "top_docs = retrieve_top_docs(query)\n",
    "print(top_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a16320dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_context(top_docs):\n",
    "    \n",
    "#     context = \"\\n\\n\".join([doc for doc, meta, dist in top_docs])\n",
    "#     return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a182b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groq LLM initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        model_name=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=400\n",
    "    )\n",
    "print(f\"✅ Groq LLM initialized\")\n",
    "    \n",
    "def build_context(top_docs):\n",
    "    context_parts = []\n",
    "    for i, (doc, meta, dist) in enumerate(top_docs):\n",
    "        context_parts.append(\n",
    "            f\"[Source {i+1} | Page {meta.get('page', 'N/A')}]\\n{doc}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dfcc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"\"\"\n",
    "        You are a helpful assistant.\n",
    "        Use the following extracted parts of a document to answer the question.\n",
    "        If the answer spans multiple parts, combine them logically.\n",
    "        If the answer is not fully contained, say so clearly.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d8cfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "    top_docs = retrieve_top_docs(query)\n",
    "    context = build_context(top_docs)\n",
    "    formatted_prompt = prompt.format(context=context, question=query)\n",
    "    response = llm.invoke(formatted_prompt)  \n",
    "    return response.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8847ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Answer:\n",
      " To provide the code for reversing and deleting a node from a linked list, we'll need to combine the provided information with standard linked list node and linked list class structures. However, since there's no explicit code provided in the given extracts, I'll create an example implementation based on the given information.\n",
      "\n",
      "### Singly Linked List Node Structure\n",
      "\n",
      "```python\n",
      "class Node:\n",
      "    def __init__(self, data=None):\n",
      "        self.data = data\n",
      "        self.next = None\n",
      "```\n",
      "\n",
      "### Singly Linked List Class Structure\n",
      "\n",
      "```python\n",
      "class LinkedList:\n",
      "    def __init__(self):\n",
      "        self.head = None\n",
      "        self.tail = None\n",
      "```\n",
      "\n",
      "### Reversing the Linked List\n",
      "\n",
      "To reverse the linked list, we'll create two pointers, `prev` and `curr`, and traverse the list. Once we find the node to be deleted, we'll update the `prev` node's `next` pointer.\n",
      "\n",
      "```python\n",
      "def reverse_linked_list(self):\n",
      "    prev = None\n",
      "    curr = self.head\n",
      "    while curr:\n",
      "        next_node = curr.next  # Store the next node\n",
      "        curr.next = prev  # Reverse the link\n",
      "        prev = curr\n",
      "        curr = next_node\n",
      "    self.head = prev\n",
      "```\n",
      "\n",
      "### Deleting an Intermediate Node\n",
      "\n",
      "To delete an intermediate node, we'll traverse the list and find the node to be deleted. We'll then update the `prev` node's `next` pointer to skip the node to be deleted.\n",
      "\n",
      "```python\n",
      "def delete_node(self, data):\n",
      "    # Traverse the list to find the node to be deleted\n",
      "    curr = self.head\n",
      "    prev = None\n",
      "    while curr and curr.data != data:\n",
      "        prev = curr\n",
      "        curr = curr.next\n",
      "\n",
      "    # If the node to be deleted is found\n",
      "    if curr:\n",
      "        # Update the prev node's next pointer to skip the node to be deleted\n",
      "        prev.next = curr.next\n",
      "        # Dispose of the\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me the code to reverse a linked list and to delete a node from a linked list\"\n",
    "answer = answer_query(query)\n",
    "print(\"\\n🧠 Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9b8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
